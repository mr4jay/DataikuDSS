exports.id=580,exports.ids=[580],exports.modules={4780:(e,t,a)=>{"use strict";a.d(t,{cn:()=>n});var o=a(49384),i=a(82348);function n(...e){return(0,i.QP)((0,o.$)(e))}},10974:(e,t,a)=>{"use strict";a.d(t,{cn:()=>n});var o=a(75986),i=a(8974);function n(...e){return(0,i.QP)((0,o.$)(e))}},18514:(e,t,a)=>{Promise.resolve().then(a.t.bind(a,4536,23)),Promise.resolve().then(a.bind(a,79737))},18864:(e,t,a)=>{Promise.resolve().then(a.t.bind(a,86346,23)),Promise.resolve().then(a.t.bind(a,27924,23)),Promise.resolve().then(a.t.bind(a,35656,23)),Promise.resolve().then(a.t.bind(a,40099,23)),Promise.resolve().then(a.t.bind(a,38243,23)),Promise.resolve().then(a.t.bind(a,28827,23)),Promise.resolve().then(a.t.bind(a,62763,23)),Promise.resolve().then(a.t.bind(a,97173,23))},33486:(e,t,a)=>{"use strict";a.d(t,{Toaster:()=>C});var o=a(60687),i=a(43210);let n=0,r=new Map,s=e=>{if(r.has(e))return;let t=setTimeout(()=>{r.delete(e),d({type:"REMOVE_TOAST",toastId:e})},1e6);r.set(e,t)},l=(e,t)=>{switch(t.type){case"ADD_TOAST":return{...e,toasts:[t.toast,...e.toasts].slice(0,1)};case"UPDATE_TOAST":return{...e,toasts:e.toasts.map(e=>e.id===t.toast.id?{...e,...t.toast}:e)};case"DISMISS_TOAST":{let{toastId:a}=t;return a?s(a):e.toasts.forEach(e=>{s(e.id)}),{...e,toasts:e.toasts.map(e=>e.id===a||void 0===a?{...e,open:!1}:e)}}case"REMOVE_TOAST":if(void 0===t.toastId)return{...e,toasts:[]};return{...e,toasts:e.toasts.filter(e=>e.id!==t.toastId)}}},c=[],u={toasts:[]};function d(e){u=l(u,e),c.forEach(e=>{e(u)})}function h({...e}){let t=(n=(n+1)%Number.MAX_SAFE_INTEGER).toString(),a=()=>d({type:"DISMISS_TOAST",toastId:t});return d({type:"ADD_TOAST",toast:{...e,id:t,open:!0,onOpenChange:e=>{e||a()}}}),{id:t,dismiss:a,update:e=>d({type:"UPDATE_TOAST",toast:{...e,id:t}})}}var p=a(82370),m=a(24224),g=a(78726),y=a(4780);let f=p.Kq,w=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)(p.LM,{ref:a,className:(0,y.cn)("fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",e),...t}));w.displayName=p.LM.displayName;let b=(0,m.F)("group pointer-events-auto relative flex w-full items-center justify-between space-x-4 overflow-hidden rounded-md border p-6 pr-8 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",{variants:{variant:{default:"border bg-background text-foreground",destructive:"destructive group border-destructive bg-destructive text-destructive-foreground"}},defaultVariants:{variant:"default"}}),v=i.forwardRef(({className:e,variant:t,...a},i)=>(0,o.jsx)(p.bL,{ref:i,className:(0,y.cn)(b({variant:t}),e),...a}));v.displayName=p.bL.displayName,i.forwardRef(({className:e,...t},a)=>(0,o.jsx)(p.rc,{ref:a,className:(0,y.cn)("inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium ring-offset-background transition-colors hover:bg-secondary focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive",e),...t})).displayName=p.rc.displayName;let k=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)(p.bm,{ref:a,className:(0,y.cn)("absolute right-2 top-2 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-2 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",e),"toast-close":"",...t,children:(0,o.jsx)(g.A,{className:"h-4 w-4"})}));k.displayName=p.bm.displayName;let S=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)(p.hE,{ref:a,className:(0,y.cn)("text-sm font-semibold",e),...t}));S.displayName=p.hE.displayName;let T=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)(p.VY,{ref:a,className:(0,y.cn)("text-sm opacity-90",e),...t}));function C(){let{toasts:e}=function(){let[e,t]=i.useState(u);return i.useEffect(()=>(c.push(t),()=>{let e=c.indexOf(t);e>-1&&c.splice(e,1)}),[e]),{...e,toast:h,dismiss:e=>d({type:"DISMISS_TOAST",toastId:e})}}();return(0,o.jsxs)(f,{children:[e.map(function({id:e,title:t,description:a,action:i,...n}){return(0,o.jsxs)(v,{...n,children:[(0,o.jsxs)("div",{className:"grid gap-1",children:[t&&(0,o.jsx)(S,{children:t}),a&&(0,o.jsx)(T,{children:a})]}),i,(0,o.jsx)(k,{})]},e)}),(0,o.jsx)(w,{})]})}T.displayName=p.VY.displayName},39648:(e,t,a)=>{"use strict";a.d(t,{ab:()=>o});let o=[{id:401,slug:"defining-onboarding-workflows-for-new-dataiku-users",question:"How to get started with defining onboarding workflows for new Dataiku users?",answer:'### 1. Introduction/Overview\nDefining a structured onboarding workflow is crucial for enabling new users to become productive in Dataiku efficiently. A good onboarding plan blends self-service learning with guided, practical exercises.\n\n### 2. Prerequisites\n- A Dataiku instance with a "Sandbox" or "Training" project space.\n- A list of core competencies you expect a new user to learn in their first month.\n\n### 3. Step-by-Step Instructions\n1.  **Create a "Welcome" Project:** Build a dedicated Dataiku project that serves as the starting point for all new users.\n2.  **Develop a Checklist in the Wiki:** In the project\\\'s Wiki, create an "Onboarding Checklist" with links to Dataiku Academy courses, key documentation, and a sequence of hands-on tasks.\n3.  **Provide Sample Datasets:** Populate the project with clean, easy-to-understand sample datasets for users to practice with.\n4.  **Assign a Mentor:** Assign an experienced team member as a mentor to guide the new user and answer questions.\n\n### 4. Resources and Tools\n- **Dataiku Academy:** The "Core Designer" learning path is essential.\n- **A templated onboarding project:** To ensure a consistent experience.\n\n### 5. Next Steps and Progression\n- Have new users present a small project they built at the end of their first month to showcase their learning.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** New users feel overwhelmed.\n- **Solution:** Break down the onboarding into a week-by-week plan to make it more manageable.'},{id:402,slug:"assigning-datasets-and-flows-for-hands-on-learning",question:"How to get started with assigning datasets and flows for hands-on learning?",answer:'### 1. Introduction/Overview\nPassive learning is ineffective. New users need hands-on exercises with real data and flows. The key is to provide a safe, structured environment for this practice.\n\n### 2. Prerequisites\n- A "sandbox" project for each new user.\n- A central project with read-only "golden" datasets.\n\n### 3. Step-by-Step Instructions\n1.  **Grant Sandbox Access:** Give the new user full admin rights on their own sandbox project.\n2.  **Provide Read-Only Access to Real Data:** Give them "Reader" access to a project containing well-documented, clean production datasets.\n3.  **Create a Task List:** In their sandbox project\\\'s Wiki, create a list of tasks, such as "Build a flow to join the `customers` and `orders` datasets and calculate the total sales per customer."\n\n### 4. Resources and Tools\n- **Sandbox Projects:** To provide a safe experimentation space.\n- **Read-only Shared Projects:** To provide access to realistic data.\n\n### 5. Next Steps and Progression\n- Gradually increase the complexity of the assigned tasks as their skills grow.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The user breaks something in their sandbox.\n- **Solution:** This is a good thing! A sandbox is for learning. Help them troubleshoot the issue. It\\\'s a valuable learning experience.'},{id:403,slug:"introducing-standard-naming-conventions-for-datasets-and-recipes",question:"How to get started with introducing standard naming conventions for datasets and recipes?",answer:"### 1. Introduction/Overview\nConsistent naming is a cornerstone of maintainable projects. Establishing and enforcing a standard naming convention makes flows easier to read and understand for everyone on the team.\n\n### 2. Prerequisites\n- Agreement from the team on a standard.\n\n### 3. Step-by-Step Instructions\n1.  **Define the Convention:** Hold a team meeting to agree on a simple, clear convention. A good starting point is:\n    *   **Datasets:** `SOURCE_CONTENT_STATUS` (e.g., `SFDC_LEADS_CLEANED`).\n    *   **Recipes:** `VERB_INPUT_OUTPUT` (e.g., `prepare_leads_for_scoring`).\n2.  **Document the Standard:** Create a page in your central team Wiki that clearly documents this naming convention with examples.\n3.  **Enforce via Code Reviews:** The most effective way to enforce the standard is during peer reviews of projects or pull requests. A review checklist should include \"Adherence to naming conventions.\"\n\n### 4. Resources and Tools\n- **A central Wiki:** To document the standards.\n- **Peer Reviews:** As the enforcement mechanism.\n\n### 5. Next Steps and Progression\n- Use a linter or a custom Dataiku check to automatically flag non-compliant names.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** People forget or don\\'t follow the standard.\n- **Solution:** Consistent enforcement during reviews is key. If a developer knows their work won\\'t be approved without following the standard, they will quickly adopt the habit."},{id:404,slug:"monitoring-new-joiners-progress-across-the-flow",question:"How to get started with monitoring new joiners’ progress across the Flow?",answer:'### 1. Introduction/Overview\nAs an SME or manager, you need to track the progress of new team members to provide support and ensure they are learning effectively. This involves a combination of informal check-ins and reviewing their work directly in Dataiku.\n\n### 2. Prerequisites\n- A mentorship or management relationship with the new joiner.\n- Access to their sandbox or development projects.\n\n### 3. Step-by-Step Instructions\n1.  **Daily Stand-ups:** Use the daily team stand-up for a quick status update. "What did you work on yesterday? What are you working on today? Are you blocked by anything?"\n2.  **Weekly 1-on-1s:** Schedule a dedicated weekly meeting to review their work. Have them share their screen and walk you through a flow they\\\'ve built.\n3.  **Use the Project Timeline:** In their project, the **Timeline** feature gives you a quick, high-level audit trail of their recent activity (e.g., "Jane created recipe X," "Jane modified dataset Y").\n\n### 4. Resources and Tools\n- **Regularly scheduled meetings.**\n- **Dataiku\\\'s Project Timeline feature.**\n\n### 5. Next Steps and Progression\n- Set clear goals for each week of their onboarding so you have something concrete to measure their progress against.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The new joiner is stuck but is afraid to ask for help.\n- **Solution:** You must create a psychologically safe environment. In your 1-on-1s, explicitly state that it\\\'s okay to be stuck and that asking questions is a sign of a good developer.'},{id:405,slug:"creating-checklist-based-training-modules-for-each-feature",question:"How to get started with creating checklist-based training modules for each feature?",answer:"### 1. Introduction/Overview\nBreaking down the vast capabilities of Dataiku into small, manageable training modules with checklists helps new users learn in a structured way and allows you to track their skill acquisition.\n\n### 2. Prerequisites\n- A deep understanding of Dataiku\\'s features.\n- A platform for hosting the training materials (e.g., a Wiki).\n\n### 3. Step-by-Step Instructions\n1.  **Identify Core Competencies:** List the key features a new user must learn (e.g., Prepare Recipe, Join Recipe, Scenarios, Dashboards).\n2.  **Create a Module for Each Competency:** In your Wiki, create a separate page for each module.\n3.  **Develop a Checklist for Each Module:** For each module, create a checklist of specific, demonstrable skills.\n    *   **Example (Join Recipe Module):**\n        *   \\[ ] Can explain the difference between a left and inner join.\n        *   \\[ ] Can successfully perform a join on a single key.\n        *   \\[ ] Can successfully perform a join on a composite key.\n4.  **Link to Resources:** For each checklist item, provide a link to the relevant Dataiku Academy course or documentation page.\n\n### 4. Resources and Tools\n- **A Wiki or other documentation platform.**\n- **Dataiku Academy and official documentation.**\n\n### 5. Next Steps and Progression\n- Have users work through the checklists during their onboarding and demonstrate the skills to their mentor.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** Creating the modules is a lot of work.\n- **Solution:** Start small. Create modules for the 5-10 most critical features first. You can build out the rest over time. You can also assign the creation of a new module to a team member who has recently mastered that feature."},{id:406,slug:"enforcing-documentation-discipline-early-in-the-project",question:"How to get started with enforcing documentation discipline early in the project?",answer:'### 1. Introduction/Overview\nGood documentation is a habit. Enforcing this discipline from the very beginning of a new team member\\\'s journey is the best way to ensure it becomes a natural part of their workflow.\n\n### 2. Prerequisites\n- A clear team standard for what needs to be documented.\n\n### 3. Step-by-Step Instructions\n1.  **Lead by Example:** As an SME, ensure all of your own work is impeccably documented. New joiners will follow the examples they see.\n2.  **Define Clear Expectations:** The standard should be simple: **every object in the Flow must have a clear, one-sentence description.**\n3.  **Make it Part of the "Definition of Done":** A task or user story is not complete until the associated objects are documented. This should be a formal part of your process.\n4.  **Enforce During Reviews:** During peer reviews, if a recipe is missing a description, the reviewer should leave a comment asking for it to be added. The work should not be approved until it is documented.\n\n### 4. Resources and Tools\n- **The "Description" field** on all Dataiku objects.\n- **A documented team standard** in your Wiki.\n- **Peer reviews.**\n\n### 5. Next Steps and Progression\n- Showcase examples of good documentation in team meetings to highlight its value.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** "It slows down development."\n- **Solution:** Frame it as an investment. The 30 seconds it takes to write a good description will save hours of confusion for the next person who has to work on that flow. It actually speeds up the team in the long run.'},{id:407,slug:"building-learning-flows-that-cover-prepare-join-window-recipes",question:"How to get started with building learning flows that cover Prepare, Join, Window recipes?",answer:'### 1. Introduction/Overview\nA simple, end-to-end learning flow is a powerful tool for teaching new users the core concepts of Dataiku in a practical way.\n\n### 2. Prerequisites\n- Two simple, related CSV files (e.g., `customers.csv`, `orders.csv`).\n- A new Dataiku project.\n\n### 3. Step-by-Step Instructions\n1.  **Ingest Data:** Upload the two CSV files to create two datasets.\n2.  **First Step (Prepare):** Create a **Prepare** recipe on the `orders` dataset. Use it to parse the date column and filter out test orders.\n3.  **Second Step (Join):** Create a **Join** recipe. Join the output of your Prepare recipe with the `customers` dataset on the `customer_id` key.\n4.  **Third Step (Window):** Create a **Window** recipe on the joined data. Use it to calculate a running total of sales for each customer. Partition by `customer_id` and order by `date`.\n5.  **Document Each Step:** Add clear descriptions to each dataset and recipe explaining its purpose.\n\n### 4. Resources and Tools\n- **Visual Recipes:** Prepare, Join, Window.\n- **Sample Data:** Simple, clean data is best for a learning exercise.\n\n### 5. Next Steps and Progression\n- Have the new user try to rebuild this entire flow themselves after you have demonstrated it.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The user gets stuck on the Window recipe configuration.\n- **Solution:** The concepts of "Partition by" and "Order by" are the most common points of confusion. Use a visual diagram on a whiteboard to explain how the window frame moves over the partitioned data.'},{id:408,slug:"guiding-them-through-real-data-exploration-tasks",question:"How to get started with guiding them through real data exploration tasks?",answer:'### 1. Introduction/Overview\nData exploration is a key skill. As an SME, you can guide new users by giving them open-ended tasks that encourage them to use Dataiku\\\'s visual analysis tools to explore a dataset and find insights.\n\n### 2. Prerequisites\n- An interesting, new dataset.\n- A Dataiku project.\n\n### 3. Step-by-Step Instructions\n1.  **Provide the Data:** Upload a new dataset to a project and share it with the user.\n2.  **Ask Open-Ended Questions:** Give them a set of business questions to answer, not a set of technical instructions. For example:\n    *   "Explore this customer dataset. What can you tell me about our customer base?"\n    *   "Are there any data quality issues you can find?"\n    *   "What is the relationship between customer age and total spending?"\n3.  **Point them to the Tools:** Guide them to use the **Statistics** and **Charts** tabs on the dataset to answer these questions.\n4.  **Review their Findings:** Have them present their findings to you, explaining the charts and statistics they used.\n\n### 4. Resources and Tools\n- **Dataset Statistics Tab:** For profiling columns.\n- **Dataset Charts Tab:** For visual exploration.\n\n### 5. Next Steps and Progression\n- Encourage them to create a full **Dashboard** to summarize the key insights they found during their exploration.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The user is not sure where to start.\n- **Solution:** Give them a more specific starting point. "Start by looking at the distribution of the `country` column. What do you notice?" This can help kickstart their exploration process.'},{id:409,slug:"auditing-how-juniors-use-variables-metrics-and-flags",question:"How to get started with auditing how juniors use variables, metrics, and flags?",answer:'### 1. Introduction/Overview\nAs an SME, it\\\'s important to review a junior developer\\\'s work to ensure they are using platform features correctly. Auditing their use of variables, metrics, and other settings helps catch anti-patterns early and reinforces best practices.\n\n### 2. Prerequisites\n- Access to the junior developer\\\'s projects.\n- A checklist of best practices to review against.\n\n### 3. Step-by-Step Instructions\n1.  **Review Project Variables:**\n    *   Open their project and go to **... > Variables**. Are they parameterizing their project, or are there hardcoded values in their recipes?\n2.  **Review Metrics and Checks:**\n    *   Open their key datasets and go to the **Status** tab. Have they set up any metrics or data quality checks? This is a key sign of a mature developer.\n3.  **Review Scenario Flags:**\n    *   Open their scenarios. Are they using reporters for alerting? Are they using build modes like "Forced rebuild" correctly?\n4.  **Provide Feedback:** In a 1-on-1 session, walk them through your findings and explain the "why" behind the best practices.\n\n### 4. Resources and Tools\n- **The Project Itself:** Your primary source for the audit.\n- **A Review Checklist:** To ensure you are consistent in your audits.\n\n### 5. Next Steps and Progression\n- Turn the review checklist into a self-assessment tool that developers can use on their own work before submitting it for review.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The junior developer sees the review as criticism.\n- **Solution:** Frame the session as a collaborative learning opportunity, not a test. The goal is to help them improve and build higher-quality pipelines.'},{id:410,slug:"encouraging-best-practices-around-branching-and-git-sync",question:"How to get started with encouraging best practices around branching and Git sync?",answer:'### 1. Introduction/Overview\nUsing Git correctly is essential for team collaboration. As an SME, you must establish and encourage best practices for branching, committing, and syncing to prevent conflicts and maintain a clean project history.\n\n### 2. Prerequisites\n- The project is integrated with a Git repository.\n- The team is familiar with basic Git concepts.\n\n### 3. Step-by-Step Instructions\n1.  **Define a Branching Strategy:** Document a simple branching strategy. A common one is "feature branching":\n    *   `main` branch is for production-ready code.\n    *   All new work must be done on a new branch created from `main` (e.g., `feature/add-sales-report`).\n2.  **Enforce Pull Requests (PRs):** In your Git provider (GitHub, etc.), protect the `main` branch and require all changes to be merged via a pull request. Require at least one reviewer to approve the PR.\n3.  **Train on Commit Messages:** Teach the team to write clear, descriptive commit messages. A good message explains *why* a change was made.\n\n### 4. Resources and Tools\n- **Git Branch Protection Rules:** A feature in GitHub/GitLab to enforce PRs.\n- **A documented branching strategy** in your team Wiki.\n\n### 5. Next Steps and Progression\n- Integrate your Git repository with a CI/CD tool to automatically run tests on every pull request.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** Developers complain that the PR process is slow.\n- **Solution:** The team must build a culture of reviewing PRs quickly. Keep PRs small and focused on a single change to make them easier and faster to review.'},{id:411,slug:"defining-recipe-selection-frameworks-prepare-vs-python",question:"How to get started with defining recipe selection frameworks (e.g., when to prefer “Prepare” vs “Python”)?",answer:'### 1. Introduction/Overview\nOne of the most common questions from new developers is "which recipe should I use?". As an SME, you should provide a simple, clear framework to guide their decision, prioritizing clarity and performance.\n\n### 2. Prerequisites\n- An understanding of the capabilities of different recipe types.\n\n### 3. Step-by-Step Instructions\n1.  **Establish the "Visual First" Principle:** The default choice should always be a visual recipe.\n2.  **Create a Decision Tree:** Document a simple decision tree in your Wiki:\n    *   **Question 1: Can this transformation be done with the visual processors in a Prepare, Join, or Group recipe?**\n        *   **Yes:** Use the visual recipe. **This is the answer 90% of the time.**\n        *   **No:** Move to Question 2.\n    *   **Question 2: Does the logic require an external library or a complex algorithm not available visually?**\n        *   **Yes:** Use a **Python Recipe**.\n        *   **No:** Re-evaluate Question 1. Is there a creative way to use a visual recipe?\n3.  **Consider Performance:** Add a note about performance: "For large datasets in a database, a visual recipe set to \\\'Run on database\\\' will almost always be faster than a Python recipe."\n\n### 4. Resources and Tools\n- **A Wiki page** with the decision tree.\n\n### 5. Next Steps and Progression\n- During reviews, if you see a Python recipe that could have been a simple visual recipe, use it as a teaching moment to reinforce the framework.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A developer always defaults to Python because it\\\'s what they know.\n- **Solution:** Explain the benefits of visual recipes in Dataiku: they are easier for others to understand (self-documenting), and they can be pushed down to more powerful engines (SQL, Spark) for better performance.'},{id:412,slug:"standardizing-the-transformation-rules-in-prepare-steps",question:"How to get started with standardizing the transformation rules in “Prepare” steps?",answer:'### 1. Introduction/Overview\nMany projects require the same basic data cleaning steps. Standardizing these rules into a reusable component saves time and ensures consistency.\n\n### 2. Prerequisites\n- A set of common cleaning steps that your team performs often (e.g., trimming whitespace, converting to lowercase, handling nulls).\n\n### 3. Step-by-Step Instructions\n1.  **Create a "Standard Cleaning" Recipe:** In a shared or template project, create a Prepare recipe that contains your standard set of cleaning steps.\n2.  **Copy the Steps:** In the Prepare recipe, select all the steps in the script panel on the left and click the "Copy" button.\n3.  **Share the Logic:** This copies the JSON definition of the steps to your clipboard. You can now:\n    *   Paste this JSON into a team Wiki page called "Standard Cleaning Logic".\n    *   Email the JSON to your team.\n4.  **Reuse the Logic:** When a developer needs to apply these standard steps, they can simply copy the JSON from the Wiki and paste it into their own Prepare recipe. All the steps will be added instantly.\n\n### 4. Resources and Tools\n- **Prepare Recipe\\\'s Copy/Paste feature.**\n- **A central Wiki** to store the shared logic.\n\n### 5. Next Steps and Progression\n- For more advanced use cases, a developer can create a custom Python processor that encapsulates the logic, making it available directly in the Prepare recipe\\\'s processor list.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The shared logic doesn\\\'t apply perfectly to all datasets.\n- **Solution:** The pasted steps are just a starting point. The developer can then modify or delete individual steps to suit the specific needs of their dataset.'},{id:413,slug:"setting-ground-rules-for-using-stack-vs-join-recipes",question:"How to get started with setting ground rules for using Stack vs Join recipes?",answer:'### 1. Introduction/Overview\nNew users often confuse the Stack and Join recipes. Setting clear ground rules and providing simple visual explanations is key to preventing this common mistake.\n\n### 2. Prerequisites\n- An understanding of the difference between the two recipes.\n\n### 3. Step-by-Step Instructions\n1.  **Create a "Ground Rules" Wiki Page:** In your team\\\'s Wiki, create a page for recipe guidance.\n2.  **Define the Rule Simply:** Use clear, non-technical language.\n    *   "Use **Join** when you want to add new **columns** from another dataset." (Show a picture of two tables being combined side-by-side).\n    *   "Use **Stack** when you want to add new **rows** from another dataset." (Show a picture of two tables being combined one on top of the other).\n3.  **Provide a Use Case:**\n    *   **Join Use Case:** "Joining `orders` data with `customers` data to add the customer\\\'s name to each order."\n    *   **Stack Use Case:** "Stacking `sales_2022` data with `sales_2023` data to create a single historical dataset."\n\n### 4. Resources and Tools\n- **A Wiki page** with clear definitions and visual aids.\n\n### 5. Next Steps and Progression\n- During reviews, if you see a user misusing one of these recipes, refer them back to the Wiki page.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A user tries to join two datasets with no common key.\n- **Solution:** Explain that a join requires a "lookup key" that exists in both tables. This is a fundamental concept that needs to be understood.'},{id:414,slug:"using-pivot-and-unpivot-recipes-for-reporting-logic",question:"How to get started with using Pivot and Unpivot recipes for reporting logic?",answer:'### 1. Introduction/Overview\nPivoting and unpivoting are common data reshaping tasks needed for reporting. As an SME, you should guide developers on when and how to use Dataiku\\\'s dedicated visual recipes for these tasks.\n\n### 2. Prerequisites\n- A dataset that needs reshaping.\n- An understanding of "long" vs. "wide" data formats.\n\n### 3. Step-by-Step Instructions\n1.  **Explain the Use Cases:**\n    *   **Pivot:** Use the Pivot recipe to transform "long" data into "wide" data. This is common when you want to create summary tables for reports, where each category has its own column.\n    *   **Unpivot:** Use the Unpivot recipe for the reverse. This is useful when you get data from a source like Excel where it is already in a "pivoted" or "crosstab" format, and you need to normalize it into a long format for easier analysis.\n2.  **Provide a Clear Example:**\n    *   In a training project, provide a simple `sales_data_long` dataset (`Date`, `Category`, `Sales`).\n    *   Walk through using the **Pivot** recipe to turn it into `sales_data_wide` (`Date`, `Sales_Category_A`, `Sales_Category_B`).\n    *   Then, show how the **Unpivot** recipe can transform it back.\n\n### 4. Resources and Tools\n- **Pivot and Unpivot recipes.**\n- **A sample project** demonstrating the transformation.\n\n### 5. Next Steps and Progression\n- Discuss the performance implications. Pivoting can create very wide tables, so it\\\'s important to only pivot on columns with a limited number of unique values.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The user\\\'s pivot recipe is creating hundreds of columns.\n- **Solution:** This means the column they are pivoting on has too many unique values (high cardinality). They should first use a Prepare recipe to group the values or clean them up before pivoting.'},{id:415,slug:"preempting-edge-cases-in-window-and-top-n-recipes",question:"How to get started with preempting edge cases in Window and Top-N recipes?",answer:'### 1. Introduction/Overview\nThe Window and Top-N recipes are powerful, but they have settings that can lead to unexpected results if not understood. As an SME, you should proactively teach developers about these edge cases.\n\n### 2. Prerequisites\n- A deep understanding of how these recipes work.\n\n### 3. Step-by-Step Instructions\n1.  **For the Window Recipe:**\n    *   **Emphasize Ordering:** Stress that a Window function is meaningless without a defined **Order**. This is the most common mistake.\n    *   **Explain Partitioning:** Clearly explain that partitioning restarts the calculation for each group.\n    *   **Visualize the Frame:** Draw a diagram on a whiteboard showing how the "window frame" (e.g., "3 preceding rows") moves across the data.\n2.  **For the Top-N Recipe:**\n    *   **Focus on Ties:** The biggest edge case is how the recipe handles ties in the ranking column. Explain the difference between the "dense" ranking strategy and the sequential one.\n    *   **Explain "Retrieve all" vs. "Select N":** Clarify that "Retrieve all rows" can result in more than N rows if there are ties at the Nth position.\n\n### 4. Resources and Tools\n- **Whiteboards or diagramming tools** for visual explanations.\n- **Sample datasets** that specifically include ties and multiple partitions.\n\n### 5. Next Steps and Progression\n- Create a small quiz or a set of challenge questions to test the developers\\\' understanding of these edge cases.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A user says their running total from a Window recipe is wrong.\n- **Solution:** Ask them one question: "Did you set the ordering?". 99% of the time, they have forgotten to add an "Order by" clause to the recipe.'},{id:416,slug:"resolving-issues-when-visual-recipes-get-too-complex",question:"How to get started with resolving issues when visual recipes get too complex?",answer:'### 1. Introduction/Overview\nWhile it\\\'s possible to create a Prepare recipe with hundreds of steps, this can become difficult to manage and debug. As an SME, you should guide developers on how to refactor complex visual logic for better maintainability.\n\n### 2. Prerequisites\n- A project with a very long and complex visual recipe.\n\n### 3. Step-by-Step Instructions\n1.  **The "Single Responsibility" Principle:** Teach the principle that each recipe should have one clear, single purpose.\n2.  **Identify Logical Blocks:** Open the long Prepare recipe. Look for logical groups of steps in the script (e.g., a set of steps for address cleaning, another set for date calculations).\n3.  **Refactor into Multiple Recipes:**\n    *   Cut the steps for the first logical block (e.g., address cleaning) from the main recipe.\n    *   Create a new, intermediate Prepare recipe that performs only these steps.\n    *   Chain the original recipe to the output of this new, smaller recipe.\n    *   Repeat this process, breaking the monolithic recipe down into a chain of smaller, single-purpose recipes. The Flow will be longer, but each step will be simpler and easier to understand.\n\n### 4. Resources and Tools\n- **The Prepare Recipe\\\'s copy/paste feature** makes it easy to move steps between recipes.\n- **Flow Zones:** Organize the new, longer chain of recipes into a Flow Zone to keep the overall Flow clean.\n\n### 5. Next Steps and Progression\n- For very complex or repetitive logic, consider creating a reusable Python function in the project library or even a custom plugin.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** "But now my Flow has so many recipes!"\n- **Solution:** Explain that this is a good thing. It makes the pipeline more modular and easier to debug. If a step fails, you know exactly which small part of the logic has the problem. Use Flow Zones to manage the visual complexity.'},{id:417,slug:"validating-recipe-chains-for-schema-consistency",question:"How to get started with validating recipe chains for schema consistency?",answer:'### 1. Introduction/Overview\nIn a long chain of recipes, an upstream change can unexpectedly break a downstream recipe if it alters the schema (e.g., by changing a column name or type). As an SME, you should teach developers how to manage and validate schema consistency.\n\n### 2. Prerequisites\n- A Dataiku Flow with a multi-step recipe chain.\n\n### 3. Step-by-Step Instructions\n1.  **Proactive Check: "Propagate Schema Changes":**\n    *   When you make a change in an upstream recipe (e.g., renaming a column in a Prepare recipe), Dataiku is often aware of it.\n    *   Teach developers to look for and use the **Propagate schema changes** button, which can automatically update the configurations of downstream recipes.\n2.  **Reactive Check: The Job Log:**\n    *   If a job fails, the most common error is a schema error like "Column \\\'X\\\' not found."\n    *   Teach developers to read this error and trace it back. Open the failing recipe and look at its input dataset. Does the column exist? If not, the problem is in the upstream recipe that produced it.\n3.  **Best Practice: The "Analyze" Tool:**\n    *   Before building a new recipe on an existing dataset, encourage developers to quickly open the dataset and use the **Analyze** feature on a few key columns to check their name, type, and distribution. This can catch schema issues before a job is even run.\n\n### 4. Resources and Tools\n- **The "Propagate schema changes" feature.**\n- **The Job Log.**\n- **The Analyze feature** on datasets.\n\n### 5. Next Steps and Progression\n- Add automated schema checks to your pipeline. You can use a Python scenario step with the Dataiku API to get a dataset\\\'s schema and verify that it contains a specific set of required columns before running the main pipeline.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A downstream Join recipe is failing.\n- **Solution:** The most likely cause is that a column used in the join key was renamed or removed in an upstream Prepare recipe. The developer needs to update the Join recipe to use the new, correct key column.'},{id:418,slug:"enforcing-test-datasets-for-each-visual-recipe-module",question:"How to get started with enforcing test datasets for each visual recipe module?",answer:'### 1. Introduction/Overview\nUnit testing is not just for code. You can apply the same principle to visual recipes by creating small, dedicated test datasets that contain known edge cases. This ensures your visual logic is robust.\n\n### 2. Prerequisites\n- A visual recipe that performs a critical transformation.\n\n### 3. Step-by-Step Instructions\n1.  **Create a Test Dataset:**\n    *   Manually create a small CSV file or an inline dataset that contains sample data specifically designed to test your recipe.\n    *   Include rows that represent edge cases: null values, zeros, negative numbers, strings with trailing spaces, etc.\n2.  **Create a Test Branch of the Flow:**\n    *   Duplicate your main recipe.\n    *   Change the input of this duplicated recipe to be your new test dataset. Let\\\'s call this recipe `test_prepare_customers`.\n3.  **Define the Expected Output:** Manually create another small dataset that represents the *correct output* you expect after the recipe has processed the test input.\n4.  **Automate the Test:**\n    *   Create a **Scenario** called "Run Unit Tests".\n    *   In the scenario, add a step to build your `test_prepare_customers` recipe.\n    *   Add a second step that uses a **Sync/Compare** recipe or a Python recipe to compare the actual output with the expected output. If they do not match, the step should fail, which will fail the scenario.\n\n### 4. Resources and Tools\n- **Inline Datasets:** For creating small test datasets.\n- **The Duplicate feature:** To create a test version of your recipe.\n- **Scenarios:** To automate the execution of the test.\n\n### 5. Next Steps and Progression\n- Integrate this "Run Unit Tests" scenario into your CI/CD pipeline. No code should be merged unless all the visual recipe unit tests pass.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** "This seems like a lot of setup."\n- **Solution:** It is. You don\\\'t need to do this for every single recipe. Reserve this practice for your most critical, complex, and shared visual recipes where an error would have a significant impact.'},{id:419,slug:"mentoring-on-when-to-add-intermediate-sync-recipes",question:"How to get started with mentoring on when to add intermediate Sync recipes?",answer:'### 1. Introduction/Overview\nThe **Sync** recipe\\\'s main purpose is to move data from one storage location to another. As an SME, you should guide junior developers on the two primary strategic reasons to add a Sync recipe into a flow.\n\n### 2. Prerequisites\n- An understanding of Dataiku\\\'s different storage connections (e.g., filesystem, S3, Snowflake).\n\n### 3. Step-by-Step Instructions\n\n#### Use Case 1: Changing the Storage Engine\n- **When:** You have a series of recipes running on one engine (e.g., in-memory on the filesystem) but the next step needs to be on a different engine (e.g., a SQL database).\n- **Example:**\n    1. You upload a CSV file and use a Prepare recipe to clean it (this runs in-memory).\n    2. You need to join this cleaned data with a very large table in your Snowflake data warehouse.\n- **Solution:** Before the Join recipe, add a **Sync** recipe. Its input is the cleaned CSV data. For its output, change the connection to your Snowflake connection. This will create a new table in Snowflake. Now, both inputs to your Join recipe are in Snowflake, and you can run the join with the highly performant "Run on database" engine.\n\n#### Use Case 2: Creating a Major Checkpoint\n- **When:** You have a very long and complex data preparation flow. You want to "finalize" the result of this preparation into a stable, "golden" dataset before you start using it for multiple downstream purposes (like modeling and reporting).\n- **Solution:** After the last Prepare recipe in your preparation chain, add a **Sync** recipe. This creates a clean, materialized copy of your prepared data. It signals to other developers that this is a stable, trusted checkpoint in the flow.\n\n### 4. Resources and Tools\n- **The Sync Recipe.**\n- **Multiple data connections.**\n\n### 5. Next Steps and Progression\n- Explain how the Sync recipe is also used to move data between different environments (e.g., from a dev database to a prod database) during deployments.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A user says, "Why can\\\'t I just use a Prepare recipe and change the output connection?"\n- **Solution:** Explain that other visual recipes (like Prepare) have their engine determined by their *input*. A Sync recipe is special because its primary purpose is to let you explicitly control the output connection, making it the correct tool for moving data between systems.'},{id:420,slug:"embedding-qa-tags-within-recipe-descriptions",question:"How to get started with embedding QA tags within recipe descriptions?",answer:"### 1. Introduction/Overview\nIn a busy project, it can be hard to know the quality assurance (QA) status of a specific recipe. A lightweight but effective solution is to embed simple QA status tags directly in the recipe\\'s description, making the status visible at a glance in the Flow.\n\n### 2. Prerequisites\n- A team agreement on a simple set of QA tags.\n\n### 3. Step-by-Step Instructions\n1.  **Define Your QA Tags:** Agree on a simple set of tags. For example:\n    *   `[QA: Not Started]`\n    *   `[QA: In Progress]`\n    *   `[QA: Ready for Review]`\n    *   `[QA: Approved]`\n2.  **Use in Descriptions:**\n    *   When a developer is working on a recipe, they should add the appropriate tag to the beginning of the recipe\\'s **Description** field.\n    *   **Example Description:** `[QA: Ready for Review] This recipe joins customer and sales data.`\n3.  **Update During the Workflow:**\n    *   As the recipe moves through your QA process, the developer or reviewer updates the tag.\n    *   For example, after a peer review is complete, the reviewer changes the tag to `[QA: Approved]`.\n\n### 4. Resources and Tools\n- **The Description field** on recipes.\n\n### 5. Next Steps and Progression\n- You can use the Dataiku API to write a scenario that scans all recipes in a project and creates a report of the current QA status of each, providing a simple QA dashboard.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** People forget to update the tags.\n- **Solution:** This must be part of your team\\'s process. For example, a pull request should not be merged until the reviewer has updated the tag on the relevant recipes to `[QA: Approved]`."}]},44493:(e,t,a)=>{"use strict";a.d(t,{BT:()=>c,Wu:()=>u,ZB:()=>l,Zp:()=>r,aR:()=>s});var o=a(60687),i=a(43210),n=a(4780);let r=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("rounded-lg border bg-card text-card-foreground shadow-sm",e),...t}));r.displayName="Card";let s=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("flex flex-col space-y-1.5 p-6",e),...t}));s.displayName="CardHeader";let l=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("text-2xl font-semibold leading-none tracking-tight",e),...t}));l.displayName="CardTitle";let c=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("text-sm text-muted-foreground",e),...t}));c.displayName="CardDescription";let u=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("p-6 pt-0",e),...t}));u.displayName="CardContent",i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("flex items-center p-6 pt-0",e),...t})).displayName="CardFooter"},60720:(e,t,a)=>{Promise.resolve().then(a.t.bind(a,16444,23)),Promise.resolve().then(a.t.bind(a,16042,23)),Promise.resolve().then(a.t.bind(a,88170,23)),Promise.resolve().then(a.t.bind(a,49477,23)),Promise.resolve().then(a.t.bind(a,29345,23)),Promise.resolve().then(a.t.bind(a,12089,23)),Promise.resolve().then(a.t.bind(a,46577,23)),Promise.resolve().then(a.t.bind(a,31307,23))},61135:()=>{},65370:(e,t,a)=>{Promise.resolve().then(a.t.bind(a,85814,23)),Promise.resolve().then(a.bind(a,33486))},67393:(e,t,a)=>{"use strict";a.r(t),a.d(t,{default:()=>n});var o=a(37413),i=a(28578);function n(){return(0,o.jsx)("div",{className:"flex justify-center items-center min-h-[calc(100vh-200px)]",children:(0,o.jsx)(i.A,{className:"h-16 w-16 animate-spin text-primary"})})}},70440:(e,t,a)=>{"use strict";a.r(t),a.d(t,{default:()=>i});var o=a(31658);let i=async e=>[{type:"image/x-icon",sizes:"16x16",url:(0,o.fillMetadataSegment)(".",await e.params,"favicon.ico")+""}]},72498:(e,t,a)=>{"use strict";a.d(t,{ab:()=>o});let o=[{id:1,slug:"building-your-first-dataiku-dss-flow",question:"How to get started with building your first Dataiku DSS flow?",answer:`
### 1. Introduction/Overview
Building your first flow in Dataiku DSS is the foundational step to mastering the platform. A "Flow" is a visual representation of your data pipeline, showing how data is ingested, transformed, and processed. This guide will walk you through creating a simple yet complete flow, a process that is intuitive and can be completed in under an hour.

### 2. Prerequisites
- **Access to a Dataiku DSS instance:** You need a running instance of Dataiku DSS (Free, Cloud, or Enterprise).
- **A sample dataset:** A simple CSV file is perfect. You can use your own or find one online (e.g., from Kaggle).
- **Basic Data Concepts:** Understanding of what datasets and data transformations are.

### 3. Step-by-Step Instructions
1.  **Create a New Project:** From the Dataiku homepage, click "+ New Project" and choose "Blank project". Give it a name like "My First Flow".
2.  **Import Your Dataset:**
    *   Inside your project, click **+ DATASET**.
    *   Select **Upload your files** and drag your CSV file into the designated area.
    *   Dataiku will preview the data. Confirm the format and click **CREATE**. Your dataset now appears in the Flow.
3.  **Create a Prepare Recipe:**
    *   Select your new dataset in the Flow.
    *   From the right-hand panel, click on the **Prepare** recipe under "Visual recipes".
    *   Click **CREATE RECIPE**. This opens the Prepare recipe interface.
4.  **Perform a Transformation:**
    *   In the Prepare recipe, click on a column header. A list of suggested transformations will appear.
    *   Choose a simple action, like **Filter** on a specific value or **Clear cells with invalid values**.
    *   The changes will be previewed instantly.
5.  **Run the Recipe:** Click the **Run** button at the bottom left. Once it completes, you'll see two datasets and one recipe in your Flow, showing the input, the transformation, and the output.

### 4. Resources and Tools
- **Dataiku Academy:** Offers free courses like "Core Designer" that are perfect for beginners.
- **Sample Projects:** Dataiku comes with built-in sample projects. Explore them to see how complex flows are built.
- **Community Forum:** The Dataiku Community is a great place to ask questions if you get stuck.

### 5. Next Steps and Progression
- **Add more recipes:** Try chaining another recipe, like a **Join** or **Group**, to the output of your first recipe.
- **Explore other dataset types:** Try connecting to a SQL database or a cloud storage source.
- **Build a simple chart:** Open your output dataset and go to the "Charts" tab to visualize your results.

### 6. Common Challenges and Solutions
- **Challenge:** "My dataset schema was not detected correctly."
- **Solution:** In the dataset import screen, you can manually override the column types and format settings before creating the dataset.
- **Challenge:** "I'm not sure which transformation to use."
- **Solution:** The processor library in the Prepare recipe is searchable. Try typing a keyword like "date" or "split" to find the right tool.
`},{id:2,slug:"designing-end-to-end-etl-pipelines-in-dataiku-dss",question:"How to get started with designing end‑to‑end ETL pipelines in Dataiku DSS?",answer:`
### 1. Introduction/Overview
Designing an end-to-end ETL (Extract, Transform, Load) pipeline is a core competency for any data professional. In Dataiku, this process is visualized in the Flow, making it transparent and manageable. This guide outlines the strategic approach to designing a robust ETL pipeline from source to final output.

### 2. Prerequisites
- **Clear Business Goal:** Understand what you want to achieve with the data (e.g., create a sales report, prepare data for a model).
- **Access to Data Sources:** You'll need credentials and connection details for your source systems (databases, APIs, etc.).
- **Familiarity with Dataiku Flow:** Basic knowledge of how to create datasets and recipes.

### 3. Step-by-Step Instructions
1.  **Plan Your Pipeline (The "Whiteboard" Phase):**
    *   **Extract:** Identify all your data sources.
    *   **Transform:** List the key cleaning, joining, and aggregation steps required.
    *   **Load:** Define the final destination and format of your output data.
2.  **Organize with Flow Zones:** Before you build, create Flow Zones in your project for each stage of your plan (e.g., "1_Ingestion", "2_Transformation", "3_Outputs"). This keeps your pipeline organized.
3.  **Implement the "Extract" Stage:** In the "Ingestion" zone, create datasets for each of your sources (e.g., from SQL, S3, or APIs).
4.  **Implement the "Transform" Stage:** In the "Transformation" zone, use a series of visual recipes (Prepare, Join, Group, etc.) to implement your logic. Chain the recipes together, with the output of one becoming the input for the next.
5.  **Implement the "Load" Stage:** In the "Outputs" zone, take your final transformed dataset and add an **Export** recipe. Configure it to write the data to your target system (e.g., a Snowflake table or a cloud storage bucket).

### 4. Resources and Tools
- **Flow Zones:** The primary tool for organizing complex pipelines.
- **Project Wiki:** Use the project's Wiki to document your pipeline design, data sources, and business logic.
- **Dataiku Academy:** The "Advanced Designer" path covers complex pipeline architecture.

### 5. Next Steps and Progression
- **Automation:** Once the pipeline is built, create a **Scenario** to schedule it to run automatically.
- **Data Quality:** Add **Metrics and Checks** to your key datasets to monitor data quality.
- **Optimization:** Use the Job Inspector to find and optimize any slow-running parts of your pipeline.

### 6. Common Challenges and Solutions
- **Challenge:** "My flow is becoming a 'spaghetti mess'."
- **Solution:** Be disciplined with Flow Zones. Group related items and collapse zones you aren't working on to keep the view clean.
- **Challenge:** "How do I handle environment differences (dev vs. prod)?"
- **Solution:** Use **Project Variables** for connections and paths. This allows you to promote the pipeline to a production environment by simply changing the variables, without editing the recipes.
`},{id:3,slug:"ingesting-data-from-databases-into-dataiku",question:"How to get started with ingesting data from databases into Dataiku?",answer:`
### 1. Introduction/Overview
Connecting to a database is often the first step in a data project. Dataiku provides native connectors for most SQL databases, making it simple to ingest tables as datasets in your Flow. This guide covers the two-step process: creating a connection and then importing a table.

### 2. Prerequisites
- **Database Credentials:** You'll need the hostname, port, database name, username, and password for your database.
- **Permissions:** Your database user needs at least read access to the tables you want to ingest.
- **Network Access:** The Dataiku instance must be able to reach the database server. This may require firewall rules to be configured by your IT team.

### 3. Step-by-Step Instructions
1.  **Create the Connection (Admin Task):**
    *   Navigate to **Administration > Connections**. *(Note: You may need admin rights for this step.)*
    *   Click **+ NEW CONNECTION** and select your database type (e.g., PostgreSQL, Snowflake).
    *   Fill in the connection details (host, credentials, etc.) and give the connection a clear name.
    *   Click **Test** to verify that Dataiku can connect successfully, then click **Create**.
2.  **Create a Dataset from the Connection:**
    *   Go to your project's Flow and click **+ DATASET**.
    *   Select your database type from the list of sources.
    *   A list of tables will appear. Select the table you want to import.
    *   Dataiku will show a preview. Click **CREATE**. The database table is now represented as a dataset in your Flow.

### 4. Resources and Tools
- **Connections Page:** The central place in Dataiku to manage all external connections.
- **SQL Query Recipe:** Once you have a dataset from a database, you can use a SQL recipe to directly query it, pushing the computation to the database.

### 5. Next Steps and Progression
- **Import multiple tables:** Bring in several tables from the same database.
- **Join database tables:** Use a **Join** recipe to combine the datasets you've imported.
- **Write data back:** Use an **Export** recipe to write the results of your flow back to a new table in the same database.

### 6. Common Challenges and Solutions
- **Challenge:** "The connection test fails."
- **Solution:** This is almost always a network issue. Double-check all credentials. Confirm with your network team that the Dataiku server's IP address is allowed to access the database port.
- **Challenge:** "I can't see the table I'm looking for."
- **Solution:** This is likely a permissions issue. Check with your database administrator to ensure the user account Dataiku is using has 'SELECT' privileges on that specific table or schema.
`},{id:4,slug:"loading-csv-excel-data-into-dataiku-dss",question:"How to get started with loading CSV/Excel data into Dataiku DSS?",answer:`
### 1. Introduction/Overview
Loading flat files like CSVs and Excel spreadsheets is a fundamental skill in Dataiku. The platform provides a simple, user-friendly interface to upload files directly into your project's Flow, automatically detecting formats and schemas.

### 2. Prerequisites
- **A CSV or Excel file:** Have the file ready on your local machine.
- **A Dataiku Project:** You need an existing project to add the dataset to.

### 3. Step-by-Step Instructions
1.  **Navigate to Your Project Flow:** Open the project where you want to add the data.
2.  **Select "Upload your files":**
    *   Click the **+ DATASET** button.
    *   From the menu, choose **Upload your files**.
3.  **Upload the File:** You can either drag and drop your CSV/Excel file into the browser window or click to browse your computer.
4.  **Preview and Configure:**
    *   Dataiku will automatically upload the file and show a preview.
    *   It will try to detect the file format (e.g., separator for CSVs) and infer the column names and types.
    *   Carefully review this preview. You can make adjustments here if needed (e.g., change a column type from string to number).
5.  **Create the Dataset:** Once you are satisfied with the preview, click the **CREATE** button. The file is now a dataset in your Flow, ready to be used in recipes.

### 4. Resources and Tools
- **The Upload Interface:** The primary tool for this task, offering previews and configuration options.
- **Prepare Recipe:** The immediate next step is usually to use a Prepare recipe to clean the uploaded data.

### 5. Next Steps and Progression
- **Upload multiple files:** Practice by uploading a few different files.
- **Explore Format Options:** In the preview screen, explore the settings for different delimiters, encodings, and how to handle headers.
- **Use the Dataset:** Select your newly created dataset and apply a **Prepare** recipe to start transforming it.

### 6. Common Challenges and Solutions
- **Challenge:** "My column headers are in the second row, not the first."
- **Solution:** In the preview screen, there is an option to "Skip first N rows". Set this to 1 to correctly detect the headers.
- **Challenge:** "Dataiku misidentified a column's data type."
- **Solution:** In the preview screen, you can click on any column header to manually change its type before creating the dataset.
- **Challenge:** "My international characters are not displaying correctly."
- **Solution:** This is an encoding issue. In the preview settings, try changing the file encoding from the default to \`UTF-8\` or another appropriate standard.
`},{id:5,slug:"integrating-rest-apis-as-dataiku-datasets",question:"How to get started with integrating REST APIs as Dataiku datasets?",answer:`
### 1. Introduction/Overview
Many modern services expose their data via REST APIs. Dataiku can connect to these APIs and treat their JSON responses as regular datasets. This allows you to pull live data directly into your flows for analysis.

### 2. Prerequisites
- **API Documentation:** You need the documentation for the API you want to connect to. This will provide the endpoint URL, required parameters, and authentication details.
- **API Key (if required):** If the API requires authentication, you will need an API key or other credentials.
- **Network Access:** The Dataiku server must be able to make outbound requests to the API's domain.

### 3. Step-by-Step Instructions
1.  **Select the API Connector:** In your Flow, click **+ DATASET** and search for or select the **API** plugin.
2.  **Configure the API Endpoint:**
    *   Give your new dataset a name (e.g., \`api_weather_data\`).
    *   In the API configuration screen, enter the base URL of the API endpoint.
    *   If authentication is needed, add a new header and provide your API key (e.g., Header name: \`Authorization\`, Header value: \`Bearer YOUR_API_KEY\`).
3.  **Test the Connection:** Click the **Test** button. Dataiku will call the API and show you the raw JSON response.
4.  **Configure JSON Parsing:**
    *   Dataiku will try to automatically identify the array in the JSON response that contains the data records.
    *   If the automatic parsing isn't correct, you may need to manually specify the path to the array of records.
5.  **Create the Dataset:** Once the preview shows the data correctly parsed into rows and columns, click **CREATE**. You now have a dataset that pulls live data from the API every time it's built.

### 4. Resources and Tools
- **API Plugin:** The dedicated Dataiku connector for this task.
- **Postman:** A great external tool for testing API calls and understanding the JSON structure before you configure it in Dataiku.
- **Project Variables:** Store your API key in a project variable (marked as a password) instead of hardcoding it in the recipe for better security.

### 5. Next Steps and Progression
- **Handle Pagination:** Many APIs return data in pages. Explore the API connector's settings for handling pagination to retrieve all records.
- **Parameterize API Calls:** Use project variables in the API URL or parameters to make dynamic calls (e.g., fetch data for a specific date).
- **Schedule a Refresh:** Create a **Scenario** to rebuild this API dataset on a schedule, keeping your data fresh.

### 6. Common Challenges and Solutions
- **Challenge:** "The API call fails with an authentication error."
- **Solution:** Double-check your API key and how you are providing it. Read the API documentation carefully; some APIs expect the key in a header, others as a URL parameter.
- **Challenge:** "The JSON is not parsing correctly."
- **Solution:** Use the "Test" feature to view the raw JSON. Manually inspect its structure to find the correct path to the array of data you want to extract. The path might be nested, like \`results.records\`.
`},{id:6,slug:"working-with-cloud-storage-sources-s3-gcs-azure-blob",question:"How to get started with working with cloud storage sources (S3, GCS, Azure Blob)?",answer:`
### 1. Introduction/Overview
Cloud storage is a common place to store raw data files. Dataiku integrates seamlessly with major providers like AWS S3, Google Cloud Storage (GCS), and Azure Blob Storage, allowing you to access these files as if they were local.

### 2. Prerequisites
- **Cloud Account Credentials:** You'll need an access key and secret key (or equivalent credentials) for a user with read permissions on the storage location.
- **Bucket/Container and Path:** You need to know the name of the storage bucket and the path to the file(s) you want to access.
- **Admin Access (for first-time setup):** You need Dataiku admin rights to configure the connection initially.

### 3. Step-by-Step Instructions
1.  **Configure the Cloud Connection:**
    *   In Dataiku, navigate to **Administration > Connections**.
    *   Click **+ NEW CONNECTION** and select your cloud provider (e.g., Amazon S3).
    *   Enter your credentials (access key, secret key). It is highly recommended to use instance roles or more secure methods if available.
    *   Save and test the connection.
2.  **Create a Dataset from Cloud Storage:**
    *   In your project's Flow, click **+ DATASET**.
    *   Select your cloud provider (e.g., Amazon S3).
    *   Dataiku will now use the configured connection to let you browse your buckets.
    *   Navigate to the correct bucket and folder and select the file you want to use.
3.  **Preview and Create:**
    *   Dataiku will preview the file and infer its schema, just like with an uploaded file.
    *   Confirm the settings and click **CREATE**. The cloud file is now a dataset in your Flow.

### 4. Resources and Tools
- **Cloud Provider IAM Console:** Where you manage users and permissions to grant Dataiku access.
- **Dataiku Connections:** The central hub for managing all external data source connections.

### 5. Next Steps and Progression
- **Access a folder of files:** Instead of a single file, you can point a dataset to an entire folder of identically structured files (e.g., daily log files). Dataiku will treat them as a single, larger dataset.
- **Use Partitioning:** If your files are organized into folders by date (e.g., \`/logs/2023/01/01/\`), you can set up partitioning on the dataset. This allows for much more efficient processing.
- **Write back to cloud storage:** Use an **Export** recipe to save your final results back to a different bucket or path in your cloud storage.

### 6. Common Challenges and Solutions
- **Challenge:** "Connection failed or Access Denied error."
- **Solution:** This is a permissions issue. In your cloud provider's console (e.g., AWS IAM), ensure the user or role whose credentials Dataiku is using has a policy attached that grants it permission to list buckets and read objects from the specific bucket you're trying to access.
- **Challenge:** "I can't see my files when browsing."
- **Solution:** Double-check the path. Bucket and folder names are case-sensitive. Also, verify you are in the correct cloud region.
`},{id:7,slug:"combining-disparate-data-sources-into-unified-datasets",question:"How to get started with combining disparate data sources into unified datasets?",answer:`
### 1. Introduction/Overview
A key task in data engineering is combining data from different sources to create a single, unified view. Dataiku excels at this with its visual **Join** and **Stack** recipes, allowing you to merge datasets regardless of their original source (e.g., joining a SQL table with a CSV file).

### 2. Prerequisites
- **At least two datasets:** You need to have already imported the datasets you want to combine into your Flow.
- **Understanding of Join vs. Stack:**
    - **Join:** Adds new *columns* based on a common key (like a VLOOKUP).
    - **Stack:** Appends new *rows* from datasets that have the same columns (like a UNION ALL).

### 3. Step-by-Step Instructions
#### To Join Two Datasets:
1.  In your Flow, select one of the datasets you want to join (this will be the "left" dataset).
2.  From the right-hand panel, choose the **Join with...** recipe.
3.  Select the second dataset you want to join with.
4.  In the Join recipe screen:
    *   Select the **Join Type** (e.g., Left, Inner, Outer).
    *   Define the **Join Condition** by clicking on the column(s) that serve as the common key between the two datasets.
    *   In the "Selected Columns" panel, choose which columns from both datasets you want to keep in the final output.
5.  Click **Run** to create the new, unified dataset.

#### To Stack Two Datasets:
1.  Select one of the datasets.
2.  From the right-hand panel, choose the **Stack with...** recipe.
3.  Select the second dataset. Dataiku will automatically map columns with the same name.
4.  Review the column mappings. You can manually adjust them if needed.
5.  Click **Run** to create the new, appended dataset.

### 4. Resources and Tools
- **Join Recipe:** The primary tool for combining data based on a key.
- **Stack Recipe:** The tool for appending rows.
- **Prepare Recipe:** Often used before a Join or Stack to clean up join keys or align column names.

### 5. Next Steps and Progression
- **Multi-dataset Joins:** You can chain Join recipes to combine three or more datasets.
- **Fuzzy Joins:** If your join keys are similar but not identical (e.g., due to typos), explore the **Fuzzy Join** recipe.
- **Post-Join Cleanup:** Add a **Prepare** recipe after your join to rename columns or remove duplicate join keys.

### 6. Common Challenges and Solutions
- **Challenge:** "My join is creating duplicate rows."
- **Solution:** This happens if your join key is not unique in the "right" dataset. Before the join, use a **Group** recipe to deduplicate the right dataset based on the key, ensuring there is only one row per key.
- **Challenge:** "My stack failed because of a schema mismatch."
- **Solution:** Before stacking, use separate **Prepare** recipes on each input dataset to ensure the column names and data types are identical.
`},{id:8,slug:"creating-multi-step-recipe-chains",question:"How to get started with + creating multi‑step recipe chains?",answer:`
### 1. Introduction/Overview
A "recipe chain" is the fundamental structure of a Dataiku Flow. It's a sequence of recipes where the output of one step becomes the input for the next. This creates a clear, traceable data pipeline. This guide explains how to build these chains.

### 2. Prerequisites
- **A Dataiku Project:** An existing project with at least one dataset.
- **A Goal:** A clear idea of the sequence of transformations you want to perform.

### 3. Step-by-Step Instructions
1.  **Create the First Link:**
    *   Start with your raw dataset in the Flow.
    *   Select it and add your first recipe, for example, a **Prepare** recipe to clean the data.
    *   Run this recipe. You now have a chain of two datasets and one recipe.
2.  **Add the Second Link:**
    *   Now, select the *output dataset* from your first recipe.
    *   From the right-hand panel, choose your next recipe. For instance, a **Join** recipe to combine it with another dataset.
    *   Configure and run this second recipe.
3.  **Continue the Chain:**
    *   Select the output of the Join recipe.
    *   Add a third recipe, perhaps a **Group** recipe to aggregate the data.
    *   Run this recipe.
4.  **Visualize the Chain:** Your Flow now shows a clear, left-to-right pipeline: \`[Raw Data] -> (Prepare) -> [Cleaned Data] -> (Join) -> [Joined Data] -> (Group) -> [Aggregated Data]\`. This is a multi-step recipe chain.

### 4. Resources and Tools
- **The Flow:** The canvas where you build and visualize your recipe chains.
- **The Actions Panel:** The right-hand panel where you select recipes to add to the chain.

### 5. Next Steps and Progression
- **Complex Chains:** Build longer chains with more complex logic.
- **Branching Chains:** A single dataset can be the input to multiple recipes, creating branches in your Flow. For example, you could create two different sets of aggregations from the same cleaned dataset.
- **Refactoring:** As chains get long, use **Flow Zones** to group logical segments of the chain.

### 6. Common Challenges and Solutions
- **Challenge:** "I accidentally connected a recipe to the wrong input."
- **Solution:** In the recipe's settings, go to the "Input/Output" tab. Here you can change the input dataset for the recipe without having to delete and recreate it.
- **Challenge:** "My flow is hard to read because the chain goes all over the place."
- **Solution:** Use the **Arrange** button in the Flow to automatically clean up the layout. For more complex flows, manually drag and drop the items to create a clean, logical layout.
`},{id:9,slug:"structuring-reusable-flow-zones",question:"How to get started with + structuring reusable Flow Zones?",answer:`
### 1. Introduction/Overview
As your projects grow, the Flow can become crowded and difficult to navigate. Flow Zones are the solution. They are visual containers that let you group related datasets and recipes, creating a clean, high-level view of your pipeline.

### 2. Prerequisites
- **A Dataiku project with a moderately complex Flow:** You need a Flow with several recipe chains to see the benefit.
- **A Logical Project Structure:** You should have a mental map of the different stages of your project (e.g., ingestion, preparation, modeling).

### 3. Step-by-Step Instructions
1.  **Create a Flow Zone:**
    *   In your Flow, right-click on an empty area of the canvas.
    *   Select **Create Flow Zone**.
    *   Give it a name that represents a stage of your pipeline, for example, \`1_Data_Ingestion\`. Using numbers helps enforce a logical order.
2.  **Move Items into the Zone:**
    *   Select the datasets and recipes that belong to this stage by holding \`Shift\` and clicking on them.
    *   Drag the selected items and drop them anywhere inside the new Flow Zone.
3.  **Create More Zones:** Repeat the process for other logical stages. Common zone structures include:
    *   \`1_Data_Ingestion\` (raw data sources)
    *   \`2_Data_Preparation\` (cleaning and joining)
    *   \`3_Feature_Engineering\` (creating features for modeling)
    *   \`4_Modeling\` (training and scoring models)
    *   \`5_Outputs\` (final datasets for reporting)
4.  **Collapse and Expand Zones:** Click the \`-\` icon on a Flow Zone to collapse it. This hides the details and shows only the connections between zones, providing a clean, high-level overview of your entire project architecture.

### 4. Resources and Tools
- **The Flow Canvas:** The primary interface for creating and managing zones.
- **Project Wiki:** Document your Flow Zone strategy in the Wiki so that all team members follow the same structure.

### 5. Next Steps and Progression
- **Reusable Flows via Duplication:** A well-structured project with clear zones can serve as a template. You can duplicate the entire project to kickstart new work with a proven architecture.
- **Cross-Project Standardization:** Define a standard set of Flow Zones for your entire organization to ensure all projects are structured consistently.
- **Sharing Work:** Collapsed zones make it easy to present your project architecture to stakeholders without overwhelming them with details.

### 6. Common Challenges and Solutions
- **Challenge:** "I have items that seem to belong to two zones."
- **Solution:** Choose the zone that represents the item's primary purpose. A dataset's zone is determined by the recipe that *creates* it.
- **Challenge:** "The connections between my collapsed zones look like a mess."
- **Solution:** Manually arrange the Flow Zones on the canvas to create a clean, left-to-right flow of data between them.
`},{id:10,slug:"implementing-branching-and-looping-in-dataiku-flows",question:"How to get started with + implementing branching and looping in Dataiku flows?",answer:`
### 1. Introduction/Overview
While Dataiku Flows are primarily Directed Acyclic Graphs (DAGs), you can implement more complex control flows like branching and looping using Scenarios and variables. Branching allows for conditional execution, while looping enables iterative processing.

### 2. Prerequisites
- **Understanding of Dataiku Scenarios:** Knowledge of how to create scenarios and add steps.
- **Basic Python Knowledge:** Required for implementing the conditional logic.
- **Familiarity with Project Variables:** Understanding how to create and use variables.

### 3. Step-by-Step Instructions
#### Branching (Conditional Execution)
1.  **Build Both Branches:** In your Flow, build out the different logical paths. For example, have a dataset \`input_data\` that can be processed by either \`recipe_A\` or \`recipe_B\`.
2.  **Create a Scenario:** Create a new scenario to control the execution.
3.  **Add a Python Step:** Add a "Execute Python code" step. This script will contain your conditional logic.
4.  **Write the Logic:** In the Python step, write code to check a condition (e.g., by querying a database or checking a project variable). Based on the result, use the Dataiku API to run the appropriate job.
    > \`\`\`python
    > if condition:
    >     project.build("output_of_recipe_A")
    > else:
    >     project.build("output_of_recipe_B")
    > \`\`\`
#### Looping (Iterative Execution)
1.  **Parameterize Your Flow:** Build a flow that depends on a project variable. For example, a filter in a recipe could be \`date == '\${run_date}'\`.
2.  **Create a "Controller" Scenario:** Create a new scenario.
3.  **Add a Python Step for Looping:** In a Python step, define the list of values you want to loop over.
4.  **Write the Loop:** Use a \`for\` loop. Inside the loop, set the project variable and then trigger the build of your parameterized flow.
    > \`\`\`python
    > dates_to_process = ["2023-01-01", "2023-01-02"]
    > for d in dates_to_process:
    >     vars = project.get_variables()
    >     vars["standard"]["run_date"] = d
    >     project.set_variables(vars)
    >     project.build("final_output")
    > \`\`\`

### 4. Resources and Tools
- **Scenarios:** The orchestration engine of Dataiku.
- **Python Recipes/Steps:** Provide the flexibility to implement custom control flow.
- **Dataiku Python API Documentation:** Essential for programmatically controlling jobs and variables.

### 5. Next Steps and Progression
- **Recursive Loops:** For more advanced cases, a scenario can even trigger itself with new parameters, creating a recursive loop.
- **Dynamic Branching:** The condition in your Python script can be dynamic, for example, based on the number of rows in a dataset.

### 6. Common Challenges and Solutions
- **Challenge:** "How do I get the list of items to loop over?"
- **Solution:** The list can be hardcoded, or for a more dynamic approach, you can read it from a Dataiku dataset at the start of your Python script.
- **Challenge:** "My loop is not working in parallel."
- **Solution:** Standard scenario loops run sequentially. For parallel execution, you would need a more advanced setup using containerized execution where your Python script could launch multiple jobs simultaneously.
`},{id:11,slug:"using-prepare-recipes-for-data-cleaning",question:"How to get started with + using Prepare recipes for data cleaning?",answer:`
### 1. Introduction/Overview
The **Prepare recipe** is the cornerstone of data transformation in Dataiku. It provides a powerful, interactive, and visual interface for cleaning, normalizing, and enriching your data without writing code. Mastering the Prepare recipe is essential for any Dataiku user.

### 2. Prerequisites
- **A dataset in your Flow:** You need a dataset to clean.
- **An idea of what "clean" means:** Understand the data quality issues you need to address (e.g., missing values, incorrect formats, inconsistent text).

### 3. Step-by-Step Instructions
1.  **Create a Prepare Recipe:**
    *   In your Flow, select the dataset you want to clean.
    *   From the right-hand panel, click on the **Prepare** recipe.
    *   Give the output dataset a name (e.g., \`your_dataset_prepared\`) and click **CREATE RECIPE**.
2.  **Explore the Interface:** You are now in the Prepare recipe. You see your data in a spreadsheet-like view. On the left is the list of transformation "steps".
3.  **Add a Cleaning Step:**
    *   Click on the header of a column that needs cleaning.
    *   A panel will appear with context-aware suggestions.
    *   For example, if a column has missing values, you can select **Clear rows with no value** or **Impute missing values**.
4.  **Use the Processor Library:**
    *   Click the **+ ADD A NEW STEP** button to open the full library of over 100 processors.
    *   You can search for processors by name (e.g., "Filter", "Parse date", "Split column").
5.  **Chain Multiple Steps:** Continue adding steps to the list on the left. Each step operates on the result of the previous one. You can reorder or delete steps as needed.
6.  **Run the Recipe:** Once you are happy with your cleaning script, click the **Run** button to apply the transformations and create the new, cleaned output dataset.

### 4. Resources and Tools
- **The Processor Library:** Your main tool, containing all available transformation functions.
- **The Formula Language:** For custom transformations, the "Formula" processor lets you write Excel-like expressions.
- **Dataiku Academy:** The "Visual Recipes" courses provide deep dives into the Prepare recipe.

### 5. Next Steps and Progression
- **Advanced Processors:** Explore more complex processors like "GeoIP lookup" or "Currency conversion".
- **Regular Expressions:** Use the "Find & Replace" or "Filter" processors with regular expressions for powerful pattern matching.
- **Copy/Paste Steps:** You can copy the list of steps from one recipe and paste them into another to reuse cleaning logic.

### 6. Common Challenges and Solutions
- **Challenge:** "My changes are not being saved."
- **Solution:** Changes in the Prepare recipe are only applied to the output dataset when you click the **Run** button.
- **Challenge:** "I made a mistake in a step."
- **Solution:** You can click on any step in the list to edit its configuration or click the "X" to delete it. The data preview will update instantly.
`},{id:12,slug:"writing-python-recipes-in-dataiku-dss",question:"How to get started with + writing Python recipes in Dataiku DSS?",answer:`
### 1. Introduction/Overview
While visual recipes are powerful, sometimes you need the full flexibility of a programming language. Python recipes allow you to use custom code and leverage the vast ecosystem of Python libraries (like Pandas) to perform complex transformations within your Dataiku Flow, while still maintaining full lineage.

### 2. Prerequisites
- **Basic Python and Pandas knowledge:** You should be comfortable with Python syntax and basic Pandas DataFrame operations.
- **An input dataset:** A dataset in your Flow that you want to process.
- **A Code Environment:** Ensure a Python environment is configured for your project.

### 3. Step-by-Step Instructions
1.  **Create a Python Recipe:**
    *   In your Flow, select your input dataset.
    *   From the right-hand panel, click **+ RECIPE** and choose **Python**.
    *   Dataiku will automatically create an output dataset. Confirm the details and click **CREATE RECIPE**.
2.  **Understand the Boilerplate Code:** The recipe editor will open with some pre-filled code. The key parts are:
    *   \`import dataiku\`: Imports the Dataiku API library.
    *   \`input_dataset = dataiku.Dataset("your_input_name")\`: Gets a handle on your input.
    *   \`df = input_dataset.get_dataframe()\`: Reads the input data into a Pandas DataFrame.
    *   \`output_dataset = dataiku.Dataset("your_output_name")\`: Gets a handle on your output.
    *   \`output_dataset.write_with_schema(df)\`: Writes the transformed DataFrame to the output.
3.  **Add Your Transformation Logic:** Between reading the input DataFrame and writing the output, insert your custom Pandas code. For example:
    > \`\`\`python
    > # Example: Create a new column
    > df['new_column'] = df['existing_column'] * 2
    > \`\`\`
4.  **Validate and Run:**
    *   Click the **Validate** button to check for syntax errors.
    *   Click the **Run** button to execute the recipe and generate the output dataset.

### 4. Resources and Tools
- **Dataiku Python API:** The core library for interacting with Flow items in code. The documentation is essential.
- **Code Environments:** In **Administration > Code Envs**, you can manage the Python packages available to your recipes.
- **Jupyter Notebooks:** For exploratory coding, you can experiment in a notebook first, then copy the finalized code into a recipe.

### 5. Next Steps and Progression
- **Multiple Inputs/Outputs:** A single Python recipe can have multiple input and output datasets.
- **Using Project Variables:** Access project variables in your code with \`dataiku.get_custom_variables()\`.
- **Custom Libraries:** Write reusable functions in your project's **Libraries** folder and import them into your recipes.

### 6. Common Challenges and Solutions
- **Challenge:** "\`ModuleNotFoundError\`: No module named 'some_library'"
- **Solution:** The required library is not in your code environment. Go to your project's settings, find the code environment, and add the missing package.
- **Challenge:** "Out of Memory Error."
- **Solution:** Your dataset is too large to fit in memory as a single Pandas DataFrame. You can either process the data in chunks (using an iterator) or, for very large data, switch to a **PySpark** recipe to use distributed computing.
`},{id:13,slug:"writing-sql-recipes-inside-dataiku",question:"How to get started with + writing SQL recipes inside Dataiku?",answer:`
### 1. Introduction/Overview
When your data resides in a SQL database, the most efficient way to transform it is often with a SQL recipe. This pushes the computation directly to the database, leveraging its native power and avoiding unnecessary data movement. Dataiku provides a clean interface for writing and executing SQL against your connected data sources.

### 2. Prerequisites
- **SQL Datasets:** Your input datasets must be stored in a connected SQL database (e.g., Snowflake, Redshift, PostgreSQL).
- **Basic SQL Knowledge:** You should be comfortable writing \`SELECT\` statements, joins, and aggregations.
- **A configured SQL connection:** The connection to your database must be set up in Dataiku.

### 3. Step-by-Step Instructions
1.  **Create a SQL Recipe:**
    *   In your Flow, select a SQL-based input dataset.
    *   From the right-hand panel, click **+ RECIPE** and choose **SQL**.
    *   An output dataset will be proposed. Click **CREATE RECIPE**.
2.  **Write Your SQL Query:** The recipe editor provides a standard SQL editor. Write your query.
    *   **Crucially**, you don't need to use the full table names. You can refer to your input datasets directly by their names (e.g., \`customers_prepared\`). Dataiku handles the translation.
    > \`\`\`sql
    > SELECT
    >   c.customer_id,
    >   c.name,
    >   COUNT(o.order_id) AS number_of_orders
    > FROM customers_prepared c
    > LEFT JOIN orders_prepared o ON c.customer_id = o.customer_id
    > GROUP BY 1, 2
    > \`\`\`
3.  **Validate and Run:**
    *   Click **Validate** to check your SQL syntax against the database.
    *   The **Preview** pane will show you the results on a sample of the data.
    *   Click **Run** to execute the full query on the database and create the output dataset.

### 4. Resources and Tools
- **Database Explorer:** From a dataset's view, you can explore the underlying database schema.
- **Execution Engine:** In the recipe settings, ensure the execution is set to "Run on database".

### 5. Next Steps and Progression
- **Parameterization:** Use project variables directly in your SQL code (e.g., \`WHERE order_date > '\${start_date}'\`).
- **SQL Notebooks:** For exploratory SQL queries, use a SQL notebook before finalizing your logic in a recipe.
- **Complex Queries:** Write queries with Common Table Expressions (CTEs), window functions, and other advanced SQL features.

### 6. Common Challenges and Solutions
- **Challenge:** "Query failed: Table or view not found."
- **Solution:** Make sure you are using the Dataiku dataset name, not the raw database table name, in your \`FROM\` and \`JOIN\` clauses. Also check for typos.
- **Challenge:** "My query is very slow."
- **Solution:** The performance depends on your database. Use your database's query execution plan tools (like \`EXPLAIN\`) to analyze and optimize your SQL. Ensure the join columns are indexed in the database.
`},{id:14,slug:"adding-custom-python-code-into-workflows",question:"How to get started with + adding custom Python code into workflows?",answer:`
### 1. Introduction/Overview
There are several ways to incorporate custom Python code into Dataiku, each suited for different purposes. This guide provides a framework for deciding which method to use, from one-off transformations to building reusable, governed components.

### 2. Prerequisites
- **A clear goal:** Understand what you want to achieve with your code (e.g., perform a complex calculation, call an external service, integrate a specific library).
- **Python programming skills.**

### 3. Step-by-Step Instructions: Choosing the Right Tool
1.  **For Exploration and Prototyping: Use a Jupyter Notebook.**
    *   **When:** When you are exploring data or developing a new algorithm and don't know the final logic yet.
    *   **How:** In your project, go to **Notebooks > + New Notebook > Python**. You can read datasets, write code in cells, and see immediate results.
    *   **Outcome:** Once your logic is finalized, you can copy the code from the notebook and "productionize" it in a recipe.

2.  **For In-Flow Data Transformation: Use a Python Recipe.**
    *   **When:** When your code is a standard part of your data pipeline, taking one or more datasets as input and producing a dataset as output.
    *   **How:** Select your input dataset(s) and choose the **Python** recipe. Use the Dataiku API to read and write data.
    *   **Outcome:** A fully versioned, traceable, and schedulable step in your Flow.

3.  **For Reusable Functions: Use the Project Library.**
    *   **When:** When you have a helper function (e.g., for data cleaning or a specific calculation) that you want to use in multiple different Python recipes or notebooks.
    *   **How:** Go to the **Libraries** section of your project. Create a \`.py\` file and define your functions. You can then \`import\` this library in any recipe or notebook in the project.
    *   **Outcome:** Modular, maintainable, and reusable code.

4.  **For Operational Logic in Scenarios: Use a Python Scenario Step.**
    *   **When:** When you need to automate operational tasks, like conditional execution, looping, or calling external APIs as part of your orchestration.
    *   **How:** In a Scenario, add a new step of type "Execute Python code". Use the Dataiku API to interact with projects, datasets, and jobs.
    *   **Outcome:** Powerful, automated control over your project's execution.

### 4. Resources and Tools
- **Dataiku API Documentation:** Your guide to interacting with Dataiku objects programmatically.
- **Code Environments:** The mechanism for managing Python package dependencies for all your custom code.

### 5. Next Steps and Progression
- **Custom Plugins:** For highly reusable, user-friendly components, consider developing a custom plugin that provides a visual interface for your Python code.
- **API Node:** Deploy your Python code as a real-time REST API endpoint using the API Deployer.

### 6. Common Challenges and Solutions
- **Challenge:** "My code works in a notebook but fails in a recipe."
- **Solution:** The execution context can be different. Ensure the code environment is the same for both. Also, make sure you are using the Dataiku API correctly to read and write datasets in the recipe, which is different from how you might load data in a notebook.
`},{id:15,slug:"combining-r-code-and-dataiku-recipes",question:"How to get started with + combining R code and Dataiku recipes?",answer:`
### 1. Introduction/Overview
For teams that have existing skills or libraries in R, Dataiku provides first-class support for integrating R code directly into your data pipelines. An R recipe functions similarly to a Python recipe, allowing you to read data, apply custom R logic, and write the results back to the Flow.

### 2. Prerequisites
- **R Programming Skills:** Familiarity with R syntax and data frames.
- **An R Code Environment:** Your Dataiku administrator must have configured an R engine and you should have a code environment with the R packages you need (e.g., \`dplyr\`, \`ggplot2\`).
- **An Input Dataset:** A dataset in your Flow to process with R.

### 3. Step-by-Step Instructions
1.  **Create an R Recipe:**
    *   In your Flow, select your input dataset.
    *   From the right-hand panel, click **+ RECIPE** and choose **R**.
    *   Confirm the output dataset and click **CREATE RECIPE**.
2.  **Understand the Boilerplate Code:** The R recipe editor will provide starter code that shows you how to use the \`dataiku\` R library.
    *   \`library(dataiku)\`: Loads the necessary library.
    *   \`input_dataset <- dkuReadDataset("your_input_name")\`: Reads the input dataset into an R data frame.
    *   \`output_dataset <- ... # Your R code here\`: This is where you insert your transformation logic.
    *   \`dkuWriteDataset(output_dataset, "your_output_name")\`: Writes the transformed data frame to the output dataset.
3.  **Add Your R Logic:** Use your preferred R packages and functions to manipulate the \`input_dataset\` data frame. For example, using \`dplyr\`:
    > \`\`\`R
    > library(dplyr)
    > output_dataset <- input_dataset %>%
    >   mutate(new_column = existing_column * 2) %>%
    >   filter(some_category == 'A')
    > \`\`\`
4.  **Install Packages and Run:**
    *   In the recipe settings, ensure you have selected the correct R code environment. You can install required packages in this environment.
    *   Click **Run** to execute the R script.

### 4. Resources and Tools
- **R Code Environments:** Manage the R packages available to your recipes in **Administration > Code Envs**.
- **Dataiku R API Documentation:** The official reference for the \`dataiku\` R library.
- **R Notebooks:** Use an R notebook for exploratory analysis before finalizing your script in a recipe.

### 5. Next Steps and Progression
- **Visualization:** Use \`ggplot2\` within a notebook to create plots and add them as insights to your project dashboards.
- **Statistical Modeling:** Leverage R's powerful statistical modeling packages within your recipes.
- **R Markdown Reports:** Create dynamic reports using R Markdown in a notebook.

### 6. Common Challenges and Solutions
- **Challenge:** "Error: could not find function 'some_function'"
- **Solution:** You are missing a package. Make sure the package containing the function is listed in your code environment's packages, and that you have loaded it with \`library(package_name)\` at the start of your script.
- **Challenge:** "The recipe is very slow."
- **Solution:** R recipes, like Python recipes, run in-memory on the Dataiku server. For very large datasets, this can be slow. Consider using a SQL recipe to pre-aggregate the data before bringing it into R, or use SparkR in a Spark code recipe for distributed computation.
`},{id:16,slug:"using-macros-and-global-variables-in-recipes",question:"How to get started with + using macros and global variables in recipes?",answer:`
### 1. Introduction/Overview
Hardcoding values like file paths, server names, or thresholds into your recipes is a bad practice. It makes your flows brittle and difficult to move between environments. Dataiku's solution is **Project Variables**, which act as global variables that can be used across all your recipes and scenarios.

### 2. Prerequisites
- **A Dataiku Project.**
- **A need for parameterization:** Identify values in your project that might change (e.g., a date filter, a file name, a model version).

### 3. Step-by-Step Instructions
1.  **Create a Project Variable:**
    *   From your project's top navigation bar, go to **... > Variables**.
    *   Click **Edit Variables**.
    *   Click **+ ADD VARIABLE**.
    *   Give your variable a \`name\` (e.g., \`start_date\`) and a \`value\` (e.g., \`2023-01-01\`).
    *   Click **SAVE**.
2.  **Use the Variable in a Visual Recipe:**
    *   Open a recipe, for example, a **Prepare** recipe with a **Filter** step.
    *   In the filter expression, you can use the variable with the syntax \`\${variable_name}\`.
    *   Example: \`date_column > '\${start_date}'\`
3.  **Use the Variable in a Code Recipe (Python):**
    *   In a Python recipe, you can retrieve all variables as a dictionary.
    > \`\`\`python
    > import dataiku
    > # Get all variables
    > variables = dataiku.get_custom_variables()
    > # Access a specific variable
    > my_variable_value = variables.get('my_variable_name')
    > \`\`\`
4.  **Change the Variable's Value:** To change the behavior of all recipes that use the variable, you only need to update its value in one place: the **Variables** page.

### 4. Resources and Tools
- **Project Variables Page:** The central place to manage all your project's parameters.
- **Scenarios:** Scenarios can override the default values of variables for a specific run, which is extremely powerful for automation.

### 5. Next Steps and Progression
- **Environment Promotion:** This is the primary use case. You can have different variable values for your \`dev\` and \`prod\` Dataiku instances. When you deploy your project, the flow automatically picks up the correct production values.
- **Password Protection:** When creating a variable, you can mark it as a "Password" type. This will hide its value in the UI, making it suitable for storing secrets like API keys.
- **Dataiku Macros:** For more advanced programmatic control, explore Dataiku Macros, which are Python functions that can be triggered from scenarios to perform complex actions.

### 6. Common Challenges and Solutions
- **Challenge:** "My variable is not being replaced in the recipe."
- **Solution:** Check the syntax carefully. It must be exactly \`\${variable_name}\`. Make sure you saved the variable after creating it and that there are no typos in the name.
- **Challenge:** "How do I manage variables for multiple environments?"
- **Solution:** When you deploy a project using a bundle, the import wizard will prompt you to remap the variables for the new environment. You can define the production values at that time.
`},{id:17,slug:"parsing-json-and-nested-data-in-dataiku",question:"How to get started with + parsing JSON and nested data in Dataiku?",answer:`
### 1. Introduction/Overview
Data from APIs or NoSQL databases often comes in a nested JSON format. While this is efficient for applications, it's not ideal for analytics. Dataiku provides powerful visual tools in the Prepare recipe to "flatten" or "unnest" this JSON into a standard tabular format (rows and columns).

### 2. Prerequisites
- **A dataset with a JSON column:** You need a dataset where one of the columns contains data as a JSON string or object.
- **Understanding of your JSON structure:** You should know whether your data is a single JSON object or an array of objects.

### 3. Step-by-Step Instructions
1.  **Create a Prepare Recipe:** Start by creating a **Prepare** recipe on your dataset that contains the JSON column.
2.  **Ensure the Column is Parsed as JSON:**
    *   Dataiku will often detect that a column contains JSON and show a special icon.
    *   If it's just a string, click the column header and use the **Parse to JSON** processor first.
3.  **Unnest the JSON:**
    *   Click on the header of the now-parsed JSON column.
    *   From the processor library, choose **Unnest object**.
    *   Dataiku will show a dialog where you can select which nested keys you want to extract into new columns.
    *   You can also specify a prefix for the new column names to avoid naming conflicts.
4.  **Handle Nested Arrays:**
    *   If your JSON contains an array of objects that you want to turn into new rows, use the **Unfold array** processor. This will create a new row for each element in the array.
5.  **Run the Recipe:** After configuring the unnesting, run the recipe to generate the new, flattened dataset.

### 4. Resources and Tools
- **Prepare Recipe Processors:**
    - \`Parse to JSON\`: Converts a JSON string into a native JSON object.
    - \`Unnest object\`: Extracts keys from a JSON object into new columns.
    - \`Flatten object\`: Recursively unnests a deeply nested object.
    - \`Unfold array\`: Creates new rows from a JSON array.
- **Python Recipe:** For extremely complex or malformed JSON, you can always fall back to a Python recipe and use libraries like \`json\` to parse it manually.

### 5. Next Steps and Progression
- **Chained Unnesting:** If you have nested JSON within nested JSON, you can apply the unnesting processors multiple times in sequence.
- **Extracting Single Keys:** If you only need one value from the JSON, you can use the **Formula** processor with the \`get\` function (e.g., \`get(json_column, "key_name")\`).

### 6. Common Challenges and Solutions
- **Challenge:** "The 'Unnest object' processor is greyed out."
- **Solution:** This means Dataiku does not recognize the column as a valid JSON object. You must first apply the **Parse to JSON** processor to the column.
- **Challenge:** "My JSON has an inconsistent schema (some keys are missing in some rows)."
- **Solution:** The \`Unnest object\` processor handles this gracefully. If a key is missing in a particular row, the corresponding new column will simply have a null value for that row.
`},{id:18,slug:"feature-engineering-using-formula-steps",question:"How to get started with + feature engineering using formula steps?",answer:`
### 1. Introduction/Overview
Feature engineering is the art of creating new, informative features from your existing data to improve model performance. The **Formula** processor in Dataiku's Prepare recipe is a versatile and powerful tool for this task, allowing you to use an Excel-like expression language to create new columns.

### 2. Prerequisites
- **A dataset in a Prepare recipe.**
- **An idea for a new feature:** You should have a hypothesis about what new information could be useful (e.g., "the ratio of two columns," "the length of a text field," "a binary flag for a specific condition").

### 3. Step-by-Step Instructions
1.  **Open a Prepare Recipe:** Select your dataset and create a new **Prepare** recipe.
2.  **Add a Formula Step:**
    *   Click the **+ ADD A NEW STEP** button.
    *   Search for and select the **Formula** processor.
3.  **Write Your Expression:**
    *   The Formula editor opens. In the "Output column" field, give your new feature a name.
    *   In the "Expression" box, write your formula. You can refer to other columns directly by their names.
    *   The syntax is similar to spreadsheet functions. The editor has autocomplete for functions and column names.
4.  **Explore Different Formula Types:**
    *   **Mathematical:** \`(col_A + col_B) / 2\`
    *   **String Manipulation:** \`substring(name_column, 0, 5)\` or \`length(text_column)\`
    *   **Conditional (If/Then):** \`if(category_column == 'A', 'Group1', 'Group2')\`
    *   **Date Functions:** \`diff(date1, date2, "days")\`
5.  **Preview and Run:** The preview pane will instantly show the result of your new feature column. Once satisfied, **Run** the recipe.

### 4. Resources and Tools
- **Formula Processor:** The main tool for this task.
- **Formula Language Documentation:** Click the "?" icon in the Formula editor to see a full list of all available functions. This is an essential reference.
- **Visual Analysis Lab:** Use the "Correlation Matrix" or "Feature Importance" in the modeling lab to see if your newly engineered feature is actually predictive.

### 5. Next Steps and Progression
- **Chained Formulas:** You can create complex features by using the output of one Formula step as an input to another.
- **Advanced Functions:** Explore powerful functions like \`dummify\` (for one-hot encoding), \`greatest\` (to find the max of several columns), or \`floor\` and \`ceil\` for rounding.
- **Regular Expressions:** Combine formulas with conditional functions that use \`match\` to create features based on complex text patterns.

### 6. Common Challenges and Solutions
- **Challenge:** "My formula has an error."
- **Solution:** The editor will highlight syntax errors. The most common issues are mismatched parentheses or incorrect function names. Use the function reference to double-check the syntax.
- **Challenge:** "The formula is giving a wrong data type."
- **Solution:** You may need to explicitly convert types within the formula using functions like \`toNumber()\` or \`toString()\`. For example, \`toNumber(string_column) * 2\`.
`},{id:19,slug:"implementing-fuzzy-joins-in-dataiku",question:"How to get started with + implementing fuzzy joins in Dataiku?",answer:`
### 1. Introduction/Overview
A standard join requires an exact match on the join key. But what if your keys are messy due to typos or different naming conventions (e.g., "Dataiku" vs. "Dataiku, Inc.")? A **Fuzzy Join** solves this problem by joining records that are "similar" but not identical. Dataiku provides a dedicated visual recipe for this powerful technique.

### 2. Prerequisites
- **Two datasets to join:** One "left" and one "right" dataset.
- **A messy join key:** The column you want to join on should have similar but inconsistent values in the two datasets.

### 3. Step-by-Step Instructions
1.  **Select the Fuzzy Join Recipe:**
    *   In your Flow, select your "left" dataset.
    *   From the right-hand panel, click **+ RECIPE** and search for **Fuzzy Join**.
    *   Select the "right" dataset you want to join with.
2.  **Configure the Join:**
    *   In the Fuzzy Join recipe screen, select the key columns from both the left and right datasets.
    *   **Choose a Similarity Metric:** This is the core of the fuzzy join. Common choices include:
        *   \`Levenshtein\`: Measures the number of edits (inserts, deletes, substitutions) needed to change one string to the other. Good for typos.
        *   \`Jaccard\`: Measures the similarity of word sets. Good for different word orders.
    *   **Set a Threshold:** Define a similarity score (e.g., 0.8) above which two values will be considered a match.
3.  **Preview the Matches:** The recipe will show a preview of the matches it found based on your settings. You can adjust the metric and threshold to fine-tune the results.
4.  **Select Output Columns:** As in a normal join, choose which columns from both datasets to keep.
5.  **Run the Recipe:** Click **Run** to perform the fuzzy join and create the output dataset.

### 4. Resources and Tools
- **Fuzzy Join Recipe:** The dedicated visual recipe for this task.
- **Prepare Recipe:** It's often a good practice to use a Prepare recipe *before* the fuzzy join to standardize the key columns as much as possible (e.g., convert to lowercase, remove punctuation). This improves the quality of the fuzzy match.

### 5. Next Steps and Progression
- **Experiment with Metrics:** Try different similarity metrics to see which one works best for your specific data.
- **Stemming/Normalization:** In the recipe settings, you can enable text normalization options like stemming (reducing words to their root form), which can further improve match quality.
- **Manual Review:** For critical applications, you may want to create a process to manually review the matches found by the fuzzy join.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm getting too many incorrect matches."
- **Solution:** Your threshold is too low. Increase the similarity threshold (e.g., from 0.7 to 0.9) to require a closer match.
- **Challenge:** "I'm missing some obvious matches."
- **Solution:** Your threshold might be too high, or you're using the wrong metric. Try lowering the threshold or switching to a different similarity algorithm. Also, make sure you've pre-processed the keys to remove noise.
`},{id:20,slug:"handling-missing-values-via-recipes",question:"How to get started with + handling missing values via recipes?",answer:`
### 1. Introduction/Overview
Missing values (or nulls) are a common problem in real-world data and can cause issues for analysis and modeling. Dataiku's **Prepare recipe** provides a comprehensive set of tools to identify, remove, or intelligently fill in (impute) these missing values.

### 2. Prerequisites
- **A dataset with missing values:** You need a dataset where some cells are empty.
- **A strategy:** Decide how you want to handle the nulls. Your strategy might be different for different columns.

### 3. Step-by-Step Instructions
1.  **Open a Prepare Recipe:** Create a **Prepare** recipe on your dataset.
2.  **Identify Missing Values:** Dataiku's data quality bars at the top of each column immediately show you the percentage of empty values.
3.  **Choose a Handling Strategy and Processor:**
    *   **Strategy 1: Remove Rows with Missing Values**
        *   Click the header of the column with nulls.
        *   Choose the **Remove rows where value is empty** processor. Use this carefully, as it can lead to significant data loss if many rows have missing values.
    *   **Strategy 2: Fill (Impute) Missing Values**
        *   Click the column header.
        *   Choose the **Impute missing values** processor.
        *   You can then choose how to fill the nulls:
            *   **For Numerical Columns:** With the column's \`Mean\`, \`Median\`, \`Mode\`, or a \`Constant\` value (like 0).
            *   **For Categorical Columns:** With the column's \`Mode\` or a \`Constant\` value (like "Missing").
    *   **Strategy 3: Create an Indicator Column**
        *   Sometimes, the fact that a value is missing is itself useful information.
        *   Use the **Create indicator for missing values** processor. This will create a new binary (0/1) column that flags which rows had a null in the original column.

4.  **Apply and Run:** Add the desired processor steps and click **Run** to create the cleaned dataset.

### 4. Resources and Tools
- **Prepare Recipe:** The central location for all missing value handling tools.
- **Data Quality Bars:** Provide a quick visual guide to which columns need attention.
- **Analyze Tool:** In a dataset, the "Analyze" feature can give you more detailed statistics on missing values.

### 5. Next Steps and Progression
- **Advanced Imputation:** For more sophisticated imputation, you can use a Python recipe to implement algorithms like k-Nearest Neighbors (k-NN) imputation or model-based imputation.
- **Column-Specific Strategies:** Apply different imputation strategies to different columns based on their meaning and distribution.
- **Combining Strategies:** You can first create an indicator column and then impute the missing values, preserving the information about which values were originally missing.

### 6. Common Challenges and Solutions
- **Challenge:** "Which imputation method should I choose for a numerical column?"
- **Solution:** If the data has a normal distribution, \`Mean\` is often fine. If the data is skewed or has outliers, \`Median\` is more robust and generally a safer choice.
- **Challenge:** "After imputation, my model performance got worse."
- **Solution:** Simple imputation can sometimes distort the original data distribution. Try a different method or consider using a model that can handle missing values natively (like LightGBM). Creating an indicator column is often a good strategy.
`},{id:21,slug:"using-automl-in-dataiku-dss",question:"How to get started with + using AutoML in Dataiku DSS?",answer:`
### 1. Introduction/Overview
AutoML (Automated Machine Learning) in Dataiku empowers you to build powerful predictive models without writing complex code. It automates the tedious parts of modeling, such as feature handling, algorithm selection, and hyperparameter tuning, allowing you to focus on interpreting the results.

### 2. Prerequisites
- **A clean, prepared dataset:** Your dataset should be in a "tidy" format, with rows representing observations and columns representing features. The target variable (what you want to predict) must be one of the columns.
- **A clear prediction goal:** Know whether you are solving a **Classification** problem (predicting a category, e.g., "churn" or "no churn") or a **Regression** problem (predicting a number, e.g., "price").

### 3. Step-by-Step Instructions
1.  **Launch the Visual Analysis Lab:**
    *   In your Flow, select your prepared dataset.
    *   From the right-hand panel, click **Lab**.
    *   Click **+ NEW ANALYSIS**.
2.  **Select the Task and Target:**
    *   Choose **Prediction**.
    *   Select the column you want to predict as your **Target variable**.
    *   Dataiku will automatically detect if it's a classification or regression task based on the target's type.
3.  **Design the Model:**
    *   Navigate to the **Design** tab. Here you can configure feature handling, select which algorithms to train, and set the evaluation metric.
    *   For your first time, the default settings are excellent.
4.  **Train the Models:** Click the **Train** button. Dataiku's AutoML engine will now:
    *   Preprocess the features (e.g., one-hot encode categoricals, scale numerics).
    *   Train multiple different algorithms (like Logistic Regression, Random Forest, Gradient Boosted Trees).
    *   Tune their hyperparameters.
5.  **Analyze the Results:**
    *   Once training is complete, the **Results** tab will show a leaderboard of all trained models, ranked by performance.
    *   Click on a model to explore its details, including feature importance, confusion matrices, and performance charts.

### 4. Resources and Tools
- **Visual Analysis Lab:** The dedicated interface for AutoML in Dataiku.
- **Model Leaderboard:** The main results view for comparing different models.
- **Dataiku Academy:** The "Machine Learning Basics" learning path is a perfect introduction.

### 5. Next Steps and Progression
- **Deploy a Model:** Select the best-performing model from the leaderboard and click **Deploy**. This creates a "Saved Model" object in your Flow, ready for scoring new data.
- **Tune Hyperparameters:** For any model, you can go back to the Design tab and manually adjust its hyperparameters to try and improve performance.
- **Interactive Scoring:** Use the "What-If" analysis on the model results page to see how changing input features affects the prediction for a single record.

### 6. Common Challenges and Solutions
- **Challenge:** "My model performance is poor."
- **Solution:** The most common reason is not the model, but the features. Go back to your data preparation and try to engineer more informative features.
- **Challenge:** "Training is taking too long."
- **Solution:** In the Design tab, you can reduce the number of algorithms to train or use simpler, faster models like Logistic Regression. You can also work with a smaller sample of your data for faster iteration.
`},{id:22,slug:"building-a-random-forest-classifier-visually",question:"How to get started with + building a random forest classifier visually?",answer:`
### 1. Introduction/Overview
A Random Forest is a powerful and popular machine learning algorithm for classification. In Dataiku, you can train, evaluate, and tune a Random Forest model using a completely visual interface, making it accessible even without deep coding knowledge.

### 2. Prerequisites
- **A prepared dataset for classification:** This means a clean dataset with a categorical target variable (the column you want to predict).
- **A Visual Analysis created:** You should have already started a new prediction analysis in the Lab, with your target variable selected.

### 3. Step-by-Step Instructions
1.  **Navigate to the Model Design:** In the Visual Analysis Lab, click on the **Design** tab.
2.  **Select the Algorithm:**
    *   Go to the **Algorithms** section.
    *   By default, Dataiku selects a few common algorithms. You can uncheck the others and ensure that **Random Forest** is checked.
3.  **(Optional) Tune Hyperparameters:**
    *   Click on the "Random Forest" algorithm to open its settings.
    *   Here you can adjust key hyperparameters:
        *   \`Number of trees\`: The number of decision trees to build. More trees are generally better but take longer to train.
        *   \`Max depth\`: The maximum depth of each tree. Controls model complexity.
        *   \`Min samples per leaf\`: The minimum number of samples required to be at a leaf node.
    *   For a first attempt, the default values are usually a good starting point.
4.  **Train the Model:** Click the **Train** button at the top right. Dataiku will now train the Random Forest model on your data.
5.  **Review the Results:**
    *   Once training is complete, click on the "Random Forest" model in the results list.
    *   Explore its performance: check the **ROC Curve**, **Confusion Matrix**, and **Feature Importance** to understand how it works and how well it performs.

### 4. Resources and Tools
- **Algorithms Panel:** In the Design tab, this is where you select and configure the models to be trained.
- **Model Results Page:** The dashboard for deep-diving into the performance and explainability of your trained model.

### 5. Next Steps and Progression
- **Compare to Other Models:** Train other algorithms (like Logistic Regression or Gradient Boosted Trees) at the same time to see if Random Forest is truly the best choice for your problem.
- **Deploy the Model:** If you are happy with its performance, click the **Deploy** button to create a usable model object in your Flow.
- **Grid Search:** For more advanced tuning, you can enable "Grid Search" on the algorithm to have Dataiku systematically test multiple combinations of hyperparameters to find the best ones.

### 6. Common Challenges and Solutions
- **Challenge:** "My model is overfitting (performs well on training data but poorly on test data)."
- **Solution:** Your model is too complex. Try reducing the \`Max depth\` of the trees or increasing the \`Min samples per leaf\`. This will make the model simpler and more generalizable.
- **Challenge:** "I don't understand what the feature importances mean."
- **Solution:** Feature importance shows which variables the model relied on most to make its predictions. High-importance features are the most influential drivers of the outcome.
`},{id:23,slug:"building-regression-models-in-dataiku-dss",question:"How to get started with + building regression models in Dataiku DSS?",answer:`
### 1. Introduction/Overview
Regression is a type of machine learning used to predict a continuous numerical value, such as a price, a sales forecast, or a temperature. Dataiku's AutoML capabilities make it straightforward to build, compare, and deploy various regression models visually.

### 2. Prerequisites
- **A prepared dataset for regression:** You need a dataset where your target variable (the column you want to predict) is numerical.
- **A clear business question:** Understand what numerical value you are trying to predict and why.

### 3. Step-by-Step Instructions
1.  **Launch a New Analysis:**
    *   In your Flow, select your prepared dataset.
    *   Click **Lab > + NEW ANALYSIS**.
2.  **Select the Regression Task:**
    *   Choose **Prediction**.
    *   Select your numerical target variable. Dataiku will automatically recognize this as a **Regression** task.
3.  **Design and Train:**
    *   Go to the **Design** tab. In the **Algorithms** section, you will see that Dataiku has pre-selected common regression models like \`Ridge Regression\`, \`Random Forest Regression\`, and \`Gradient Boosted Regression\`.
    *   For your first time, the defaults are fine. Click **Train**.
4.  **Evaluate the Results:**
    *   After training, the Results tab will show a leaderboard of your models.
    *   For regression, the key metrics are different from classification. Look for:
        *   **R\xb2 (R-squared):** Represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Higher is better.
        *   **RMSE (Root Mean Squared Error):** Measures the average magnitude of the errors. Lower is better.
5.  **Analyze a Specific Model:** Click on a model (e.g., Random Forest Regression) to see more details, including feature importance and error distribution plots.

### 4. Resources and Tools
- **Visual Analysis Lab:** The same tool used for classification, which automatically adapts to regression tasks.
- **Regression Metrics:** Understand the meaning of R\xb2, RMSE, and MAE (Mean Absolute Error) to correctly evaluate your models.
- **Dataiku Academy:** The "Machine Learning Basics" courses cover regression concepts.

### 5. Next Steps and Progression
- **Feature Engineering:** If your models have low R\xb2 values, the most effective way to improve them is often by creating better features in a Prepare recipe.
- **Model Tuning:** Adjust the hyperparameters of the algorithms in the Design tab to see if you can improve the RMSE.
- **Deploy and Score:** Deploy your best model to the Flow and use a **Score** recipe to predict values for a new dataset.

### 6. Common Challenges and Solutions
- **Challenge:** "My model's predictions are not very accurate (high RMSE)."
- **Solution:** This is the central challenge of modeling. First, ensure your features are relevant. Try engineering new features. Also, make sure you have enough data. Finally, experiment with more complex models like Gradient Boosted Trees.
- **Challenge:** "The feature importance list doesn't make business sense."
- **Solution:** This could indicate a data leakage problem, where a feature that is a proxy for the answer has snuck into your training data. Carefully review your features to ensure they are all things you would know *before* the event you are trying to predict.
`},{id:24,slug:"evaluating-models-roc-f1-precision-recall",question:"How to get started with + evaluating models (ROC, F1, precision/recall)?",answer:`
### 1. Introduction/Overview
Building a model is easy; knowing if it's a *good* model is the hard part. Model evaluation involves using specific metrics to measure a model's performance. For classification models, the most important metrics are often Precision, Recall, F1-Score, and the ROC AUC. Dataiku calculates and visualizes these automatically.

### 2. Prerequisites
- **A trained classification model:** You need to have trained at least one classifier in the Visual Analysis Lab.
- **Basic understanding of classification concepts:** Know the difference between True Positives, False Positives, True Negatives, and False Negatives.

### 3. Step-by-Step Instructions
1.  **Navigate to Model Results:** After training a model, go to the **Results** tab in the Visual Analysis Lab and click on your model.
2.  **Analyze the Confusion Matrix:**
    *   Go to the **Performance** section. The **Confusion Matrix** is a key visualization. It shows you:
        *   How many positive cases were correctly predicted (True Positives).
        *   How many negative cases were incorrectly predicted as positive (False Positives).
3.  **Understand Precision and Recall:**
    *   **Precision:** Of all the predictions your model made for the positive class, how many were actually correct? (TP / (TP + FP)). High precision means a low false positive rate.
    *   **Recall (Sensitivity):** Of all the actual positive cases in your data, how many did your model find? (TP / (TP + FN)). High recall means a low false negative rate.
    *   Dataiku displays these values next to the confusion matrix. You'll notice there is often a trade-off between them.
4.  **Use the F1-Score:**
    *   The **F1-Score** is the harmonic mean of Precision and Recall. It provides a single score that balances both metrics. It's a very common metric for comparing classifiers.
5.  **Examine the ROC Curve:**
    *   Go to the **ROC Curve** tab. This plot shows the model's performance across all possible probability thresholds.
    *   The **AUC (Area Under the Curve)** is a single number that summarizes the curve. An AUC of 1.0 is a perfect model, while an AUC of 0.5 is no better than random guessing. It's one of the most important metrics for overall model quality.

### 4. Resources and Tools
- **Model Performance Tab:** The central place in Dataiku for all evaluation metrics and charts.
- **Interactive Threshold Adjustment:** In the confusion matrix view, you can drag the probability threshold slider to see how it affects precision and recall in real-time.

### 5. Next Steps and Progression
- **Choose the Right Metric:** The best metric depends on your business problem. Is it worse to have a false positive or a false negative? This will determine whether you should optimize for precision or recall.
- **Compare Models:** Use the AUC and F1-Score on the main results leaderboard to compare different algorithms.
- **Profit Curves:** Explore the "Cost Matrix" and "Profit Curve" sections in Dataiku to perform evaluation based on the business cost/benefit of correct and incorrect predictions.

### 6. Common Challenges and Solutions
- **Challenge:** "My model has high precision but low recall."
- **Solution:** This is a common trade-off. Your model is being very conservative. To increase recall, you can try lowering the probability threshold for classifying a positive case, but this will likely decrease precision.
- **Challenge:** "What is a 'good' AUC score?"
- **Solution:** This is highly context-dependent. Generally, > 0.9 is excellent, > 0.8 is great, > 0.7 is good, and < 0.6 is poor. But for a very difficult problem, an AUC of 0.7 could be a major success.
`},{id:25,slug:"extracting-feature-importance-from-model-runs",question:"How to get started with + extracting feature importance from model runs?",answer:`
### 1. Introduction/Overview
After training a model, one of the most important questions is: "What did it learn?" **Feature Importance** helps answer this by showing which input variables had the most influence on the model's predictions. This is crucial for model explainability and for gaining business insights.

### 2. Prerequisites
- **A trained model:** You need a trained model in the Visual Analysis Lab. Most model types (trees, linear models, etc.) support feature importance.

### 3. Step-by-Step Instructions
1.  **Navigate to the Model Results:** In the Visual Analysis Lab, go to the **Results** tab and click on the specific model you want to analyze.
2.  **Open the Feature Importance View:** In the model's detail view, click on the **Feature Importance** tab.
3.  **Interpret the Chart:**
    *   You will see a bar chart listing the features (your input columns) on the y-axis.
    *   The length of the bar on the x-axis represents the importance or weight of that feature.
    *   The features are ranked from most important at the top to least important at the bottom.
4.  **Understand the Method:** The method used to calculate importance depends on the model. For tree-based models like Random Forest, it's often based on "Gini impurity" or "information gain." For linear models, it's based on the coefficient weights.
5.  **Use the Insights:** This chart tells you which factors are the primary drivers of the outcome you are predicting. This can be a valuable insight for business stakeholders, even independent of the model itself.

### 4. Resources and Tools
- **Feature Importance Tab:** The dedicated view for this analysis within the model results page.
- **SHAP Explanations:** For more advanced, instance-level explanations, explore the **Individual Explanations** tab, which uses SHAP values.

### 5. Next Steps and Progression
- **Feature Selection:** If you have a very large number of features, you can use the feature importance results to select only the top N most important features and retrain your model. This can sometimes lead to a simpler, faster, and even more robust model.
- **Compare Across Models:** Look at the feature importance for different algorithms. Do they agree on which features are most important? This can increase your confidence in the results.
- **Present to Stakeholders:** The feature importance chart is an excellent, easy-to-understand visual to share with business users to explain what your model has learned.

### 6. Common Challenges and Solutions
- **Challenge:** "A feature I thought would be important has very low importance."
- **Solution:** This is a common and often insightful result. It may be that the feature is highly correlated with another, more important feature (so its effect is redundant), or it simply may not be as predictive as you thought.
- **Challenge:** "The feature names are hard to read (e.g., after one-hot encoding)."
- **Solution:** Dataiku's chart is interactive. You can hover over the feature names to see the full name. The naming convention \`dummy:column_name:value\` indicates a feature created by one-hot encoding.
`},{id:26,slug:"implementing-cross‑validation-and-a-b-tests",question:"How to get started with + implementing cross‑validation and A/B tests?",answer:`
### 1. Introduction/Overview
This guide covers two distinct but related validation techniques: **Cross-validation**, used during model training to get a robust estimate of performance, and **A/B testing**, used after deployment to compare two model versions in a live environment.

### 2. Prerequisites
- **For Cross-Validation:** A dataset ready for modeling in Dataiku's Visual Analysis Lab.
- **For A/B Testing:** Two different trained and deployed models in your Flow that you want to compare.

### 3. Step-by-Step Instructions
#### Part A: Cross-Validation (During Training)
1.  **Open the Model Design:** In the Visual Analysis Lab, go to the **Design** tab.
2.  **Configure the Validation Strategy:**
    *   Find the **Train / Test Set** section.
    *   The default is a simple train/test split. To use cross-validation, change the **Splitting strategy** to **K-fold cross-validation**.
3.  **Set the Number of Folds (K):**
    *   Choose the number of folds (e.g., 5 or 10). A higher number is more robust but takes longer.
4.  **Train the Model:** When you click **Train**, Dataiku will now perform k-fold cross-validation. It will split your data into k-folds, then train and evaluate the model k times, each time holding out a different fold for testing.
5.  **Review the Results:** The performance metrics you see on the results page will be the *average* performance across all k-folds, giving you a more reliable estimate of how the model will perform on unseen data.

#### Part B: A/B Testing (Post-Deployment)
1.  **Deploy Two Models:** You need two "Saved Model" objects in your Flow. These can be two different algorithms or two different versions of the same algorithm. Let's call them \`model_A\` and \`model_B\`.
2.  **Deploy as API Endpoints:** Deploy both models to the **API Deployer** as two separate endpoints within the same API service.
3.  **Set Up Traffic Splitting:** In the API service settings, you can configure the endpoint to function as a "Champion/Challenger".
    *   Set \`model_A\` as the champion and \`model_B\` as the challenger.
    *   Route a percentage of traffic to each (e.g., 90% to A, 10% to B).
4.  **Monitor Performance:** The API Deployer will log the requests and performance for each model version separately. By analyzing these logs, you can compare their live performance and decide which one is superior.

### 4. Resources and Tools
- **Model Design Tab:** Where you configure the cross-validation strategy.
- **API Deployer:** The service used for deploying models and setting up A/B tests (Champion/Challenger).

### 5. Next Steps and Progression
- **Stratified K-Fold:** For classification with imbalanced classes, Dataiku automatically uses stratified k-fold to ensure each fold has a similar class distribution.
- **Gradual Rollout:** For A/B testing, you can start with a 99%/1% split and gradually increase the traffic to the challenger model as you gain confidence in its performance.

### 6. Common Challenges and Solutions
- **Challenge:** "Cross-validation is taking a very long time."
- **Solution:** Reduce the number of folds (K) or the number of algorithms you are training. CV is computationally intensive by design.
- **Challenge:** "How do I analyze the A/B test results?"
- **Solution:** You will need to capture the prediction logs from the API node and join them with the actual outcomes (ground truth). This allows you to calculate accuracy, business KPIs, and other metrics for both model A and model B to determine the winner.
`},{id:27,slug:"training-deep-learning-models-using-notebooks",question:"How to get started with + training deep learning models using notebooks?",answer:`
### 1. Introduction/Overview
For cutting-edge tasks in computer vision or NLP, you'll often need Deep Learning. While Dataiku's visual tools focus on tabular data, its code-based tools like **Jupyter Notebooks** and **Code Recipes** provide a perfect environment for training deep learning models with frameworks like TensorFlow or PyTorch.

### 2. Prerequisites
- **A Deep Learning Code Environment:** You need a code environment configured with the necessary libraries (\`tensorflow\`, \`pytorch\`, \`transformers\`, etc.) and a Python version they support. This may require a GPU-enabled environment for acceptable performance.
- **Your Data:** Your training data (e.g., images, text files) should be accessible to Dataiku, for example, in a managed folder.
- **Python and Deep Learning Knowledge:** You should be familiar with the basics of the chosen framework (e.g., Keras/TensorFlow).

### 3. Step-by-Step Instructions
1.  **Create a Code Environment:**
    *   Go to **Administration > Code Envs** and create a new environment.
    *   Add \`tensorflow\` or \`pytorch\` and other required packages to the list of packages to install.
2.  **Create a Jupyter Notebook:** In your project, go to **Notebooks > + New Notebook > Python**. Make sure to select your new deep learning code environment for this notebook.
3.  **Load Your Data:** Use the Dataiku API to get the paths to your data. For image data in a managed folder, you can get a list of file paths.
    > \`\`\`python
    > import dataiku
    > # Get handle on folder and list paths
    > folder = dataiku.Folder("my_images_folder")
    > image_paths = folder.list_paths_in_partition()
    > \`\`\`
4.  **Write Your Training Code:** Write your standard TensorFlow/Keras or PyTorch code in the notebook cells. This includes:
    *   Creating data generators to load and preprocess the data.
    *   Defining your neural network architecture.
    *   Compiling the model and running the training loop (\`model.fit()\`).
5.  **Save the Trained Model:** After training, save the model's weights and architecture to a new managed folder so it can be reused later.
    > \`\`\`python
    > # Save the model
    > model.save("model_architecture.h5")
    > # Write to a Dataiku managed folder
    > output_folder = dataiku.Folder("trained_model_folder")
    > with open("model_architecture.h5", "rb") as f:
    >     output_folder.upload_stream("model.h5", f)
    > \`\`\`
### 4. Resources and Tools
- **Code Environments:** The key to managing your complex deep learning dependencies.
- **Managed Folders:** The standard way to store unstructured data like images and model files in Dataiku.
- **Python Notebooks:** The interactive environment for developing and running your training code.

### 5. Next Steps and Progression
- **GPU Acceleration:** For serious deep learning, work with your administrator to set up a code environment that can run on a machine with GPUs.
- **Productionize in a Recipe:** Once your notebook code is stable, move it into a **Python recipe** to make it a repeatable, schedulable part of your Flow.
- **Custom Model Plugin:** For advanced integration, you can wrap your deep learning model in a custom model plugin, allowing it to be evaluated and deployed like a standard Dataiku visual model.

### 6. Common Challenges and Solutions
- **Challenge:** "Training is extremely slow."
- **Solution:** Deep learning without a GPU is often impractical. You must ensure your code environment is configured to run on a GPU-enabled node.
- **Challenge:** "Dependency conflicts or CUDA errors."
- **Solution:** Deep learning libraries have very specific dependencies (e.g., specific CUDA and cuDNN versions). It is critical to build your code environment carefully, ensuring all package versions are compatible. This can be a complex task that requires collaboration with a Dataiku admin.
`},{id:28,slug:"integrating-scikit‑learn-and-tensorflow-in-dataiku",question:"How to get started with + integrating scikit‑learn and TensorFlow in Dataiku?",answer:`
### 1. Introduction/Overview
Dataiku is not a closed black box; it's an open platform designed to integrate with the most popular data science libraries. Scikit-learn is the engine behind Dataiku's visual machine learning, and both it and TensorFlow can be used directly in code for maximum flexibility and power.

### 2. Prerequisites
- **A Code Environment:** You must have a code environment set up with the desired versions of \`scikit-learn\` and \`tensorflow\`.
- **Familiarity with the Libraries:** You should know how to use these libraries in a standard Python environment.

### 3. Step-by-Step Instructions: Integration Methods

#### Method 1: Using Them in Visual ML (Scikit-learn)
- **What it is:** Dataiku's AutoML features (in the Visual Analysis Lab) are built on top of Scikit-learn.
- **How to use:** When you visually train a Random Forest or Logistic Regression, you are already using Scikit-learn. You can even add a custom Python model to the visual lab that uses Scikit-learn code.
- **Best for:** Standard tabular machine learning, rapid prototyping, and model comparison.

#### Method 2: Using Them in a Python Recipe or Notebook
- **What it is:** This gives you complete control to write your own custom code using these libraries.
- **How to use:**
    1.  Create a new **Python Recipe** or **Jupyter Notebook**.
    2.  Ensure it is configured to use your code environment that contains \`scikit-learn\` or \`tensorflow\`.
    3.  Use the \`dataiku\` Python library to read your dataset into a Pandas DataFrame.
    4.  Write your standard Scikit-learn or TensorFlow code to process the DataFrame, train a model, and generate predictions.
    5.  Use the \`dataiku\` library to write the results (e.g., a DataFrame with predictions, or a saved model file) back to a Dataiku dataset or managed folder.
- **Best for:** Custom algorithms, deep learning, complex pre-processing pipelines, or when you need a specific feature not available in the visual interface.

### 4. Resources and Tools
- **Code Environments:** Essential for managing your dependencies. You can have one environment for general Scikit-learn tasks and another for a specific TensorFlow version.
- **Dataiku Python API:** The bridge that connects your custom code to the Dataiku Flow.
- **Official Library Documentation:** The Scikit-learn and TensorFlow websites are the ultimate source for how to use their functions.

### 5. Next Steps and Progression
- **Custom Model Plugins:** Wrap your Scikit-learn model in a custom plugin to make it fully integrated with Dataiku's evaluation and deployment framework.
- **GPU Environments:** For TensorFlow, work with your admin to set up a GPU-enabled code environment for performant training.
- **API Deployment:** Deploy your custom-coded model as a real-time API using the API Deployer.

### 6. Common Challenges and Solutions
- **Challenge:** "Import Error: My library is not found."
- **Solution:** The most common issue. Double-check that your recipe/notebook is using the correct code environment and that the library (with the correct version) is listed in that environment's installed packages.
- **Challenge:** "How do I save my trained TensorFlow model?"
- **Solution:** Use the standard \`model.save()\` method from Keras. Then, write the resulting model file (e.g., an \`.h5\` file) into a Dataiku **Managed Folder**, which is the correct place to store binary artifacts.
`},{id:29,slug:"deploying-ml-models-inside-dataiku",question:"How to get started with + deploying ML models inside Dataiku?",answer:`
### 1. Introduction/Overview
"Deploying" a model means moving it from the experimental phase (in the lab) to a stable, versioned artifact in your Flow that can be used to make predictions on new data. This is a critical step in productionizing your machine learning work. Dataiku makes this process simple and traceable.

### 2. Prerequisites
- **A trained model:** You need a model that you have trained and evaluated in the Visual Analysis Lab.
- **You have selected the "best" model:** From the results leaderboard, you should have identified the model you want to deploy.

### 3. Step-by-Step Instructions
1.  **Find the Deploy Button:**
    *   In the Visual Analysis Lab, go to the **Results** tab.
    *   Select the model you wish to deploy from the list.
    *   In the top right corner, you will see a blue **Deploy** button. Click it.
2.  **Create the Saved Model:**
    *   A dialog will appear. It will ask you to name the new "Saved Model" that will be created in your Flow.
    *   Click **CREATE**.
3.  **View the Deployed Model in the Flow:**
    *   Go back to your project's Flow. You will now see a new, green, diamond-shaped object. This is your **Saved Model**. It contains the trained model, its configuration, and its version.
4.  **Use the Deployed Model for Scoring:**
    *   Now that the model is deployed, you can use it.
    *   Bring a new dataset into your Flow that has the same schema as your training data (but without the target column).
    *   Select this new dataset, and from the right-hand panel, choose the **Score** recipe.
    *   Select your Saved Model as the model to use.
    *   Run the Score recipe. The output will be a new dataset containing the original data plus new columns for the predictions and probabilities.

### 4. Resources and Tools
- **Saved Model Object:** The versioned, governed artifact in the Flow that represents your deployed model.
- **Score Recipe:** The visual recipe used to apply a Saved Model to a new dataset for batch predictions.
- **API Deployer:** The service used to deploy a Saved Model as a real-time REST API endpoint.

### 5. Next Steps and Progression
- **Versioning:** If you retrain your model, you can deploy the new version to the *same* Saved Model object. This creates a new version, and you can easily switch between them or compare their performance.
- **Monitoring:** Once deployed, set up monitoring on your Saved Model using the "Model Views" to track performance and data drift over time.
- **Real-time Deployment:** Take your Saved Model and deploy it to the API Deployer to get a live REST API for on-demand predictions.

### 6. Common Challenges and Solutions
- **Challenge:** "The Score recipe failed with a schema mismatch."
- **Solution:** The columns in your new dataset do not match the columns the model was trained on. Ensure the column names and types are identical. You may need to apply the same **Prepare** recipe to your new data that you used on your training data.
- **Challenge:** "I've retrained my model. How do I update the one in the Flow?"
- **Solution:** Go back to the lab, find your new model version, and click **Deploy** again. In the dialog, choose to "Update existing" and select your original Saved Model. This will add the new model as a new version.
`},{id:30,slug:"monitoring-model-performance-over-time",question:"How to get started with + monitoring model performance over time?",answer:`
### 1. Introduction/Overview
A model's performance is not static; it can degrade over time as the real world changes. This is called "model drift." Monitoring your deployed models is essential to detect this degradation and know when it's time to retrain. Dataiku provides built-in tools for automated performance and data drift monitoring.

### 2. Prerequisites
- **A deployed "Saved Model":** You need a model that has been deployed to your Flow.
- **New Labeled Data:** You need ongoing access to new data that includes the actual outcomes (the "ground truth") to compare against your model's predictions.

### 3. Step-by-Step Instructions
1.  **Open the Saved Model:** In your Flow, double-click on your green "Saved Model" object.
2.  **Explore Model Views:** On the left panel, you'll see "Model Views." This is the hub for monitoring.
3.  **Set Up Data Drift Monitoring:**
    *   Click on **Drift Analysis**.
    *   Click **Compute** to analyze the statistical drift between your original training data and a new dataset.
    *   This will show you which input features have changed distribution the most, which can be an early warning sign of problems.
4.  **Set Up Performance Monitoring (using an Evaluation Recipe):**
    *   In your Flow, you need a dataset that contains both the model's predictions and the actual outcomes.
    *   Select this dataset and choose the **Evaluate** recipe from the right-hand panel.
    *   Configure the recipe, telling it which columns contain the prediction and which contain the actuals.
5.  **Automate Monitoring with a Scenario:**
    *   Create a scenario that runs periodically (e.g., weekly).
    *   **Step 1:** Build your latest input data.
    *   **Step 2:** Run a **Score** recipe to get fresh predictions.
    *   **Step 3:** Run your **Evaluate** recipe to generate the latest performance metrics.
    *   **Step 4:** Add a "Run checks" step on your Saved Model to check for performance degradation or data drift.
    *   **Step 5:** Configure a reporter to send an alert if any checks fail.

### 4. Resources and Tools
- **Saved Model Views:** The central UI for drift analysis and performance history.
- **Evaluate Recipe:** The tool for calculating performance metrics on new data.
- **Scenarios:** The engine for automating the entire monitoring process.

### 5. Next Steps and Progression
- **Custom Checks:** Define custom checks, such as "AUC must be greater than 0.8" or "Drift score for feature 'X' must be less than 0.2".
- **Dashboards:** Publish the metrics from your Evaluate recipe to a dashboard to create a visual history of your model's performance over time.
- **Automated Retraining:** If your monitoring scenario detects a significant performance drop, it can automatically trigger another scenario to retrain and redeploy the model.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't have the actual outcomes (ground truth) right away."
- **Solution:** This is common. Your monitoring pipeline will have a delay. You'll need to store your model's predictions and then join them with the ground truth data whenever it becomes available, before running the Evaluate recipe.
- **Challenge:** "What does a high drift score mean?"
- **Solution:** It means the statistical distribution of an input feature in your new data is significantly different from the data the model was trained on. The model may not make reliable predictions on this new data because it looks different from what it has seen before.
`},{id:31,slug:"building-scenarios-in-dataiku-dss",question:"How to get started with + building Scenarios in Dataiku DSS?",answer:`
### 1. Introduction/Overview
A Scenario is an automated sequence of actions in Dataiku. It's the tool you use to productionize your Flow, allowing you to schedule jobs, run tests, send alerts, and orchestrate complex pipelines. Building a basic scenario is a fundamental skill for any Dataiku developer.

### 2. Prerequisites
- **A Dataiku project with a Flow:** You need something to automate, like a data pipeline that builds a final dataset or a model that needs to be retrained.
- **A clear goal:** Know what you want to automate (e.g., "rebuild my final report dataset every morning").

### 3. Step-by-Step Instructions
1.  **Navigate to the Scenarios Page:** In your project's top navigation bar, click on **Scenarios**.
2.  **Create a New Scenario:**
    *   Click **+ NEW SCENARIO**.
    *   Give your scenario a descriptive name, like \`Daily_Report_Build\`.
3.  **Define the Steps:**
    *   Go to the **Steps** tab. This is where you define what the scenario does.
    *   Click **+ ADD STEP**.
    *   The most common step is **Build / Train**. Select this.
    *   In the step's configuration, click **+ ADD** and choose the dataset(s) or model(s) you want to build. **Best practice:** Only add the *final* outputs. Dataiku will automatically figure out and build all the necessary upstream dependencies.
    *   You can add other step types, like "Run checks" or "Execute Python code".
4.  **Run the Scenario Manually:**
    *   Before scheduling, it's a good idea to test your scenario.
    *   Click the **Run** button at the top right.
    *   You can monitor the progress in the "Last runs" tab.

### 4. Resources and Tools
- **Scenarios Page:** The central hub for creating, managing, and monitoring all your automations.
- **Step Library:** The list of available actions you can add to a scenario, from building datasets to calling external APIs.

### 5. Next Steps and Progression
- **Scheduling:** Go to the **Settings > Triggers** tab to make your scenario run automatically on a schedule.
- **Alerting:** Go to the **Reporters** tab to configure email or Slack notifications on success or failure.
- **Parameterization:** Use project variables to make your scenarios more dynamic. You can override these variables for a specific scenario run.

### 6. Common Challenges and Solutions
- **Challenge:** "My scenario failed, but I don't know why."
- **Solution:** Go to the "Last runs" tab and click on the failed run. This will take you to the job log, which provides a detailed error message and shows exactly which step failed.
- **Challenge:** "Do I need to add a step for every recipe in my flow?"
- **Solution:** No, and you shouldn't. This is a common beginner mistake. You only need to add a step to build the final artifact (e.g., the last dataset). Dataiku's dependency engine is smart enough to trace the lineage backward and build everything required in the correct order.
`},{id:32,slug:"scheduling-pipelines-via-triggers-or-cron",question:"How to get started with + scheduling pipelines via triggers or CRON?",answer:`
### 1. Introduction/Overview
Once you have built a scenario to automate your pipeline, the next step is to schedule it to run automatically. Dataiku provides a powerful and flexible trigger system that can launch scenarios based on time (like a CRON job), data changes, or external API calls.

### 2. Prerequisites
- **A working scenario:** You need an existing scenario that you have tested by running it manually.

### 3. Step-by-Step Instructions
1.  **Navigate to Scenario Settings:**
    *   In your project, go to **Scenarios**.
    *   Click on the scenario you want to schedule.
    *   Go to the **Settings** tab.
2.  **Add a Trigger:**
    *   In the **Triggers** section, click **+ ADD TRIGGER**.
3.  **Choose a Trigger Type:**
    *   **Time-based (CRON-like):**
        *   Select the **Time-based** trigger.
        *   Choose a repeat frequency (e.g., \`Daily\`, \`Hourly\`, \`Weekly\`).
        *   You can set the exact time of day for the run. For more complex schedules, you can switch to the "CRON" format.
    *   **Data-driven:**
        *   Select the **Dataset change** trigger.
        *   Choose a dataset from your project.
        *   The scenario will automatically run whenever this specific dataset is modified (e.g., by another scenario or an external process).
4.  **Enable the Trigger:**
    *   After configuring the trigger, make sure the toggle switch next to it is turned **ON**.
    *   The "Next run" time will now be displayed.
5.  **Save and Activate:** Remember to save your scenario. The scenario must also be active (the master toggle at the top of the scenario page must be on).

### 4. Resources and Tools
- **Triggers Panel:** The UI within the scenario settings for configuring all types of triggers.
- **CRON Schedulers:** For advanced time-based schedules, you can use online tools to help you build a valid CRON expression.

### 5. Next Steps and Progression
- **Multiple Triggers:** A single scenario can have multiple triggers. For example, it could run every hour AND be triggered if a specific input file changes.
- **API Triggering:** Any scenario can also be triggered externally by making a call to its specific REST API endpoint. This is useful for integration with external schedulers like Airflow.
- **Conditional Execution:** For more complex scheduling logic (e.g., "run only on the last day of the month"), you can use a time-based trigger that runs daily, with the first step being a Python script that checks the date and exits if the condition is not met.

### 6. Common Challenges and Solutions
- **Challenge:** "My scenario didn't run at the scheduled time."
- **Solution:** First, check that the trigger and the scenario itself are both enabled (toggled on). Second, check the "Last runs" tab for any error messages. Finally, check with your Dataiku administrator; there might be instance-level settings that limit the number of concurrent jobs.
- **Challenge:** "My data-driven trigger is firing too often."
- **Solution:** The trigger will fire every time the input dataset is modified. If an upstream process is rebuilding that dataset frequently, your scenario will also run frequently. You may need to adjust the logic of the upstream process or switch to a time-based trigger.
`},{id:33,slug:"automating-model-retraining-workflows",question:"How to get started with + automating model retraining workflows?",answer:`
### 1. Introduction/Overview
Models are not static; they need to be retrained on new data to stay accurate. Automating this retraining process is a core MLOps practice. In Dataiku, you can build a scenario that automatically retrains your model, evaluates its new performance, and even deploys it if it's better than the current version.

### 2. Prerequisites
- **A deployed "Saved Model" in your Flow.**
- **A pipeline that generates fresh training data:** Your Flow must have a recipe chain that prepares the data needed to train the model.

### 3. Step-by-Step Instructions
1.  **Create a Retraining Scenario:**
    *   Go to **Scenarios** and create a new scenario called \`Retrain_My_Model\`.
2.  **Add the "Train" Step:**
    *   In the scenario's **Steps** tab, click **+ ADD STEP**.
    *   Choose **Build / Train**.
    *   In the step's configuration, **do not add a dataset**. Instead, click **+ ADD** and select your **Saved Model** object from the Flow.
    *   This tells the scenario to retrain the model, which will automatically first rebuild its input training dataset.
3.  **(Optional) Add an Evaluation Step:**
    *   After the training step, you can add another step to build a **Model Evaluation** recipe. This will score the newly trained model version against a test set and compute its performance metrics.
4.  **(Optional) Add a Deployment Step:**
    *   For full automation, add a **Python code** step after the evaluation.
    *   This script can use the Dataiku API to:
        1.  Get the performance of the newly trained model version and the currently active version.
        2.  If the new version is better, the script can activate it, effectively deploying it.
5.  **Schedule the Scenario:**
    *   Go to the **Settings > Triggers** tab and add a **Time-based** trigger to run this retraining pipeline on a schedule (e.g., weekly or monthly).

### 4. Resources and Tools
- **Train Step:** The specific scenario step used to retrain a Saved Model.
- **Evaluate Recipe:** The tool used to measure the performance of the newly trained model version.
- **Dataiku Python API:** Required for the advanced step of automatically deploying the best model.

### 5. Next Steps and Progression
- **Champion/Challenger:** Instead of immediately deploying the new model, your scenario could deploy it as a "challenger" to be A/B tested against the current "champion" in production.
- **Monitoring:** Ensure you have model monitoring set up to track the performance of your automatically retrained models over time.

### 6. Common Challenges and Solutions
- **Challenge:** "How do I know if the retrained model is actually better?"
- **Solution:** The **Evaluate** recipe is key. It creates a dataset containing the performance metrics for each model version. Your deployment script must read this data and compare the new model's performance (e.g., its AUC score) against the old one before making a deployment decision.
- **Challenge:** "The model is retraining, but the performance is getting worse."
- **Solution:** This is a serious issue. It could mean there's a problem with your new training data (e.g., data quality issues, changes in data distribution). Your automation script should have a safety check to *never* deploy a model that is worse than the current one.
`},{id:34,slug:"configuring-success-failure-email-alerts",question:"How to get started with + configuring success/failure email alerts?",answer:`
### 1. Introduction/Overview
Monitoring your automated pipelines is crucial. You need to know if they succeeded or, more importantly, if they failed. Dataiku Scenarios have a built-in reporting system that can automatically send notifications via email, Slack, or other services.

### 2. Prerequisites
- **A working scenario.**
- **Email server configuration:** Your Dataiku administrator must have configured the instance to be able to send emails.

### 3. Step-by-Step Instructions
1.  **Navigate to your Scenario:** In your project, go to **Scenarios** and select the one you want to add alerts to.
2.  **Open the Reporters Tab:** Click on the **Reporters** tab. This is where all notifications are configured.
3.  **Add a Mail Reporter:**
    *   Click **+ ADD REPORTER**.
    *   Select **Mail** from the dropdown list.
4.  **Configure the Reporter:**
    *   **Give it a name:** e.g., \`Failure_Alerts_To_Dev_Team\`.
    *   **Run condition:** This is the most important setting. Choose when the email should be sent. Common choices are:
        *   \`On failure\`: The most common setting, for sending error alerts.
        *   \`On success\`: Useful for confirming that a critical job has completed.
        *   \`On completion\`: Sends an email regardless of outcome.
    *   **Recipients:** Enter the email address or distribution list to send the alert to.
    *   **Customize the Message:** You can customize the subject and body of the email. It's useful to use variables like \`\${scenarioName}\` and \`\${outcome}\` to provide context.
5.  **Save the Scenario:** Click **Save**. The reporter is now active and will send an email the next time the scenario runs and the run condition is met.

### 4. Resources and Tools
- **Reporters Tab:** The UI for managing all scenario notifications.
- **Variables:** Use built-in variables like \`\${projectKey}\`, \`\${scenarioName}\`, \`\${outcome}\`, and \`\${jobURL}\` in your email body to create informative and actionable alerts.

### 5. Next Steps and Progression
- **Multiple Reporters:** You can add multiple reporters to a single scenario. For example, you could have one reporter that sends a detailed failure log to the development team and another that sends a simple success/failure notification to a business stakeholder.
- **Slack Integration:** If your team uses Slack, set up a Slack reporter for more immediate notifications in a shared channel.
- **Custom Reports:** You can attach the content of a dashboard or dataset to your email, creating automated reports.

### 6. Common Challenges and Solutions
- **Challenge:** "No emails are being sent."
- **Solution:** First, check that the scenario actually ran and that the outcome matched the reporter's run condition. Second, confirm with your Dataiku administrator that the instance's mail server is configured correctly and is able to send emails to external addresses.
- **Challenge:** "The email alert isn't helpful."
- **Solution:** Customize the email body to be more informative. Always include the project key, scenario name, and the \`\${jobURL}\` variable. This provides a direct link to the logs so the recipient can immediately start investigating the problem.
`},{id:35,slug:"building-data-quality-validation-steps-in-scenarios",question:"How to get started with + building data quality validation steps in Scenarios?",answer:`
### 1. Introduction/Overview
Preventing bad data from flowing through your pipelines is a critical governance task. Dataiku allows you to define data quality rules on your datasets and then use Scenarios to automatically run these checks and stop the pipeline if the data doesn't meet your standards.

### 2. Prerequisites
- **A key dataset in your Flow:** You need a dataset where data quality is important (e.g., a cleaned dataset before it's used for modeling or reporting).
- **An idea of what "good quality" means:** Know what rules your data should follow (e.g., "this column should never be empty," "this value should be within a certain range").

### 3. Step-by-Step Instructions
#### Part 1: Define the Quality Rules (Metrics & Checks)
1.  **Open the Dataset:** In your Flow, open the dataset you want to validate.
2.  **Go to the Status Tab:** Navigate to the **Status** tab.
3.  **Define Metrics:**
    *   Click on the **Metrics** section. This is where you tell Dataiku what to measure.
    *   Click **+ ADD METRIC** and choose a metric type, for example, "Column statistics" or "Record count".
    *   Configure the metric (e.g., compute statistics on the \`price\` column).
    *   Run the metric computation to see the current values.
4.  **Define Checks:**
    *   Click on the **Checks** section. This is where you define your pass/fail rules.
    *   Click **+ ADD CHECK**. For example, choose **Column value in numerical range**.
    *   Configure the check: Select the \`price\` column and set the valid range (e.g., \`0\` to \`1000\`).
    *   Run the check to see if the current data passes.

#### Part 2: Automate the Validation in a Scenario
1.  **Create a Scenario:** Go to the **Scenarios** page and create a new scenario.
2.  **Add a Build Step:** The first step should be to build the dataset you want to validate.
3.  **Add the "Run Checks" Step:**
    *   Click **+ ADD STEP** and select **Run checks**.
    *   In the step's configuration, select your dataset.
    *   The scenario will now run the checks you defined in Part 1. By default, if any check fails, the entire scenario will fail.
4.  **Schedule and Alert:** Schedule your scenario to run after your data ingestion, and add a **Reporter** to send an alert if it fails.

### 4. Resources and Tools
- **Status Tab (Metrics & Checks):** The UI for defining your data quality rules.
- **Run Checks Step:** The scenario step that executes your predefined checks.

### 5. Next Steps and Progression
- **Custom Python Checks:** For very complex quality rules that can't be expressed with the built-in checks, you can write a custom check in Python.
- **Data Quality Dashboard:** Create a dashboard that visualizes the history of your metric values over time, allowing you to track data quality trends.

### 6. Common Challenges and Solutions
- **Challenge:** "My check is failing, but I don't know why."
- **Solution:** Go to the **Checks** section on the dataset. Click on the failing check. It will show you the computed value and the condition that failed. For example, "Value was 1050, which is outside the valid range of 0-1000."
- **Challenge:** "I want to be warned but not fail the whole pipeline."
- **Solution:** In the "Checks" settings, you can change the severity of a check from "Error" (which fails the job) to "Warning" (which allows the job to continue but still flags the issue).
`},{id:36,slug:"using-scenarios-to-automate-data-ingestion-jobs",question:"How to get started with + using scenarios to automate data ingestion jobs?",answer:`
### 1. Introduction/Overview
Data ingestion is the process of loading data from external sources (like databases, cloud storage, or APIs) into Dataiku. Automating this process with Scenarios ensures that your projects always have access to the latest data without manual intervention.

### 2. Prerequisites
- **Source datasets configured:** You must have already created datasets in your Flow that connect to your external sources (e.g., a SQL table dataset, an S3 dataset).
- **A clear ingestion schedule:** Know how often you need to refresh your data (e.g., daily, hourly).

### 3. Step-by-Step Instructions
1.  **Identify Your Ingestion Datasets:** In your Flow, locate the datasets that represent your raw data sources. These are the "entry points" of your pipeline.
2.  **Create an Ingestion Scenario:**
    *   Go to **Scenarios** and create a new scenario. Name it something clear, like \`Ingest_Source_Data_Daily\`.
3.  **Add a "Build" Step:**
    *   In the **Steps** tab, click **+ ADD STEP** and choose **Build / Train**.
    *   In the step's configuration, click **+ ADD** and select all of your source datasets.
4.  **Configure the Build Behavior:**
    *   For the "Build mode", the default \`Since last successful build\` is often good.
    *   However, for ingestion, you often want a full refresh. You can change the build mode for each dataset to **Forced rebuild**. This tells Dataiku to completely drop the old data and reload it from the source.
5.  **Schedule the Scenario:**
    *   Go to the **Settings > Triggers** tab.
    *   Add a **Time-based** trigger and configure it to run on your desired schedule (e.g., every day at 2 AM).
6.  **Add Alerts:** Go to the **Reporters** tab and configure an email or Slack alert to notify you if the ingestion job fails.

### 4. Resources and Tools
- **Build Step:** The primary scenario step for running data pipelines.
- **Forced Rebuild Mode:** A key setting for ensuring your ingestion datasets are completely refreshed from the source.
- **Triggers:** The tool for scheduling your automated ingestion.

### 5. Next Steps and Progression
- **Incremental Loads:** For very large tables, a full rebuild can be slow. If your source data has a timestamp, you can set up an incremental load pattern using partitioning and project variables to only fetch new records.
- **Chained Scenarios:** You can have your main data processing scenario be triggered by the successful completion of your ingestion scenario.
- **Data Quality Checks:** Add a "Run checks" step immediately after your build step to validate the incoming data before it's used by downstream processes.

### 6. Common Challenges and Solutions
- **Challenge:** "The ingestion job failed due to a source connection error."
- **Solution:** This indicates a problem with the external system (e.g., the database was down). A well-configured scenario with failure alerts will notify you of this problem so you can investigate.
- **Challenge:** "My ingested data is not up-to-date."
- **Solution:** Check the build mode. If it's not set to "Forced rebuild", Dataiku might be using a cached version. Ensure you are forcing a refresh for ingestion datasets. Also, verify that your scenario's schedule is running as expected.
`},{id:37,slug:"managing-flow-dependencies-programmatically",question:"How to get started with + managing flow dependencies programmatically?",answer:`
### 1. Introduction/Overview
While Dataiku's visual dependency engine is powerful, sometimes you need finer-grained control over your pipeline's execution. Using a **Python scenario step** and the **Dataiku Python API**, you can programmatically check conditions, build specific parts of your flow, and create complex, dynamic workflows.

### 2. Prerequisites
- **A good understanding of your Flow's structure.**
- **Basic Python skills.**
- **Familiarity with the Dataiku Python API documentation.**

### 3. Step-by-Step Instructions
1.  **Create a Scenario with a Python Step:**
    *   In your project, go to **Scenarios** and create a new scenario.
    *   Click **+ ADD STEP** and choose **Execute Python code**.
2.  **Get a Handle on Flow Items:**
    *   The script starts with access to a \`dataiku\` client object. Use it to get handles on your project and its datasets, recipes, etc.
    > \`\`\`python
    > import dataiku
    > client = dataiku.api_client()
    > project = client.get_project("MY_PROJECT_KEY")
    > dataset_to_build = project.get_dataset("my_output_dataset")
    > \`\`\`
3.  **Implement Your Logic:** Now you can write code to manage your dependencies.
    *   **Example: Conditional Build**
        > \`\`\`python
        > # Get the row count of a control dataset
        > control_dataset = project.get_dataset("control_ds")
        > row_count = control_dataset.get_metadata()["metrics"]["recordsCount"]
        > # If count > 100, build the main dataset
        > if row_count > 100:
        >     job = dataset_to_build.build()
        >     print(f"Started job: {job.id}")
        > \`\`\`
    *   **Example: Building only some partitions**
        > \`\`\`python
        > # Build only the latest partition of a partitioned dataset
        > job = dataset_to_build.build(partitions="LATEST")
        > \`\`\`
4.  **Run the Scenario:** The scenario will execute your Python script, which in turn will run the specific jobs you've defined.

### 4. Resources and Tools
- **Dataiku Python API Documentation:** This is your essential reference. Look for the \`DSSProject\`, \`DSSDataset\`, and \`DSSJob\` classes.
- **Python Scenario Step:** The environment where your programmatic logic runs.
- **Job Log:** The output of your \`print\` statements and the status of the jobs you launch will appear in the scenario's log.

### 5. Next Steps and Progression
- **Error Handling:** Your script should include \`try...except\` blocks to gracefully handle cases where a job fails.
- **Looping:** Programmatically build a flow for a list of inputs (e.g., loop through a list of countries and run a specific part of the flow for each).
- **Chaining Projects:** Use the API to trigger scenarios in other projects, creating dependencies between them.

### 6. Common Challenges and Solutions
- **Challenge:** "How do I get my project key or dataset name?"
- **Solution:** These are visible in the UI. The project key is in the URL of your browser. The dataset name is what you see in the Flow.
- **Challenge:** "The script runs, but no job is started."
- **Solution:** Add \`print\` statements to your script to debug the logic. Check your conditions. Ensure your user has the correct permissions to build the target dataset. The job might also be queued if the Dataiku instance is busy.
`},{id:38,slug:"integrating-dataiku-jobs-into-ci-cd-pipelines",question:"How to get started with + integrating Dataiku jobs into CI/CD pipelines?",answer:`
### 1. Introduction/Overview
CI/CD (Continuous Integration/Continuous Deployment) is a DevOps practice for automating software delivery. You can integrate Dataiku into this process, using tools like Jenkins, GitLab CI, or GitHub Actions to automatically test and deploy your Dataiku projects. The key to this integration is the **Dataiku REST API**.

### 2. Prerequisites
- **A CI/CD tool:** Jenkins, GitLab CI, GitHub Actions, etc.
- **A Dataiku project connected to Git:** Your project must be version-controlled.
- **An API Key:** Generate a Dataiku API key for your CI/CD tool to use, with permissions to run scenarios and create bundles.

### 3. Step-by-Step Instructions: A Typical CI/CD Workflow
1.  **Trigger (Code Commit):** A developer commits and pushes a change to the project's Git repository. This automatically triggers the CI/CD pipeline.
2.  **CI Step 1: Update Project from Git:**
    *   Your CI/CD script (e.g., a \`Jenkinsfile\`) makes a REST API call to Dataiku to pull the latest changes from the Git repository into the Dataiku project.
3.  **CI Step 2: Run Automated Tests:**
    *   The script then makes a second API call to run a specific "test" scenario in Dataiku.
    *   This test scenario should contain steps to "Run checks" for data quality and "Run tests" for any custom Python recipes.
    *   The script polls the job status until it completes. If the test scenario fails, the CI/CD pipeline fails and stops.
4.  **CD Step 1: Create a Project Bundle:**
    *   If the tests pass, the script makes another API call to create a "bundle" of the project. A bundle is a \`.zip\` file containing the entire project, which is the artifact for deployment.
5.  **CD Step 2: Deploy the Bundle:**
    *   The script then makes a final API call to the *production* Dataiku instance, uploading the bundle and deploying it. This updates the production project to the new, tested version.

### 4. Resources and Tools
- **Dataiku REST API:** The interface for all external control. The key endpoints are for \`project_update_from_git\`, \`scenario_run\`, and \`bundle_create/import\`.
- **Your CI/CD Tool's Scripting Language:** (e.g., Groovy for Jenkins, YAML for GitLab/GitHub).
- **\`curl\` or a scripting language:** Used in your CI/CD pipeline to make the HTTP requests to the Dataiku API.

### 5. Next Steps and Progression
- **Environment Promotion:** A full CI/CD pipeline will have multiple stages, deploying the bundle first to a "Staging" or "UAT" environment for further testing before a final manual approval step to deploy to production.
- **Infrastructure as Code:** For fully automated environments, the CI/CD pipeline could also trigger Terraform or CloudFormation scripts to provision the necessary cloud infrastructure for Dataiku.

### 6. Common Challenges and Solutions
- **Challenge:** "How do I handle API keys securely in my CI/CD tool?"
- **Solution:** Do not hardcode the API key in your script. All modern CI/CD tools have a "Secrets Management" or "Credentials" store. Store the Dataiku API key there, and inject it into your pipeline as an environment variable at runtime.
- **Challenge:** "The API call is failing."
- **Solution:** Use the \`curl\` command line tool to test your API calls manually from the CI/CD runner to debug connection or authentication issues. Check the Dataiku backend logs for more detailed error messages.
`},{id:39,slug:"linking-dataiku-with-jenkins-or-azure-devops",question:"How to get started with + linking Dataiku with Jenkins or Azure DevOps?",answer:`
### 1. Introduction/Overview
Integrating Dataiku with CI/CD tools like Jenkins or Azure DevOps allows you to automate the testing and deployment of your data projects, bringing robust software engineering practices to your analytics workflows. The integration is achieved by having the CI/CD tool call the **Dataiku REST API**.

### 2. Prerequisites
- **An installed CI/CD tool:** A running instance of Jenkins or an Azure DevOps organization.
- **A Dataiku project using Git:** Your project should be linked to a Git repository that your CI/CD tool can access.
- **A Dataiku API Key:** In Dataiku, create an API key with sufficient permissions (e.g., project admin rights).

### 3. Step-by-Step Instructions (Conceptual Workflow)

#### In Jenkins (using a \`Jenkinsfile\`):
1.  **Store Credentials:** In Jenkins, go to "Manage Jenkins" > "Credentials" and add a new "Secret text" credential. Store your Dataiku API key here and give it an ID (e.g., \`DATAİKU_API_KEY\`).
2.  **Create a Pipeline Job:** Create a new Jenkins Pipeline job linked to your project's Git repository.
3.  **Write the \`Jenkinsfile\`:** This script defines your pipeline.
    > \`\`\`groovy
    > pipeline {
    >     agent any
    >     environment {
    >         API_KEY = credentials('DATAİKU_API_KEY')
    >     }
    >     stages {
    >         stage('Run Tests in Dataiku') {
    >             steps {
    >                 // Use sh to call curl with the API key to trigger a test scenario
    >                 sh 'curl -u $API_KEY: -X POST https://YOUR_DSS_URL/public/api/projects/MY_PROJECT/scenarios/test_scenario/run'
    >             }
    >         }
    >         // ... other stages for bundling and deploying ...
    >     }
    > }
    > \`\`\`

#### In Azure DevOps (using a YAML pipeline):
1.  **Store Secrets:** In your Azure DevOps project, go to "Pipelines" > "Library" and create a "Variable group". Add your Dataiku API key as a secret variable.
2.  **Create a YAML Pipeline:** Create a file named \`azure-pipelines.yml\` in your Git repository.
3.  **Write the YAML:** This file defines your pipeline.
    > \`\`\`yaml
    > trigger:
    > - main
    >
    > pool:
    >   vmImage: 'ubuntu-latest'
    >
    > steps:
    > - script: |
    >     curl -u $(DATAİKU_API_KEY): -X POST https://YOUR_DSS_URL/public/api/projects/MY_PROJECT/scenarios/test_scenario/run
    >   displayName: 'Run Tests in Dataiku'
    >   env:
    >     DATAİKU_API_KEY: $(your-secret-variable-name) # Link to the secret
    > \`\`\`

### 4. Resources and Tools
- **Dataiku REST API Documentation:** Essential for finding the correct endpoints and parameters.
- **Jenkins/Azure DevOps Documentation:** For learning how to write pipeline scripts and manage secrets.
- **\`curl\`:** A command-line tool that is invaluable for making HTTP requests and testing your API calls.

### 5. Next Steps and Progression
- **Polling for Job Status:** The simple examples above just trigger the job. A real pipeline needs to poll the job status API endpoint in a loop until the job is finished and then check if the outcome was "SUCCESS".
- **Multi-Stage Pipelines:** Build out the full CI/CD process with stages for linting, testing, building, and deploying.

### 6. Common Challenges and Solutions
- **Challenge:** "Authentication (401) Error."
- **Solution:** Ensure the API key is correct and stored properly in your CI/CD tool's secret manager. The syntax for \`curl\` with an API key as the username and a blank password (\`-u key:\`) is very specific.
- **Challenge:** "Network/Connection Error."
- **Solution:** Ensure your Jenkins agent or Azure DevOps agent has network access to the Dataiku instance's URL and port. Firewalls can often block this traffic.
`},{id:40,slug:"using-rest-apis-to-trigger-dataiku-scenarios",question:"How to get started with + using REST APIs to trigger Dataiku scenarios?",answer:`
### 1. Introduction/Overview
The Dataiku REST API is a powerful feature that allows external systems to control and interact with your Dataiku projects. One of the most common use cases is to programmatically trigger a scenario run. This is the key to integrating Dataiku with external schedulers, CI/CD tools, or any other application.

### 2. Prerequisites
- **A Dataiku Scenario:** You need an existing scenario in a project that you want to trigger.
- **Your Dataiku Instance URL:** The URL of your Dataiku server (e.g., \`http://localhost:11000\`).
- **Project Key and Scenario ID:**
    - The **Project Key** is a short, unique identifier for your project (e.g., \`DKU_CHURN\`).
    - The **Scenario ID** is the unique identifier for your scenario (e.g., \`run_daily\`). You can find these in the URL when you are viewing the scenario.
- **An API Key:** You must generate an API key in your Dataiku profile with at least "run scenarios" permission for the target project.

### 3. Step-by-Step Instructions
1.  **Generate an API Key:**
    *   Click on your profile icon in Dataiku (top right) > **Profile & settings**.
    *   Go to the **API keys** tab.
    *   Click **+ NEW API KEY**. Give it a description and grant it the necessary permissions.
    *   Copy the generated secret key and store it securely.
2.  **Construct the API Endpoint URL:** The URL to trigger a scenario follows a standard format:
    > \`YOUR_DSS_URL/public/api/projects/YOUR_PROJECT_KEY/scenarios/YOUR_SCENARIO_ID/run\`
3.  **Make the API Call:** You can use any tool or programming language that can make an HTTP POST request. The most common tool for testing is \`curl\`.
    *   Open a terminal or command prompt.
    *   Use the following \`curl\` command, replacing the placeholders with your values. The API key is used as the username, and the password is left blank.
    > \`\`\`bash
    > curl -X POST -u 'YOUR_API_KEY:' 'http://localhost:11000/public/api/projects/DKU_CHURN/scenarios/run_daily/run'
    > \`\`\`
4.  **Check the Response:** If the call is successful, the API will return a JSON response containing information about the triggered scenario run, including its \`id\`.

### 4. Resources and Tools
- **\`curl\`:** A universal command-line tool for making HTTP requests. Perfect for testing.
- **Postman:** A graphical API client that is also excellent for testing and exploring APIs.
- **Dataiku REST API Documentation:** Available directly from your instance (\`Help > REST API Doc\`), it provides a complete reference for all available endpoints.

### 5. Next Steps and Progression
- **Polling for Results:** After triggering a scenario, you'll often need to know when it's finished and if it succeeded. You can use the \`jobId\` from the initial response to call another API endpoint in a loop to poll for the job's status.
- **Passing Parameters:** You can pass a JSON payload in your POST request to override project variables for that specific run, making your triggered jobs dynamic.
- **Integration:** Use this API call within a Python script, a Jenkinsfile, or any other application to integrate Dataiku into your broader ecosystem.

### 6. Common Challenges and Solutions
- **Challenge:** "401 Unauthorized Error."
- **Solution:** Your API key is incorrect, expired, or doesn't have permission for the target project. Double-check the key and its permissions. Ensure you are using the correct \`-u 'key:'\` syntax.
- **Challenge:** "404 Not Found Error."
- **Solution:** You have a typo in your Project Key or Scenario ID. Check them carefully. They are case-sensitive.
`},{id:41,slug:"connecting-dataiku-to-aws-redshift",question:"How to get started with + connecting Dataiku to AWS Redshift?",answer:`
### 1. Introduction/Overview
Amazon Redshift is a popular cloud data warehouse. Dataiku provides a native connector that makes it easy to read data from Redshift, write data back to it, and, most importantly, push down computation to leverage Redshift's powerful parallel processing engine.

### 2. Prerequisites
- **Redshift Cluster Details:** You need the endpoint (hostname), port, database name, username, and password for your cluster.
- **AWS Security Group Configuration:** The security group for your Redshift cluster must be configured to allow inbound traffic from the IP address of your Dataiku server on the Redshift port (usually 5439).
- **Dataiku Admin Rights:** You need administrator rights in Dataiku to create the connection.

### 3. Step-by-Step Instructions
1.  **Download the JDBC Driver:**
    *   Redshift requires a specific JDBC driver. Download it from the AWS documentation.
    *   In Dataiku, go to **Administration > Settings > Misc** and upload the driver JAR file to the "lib/java" directory. You may need to restart Dataiku for it to be recognized.
2.  **Create the Connection in Dataiku:**
    *   Go to **Administration > Connections**.
    *   Click **+ NEW CONNECTION** and select **Amazon Redshift**.
3.  **Configure Connection Details:**
    *   Give the connection a name (e.g., \`aws_redshift_main\`).
    *   Fill in the **Host**, **Port**, and **Database** from your Redshift cluster details.
    *   Enter the **User** and **Password**.
    *   **Important:** You will also need to specify an S3 bucket for temporary data. Redshift uses S3 for certain operations like \`COPY\` and \`UNLOAD\`. Create a dedicated S3 bucket for this purpose and enter its name in the settings.
4.  **Test and Create:**
    *   Click the **Test** button. If it's successful, it means your credentials and network settings are correct.
    *   Click **CREATE**.
5.  **Using the Connection:** Now, in any project, you can click **+ DATASET > Amazon Redshift** to browse schemas and tables in your Redshift cluster and import them into your Flow.

### 4. Resources and Tools
- **AWS Console:** To find your Redshift cluster details and configure security groups.
- **Dataiku Connections Page:** The central place to manage the Redshift connection.
- **SQL Recipes:** The most efficient way to work with Redshift data once it's in Dataiku.

### 5. Next Steps and Progression
- **Push-down Execution:** When you use a visual recipe (like Prepare or Join) on Redshift datasets, go to the "Advanced" settings and ensure the "Execution Engine" is set to "Run on database". This will translate the recipe into SQL and run it directly in Redshift, which is much faster for large data.
- **Writing to Redshift:** Use an **Export** recipe to write your final results from Dataiku into a new table in Redshift.

### 6. Common Challenges and Solutions
- **Challenge:** "Connection Test Times Out."
- **Solution:** This is almost always a network issue. The Dataiku server cannot reach the Redshift cluster. Go to your AWS VPC settings, find the Security Group attached to your Redshift cluster, and add an Inbound Rule that allows TCP traffic on port 5439 from the source IP of your Dataiku instance.
- **Challenge:** "JDBC Driver not found."
- **Solution:** You haven't installed the Redshift JDBC driver correctly. Ensure you've uploaded the correct JAR file to the \`lib/java\` folder in Dataiku's installation directory and restarted the instance.
`},{id:42,slug:"connecting-dataiku-to-snowflake-data-warehouse",question:"How to get started with + connecting Dataiku to Snowflake data warehouse?",answer:`
### 1. Introduction/Overview
Snowflake is a leading cloud data platform. Dataiku has a deep, native integration with Snowflake, allowing you to not only read and write data but also to fully leverage Snowflake's compute power by pushing down transformations directly from visual recipes.

### 2. Prerequisites
- **Snowflake Account Details:** You need your Snowflake account URL (e.g., \`myaccount.snowflakecomputing.com\`), a username, password, and the names of the warehouse, database, and schema you want to use.
- **Permissions:** The Snowflake user needs appropriate permissions (\`USAGE\` on the warehouse, database, and schema, and \`SELECT\` on tables).
- **Dataiku Admin Rights:** Required to set up the shared connection.

### 3. Step-by-Step Instructions
1.  **Create the Connection in Dataiku:**
    *   Go to **Administration > Connections**.
    *   Click **+ NEW CONNECTION** and select **Snowflake**.
2.  **Configure Connection Settings:**
    *   Give the connection a clear name (e.g., \`snowflake_prod\`).
    *   Enter your Snowflake **Account** name (the part before \`.snowflakecomputing.com\`).
    *   Enter your **Username** and **Password**.
    *   Specify the default **Warehouse**, **Database**, and **Schema** to use.
    *   The JDBC driver for Snowflake is typically bundled with Dataiku, so you usually don't need to install it separately.
3.  **Test and Create:**
    *   Click **Test**. A successful test confirms that your credentials and account details are correct.
    *   Click **CREATE**.
4.  **Use the Connection in a Project:**
    *   In your project Flow, click **+ DATASET > Snowflake**.
    *   You can now browse the databases and schemas accessible to your user and select a table to import as a Dataiku dataset.

### 4. Resources and Tools
- **Snowflake UI (Snowsight):** To find your account details and manage user permissions.
- **Dataiku Connections Page:** To manage the Snowflake connection.
- **Visual Recipes (Prepare, Join, Group):** These recipes can run directly inside Snowflake for maximum performance.

### 5. Next Steps and Progression
- **Full Push-down:** This is the key benefit. When using visual recipes on Snowflake datasets, check the recipe's "Advanced" settings and set the execution engine to "Run on database (SQL)". Dataiku will generate Snowflake-optimized SQL instead of pulling data out.
- **Time Travel:** Use a SQL recipe to leverage Snowflake's \`AT\` or \`BEFORE\` clauses to query historical versions of your data.
- **Writing to Snowflake:** Use an **Export** recipe to create new tables in Snowflake from your Dataiku flow.

### 6. Common Challenges and Solutions
- **Challenge:** "Connection Test Fails."
- **Solution:** Double-check your account name, username, and password. Account names can be tricky; ensure you are using the correct format. If your company uses network policies in Snowflake, the IP address of your Dataiku instance may need to be whitelisted.
- **Challenge:** "I run a recipe, and it's very slow."
- **Solution:** You are likely not using push-down. Open the recipe and verify the execution engine is set to "Run on database". If it's running in-memory, Dataiku is pulling all the data from Snowflake first, which is inefficient for large datasets.
`},{id:43,slug:"using-dataiku-with-gcp-big-query",question:"How to get started with + using Dataiku with GCP big‑query?",answer:`
### 1. Introduction/Overview
Google BigQuery is a serverless, highly scalable data warehouse. Dataiku's integration allows you to easily access your BigQuery tables and leverage its powerful query engine for large-scale data transformation directly from the Dataiku interface.

### 2. Prerequisites
- **GCP Project:** You need a Google Cloud Platform project where BigQuery is enabled.
- **Service Account:** It is best practice to create a GCP Service Account for Dataiku. This service account needs the "BigQuery Data Viewer" and "BigQuery Job User" roles, at a minimum.
- **Service Account Key:** You need to create and download a JSON key file for this service account.
- **Dataiku Admin Rights:** Required to configure the connection.

### 3. Step-by-Step Instructions
1.  **Create the Connection in Dataiku:**
    *   Go to **Administration > Connections**.
    *   Click **+ NEW CONNECTION** and select **Google BigQuery**.
2.  **Configure Authentication:**
    *   Give the connection a name (e.g., \`gcp_bigquery_main\`).
    *   For authentication, choose **Service Account**.
    *   Paste the entire contents of the JSON key file you downloaded into the "Service account credentials" text box.
    *   Enter the GCP **Project ID** where your BigQuery data resides.
3.  **Test and Create:**
    *   Click **Test**. A successful result means Dataiku was able to authenticate with GCP and access the BigQuery service.
    *   Click **CREATE**.
4.  **Use the Connection:**
    *   In a project Flow, click **+ DATASET > Google BigQuery**.
    *   You will now be able to browse the datasets and tables within your BigQuery project and import them as Dataiku datasets.

### 4. Resources and Tools
- **GCP IAM & Admin Console:** Where you create service accounts and manage permissions.
- **BigQuery Console:** To view your tables and find your project ID.
- **Visual Recipes and SQL Recipes:** Both can be pushed down to run directly in BigQuery.

### 5. Next Steps and Progression
- **Push-down Execution:** Just like with other databases, ensure your visual recipes are set to "Run on database (SQL)" in their advanced settings to leverage BigQuery's engine.
- **Writing to BigQuery:** Use an **Export** recipe to write the results of your Dataiku pipeline into new tables in BigQuery.
- **Partitioned Tables:** Dataiku can read from and write to tables partitioned by date in BigQuery, which is essential for managing large time-series data.

### 6. Common Challenges and Solutions
- **Challenge:** "Connection Test Fails with a Permissions Error."
- **Solution:** This is the most common issue. Go to the GCP IAM console. Find your service account and make sure it has the necessary roles (\`BigQuery Data Viewer\`, \`BigQuery Job User\`) assigned to it for the target project.
- **Challenge:** "My query is costing a lot of money."
- **Solution:** BigQuery charges based on the amount of data scanned. Ensure your SQL queries are efficient. Use \`WHERE\` clauses on partitioned columns to limit the amount of data scanned. Use the BigQuery console's query validator to estimate the cost before running a query in Dataiku.
`},{id:44,slug:"integrating-hadoop-or-spark-with-dataiku",question:"How to get started with + integrating Hadoop or Spark with Dataiku?",answer:`
### 1. Introduction/Overview
For big data processing, Dataiku can be deeply integrated with a Hadoop/Spark cluster. This allows Dataiku to use HDFS for storage and to submit computation jobs to Spark or Hive, enabling you to process massive datasets in a distributed fashion. This is an advanced configuration typically performed by a Dataiku administrator.

### 2. Prerequisites
- **A Hadoop/Spark Cluster:** A running cluster (e.g., Cloudera, Hortonworks, EMR) with HDFS, YARN, and Spark/Hive.
- **Network Connectivity:** The Dataiku server needs to be installed on an "edge node" of the cluster, meaning it has the necessary Hadoop client libraries and configuration files to communicate with the cluster's services.
- **Administrator-level Access:** Both to the cluster and the Dataiku instance for the installation and configuration.

### 3. Step-by-Step Instructions (High-Level)
1.  **Install Dataiku on an Edge Node:** The Dataiku server software must be installed on a node that is part of the Hadoop cluster.
2.  **Configure Hadoop Integration:**
    *   During the installation process, the Dataiku setup script will prompt for Hadoop integration.
    *   You will point Dataiku to the location of the Hadoop configuration files (like \`core-site.xml\`, \`hdfs-site.xml\`).
    *   Dataiku will use these files to automatically configure itself to work with your cluster's HDFS and YARN resource manager.
3.  **Verify Spark/Hive Integration:**
    *   After setup, go to **Administration > Settings > Spark** or **Hive**.
    *   Dataiku should have auto-detected the Spark and Hive installations. You can verify the settings here.
4.  **Using the Integration:**
    *   **For Storage:** You can now create datasets that read from and write to HDFS.
    *   **For Compute:** In the settings of visual recipes (Prepare, Join, etc.), you can now choose **Spark** or **Hive** as the execution engine. You can also create PySpark, SparkR, or SparkSQL code recipes.

### 4. Resources and Tools
- **Dataiku Installation & Administration Guide:** The official documentation provides detailed, step-by-step instructions for integrating with different Hadoop distributions.
- **Hadoop/Spark UI:** The native UIs for YARN and Spark are useful for monitoring the jobs that Dataiku submits to the cluster.

### 5. Next Steps and Progression
- **PySpark Recipes:** Move beyond visual recipes and write custom distributed data transformations using PySpark code in a Python recipe.
- **Performance Tuning:** Learn to configure Spark settings within Dataiku (e.g., number of executors, memory per executor) to optimize the performance of your jobs.
- **Containerized Spark:** For modern deployments, Dataiku can also be configured to submit Spark jobs to a Kubernetes cluster instead of a traditional YARN-based one.

### 6. Common Challenges and Solutions
- **Challenge:** "Dataiku can't connect to HDFS."
- **Solution:** This is usually a configuration or permissions issue. Double-check that Dataiku is using the correct Hadoop configuration files. Ensure that the user running the Dataiku process has the necessary permissions to read/write in HDFS.
- **Challenge:** "My Spark recipe fails with a YARN error."
- **Solution:** Debugging Spark jobs can be complex. The first step is to go to the YARN ResourceManager UI. Find your failed application and look at its detailed logs (in the \`stderr\`/\`stdout\` of the containers). The root cause is often found there.
`},{id:45,slug:"running-spark-based-recipes-in-dss",question:"How to get started with + running Spark-based recipes in DSS?",answer:`
### 1. Introduction/Overview
When dealing with very large datasets, processing data in-memory on a single server is not feasible. Spark provides a framework for distributed, in-memory computing across a cluster. Dataiku allows you to leverage Spark's power through both visual recipes and code recipes, enabling you to process terabytes of data efficiently.

### 2. Prerequisites
- **Dataiku integrated with a Spark cluster:** Your Dataiku instance must be configured to submit jobs to a Spark cluster (either via YARN or Kubernetes).
- **Data stored in a distributed file system:** Your input data should be in a location accessible by Spark, such as HDFS, S3, GCS, or Azure Blob Storage.

### 3. Step-by-Step Instructions
#### Method 1: Using Visual Recipes with the Spark Engine
1.  **Select a Compatible Recipe:** Choose a visual recipe that can be executed by Spark, such as **Prepare**, **Join**, **Group**, or **Sync**.
2.  **Ensure Inputs are Spark-compatible:** Your input datasets must be stored on a distributed filesystem like HDFS or S3.
3.  **Change the Execution Engine:**
    *   In the recipe's settings, click on the **Advanced** tab.
    *   Find the **Execution engine** dropdown.
    *   Change it from the default "In-Memory" to **Spark**.
4.  **Run the Recipe:** Click **Run**. Dataiku will now translate the visual steps of your recipe into a Spark job and submit it to the cluster for execution.

#### Method 2: Using Spark Code Recipes
1.  **Create a Code Recipe:** From your Flow, select **+ RECIPE** and choose **PySpark**, **Spark R**, or **Spark SQL**.
2.  **Write Your Spark Code:**
    *   The recipe editor will open. You can write standard Spark code.
    *   Dataiku automatically provides a \`spark\` session object.
    *   Use the Spark DataFrame API to read your input datasets and perform transformations.
    > \`\`\`python
    > # In a PySpark recipe
    > input_df = spark.read.format("parquet").load(dataiku.Dataset("my_input").get_path())
    >
    > # Your Spark transformation logic
    > output_df = input_df.withColumn("new_col", input_df["old_col"] * 2)
    >
    > output_df.write.format("parquet").mode("overwrite").save(dataiku.Dataset("my_output").get_path())
    > \`\`\`
3.  **Run the Recipe:** Click **Run**. Dataiku will execute your script as a Spark application on the cluster.

### 4. Resources and Tools
- **Recipe Execution Engine Setting:** The key UI element for switching a visual recipe to run on Spark.
- **Spark UI:** The native Spark UI is essential for monitoring and debugging the jobs submitted by Dataiku.
- **Apache Spark Documentation:** The official source for learning the Spark DataFrame API.

### 5. Next Steps and Progression
- **Performance Tuning:** In the recipe settings, you can configure Spark properties to tune your job's performance, such as setting the number of executors and their memory.
- **Window Functions:** Use Spark window functions for complex analytical tasks like calculating running totals or moving averages on massive datasets.
- **Spark Streaming:** For real-time use cases, explore Dataiku's streaming capabilities, which can run on Spark Streaming.

### 6. Common Challenges and Solutions
- **Challenge:** "My Spark job is very slow or failing."
- **Solution:** This is a broad problem, but the first place to look is the **Spark UI**. Find your job and look at the "Stages" and "Executors" tabs. Are there data skews? Are executors failing? The Spark UI provides a wealth of information for debugging performance issues.
- **Challenge:** "The 'Spark' option is not available in the execution engine dropdown."
- **Solution:** This means either your Dataiku instance is not configured for Spark, or your input/output datasets are not on a Spark-compatible storage system. They must be on a distributed filesystem, not the local filesystem of the Dataiku server.
`},{id:46,slug:"leveraging-cloud-compute-for-large-scale-pipelines",question:"How to get started with + leveraging cloud compute for large-scale pipelines?",answer:`
### 1. Introduction/Overview
Modern cloud platforms offer virtually unlimited, on-demand compute resources. Dataiku is designed to leverage this elasticity. Instead of being limited by a single server, you can configure Dataiku to push down computation to powerful cloud data warehouses or to spin up temporary, containerized environments for resource-intensive tasks.

### 2. Prerequisites
- **Dataiku running in the cloud:** While not strictly necessary, it's most common for the Dataiku instance itself to be running on a cloud VM (e.g., AWS EC2, Azure VM, GCP Compute Engine).
- **Cloud data services:** You should be using cloud-native data storage and warehousing (e.g., S3/Redshift, ADLS/Snowflake, GCS/BigQuery).

### 3. Step-by-Step Instructions: Key Strategies

#### Strategy 1: Push-down to a Cloud Data Warehouse
- **Concept:** This is the most efficient strategy. The data never leaves your data warehouse. Dataiku sends the computation *to* the data.
- **How:**
    1.  Connect Dataiku to your cloud data warehouse (Snowflake, Redshift, BigQuery, etc.).
    2.  When using visual recipes (Prepare, Join) on datasets from this warehouse, go to the recipe's **Advanced** settings.
    3.  Set the **Execution engine** to **Run on database (SQL)**.
    4.  When you run the recipe, Dataiku will generate SQL and execute it directly in your powerful cloud warehouse, not on the Dataiku server.

#### Strategy 2: Containerized Execution with Kubernetes
- **Concept:** For tasks that can't be pushed down to SQL (like a custom Python recipe), Dataiku can temporarily spin up a container on a Kubernetes cluster (e.g., AWS EKS, Azure AKS, Google GKE) to run the job.
- **How (Admin Task):**
    1.  An administrator configures the connection between Dataiku and the Kubernetes cluster in **Administration > Containerized Execution**.
    2.  As a user, in your Python recipe's **Advanced** settings, you can now select a **Container** configuration.
    3.  When you run the recipe, Dataiku packages it up, sends it to Kubernetes which runs it in a dedicated pod, and then streams the logs and results back.

### 4. Resources and Tools
- **Cloud Data Warehouse Connections:** The foundation for push-down execution.
- **Containerized Execution Settings:** The admin-level configuration for linking to Kubernetes.
- **Recipe Execution Engine Dropdown:** The user-facing control for choosing where a recipe runs.

### 5. Next Steps and Progression
- **Autoscaling:** Configure your cloud resources (like a Kubernetes node pool or a Snowflake warehouse) to autoscale. They will automatically scale up for large jobs and scale down when idle, optimizing costs.
- **Spot Instances:** Use cheaper spot/preemptible instances for the containerized execution of non-critical development or testing jobs to further reduce costs.
- **GPU Instances:** For deep learning tasks, configure a containerized environment that uses GPU-enabled nodes in your cloud.

### 6. Common Challenges and Solutions
- **Challenge:** "My SQL push-down recipe is slow."
- **Solution:** The performance is now dependent on your cloud data warehouse. You may need to resize your warehouse (e.g., use a larger Snowflake warehouse size) or optimize the generated SQL.
- **Challenge:** "My containerized job is stuck in 'pending'."
- **Solution:** This usually means the Kubernetes cluster doesn't have enough resources (CPU or memory) to schedule the pod for your job. You may need to add more nodes to your cluster or adjust the resource requests for the container configuration.
`},{id:47,slug:"deploying-dataiku-in-cloud-environments",question:"How to get started with + deploying Dataiku in cloud environments?",answer:`
### 1. Introduction/Overview
Deploying Dataiku in the cloud (AWS, Azure, or GCP) is the standard for building scalable and robust analytics platforms. Cloud deployment offers flexibility, scalability, and easy integration with a rich ecosystem of cloud data services. This guide provides a high-level overview of the common deployment patterns.

### 2. Prerequisites
- **A cloud provider account:** An account with AWS, Azure, or GCP.
- **Basic cloud knowledge:** Understanding of core concepts like Virtual Machines (VMs), storage (S3, Blob), and networking (VPCs, VNETs).
- **A Dataiku License:** You need a valid license file from Dataiku.

### 3. Step-by-Step Instructions: Common Deployment Patterns

#### Pattern 1: Single VM Deployment (Good for small teams, dev/test)
1.  **Launch a Virtual Machine:** In your cloud provider's console, launch a new VM (e.g., AWS EC2, Azure VM). Choose a suitable OS (e.g., Ubuntu, RHEL) and instance size.
2.  **Install Dataiku:** SSH into the newly created VM. Follow the standard Dataiku installation instructions to install the software.
3.  **Configure Networking:** Set up the necessary security group or network rules to allow access to the Dataiku web interface (e.g., open port 11000 to your IP address).
4.  **Connect to Cloud Services:** From within this Dataiku instance, you can then configure connections to other cloud services like S3, Redshift, or BigQuery.

#### Pattern 2: Cloud Marketplace Deployment (Easiest start)
1.  **Go to the Marketplace:** Navigate to your cloud provider's marketplace (e.g., AWS Marketplace).
2.  **Search for Dataiku:** Search for the official Dataiku DSS offering.
3.  **Launch the Template:** The marketplace provides a pre-configured template (like an AMI or an ARM template) that automates the deployment of a Dataiku instance onto a VM with recommended settings. This is often the fastest way to get started.

#### Pattern 3: Kubernetes Deployment (For scalable, production environments)
1.  **Set up a Kubernetes Cluster:** Provision a managed Kubernetes cluster (e.g., AWS EKS, Azure AKS, Google GKE).
2.  **Use Dataiku's Helm Chart:** Dataiku provides an official Helm chart for deploying to Kubernetes. Helm is a package manager that simplifies complex application deployments.
3.  **Deploy Dataiku:** Use the Helm chart to deploy the various components of Dataiku (backend server, execution nodes, etc.) as pods within your cluster. This pattern provides high availability and scalability.

### 4. Resources and Tools
- **Cloud Provider Marketplaces:** The quickest way to launch a pre-configured instance.
- **Dataiku Installation Documentation:** Provides detailed guides for different cloud providers.
- **Dataiku Helm Chart:** The official, supported method for deploying on Kubernetes.

### 5. Next Steps and Progression
- **Infrastructure as Code (IaC):** Use tools like Terraform or AWS CloudFormation to define your entire Dataiku deployment (VMs, networking, databases) in code. This makes your deployments repeatable and version-controlled.
- **High Availability (HA):** For critical production instances, set up a High Availability architecture, which typically involves multiple backend nodes and an external PostgreSQL database for Dataiku's configuration.
- **Automated Backups:** Configure automated snapshots of your Dataiku instance's disk for disaster recovery.

### 6. Common Challenges and Solutions
- **Challenge:** "I can't access the Dataiku UI after installation."
- **Solution:** This is a network configuration issue. Check the security group or firewall rules associated with your VM. Ensure the port Dataiku is running on is open to your IP address.
- **Challenge:** "Which instance size should I choose?"
- **Solution:** The Dataiku documentation provides sizing guidelines based on the number of users and the size of your data. Start with the recommended size and monitor resource usage. You can always resize the VM later if needed.
`},{id:48,slug:"using-dataiku-on-kubernetes-docker",question:"How to get started with + using Dataiku on Kubernetes/Docker?",answer:`
### 1. Introduction/Overview
Running Dataiku on containers with Docker and Kubernetes is the modern approach for scalable, resilient, and manageable deployments. Docker packages the application, and Kubernetes orchestrates it. This guide covers the concepts of how to use these technologies with Dataiku.

### 2. Prerequisites
- **Familiarity with container concepts:** Understand what Docker images and containers are.
- **A Kubernetes cluster:** A running cluster, such as a managed one from a cloud provider (EKS, AKS, GKE) or a local one like Minikube for testing.
- **\`kubectl\` and \`helm\` CLIs:** You need these command-line tools to interact with your cluster.

### 3. Step-by-Step Instructions: Key Concepts

#### Concept 1: Running Dataiku Itself on Kubernetes
This involves deploying the entire Dataiku platform as a set of services within your Kubernetes cluster.
1.  **Use the Official Helm Chart:** This is the recommended method. Helm charts are packages of pre-configured Kubernetes resources.
2.  **Configure the Deployment:** You will provide a \`values.yaml\` file to the Helm chart to configure your deployment, specifying things like your license key, the type of storage to use, and resource requests.
3.  **Deploy:** Run the \`helm install\` command. Helm will create all the necessary Kubernetes objects (Deployments, Services, PersistentVolumeClaims, etc.) to run Dataiku.
4.  **Benefits:** This gives you a highly available and scalable Dataiku instance. Kubernetes can automatically restart failed pods and you can easily scale the number of replicas.

#### Concept 2: Using Kubernetes to Run Dataiku Jobs (Containerized Execution)
In this model, the main Dataiku instance might run on a VM, but it offloads the execution of specific, heavy jobs to a Kubernetes cluster.
1.  **Configure in Dataiku:** An administrator goes to **Administration > Containerized Execution** and sets up a connection to the Kubernetes cluster.
2.  **Define a Container Configuration:** The admin defines a "base image" (a Docker image with Python and necessary libraries) and resource limits.
3.  **Run a Job:** As a user, in a recipe's **Advanced** settings, you can now select this container configuration as the execution environment.
4.  **How it works:** When you run the recipe, Dataiku packages your code, sends it to Kubernetes, which spins up a temporary pod using your base image to run the job, and then terminates the pod when it's done.
5.  **Benefits:** This provides isolation and scalability for individual jobs without needing to run the entire platform on Kubernetes.

### 4. Resources and Tools
- **Dataiku's Official Helm Chart:** The best practice for deploying the full platform on Kubernetes.
- **Docker Hub:** Where you can find official base images for creating your containerized execution environments.
- **Kubernetes Documentation:** For understanding the core concepts of pods, services, and deployments.

### 5. Next Steps and Progression
- **Infrastructure as Code:** Manage your Kubernetes resources and Helm chart configurations using tools like Terraform or ArgoCD.
- **Monitoring:** Integrate your Kubernetes cluster with monitoring tools like Prometheus and Grafana to track the performance and resource usage of your Dataiku pods.
- **Custom Docker Images:** Build your own custom Docker images with specific OS-level dependencies or pre-installed libraries for your containerized execution environments.

### 6. Common Challenges and Solutions
- **Challenge:** "My Dataiku pod is CrashLooping."
- **Solution:** Use \`kubectl describe pod <pod-name>\` to see why it's failing. A common reason is that it can't connect to the backend database. Then use \`kubectl logs <pod-name>\` to view the detailed startup logs from the Dataiku container itself.
- **Challenge:** "My containerized job fails immediately."
- **Solution:** This often means the Docker image is missing a required dependency or there's a permissions issue with the service account Kubernetes is using. Check the logs of the failed job pod.
`},{id:49,slug:"optimizing-performance-on-big-data-jobs",question:"How to get started with + optimizing performance on big data jobs?",answer:`
### 1. Introduction/Overview
When working with big data, performance is key. A job that takes hours instead of minutes can severely impact productivity. Performance optimization in Dataiku is about choosing the right tool for the job and ensuring that computation happens in the most efficient location.

### 2. Prerequisites
- **A slow-running job:** You need an existing pipeline that is taking too long to run.
- **Understanding of your data's location and size.**
- **Knowledge of your available compute engines** (e.g., an in-memory server, a SQL database, a Spark cluster).

### 3. Step-by-Step Instructions: A Troubleshooting Framework
1.  **Identify the Bottleneck:**
    *   Go to the **Jobs** menu and find a run of your slow pipeline.
    *   The job view provides a Gantt chart showing the duration of every recipe.
    *   Identify the specific recipe or recipes that are taking the most time. This is where you will focus your efforts.
2.  **Analyze the Bottleneck Recipe:** Click on the slow recipe. Ask the following questions:
    *   **Where is the data?** Is it in a SQL database, HDFS, S3, or on the local filesystem?
    *   **Where is the recipe running?** Open the recipe's **Advanced** settings and check the **Execution engine**.
3.  **Apply the Core Optimization Principle: Push Down Computation.**
    *   **If the data is in a SQL Database (Snowflake, Redshift, etc.):** The recipe *must* run on the database. Change the execution engine to **Run on database (SQL)**. This is the single most important optimization.
    *   **If the data is in HDFS/S3/GCS and is very large:** The recipe *must* run on a distributed engine. Change the execution engine to **Spark**.
    *   **If the recipe is running "In-Memory":** This is a red flag for big data. It means Dataiku is pulling all the data from your source to the Dataiku server to process it. This is slow and can cause memory errors. **Avoid the in-memory engine for large datasets.**
4.  **Other Key Optimizations:**
    *   **Use efficient file formats:** Store intermediate datasets as **Parquet** instead of CSV. Parquet is a columnar format that is much more performant for analytics.
    *   **Filter data early:** In your pipeline, filter out unnecessary rows and columns as early as possible. The less data you have to process in downstream steps, the faster they will be.
    *   **Partition your data:** If your data is time-based, partition it by date. This allows jobs to process only the latest partition instead of the entire history.

### 4. Resources and Tools
- **Job Inspector:** Your primary tool for identifying which recipes are the bottlenecks.
- **Execution Engine Dropdown:** The control for pushing down computation.
- **Dataset Format Settings:** Where you can change the storage format to something efficient like Parquet.

### 5. Next Steps and Progression
- **Spark Tuning:** If using Spark, you can tune its performance by adjusting memory and executor settings in the recipe configuration.
- **SQL Query Optimization:** If pushing down to SQL, you can use a SQL recipe to write a highly optimized query, potentially outperforming the SQL generated by a visual recipe.
- **Benchmarking:** After making an optimization, always re-run the job and measure the new time to confirm that your change had a positive impact.

### 6. Common Challenges and Solutions
- **Challenge:** "I changed the engine to Spark, but it's still slow."
- **Solution:** The problem may now be with the Spark job itself. Use the Spark UI to debug the job. Look for issues like data skew or insufficient cluster resources.
- **Challenge:** "The 'Run on database' option is greyed out."
- **Solution:** This means either the recipe type is not compatible with SQL push-down, or your input/output datasets are not from the same database connection.
`},{id:50,slug:"migrating-alteryx-workflows-into-dataiku",question:"How to get started with + migrating Alteryx workflows into Dataiku?",answer:`
### 1. Introduction/Overview
Migrating from a legacy tool like Alteryx to Dataiku is a common project that can unlock significant benefits in scalability, collaboration, and governance. The process is a manual "translation" of the logic, which provides an excellent opportunity to refactor and improve the original workflow.

### 2. Prerequisites
- **Access to the Alteryx workflow:** You need to be able to see the tools and logic in the original Alteryx workflow.
- **A Dataiku instance:** The target platform for the migration.
- **Understanding of both tools:** You should know the basic concepts of both Alteryx (tools, containers) and Dataiku (recipes, Flow).

### 3. Step-by-Step Instructions: The Migration Process
1.  **Deconstruct the Alteryx Workflow:**
    *   Open the Alteryx workflow. Go through it tool by tool.
    *   For each tool, understand its purpose: Is it filtering data? Joining? Calculating a new field?
    *   Document this sequence of logical steps. A simple spreadsheet is a good tool for this.
2.  **Map Alteryx Tools to Dataiku Recipes:**
    *   Create a "translation map." Most Alteryx tools have a direct equivalent in Dataiku.
        *   Alteryx \`Input Data\` -> Dataiku **Dataset** (e.g., from SQL or a file).
        *   Alteryx \`Filter\`, \`Formula\`, \`Select\` -> Steps in a Dataiku **Prepare** recipe.
        *   Alteryx \`Join\` -> Dataiku **Join** recipe.
        *   Alteryx \`Summarize\` -> Dataiku **Group** recipe.
        *   Alteryx \`Output Data\` -> Dataiku **Export** recipe.
3.  **Rebuild the Flow in Dataiku:**
    *   Following your map, reconstruct the logic in a Dataiku Flow.
    *   Start by creating the input datasets.
    *   Chain the appropriate visual recipes together to replicate the transformation logic.
4.  **Handle Custom Macros and Code:**
    *   If the Alteryx workflow uses custom macros or R/Python code tools, you will need to rewrite that logic in a Dataiku **Python recipe**.
5.  **Validate the Output:**
    *   Run both the original Alteryx workflow and the new Dataiku flow on the same input data.
    *   Import the output from Alteryx into Dataiku.
    *   Use a **Stack** recipe followed by a **Group** recipe or a custom Python script to compare the two outputs row by row and field by field to ensure they are identical.

### 4. Resources and Tools
- **The Alteryx Workflow Canvas:** Your source of truth for the original logic.
- **Dataiku Visual Recipes:** The building blocks for the new flow.
- **Validation Techniques:** Stacking and comparing outputs is a crucial final step.

### 5. Next Steps and Progression
- **Optimization:** Don't just do a "lift and shift." Use the migration as a chance to improve the pipeline. Can you switch from file-based sources to direct database connections? Can you push down computation to a database or Spark?
- **Automation:** Once migrated and validated, create a **Scenario** to schedule and automate your new Dataiku pipeline.
- **User Training:** Train the original Alteryx users on how to use and maintain the new flow in Dataiku.

### 6. Common Challenges and Solutions
- **Challenge:** "There's no direct equivalent for a specific Alteryx tool."
- **Solution:** This is rare, but if it happens, you will need to replicate the tool's logic in a **Python recipe**. This gives you the flexibility to code any custom transformation you need.
- **Challenge:** "The outputs don't match exactly."
- **Solution:** This requires careful, step-by-step debugging. Compare the output of each intermediate step in Alteryx with the corresponding intermediate dataset in Dataiku to pinpoint exactly where the logic diverges. Common culprits are differences in join types, filter conditions, or floating-point precision.
`},{id:51,slug:"creating-dashboards-inside-dataiku",question:"How to get started with + creating dashboards inside Dataiku?",answer:`
### 1. Introduction/Overview
Dashboards are used to communicate insights from your data to a broader audience. Dataiku has a built-in dashboarding engine that allows you to combine charts, metrics, text, and other insights into a single, shareable view.

### 2. Prerequisites
- **Data to visualize:** You need at least one dataset in your project.
- **Insights to share:** You should have already created some charts or computed some metrics that you want to display.

### 3. Step-by-Step Instructions
1.  **Create Your Charts First:**
    *   Dashboards are for displaying existing insights. You must create the content first.
    *   Open a dataset and go to the **Charts** tab.
    *   Create one or more charts (e.g., a bar chart, line chart). Give each chart a clear name and save it.
2.  **Create a New Dashboard:**
    *   In your project's top navigation bar, go to **Dashboards**.
    *   Click **+ NEW DASHBOARD**.
    *   Give it a name and choose a layout (e.g., grid-based).
3.  **Add Content (Tiles):**
    *   Your new dashboard is a blank canvas. Click **+ ADD A TILE**.
    *   A dialog will appear, allowing you to add different types of content:
        *   **Chart:** Select a chart you previously saved from a dataset.
        *   **Metric:** Select a metric you computed on a dataset's "Status" tab.
        *   **Dataset View:** Show a preview of a dataset.
        *   **Text:** Add titles, explanations, and context.
4.  **Arrange the Layout:**
    *   You can drag and drop the tiles to arrange them on the dashboard.
    *   Resize the tiles by dragging their corners.
    *   Use **Text** tiles to create headers and sections to organize your dashboard.
5.  **View and Share:** Click the **View** button to see the final dashboard. You can share it with other Dataiku users by sending them the link.

### 4. Resources and Tools
- **Charts Tab:** The workspace for creating the individual visualizations.
- **Dashboards Page:** The canvas for arranging your tiles and building the final report.
- **Tile Editor:** The dialog for adding and configuring content on your dashboard.

### 5. Next Steps and Progression
- **Interactivity:** Add dashboard-level filters. This allows viewers to filter all the charts on the dashboard at once (e.g., by date range or by country).
- **Automation:** Create a **Scenario** that automatically rebuilds the datasets on your dashboard and then refreshes the dashboard caches, ensuring the data is always up-to-date.
- **Exporting and Reporting:** Use a scenario reporter to automatically export your dashboard as a PDF and email it to stakeholders on a schedule.

### 6. Common Challenges and Solutions
- **Challenge:** "My chart is not showing up in the 'Add Tile' list."
- **Solution:** You must explicitly save the chart in the "Charts" tab of the dataset first. If you don't save it, it won't be available to add to a dashboard.
- **Challenge:** "The data on my dashboard is stale."
- **Solution:** The data for charts is cached for performance. You need to set up a scenario to automatically rebuild the underlying datasets and then add a "Refresh dashboard caches" step to force the charts to update.
`},{id:52,slug:"exporting-datasets-to-tableau-power-bi",question:"How to get started with + exporting datasets to Tableau/Power BI?",answer:`
### 1. Introduction/Overview
While Dataiku has its own dashboarding tools, many organizations have standardized on BI tools like Tableau or Power BI. Dataiku integrates easily into this ecosystem by allowing you to export your final, prepared datasets to a location that these tools can access.

### 2. Prerequisites
- **A final dataset:** The Dataiku dataset you want to visualize in your BI tool.
- **A shared data location:** You need a storage system that both Dataiku and your BI tool can connect to. A **SQL database** (like Snowflake, SQL Server, or PostgreSQL) is the most common and recommended choice.
- **Connection configured:** The connection to this shared database must be configured in Dataiku.

### 3. Step-by-Step Instructions
1.  **Select Your Final Dataset:** In your Dataiku Flow, select the dataset you want to export. This should be a clean, aggregated dataset ready for reporting.
2.  **Create an Export Recipe:**
    *   From the right-hand panel, click **+ RECIPE** and choose **Export**.
3.  **Configure the Export Destination:**
    *   In the Export recipe, click **Add Export**.
    *   Choose your shared data location. Select your configured **SQL database connection**.
    *   Specify the name of the new table you want to create in the database (e.g., \`TABLEAU_SALES_REPORT\`).
    *   Choose the write mode (usually "Overwrite" to replace the table with fresh data each time).
4.  **Run the Export:** Run the Export recipe. Dataiku will now take the data from your dataset and write it into a new table in your SQL database.
5.  **Connect Your BI Tool:**
    *   In Tableau or Power BI, create a new data source.
    *   Connect to the same SQL database.
    *   Select the \`TABLEAU_SALES_REPORT\` table you just created.
    *   You can now build your dashboards in your BI tool using this data.

### 4. Resources and Tools
- **Export Recipe:** The key Dataiku tool for writing data to external systems.
- **SQL Database:** The recommended intermediary for connecting Dataiku and BI tools.
- **BI Tool Connector:** The data source connector in Tableau or Power BI for your specific database.

### 5. Next Steps and Progression
- **Automation:** Create a **Scenario** in Dataiku that rebuilds your entire pipeline and ends with the Export recipe. Schedule this scenario to run daily to keep the data in your BI tool fresh.
- **Live Connection (Advanced):** Some BI tools offer connectors that can query Dataiku directly via its REST API, allowing for live connections without an intermediate database. However, this is often less performant for large datasets than the export-to-database method.

### 6. Common Challenges and Solutions
- **Challenge:** "The export recipe fails with a database error."
- **Solution:** This is usually a permissions issue. Check that the user credentials Dataiku is using for the database connection have \`CREATE TABLE\` and \`INSERT\` permissions in the target schema.
- **Challenge:** "Should I export the raw data or prepared data?"
- **Solution:** Always export the final, prepared, and often aggregated data. The heavy data preparation and transformation should be done in Dataiku. The BI tool should be used primarily for visualization, not complex ETL.
`},{id:53,slug:"embedding-dataiku-insights-into-bi-tools",question:"How to get started with + embedding Dataiku insights into BI tools?",answer:`
### 1. Introduction/Overview
In addition to exporting full datasets, you can also embed individual charts or dashboards from Dataiku directly into other web-based platforms, including some BI tools that support web content embedding. This is useful for sharing a specific, key visualization without recreating it.

### 2. Prerequisites
- **A saved chart or dashboard in Dataiku.**
- **A BI tool or web portal that supports embedding web content** (usually via an \`<iframe>\`).
- **Shared user access:** The user viewing the embedded chart in the BI tool must also have at least read access to that chart in Dataiku.

### 3. Step-by-Step Instructions
1.  **Navigate to the Chart or Dashboard in Dataiku:** Open the insight you want to embed.
2.  **Find the Share Button:**
    *   On a chart or dashboard, look for a **Share** button or icon.
3.  **Get the Embed Code:**
    *   Clicking "Share" will open a dialog. Go to the **Embed** tab.
    *   This will provide you with an HTML snippet, which will be an \`<iframe>\` tag.
    *   Copy this \`<iframe>\` code.
4.  **Embed in Your BI Tool:**
    *   Go to your BI tool (e.g., Tableau, Power BI, or another web portal).
    *   Find the feature that allows you to add "Web Content" or "Embed" a URL or HTML.
    *   Paste the \`<iframe>\` code you copied from Dataiku.
5.  **View the Result:** The Dataiku chart should now appear as a widget inside your BI dashboard. When a user views it, their browser will render the content directly from the Dataiku server.

### 4. Resources and Tools
- **Share/Embed Feature:** The UI in Dataiku for getting the embed code.
- **\`<iframe>\` HTML Tag:** The standard web technology used for embedding.

### 5. Next Steps and Progression
- **Passing Filters:** For advanced use cases, you can sometimes pass filter values from the BI tool to the embedded Dataiku chart via URL parameters, allowing for some interactivity. This requires custom development.
- **Authentication:** This method relies on the user having an active session in both the BI tool and Dataiku. For a more seamless experience in an enterprise setting, you would need to set up Single Sign-On (SSO) between your BI tool and Dataiku.

### 6. Common Challenges and Solutions
- **Challenge:** "The embedded chart shows a login screen or an error."
- **Solution:** This is an authentication issue. The user viewing the dashboard does not have a valid, active session in Dataiku, or they do not have permission to view that specific chart. They must be logged into Dataiku and have at least read access to the project containing the chart.
- **Challenge:** "The embedded chart is interactive, but it feels slow."
- **Solution:** The performance is dependent on the Dataiku server. If the chart is complex, it may take time to render. This method is best for simple, summary charts. For highly interactive, large-scale dashboards, the recommended approach is to export the data to a database and build the dashboard natively in the BI tool.
`},{id:54,slug:"designing-kpi-dashboards-within-dss",question:"How to get started with + designing KPI dashboards within DSS?",answer:`
### 1. Introduction/Overview
A KPI (Key Performance Indicator) dashboard provides a high-level, at-a-glance view of the most important business metrics. In Dataiku, you can design these dashboards by first computing your KPIs and then displaying them as prominent "metric" tiles.

### 2. Prerequisites
- **A clear definition of your KPIs:** You must know which metrics you need to track (e.g., "Total Sales this Month," "Daily Active Users").
- **A dataset containing the data** needed to calculate these KPIs.

### 3. Step-by-Step Instructions
#### Part 1: Compute and Store the KPIs
1.  **Calculate the KPIs:** Use a recipe (like **Group** or **Window**) to calculate your KPI values. The result should be a dataset where each row represents a KPI. For example, a dataset with one row and columns like \`total_sales\`, \`active_users\`, etc.
2.  **Create Metrics from the Dataset:**
    *   Open your new KPI dataset and go to the **Status** tab.
    *   Click on **Metrics**.
    *   Click **+ ADD METRIC** and choose **Column statistics**.
    *   Select the column containing your KPI value (e.g., \`total_sales\`). For the aggregation, choose **Value of first row** (since your dataset has only one row).
    *   Give the metric a clear name, like \`KPI_Total_Sales\`.
    *   Repeat this for each KPI.

#### Part 2: Build the Dashboard
1.  **Create a New Dashboard:** Go to **Dashboards > + NEW DASHBOARD**.
2.  **Add Metric Tiles:**
    *   On your blank dashboard, click **+ ADD A TILE**.
    *   Choose **Metric** as the tile type.
    *   Select the KPI metric you just created (e.g., \`KPI_Total_Sales\`).
    *   Configure the tile's appearance (e.g., add a text label, change color).
3.  **Arrange the Dashboard:** Add a tile for each of your key KPIs. Arrange them prominently at the top of your dashboard. You can supplement them with charts for historical trends.
4.  **Automate the Refresh:** Create a **Scenario** that first rebuilds your KPI dataset, then recomputes the metrics, and finally refreshes the dashboard caches. Schedule this to run regularly.

### 4. Resources and Tools
- **Group Recipe:** Often used to aggregate data to the KPI level.
- **Status Tab (Metrics):** The UI for defining the metrics that will be displayed on the dashboard.
- **Dashboard Metric Tile:** The specific widget used to display a single, large number (your KPI).

### 5. Next Steps and Progression
- **Trend Indicators:** In the metric tile settings, you can add a "trend" by comparing the latest value to a previous value (e.g., from a different partition), showing an up or down arrow.
- **Alerting:** In your scenario, add a **Run checks** step on your metrics. You can set a check that fails if a KPI goes above or below a certain threshold, triggering an alert.

### 6. Common Challenges and Solutions
- **Challenge:** "My KPI value on the dashboard is not updating."
- **Solution:** You have missed a step in the automation. Your scenario must do three things in order: 1. **Build** the dataset containing the KPI value. 2. **Update** the metrics on that dataset. 3. **Refresh** the dashboard caches. If any step is missing, the dashboard will show stale data.
- **Challenge:** "The metric tile shows a weird number."
- **Solution:** Check the aggregation you chose when creating the metric. If your KPI dataset has multiple rows, using "Sum" or "Average" might give you an unexpected result. Ensure your KPI dataset is aggregated correctly first.
`},{id:55,slug:"scheduling-excel-report-generation-in-dataiku",question:"How to get started with + scheduling Excel report generation in Dataiku?",answer:`
### 1. Introduction/Overview
Many business processes still rely on Excel reports. Dataiku can automate the entire process of generating and distributing these reports, saving significant manual effort and ensuring consistency. The process involves creating the dataset for the report and then using a Scenario to export and distribute it.

### 2. Prerequisites
- **A final dataset:** The dataset in your Dataiku Flow that you want to export as an Excel file.
- **A destination for the file:** Decide where the generated Excel file should be saved (e.g., a shared network drive, a cloud storage bucket, or just sent via email).

### 3. Step-by-Step Instructions
1.  **Prepare the Data:** In your Flow, create the final, clean, and aggregated dataset that will be the content of your Excel report.
2.  **Create an Export Recipe:**
    *   Select your final dataset.
    *   From the right-hand panel, choose the **Export** recipe.
    *   Click **Add Export**.
    *   For the destination, you can choose a **Managed Folder** (to save it within Dataiku) or a connection to an external filesystem like **S3** or a **File Server**.
    *   Set the **Format** to **Excel (.xlsx)**.
3.  **Create an Automation Scenario:**
    *   Go to **Scenarios** and create a new scenario.
4.  **Define the Steps:**
    *   **Step 1:** Add a **Build / Train** step. In this step, build the **Export** recipe you just created. This will first build the final dataset and then run the export, generating the Excel file.
    *   **(Optional) Step 2: Distribute the Report.** See "Next Steps" below.
5.  **Schedule the Scenario:**
    *   Go to the **Settings > Triggers** tab and add a **Time-based** trigger to run this report generation on your desired schedule (e.g., every Monday at 9 AM).

### 4. Resources and Tools
- **Export Recipe:** The tool to convert a Dataiku dataset into an Excel file.
- **Managed Folders:** A convenient place to store the generated files within Dataiku's managed storage.
- **Scenarios:** The engine for automating and scheduling the entire process.

### 5. Next Steps and Progression
- **Email Distribution:** In your scenario, add a **Reporter**. Configure a **Mail** reporter. You can configure this email to attach the file directly from the managed folder where the Export recipe saved it.
- **Multiple Sheets:** The Excel export recipe has options to export multiple datasets as different sheets within the same Excel workbook.
- **Dynamic Filenames:** Use project variables in the output path of your Export recipe to create dynamically named files, e.g., \`sales_report_\${current_date}.xlsx\`.

### 6. Common Challenges and Solutions
- **Challenge:** "The Export recipe fails with a permissions error."
- **Solution:** If you are exporting to an external filesystem (like a shared network drive or S3), ensure the user account that the Dataiku server runs as has write permissions to that specific folder.
- **Challenge:** "The Excel file has formatting issues."
- **Solution:** The default Excel export produces a raw data dump. For highly customized formatting (colors, fonts, pivot tables), the standard approach is to use Dataiku to generate the clean data and then use a tool with stronger Excel integration (like a Python script with a library like \`openpyxl\`) or a BI tool to create the final, formatted report.
`},{id:56,slug:"automating-slack-or-email-report-distribution",question:"How to get started with + automating slack or email report distribution?",answer:`
### 1. Introduction/Overview
Distributing reports and alerts is a key part of any automated data pipeline. Dataiku's **Scenarios** have a powerful feature called **Reporters** that can send customized messages and attachments to services like Email and Slack, keeping stakeholders informed automatically.

### 2. Prerequisites
- **An output to deliver:** This could be a Dataiku Dashboard, a dataset, or a file saved in a managed folder.
- **A scenario** that builds this output.
- **Service Configuration (Admin Task):** Your Dataiku administrator must have configured the integration with your company's email server and/or Slack workspace in **Administration > Settings**.

### 3. Step-by-Step Instructions
1.  **Navigate to Your Scenario:** Go to **Scenarios** and open the one you want to add notifications to.
2.  **Open the Reporters Tab:** Click on the **Reporters** tab.
3.  **Add a New Reporter:** Click **+ ADD REPORTER**.
4.  **Choose a Channel:** Select either **Mail** or **Slack**.
5.  **Configure the Reporter:**
    *   **Run Condition:** Decide when the message should be sent (e.g., \`On failure\`, \`On success\`, \`On completion\`).
    *   **Recipients:** Enter the destination email addresses or the Slack channel name.
    *   **Message:** Write the body of your message. You can use variables like \`\${scenarioName}\`, \`\${outcome}\`, and \`\${jobURL}\` for context. For example: "Here is the daily sales report for \${current_date}."
6.  **Add an Attachment:**
    *   This is the key step for delivering an output. In the reporter's configuration, find the "Attachments" section.
    *   **To send a Dashboard:** Select "Dashboard" and choose the dashboard you want to send from the dropdown. You can select the format (e.g., PDF or PNG).
    *   **To send a Dataset:** Select "Dataset" and choose the dataset. It will be attached as a CSV file.
    *   **To send a File:** Select "File from managed folder" and provide the path to the file (e.g., an Excel report generated by an Export recipe).
7.  **Save and Schedule:**
    *   Save the scenario.
    *   Ensure your scenario is scheduled to run regularly using a **Time-based trigger** in the "Settings" tab.

### 4. Resources and Tools
- **Reporters:** The main feature for configuring all automated communications.
- **Attachments Configuration:** The specific section within a reporter for adding your outputs.
- **Export Recipe:** Use this before the reporter step to generate a nicely formatted Excel file if needed.

### 5. Next Steps and Progression
- **Conditional Delivery:** Use a Python scenario step to check a condition. The script could then set a project variable (e.g., \`should_send_report = True\`). You can then use this variable to make the reporter's execution conditional.
- **Customized Content:** For highly customized email bodies, a Python step can generate HTML content, save it to a variable, and you can then use that variable in the email reporter body.

### 6. Common Challenges and Solutions
- **Challenge:** "The attached file is too large for the email server."
- **Solution:** Don't attach the file directly. Instead, have your pipeline export the file to a shared location (like a cloud storage bucket or SharePoint). Then, in your reporter message, include a *link* to the file instead of the file itself.
- **Challenge:** "The dashboard PDF export looks strange or is cut off."
- **Solution:** The PDF export of a dashboard is sensitive to the layout. You may need to adjust the size and arrangement of your dashboard tiles to ensure they fit nicely onto a standard page format for the PDF export. Try to use standard slide sizes (like 16:9) for your dashboard layout.
`},{id:57,slug:"exporting-model-predictions-for-stakeholder-review",question:"How to get started with + exporting model predictions for stakeholder review?",answer:`
### 1. Introduction/Overview
Once you have trained a model and used it to make predictions, you often need to share these predictions with business stakeholders for review, validation, or action. This process can be easily automated in Dataiku.

### 2. Prerequisites
- **A deployed "Saved Model"** in your Flow.
- **A dataset to score:** A dataset containing new records for which you want to generate predictions.
- **An understanding of the desired output format** (e.g., CSV, Excel).

### 3. Step-by-Step Instructions
1.  **Generate the Predictions (Score Recipe):**
    *   In your Flow, select the dataset you want to score.
    *   From the right-hand panel, choose the **Score** recipe.
    *   Select your deployed Saved Model.
    *   Run the recipe. This creates a new output dataset that includes the original data plus new columns for the prediction, probabilities, etc.
2.  **Prepare the Output (Optional but Recommended):**
    *   The output of the Score recipe can be technical. It's good practice to add a **Prepare** recipe after it to create a clean, human-readable report.
    *   In this Prepare recipe, you can:
        *   Remove unnecessary columns.
        *   Rename technical columns (e.g., rename \`prediction_churn\` to \`Predicted_Churn_Status\`).
        *   Create a clean "confidence" score from the probabilities.
3.  **Export the Final Report (Export Recipe):**
    *   Select your cleaned-up prediction dataset.
    *   From the right-hand panel, choose the **Export** recipe.
    *   Configure it to save the data in the desired format (e.g., **Excel**) and to a desired location (e.g., a **Managed Folder**).
4.  **Automate and Distribute:**
    *   Create a **Scenario** that builds the Export recipe.
    *   Add a **Reporter** to the scenario to automatically email the exported file to your stakeholders on a recurring schedule.

### 4. Resources and Tools
- **Score Recipe:** The tool for applying a model to new data.
- **Prepare Recipe:** Essential for cleaning up the output for a non-technical audience.
- **Export Recipe:** The tool for converting the final dataset into a shareable file format.
- **Scenarios & Reporters:** The engine for automating the entire generation and distribution process.

### 5. Next Steps and Progression
- **Interactive Webapp:** For a more interactive review process, build a simple **Dataiku Webapp**. The webapp could display a filterable table of the predictions and allow stakeholders to explore the results directly in their browser.
- **Writing to a BI Tool Table:** Instead of a file, use the Export recipe to write the predictions to a table that feeds a Tableau or Power BI dashboard for stakeholder review.

### 6. Common Challenges and Solutions
- **Challenge:** "The stakeholders are confused by the raw output of the Score recipe."
- **Solution:** This is why the optional "Prepare the Output" step is so important. Never show raw prediction logs or probabilities to a business audience. Always create a clean, simple, and well-documented final report with clear column names.
- **Challenge:** "The file is too large to email."
- **Solution:** In the Export recipe, instead of exporting to a managed folder to attach, export the file to a shared cloud storage location (like S3 or SharePoint). Then, have the email reporter send a *link* to the file instead of attaching it.
`},{id:58,slug:"building-stakeholder‑ready-reports-in-dataiku",question:"How to get started with + building stakeholder‑ready reports in Dataiku?",answer:`
### 1. Introduction/Overview
A stakeholder-ready report is more than just data; it's a narrative that provides context, summarizes key findings, and presents insights in a clear, digestible way. Dataiku **Dashboards** are the perfect tool for creating these reports, as they allow you to combine data, visualizations, and explanatory text.

### 2. Prerequisites
- **A clear audience and purpose:** Who are you building this for, and what one or two key messages do you want them to take away?
- **Finalized data and charts:** Your underlying data pipelines should be complete, and you should have already created the key charts and metrics you want to present.

### 3. Step-by-Step Instructions
1.  **Structure Your Narrative (Storyboarding):** Before you build, plan your report's story. A good structure is:
    *   **Title and Executive Summary:** What is this report about and what is the main conclusion?
    *   **Key Metrics (KPIs):** The most important high-level numbers.
    *   **Supporting Visualizations:** Charts and graphs that provide details and trends.
    *   **Commentary and Next Steps:** Your interpretation of the results and what should happen next.
2.  **Create a New Dashboard:** In your project, go to **Dashboards > + NEW DASHBOARD**.
3.  **Build the Report Using Tiles:**
    *   **Title:** Use a large **Text** tile for the report title.
    *   **Executive Summary:** Add another Text tile below the title with a few bullet points summarizing the key findings.
    *   **KPIs:** Add **Metric** tiles for your most important KPIs. Arrange them in a prominent row at the top.
    *   **Charts:** Add the pre-saved **Chart** tiles that support your narrative. Give each chart a clear title.
    *   **Context:** Intersperse your charts with smaller Text tiles that explain what the viewer is looking at and what the key insight is.
4.  **Refine the Layout:** Arrange and resize the tiles to create a clean, logical flow that guides the viewer's eye through the story you've constructed.
5.  **Share with Stakeholders:** Once complete, you can share a direct link to the dashboard with other Dataiku users.

### 4. Resources and Tools
- **Dashboards:** The canvas for building your report.
- **Text Tiles:** Crucial for adding the narrative, context, and explanations that turn a collection of charts into a real report.
- **Metric and Chart Tiles:** The building blocks for displaying your data.

### 5. Next Steps and Progression
- **Automated PDF/Email Reporting:** Create a **Scenario** with a **Reporter** to automatically export a PDF of the dashboard and email it to your stakeholders on a recurring schedule.
- **Interactive Filters:** Add dashboard-level filters (e.g., a date range dropdown) to allow stakeholders to self-serve and explore the data for different time periods.
- **Dataiku Apps:** For a fully interactive, guided experience, consider building a Dataiku Webapp that walks users through the results step-by-step.

### 6. Common Challenges and Solutions
- **Challenge:** "My dashboard is too cluttered and confusing."
- **Solution:** You are trying to show too much. A good report focuses on a few key messages. Be ruthless about removing any chart or metric that doesn't directly support your main narrative. Use whitespace and text tiles to create clear sections.
- **Challenge:** "Stakeholders don't understand what the charts mean."
- **Solution:** You haven't provided enough context. Every chart should have a descriptive title, and you should add a text tile next to it that says, "This chart shows X, and the key takeaway is Y." Never assume a chart can speak for itself.
`},{id:59,slug:"documenting-pipelines-and-outputs-effectively",question:"How to get started with + documenting pipelines and outputs effectively?",answer:`
### 1. Introduction/Overview
Effective documentation is crucial for making your data projects understandable, maintainable, and trustworthy. It helps your colleagues (and your future self) understand what a pipeline does and why. Dataiku is designed for this, with built-in documentation features at every level.

### 2. Prerequisites
- **A completed Dataiku project or pipeline.**
- **An understanding of the project's purpose and logic.**

### 3. Step-by-Step Instructions: A Multi-Layered Approach
1.  **High-Level Documentation (The "What and Why"):**
    *   **Use the Project Wiki:** Every project has a built-in Wiki. Use the homepage of the Wiki as a "ReadMe" file.
    *   Document the project's **business purpose**, the **key data sources**, the **main outputs**, and who the **project owners** are.
2.  **Flow-Level Documentation (The "How"):**
    *   **Use Descriptions on Every Object:** This is the most important habit to build. Every dataset and every recipe has a **Summary** tab with a "Description" field.
        *   For a **dataset**, describe what data it contains (e.g., "Cleaned customer data with one row per active customer").
        *   For a **recipe**, describe its action (e.g., "Filters out test users and joins customer data with sales data").
    *   These descriptions are visible directly in the Flow, making it self-documenting.
    *   **Use Flow Zones and Text Boxes:** Organize your flow into named zones (e.g., "Ingestion," "Preparation"). You can also add text boxes directly to the flow to act as large comments for different sections.
3.  **Column-Level Documentation (The "Data Dictionary"):**
    *   **Use Column Descriptions:** Open a dataset, go to the "Settings" tab. You can edit the schema and add a description for each individual column, explaining what it means and how it was calculated. This creates a data dictionary.
4.  **Code-Level Documentation:**
    *   **Use Comments and Docstrings:** In any Python or SQL recipe, use comments (\`#\` or \`--\`) and docstrings to explain complex parts of your code.

### 4. Resources and Tools
- **The Description Field:** The most critical documentation feature, available on all Dataiku objects.
- **Project Wiki:** For long-form documentation, project plans, and meeting notes.
- **Flow Zones & Text Boxes:** For visually organizing and annotating the Flow.

### 5. Next Steps and Progression
- **Automated Documentation Generation:** For advanced use cases, you can write a Python script using the Dataiku API that loops through all project objects, extracts their metadata (names, descriptions, tags, schema), and generates a formal documentation website or document.
- **Standardized Templates:** Create a documentation template in your Wiki that all new projects should follow to ensure consistency.

### 6. Common Challenges and Solutions
- **Challenge:** "Nobody has time to write documentation."
- **Solution:** Make it part of the process. Documentation shouldn't be an afterthought. The best time to write the description for a recipe is right after you've built it, while the logic is still fresh in your mind. Make it a part of your team's "definition of done."
- **Challenge:** "The documentation is out of date."
- **Solution:** Make updating the documentation a part of every change request. If you modify a recipe, you must also update its description to reflect the change.
`},{id:60,slug:"training-end-users-on-self-service-analytics",question:"How to get started with + training end users on self-service analytics?",answer:`
### 1. Introduction/Overview
Empowering business users to perform their own analysis (self-service) is a key goal of modern data platforms. Training is essential for this. The goal is not to turn them into data engineers, but to give them the confidence to explore data, use pre-built applications, and answer their own questions within a safe and governed environment.

### 2. Prerequisites
- **A target group of end-users:** Identify the business team you want to enable.
- **A well-prepared Dataiku project:** You need a project with clean, certified "golden" datasets and potentially some pre-built dashboards or Dataiku Apps for them to use.
- **A training plan.**

### 3. Step-by-Step Instructions: A Training Framework
1.  **Start with the "Why":** Begin the training by explaining what Dataiku is and how it will help them in their day-to-day job (e.g., "get answers faster," "build your own reports without waiting for IT").
2.  **Focus on Consumption, Not Creation:**
    *   The first session should be entirely about how to **consume** information.
    *   Show them how to navigate to a **Dashboard**, how to use the interactive filters, and how to export charts or data.
    *   If you have a **Dataiku App**, walk them through how to use its simple UI.
3.  **Introduce Data Exploration:**
    *   Show them how to open a certified ("golden") dataset.
    *   Teach them two key skills:
        *   **Filtering:** How to use the "Filter" dialog to quickly slice the data.
        *   **Basic Charts:** How to use the **Charts** tab to create a simple bar chart or line chart by dragging and dropping columns.
4.  **Provide a Safe Sandbox:**
    *   Give them "Reader" access to production projects but "Writer" access to a dedicated "Sandbox" project.
    *   Encourage them to upload their own small CSV files and experiment with the **Prepare** recipe in their sandbox.
5.  **Hold "Office Hours":** Schedule regular, optional sessions where users can drop in and ask questions or get help with a specific analysis they are trying to perform.

### 4. Resources and Tools
- **Dataiku Dashboards and Apps:** The primary tools for end-user consumption.
- **Certified "Golden" Datasets:** Provide clean, reliable data for them to explore. Tag these datasets clearly.
- **Dataiku Academy:** The "Business User" learning path is designed specifically for this audience.

### 5. Next Steps and Progression
- **Advanced Charting:** Hold a follow-up session on more advanced chart types and customization options.
- **Visual Recipes:** For power users, introduce the basic visual recipes like **Prepare** and **Group**, showing them how they can perform their own simple data manipulations.
- **Showcase Success:** When a business user successfully builds their own report, showcase it to the wider team. Success stories are the best way to encourage adoption.

### 6. Common Challenges and Solutions
- **Challenge:** "Users are overwhelmed and afraid they will break something."
- **Solution:** Emphasize the separation between production projects (where they have read-only access) and their personal sandbox (where they are free to do anything). Reassure them that they cannot break the production data.
- **Challenge:** "Users are creating messy flows and incorrect analyses."
- **Solution:** This is expected. The goal of self-service is not to create perfectly engineered pipelines. It's about enabling quick exploration. Use office hours to gently guide them toward best practices, but don't be overly critical of their methods. Focus on whether they got the right answer.
`},{id:61,slug:"documenting-dataiku-flows-and-steps",question:"How to get started with + documenting Dataiku flows and steps?",answer:`
### 1. Introduction/Overview
Clear documentation is the difference between a maintainable, long-lasting data project and a confusing, one-off script. Dataiku is built with documentation in mind, providing features to describe your work at every level of detail, from a high-level project overview to comments on individual transformation steps.

### 2. Prerequisites
- A Dataiku project with a Flow.
- An understanding of the "why" behind your pipeline—what business problem does it solve?

### 3. Step-by-Step Instructions: A 3-Tier Documentation Strategy

#### Tier 1: The 10,000-Foot View (Project-Level)
- **What:** Document the overall purpose of the project.
- **How:** Use the **Project Wiki**. The first page should be a "ReadMe" that covers:
    - **Goal:** A one-sentence summary of what the project achieves.
    - **Key Outputs:** Links to the most important final datasets or dashboards.
    - **Data Sources:** Where the raw data comes from.
    - **Project Owner/Team:** Who to contact with questions.

#### Tier 2: The 1,000-Foot View (Flow-Level)
- **What:** Explain the purpose of each major component in your Flow.
- **How:**
    - **Use Descriptions on All Objects:** This is the most crucial habit. Click on any dataset or recipe, go to the **Summary** tab, and write a clear, one-sentence description. These descriptions appear when you hover over items in the Flow.
    - **Use Flow Zones:** Group your flow into logical zones like \`Ingestion\`, \`Preparation\`, \`Modeling\` to make the architecture obvious.
    - **Use Text Boxes:** Drag text boxes onto the Flow canvas to act as large "comments" for different sections.

#### Tier 3: The 10-Foot View (Recipe-Level)
- **What:** Explain the logic of specific, complex transformations.
- **How:**
    - **Inside a Prepare Recipe:** You can add a description to each individual processor step. This is useful for explaining a complex formula or filter.
    - **Inside a Code Recipe (Python/SQL):** Use code comments (\`#\` or \`--\`) and docstrings to explain what your code is doing.

### 4. Resources and Tools
- **Project Wiki:** For long-form, narrative documentation.
- **Description Fields:** For concise, contextual documentation on every object.
- **Flow Zones:** For high-level structural documentation.

### 5. Next Steps and Progression
- **Create a Documentation Standard:** Document a simple standard for your team (e.g., "All recipes must have a description") and enforce it during code reviews.
- **Data Dictionary:** Use the column description fields in a dataset's "Settings" to create a detailed data dictionary that can be shared with users.
- **Automated Doc Generation:** For advanced needs, use the Dataiku API to write a script that extracts all descriptions and metadata from a project to generate a formal document.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't have time to write documentation."
- **Solution:** Integrate it into your workflow. The best time to document a recipe is immediately after you build it. It only takes 30 seconds to write a good description, and it will save hours of confusion later.
- **Challenge:** "Our documentation is always out of date."
- **Solution:** Make documentation updates a required part of any change. If you modify a recipe, you must also update its description. This ensures the documentation evolves with the project.
`},{id:62,slug:"annotating-datasets-and-recipes-for-governance",question:"How to get started with + annotating datasets and recipes for governance?",answer:`
### 1. Introduction/Overview
Data governance involves managing your data assets to ensure they are high-quality, secure, and compliant. Annotation—adding metadata like tags and descriptions—is the foundational practice for good governance in Dataiku. It makes your data assets discoverable, understandable, and manageable.

### 2. Prerequisites
- **A Dataiku project with datasets and recipes.**
- **A basic governance policy:** An idea of what information you need to track (e.g., data sensitivity, ownership, status).

### 3. Step-by-Step Instructions: Key Annotation Tools

#### 1. Using Tags
- **What they are:** Tags are simple, color-coded labels you can attach to any object (dataset, recipe, model, etc.).
- **How to use:**
    1.  Open any object, like a dataset.
    2.  In the **Summary** tab, find the "Tags" section.
    3.  Type a new tag (e.g., \`PII\`, \`Source:Salesforce\`, \`Status:Validated\`) and hit Enter.
- **Why they are useful:** Tags are searchable across the entire Dataiku instance, making it easy to find all datasets containing PII or all recipes related to a specific source system.

#### 2. Using Descriptions
- **What they are:** A text field on every object to explain its purpose.
- **How to use:**
    1.  Open any object.
    2.  In the **Summary** tab, fill in the "Description" field.
    -   For a dataset: "Weekly sales transactions for the EU region."
    -   For a recipe: "Joins sales data with customer demographics."
- **Why they are useful:** Descriptions provide essential context directly in the Flow when you hover over an item.

#### 3. Using Custom Metadata
- **What it is:** A set of key-value pairs for tracking structured information.
- **How to use:**
    1.  Open any object.
    2.  In the **Summary** tab, find the "Custom Metadata" section.
    3.  Click **+ Add** and define a key and value (e.g., \`Owner\`: \`Finance Team\`, \`Data Quality Score\`: \`95%\`).
- **Why it is useful:** For tracking formal, structured attributes that are consistent across projects.

### 4. Resources and Tools
- **The Summary Tab:** The central place on every Dataiku object for adding tags, descriptions, and custom metadata.
- **The Data Catalog:** The central, searchable catalog in Dataiku that is populated by these annotations.

### 5. Next Steps and Progression
- **Establish a Tagging Taxonomy:** Define a standard set of tags for your organization to use for concepts like data sensitivity, sources, and status. Document this in your Wiki.
- **Automated Tagging:** Use a Python scenario script to automatically apply tags based on rules. For example, a script could scan column names and automatically add the \`PII\` tag to any dataset containing a column named \`email\` or \`social_security_number\`.
- **Governance Workflows:** Build scenarios that use this metadata, for example, a scenario that checks if any dataset tagged as \`PII\` is being used in an insecure way.

### 6. Common Challenges and Solutions
- **Challenge:** "People forget to add tags and descriptions."
- **Solution:** Make it part of your team's process. Include "add appropriate tags and description" as a required item on your "definition of done" checklist for any new development.
- **Challenge:** "Our tags are a mess; everyone uses different ones for the same thing."
- **Solution:** This is why establishing and communicating a standard tagging taxonomy is so important. Hold a workshop to agree on the standard tags and publish the list in a visible place.
`},{id:63,slug:"applying-data-quality-checks-automatically",question:"How to get started with + applying data quality checks automatically?",answer:`
### 1. Introduction/Overview
Automated data quality checks are your first line of defense against data issues. They allow you to programmatically validate your data against a set of rules and can be configured to stop your pipelines if bad data is detected, preventing it from corrupting downstream models and reports.

### 2. Prerequisites
- **A dataset in your Flow** that you want to monitor.
- **A clear definition of "good data":** You need to know the specific rules your data must follow.

### 3. Step-by-Step Instructions

#### Part 1: Defining the Quality Rules on the Dataset
1.  **Navigate to the Status Tab:** Open the dataset you wish to check and click on the **Status** tab.
2.  **Define Metrics:** First, you need to tell Dataiku what to measure.
    *   Click on **Metrics**.
    *   Click **+ ADD METRIC** and choose a metric to compute, such as "Record count" or "Column statistics" (which includes min, max, mean, etc. for a column).
    *   Click **SAVE AND COMPUTE**.
3.  **Define Checks:** Now, you'll define the pass/fail rules based on those metrics.
    *   Click on **Checks**.
    *   Click **+ ADD CHECK**. You'll see a library of check types.
    *   Example: Select **Column value in numerical range**. Configure it to check your \`price\` column and ensure the values are between 0 and 1000.
    *   Example: Select **Column is never empty**. Configure it to check that your \`customer_id\` column has no missing values.
4.  **Save the Checks:** Once you have defined your checks, save them. You can run them manually here to see if the current data passes.

#### Part 2: Automating the Checks in a Scenario
1.  **Create a Scenario:** Go to **Scenarios** and create a new scenario (or edit an existing one).
2.  **Add a Build Step:** The first step should be to build the dataset you want to check, to ensure you're validating the latest data.
3.  **Add the "Run Checks" Step:**
    *   Click **+ ADD STEP** and select **Run checks**.
    *   In the step's configuration, choose your dataset from the dropdown menu.
4.  **Configure Failure Handling:** By default, if any check with a severity of "Error" fails, this step will fail, and the entire scenario will stop.
5.  **Schedule and Alert:** Schedule your scenario and add a **Reporter** to send an email or Slack alert on failure. The alert will notify you that a data quality rule has been violated.

### 4. Resources and Tools
- **Status Tab (Metrics & Checks):** The interface for setting up your data quality rules.
- **"Run checks" Scenario Step:** The key automation component that executes your rules.

### 5. Next Steps and Progression
- **Custom Python Checks:** If the built-in checks are not sufficient, you can write your own custom checks using Python for more complex validation logic.
- **Data Quality Dashboard:** Create a dashboard that visualizes the history of your metrics (e.g., a line chart of the row count over time). This helps you spot trends and anomalies.

### 6. Common Challenges and Solutions
- **Challenge:** "My pipeline failed. How do I find out which quality check was violated?"
- **Solution:** Go to the log of the failed scenario run. The log for the "Run Checks" step will clearly state which check failed and why (e.g., "Check 'price_range' failed: value was 1050").
- **Challenge:** "I want to be notified of an issue, but I don't want it to stop my entire pipeline."
- **Solution:** In the "Checks" configuration on the dataset, you can set the **Severity** of a check to **Warning** instead of **Error**. A warning will be logged but will not cause the "Run Checks" step to fail.
`},{id:64,slug:"creating-reusable-metadata-and-standards",question:"How to get started with + creating reusable metadata and standards?",answer:`
### 1. Introduction/Overview
As your use of Dataiku grows across teams and projects, establishing standards is essential for preventing chaos. Creating reusable metadata standards (like a common set of tags) and project templates ensures that all projects are built in a consistent, governed, and understandable way.

### 2. Prerequisites
- **Multiple projects or teams using Dataiku.**
- **An SME or governance team** to define and champion the standards.

### 3. Step-by-Step Instructions: Establishing Standards

#### 1. Create a Tagging Taxonomy
- **What:** A predefined, documented list of tags that should be used across all projects.
- **How:**
    1.  Hold a workshop with key stakeholders to agree on a standard set of tags for important concepts.
    2.  **Examples:**
        *   **Sensitivity:** \`PII\`, \`Confidential\`, \`Public\`
        *   **Status:** \`Raw\`, \`In_Progress\`, \`Validated\`, \`Archived\`
        *   **Source System:** \`Source:Salesforce\`, \`Source:SAP\`
        *   **Data Owner:** \`Owner:Finance\`, \`Owner:Marketing\`
    3.  Document this official taxonomy in a central, highly visible place, like a company-wide Dataiku Wiki project.

#### 2. Create a Project Template
- **What:** A pre-built, empty Dataiku project that contains your standard structure and can be duplicated to start new projects.
- **How:**
    1.  Create a new, blank project named something like \`TEMPLATE_Standard_Project\`.
    2.  Inside this project, set up your standard **Flow Zone** structure (e.g., \`1_Ingestion\`, \`2_Preparation\`, \`3_Outputs\`).
    3.  Create a standard **Wiki** structure with placeholder pages for "Business Goal," "Data Dictionary," etc.
    4.  Add your standard **Tags** to the project so they are available for autocomplete.
    5.  When a user needs to start a new project, they can simply **Duplicate** this template project.

#### 3. Create a Shared Code Library
- **What:** A central project for storing reusable Python or R code.
- **How:**
    1.  Create a project named \`SHARED_CODE_LIBRARY\`.
    2.  In its **Libraries** section, create Python (\`.py\`) files with your common, reusable functions (e.g., for custom data cleaning or calculations).
    3.  Other projects can then access this shared code.

### 4. Resources and Tools
- **Tags:** The primary tool for lightweight, flexible metadata.
- **Project Templates:** A powerful feature for enforcing structural consistency.
- **Project Wikis:** The best place to document your new standards.

### 5. Next Steps and Progression
- **Training and Communication:** It's not enough to create standards; you must train users on them and communicate why they are important.
- **Automated Governance Checks:** Create a scenario that runs periodically and uses the Dataiku API to scan all projects. The script can check for compliance with your standards (e.g., "flag any dataset that is missing a 'Data Owner' tag") and generate a governance report.

### 6. Common Challenges and Solutions
- **Challenge:** "Nobody is using our new standards."
- **Solution:** Standards need to be enforced. Make compliance part of your project review or release process. Showcase the benefits: "Look how easy it is to find all the Finance data now that it's properly tagged!" Get buy-in from team leads to champion the standards.
- **Challenge:** "The standards are too rigid and slow us down."
- **Solution:** Good standards should help, not hinder. Gather feedback from your users regularly. Be willing to evolve the standards to meet their needs. The goal is consistency, not bureaucracy.
`},{id:65,slug:"implementing-lineage-and-impact-tracing",question:"How to get started with + implementing lineage and impact tracing?",answer:`
### 1. Introduction/Overview
Data lineage is a map of your data's journey—where it came from, how it was transformed, and where it's going. Impact tracing is the reverse—understanding what downstream assets will be affected if you change something. In Dataiku, this is not something you have to "implement"; it's an automatic, core feature of the platform. The key is knowing how to read and use the lineage information provided.

### 2. Prerequisites
- **A Dataiku Flow:** You need a project with a chain of datasets and recipes.

### 3. Step-by-Step Instructions: Reading the Lineage

#### 1. Flow-level Lineage (Upstream and Downstream)
- **What it is:** A high-level view of an object's dependencies.
- **How to use:**
    1.  In your Flow, right-click on any dataset or recipe.
    2.  Select **View upstream dependencies**. Dataiku will highlight everything that is used to build that object.
    3.  Select **View downstream dependencies**. Dataiku will highlight everything that would be affected if you were to change that object. This is **impact analysis**.
- **Why it is useful:** Before you delete or modify a dataset, you can instantly see the impact it will have on all downstream reports, models, and pipelines.

#### 2. Column-level Lineage
- **What it is:** A highly detailed view that traces the origin of a single column.
- **How to use:**
    1.  Open any dataset (that is the output of a recipe).
    2.  Go to the **Lineage** tab.
    3.  Select a column on the right side.
    4.  Dataiku will display a visual graph showing exactly which source columns and which transformation recipes were used to create that specific column.
- **Why it is useful:** This is incredibly powerful for debugging and for audits. If there is a problem with a column in a final report, you can trace it back to its exact origin in seconds.

### 4. Resources and Tools
- **The Flow:** The primary interface for high-level lineage.
- **Lineage Tab:** The dedicated UI within a dataset for detailed, column-level tracing.

### 5. Next Steps and Progression
- **Audits and Compliance:** Use screenshots of the lineage graph as evidence in regulatory audits to prove data provenance and demonstrate how sensitive data is being handled.
- **Debugging Data Issues:** When you find a data quality issue in a downstream table, use the column-level lineage to quickly identify the upstream recipe where the error was introduced.
- **Root Cause Analysis:** Lineage is the key to performing root cause analysis on data issues.

### 6. Common Challenges and Solutions
- **Challenge:** "The lineage is broken or incorrect."
- **Solution:** This can happen if you use code recipes in a way that bypasses Dataiku's standard data access. For lineage to be tracked correctly, your Python/R/SQL recipes must always use the Dataiku API (\`dkuReadDataset\`, \`dkuWriteDataset\`, etc.) to read and write data. If you use a generic library to read a file from a hardcoded path, Dataiku won't know about that dependency, and the lineage will be broken.
`},{id:66,slug:"applying-access-controls-and-permissions",question:"How to get started with + applying access controls and permissions?",answer:`
### 1. Introduction/Overview
Properly managing who can see and edit your data projects is fundamental to data security and governance. Dataiku provides a granular, role-based access control system that allows you to manage permissions at the project level, ensuring users only have access to the data they are authorized to see.

### 2. Prerequisites
- **Dataiku Administrator rights:** Setting up groups and global permissions is an admin task.
- **Project Administrator rights:** Assigning permissions for a specific project requires admin rights on that project.
- **A clear access policy:** You should know which teams or users should have what level of access (e.g., "The marketing team can view the sales report, but only the finance team can edit the financial data").

### 3. Step-by-Step Instructions

#### Part 1: Creating User Groups (Admin Task)
1.  **Navigate to Security Settings:** Go to **Administration > Security > Groups**.
2.  **Create a New Group:** Click **+ NEW GROUP**. Give it a name that corresponds to a user role or team (e.g., \`marketing_analysts\`, \`data_scientists_team\`).
3.  **Add Users to the Group:** Add the relevant Dataiku users to this new group. It's much easier to manage permissions for a group than for hundreds of individual users.

#### Part 2: Assigning Permissions to a Project
1.  **Go to Project Settings:** In your project, go to **Settings** (the gear icon).
2.  **Open the Permissions Panel:** Navigate to the **Permissions** tab.
3.  **Add a Group:** Click **+ ADD GROUP**. Select the group you created (e.g., \`marketing_analysts\`).
4.  **Assign a Permission Level:** Choose the level of access for that group on this specific project:
    *   **Reader:** Can view everything in the project (Flows, datasets, dashboards) but cannot make any changes.
    *   **Contributor:** Can read, write, and edit most objects in the project. This is for developers working on the project.
    *   **Administrator:** Has full control over the project, including its settings and permissions.
5.  **Save:** Save your changes. The permissions take effect immediately.

### 4. Resources and Tools
- **Administration > Security:** The central hub for managing users and groups.
- **Project Settings > Permissions:** The page for controlling access to a specific project.

### 5. Next Steps and Progression
- **Object-level Restrictions (Advanced):** While the primary security model is at the project level, you can implement more granular security (e.g., masking a column for certain users) by creating different output datasets for different user groups, controlled by a Python recipe that checks the user's group membership.
- **Connection-level Security:** Access to data sources is also controlled by permissions on the connections in the Administration section.
- **Regular Audits:** Periodically review the permissions on your critical projects to ensure they are still appropriate and that former employees or team members have had their access revoked.

### 6. Common Challenges and Solutions
- **Challenge:** "A user says they can't see my project."
- **Solution:** You haven't granted them or their group any permissions on your project yet. Go to the project's Permissions settings and add their group with at least "Reader" access.
- **Challenge:** "How do I give someone access to only one dashboard in my project but not the rest?"
- **Solution:** Dataiku's primary security model is at the project level. You cannot grant access to a single object within a project. The standard solution is to create a separate, dedicated project just for that dashboard. You can then use a **Sync** recipe to share the necessary data into this new "dashboard project" and grant the user access only to that project.
`},{id:67,slug:"version-controlling-dataiku-projects-using-git",question:"How to get started with + version controlling Dataiku projects using Git?",answer:`
### 1. Introduction/Overview
Version control is a cornerstone of modern software development, and it's just as crucial for data projects. Integrating your Dataiku project with a Git repository (like GitHub, GitLab, or Bitbucket) allows you to track every change, collaborate with team members, and maintain a full history of your work.

### 2. Prerequisites
- **A remote Git repository:** You need an empty repository created on your Git provider.
- **Git configured on the Dataiku server:** Your Dataiku administrator must have installed Git on the server and configured it.
- **Project Admin rights in Dataiku.**

### 3. Step-by-Step Instructions
1.  **Link Your Project to Git:**
    *   In your Dataiku project, go to **Settings** (the gear icon).
    *   Navigate to the **Git** tab.
    *   Click **Convert to Git project**.
    *   Enter the **Repository URL** of your empty remote Git repository.
2.  **Make Your First Commit:**
    *   After linking, a new **Git** icon will appear in your project's top navigation bar. Click it.
    *   This page shows all the changes you've made to the project. Initially, this will be every object in the project.
    *   Click the checkbox to **Stage all** changes.
    *   Write a commit message in the box at the bottom (e.g., "Initial project commit").
    *   Click the **Commit** button.
3.  **Push to the Remote Repository:**
    *   Your commit now exists locally on the Dataiku server. To share it, you need to push it.
    *   Click the **Push** button to send your commits to the remote repository.
4.  **The Standard Workflow:** From now on, the workflow is:
    *   Make changes to your project (edit a recipe, create a dataset, etc.).
    *   Go to the **Git** page, **stage** your changes, **commit** them with a clear message, and **push** them to the remote.

### 4. Resources and Tools
- **Git tab in Project Settings:** Where you initially link the project.
- **Git page in Project Navigation:** Your day-to-day interface for staging, committing, pulling, and pushing changes.
- **A Git client (optional):** Tools like GitHub Desktop can be useful for viewing the project history visually.

### 5. Next Steps and Progression
- **Branching:** Don't work directly on the \`main\` branch. Use the Git page to **Create branch**. Make your changes on a feature branch (e.g., \`feature/add-new-sales-report\`).
- **Pull Requests:** When your feature is complete, push your branch and then use your Git provider's interface (e.g., GitHub) to create a **Pull Request**. This allows for code review before merging the changes back into the \`main\` branch.
- **Resolving Conflicts:** If you and a colleague edit the same object, you may have a merge conflict when you try to **Pull** their changes. Dataiku provides a visual diff tool to help you resolve these conflicts.

### 6. Common Challenges and Solutions
- **Challenge:** "Push/Pull failed with an authentication error."
- **Solution:** This means the Dataiku server cannot authenticate with your Git provider. Your Dataiku administrator needs to set up SSH keys or other credentials to allow the server to connect to the Git repository.
- **Challenge:** "What is actually being versioned?"
- **Solution:** Dataiku versions the *definition* of your project—the structure of your Flow, the code in your recipes, the settings of your datasets, etc. It does **not** version the actual data within your datasets.
`},{id:68,slug:"maintaining-audit-trails-within-dss",question:"How to get started with + maintaining audit trails within DSS?",answer:`
### 1. Introduction/Overview
An audit trail is a chronological record of who did what, and when. This is essential for security, compliance, and debugging. Dataiku automatically creates detailed audit trails at both the project level and the instance level, requiring no special setup. The key is knowing where to find and interpret this information.

### 2. Prerequisites
- **A Dataiku instance with user activity.**
- **The appropriate permissions:** Project-level for the project timeline, and global administrator rights for the instance-level audit logs.

### 3. Step-by-Step Instructions: Finding the Audit Trails

#### 1. Project-Level Audit Trail (The Timeline)
- **What it is:** A log of all changes made to a *specific project*.
- **Who it's for:** Project developers and managers.
- **How to access:**
    1.  In your project, go to **... > Timeline**.
    2.  This view shows a chronological list of all modifications: who created a recipe, who edited a dashboard, who ran a scenario, etc.
- **Why it's useful:** Perfect for answering questions like, "Who changed this recipe last week?" or "Why did this dashboard suddenly break?"

#### 2. Instance-Level Audit Trail (Global Audit Log)
- **What it is:** A comprehensive log of all significant events across the *entire Dataiku instance*.
- **Who it's for:** Dataiku Administrators, security teams, and compliance auditors.
- **How to access (Admin only):**
    1.  Go to **Administration > Logs > Global Audit Log**.
    2.  This log captures more sensitive and global events, including:
        *   User logins (successful and failed).
        *   Changes to global settings and connections.
        *   Permissions changes.
        *   API key creation.
- **Why it's useful:** Essential for security monitoring and formal compliance audits.

#### 3. Git Integration (The Ultimate Audit Trail)
- **What it is:** If your project is linked to Git, the commit history provides the most detailed and robust audit trail of all.
- **How to access:** On your Git provider's website (e.g., GitHub), you can browse the commit history for your project.
- **Why it's useful:** Every commit has an author, a timestamp, and a descriptive message explaining the change. It shows the exact "diff" of what was modified. This is the gold standard for auditing code and configuration changes.

### 4. Resources and Tools
- **Project Timeline:** For day-to-day project-level auditing.
- **Global Audit Log:** For instance-wide security and compliance auditing.
- **Git Commit History:** For the most detailed, developer-focused audit trail.

### 5. Next Steps and Progression
- **Log Exporting:** Administrators can configure the Dataiku instance to forward its logs to an external logging system (like Splunk or the ELK stack) for long-term storage, advanced analysis, and alerting.
- **Regular Reviews:** For critical projects, schedule a periodic review of the project timeline and Git history to ensure all changes are authorized and documented.

### 6. Common Challenges and Solutions
- **Challenge:** "I need to know who viewed a specific dataset."
- **Solution:** This level of detail is not typically available in the high-level audit logs. For highly sensitive data, you would rely on the project permissions to restrict access in the first place, ensuring only authorized users can view it.
- **Challenge:** "The project timeline is too noisy."
- **Solution:** Use the filter bar at the top of the Timeline page to filter by user or by the type of object that was changed to find the specific event you are looking for.
`},{id:69,slug:"aligning-pipelines-with-compliance-policies",question:"How to get started with + aligning pipelines with compliance policies?",answer:`
### 1. Introduction/Overview
Aligning your data pipelines with compliance policies (like GDPR, CCPA, or HIPAA) is a critical responsibility. Dataiku provides the governance tools you need to build compliant pipelines, demonstrate data provenance, and enforce security rules. The process involves a combination of technical implementation and clear documentation.

### 2. Prerequisites
- **Understanding of the compliance policy:** You must know the specific rules you need to follow (e.g., "we must not store customer PII longer than 5 years," "we must be able to demonstrate where this data came from").
- **Collaboration with your legal/compliance team.**
- **A Dataiku project that handles sensitive data.**

### 3. Step-by-Step Instructions: A Compliance Framework in Dataiku

1.  **Identify and Tag Sensitive Data:**
    *   Go through your datasets and identify any columns that contain Personally Identifiable Information (PII) or other sensitive data.
    *   **Tag** these datasets and columns with a \`PII\` or \`Confidential\` tag. This makes them easily identifiable.
2.  **Enforce Access Control:**
    *   Ensure that projects containing sensitive data have strict **permissions**. Only users with a legitimate need should have access.
    *   Use different projects for different levels of sensitivity. For example, raw PII data might live in a highly restricted project, while an anonymized version is shared in a more open project.
3.  **Implement Data Privacy in Recipes:**
    *   If you need to anonymize or mask data, implement this logic in a **Prepare** recipe.
    *   Use processors to hash email addresses, remove identifiers, or generalize locations.
    *   Document these steps clearly in the recipe's description.
4.  **Use Lineage for Audits:**
    *   Dataiku's automatic **lineage** is your most powerful tool for compliance.
    *   When an auditor asks, "Where did this number in the report come from?" you can use the column-level lineage graph to show them the exact end-to-end journey of that data point, proving its provenance.
5.  **Document Everything in the Wiki:**
    *   Create a "Compliance" section in your project's **Wiki**.
    *   Document how your pipeline adheres to the specific clauses of the regulation. For example: "To comply with GDPR's Right to be Forgotten, we implement a filter in the \`prepare_customers\` recipe to exclude users who have requested deletion."

### 4. Resources and Tools
- **Tags:** For identifying and classifying sensitive data.
- **Project Permissions:** For enforcing access control.
- **Prepare Recipe:** For implementing data masking and anonymization.
- **Lineage Graph:** For proving data provenance to auditors.
- **Project Wiki:** For formally documenting your compliance strategy.

### 5. Next Steps and Progression
- **Automated Retention Policies:** Use a **Scenario** with a Python step to automatically delete old data partitions to comply with data retention rules.
- **Formal Sign-offs:** For regulated industries, Dataiku has features for formal model and project sign-offs, creating an auditable approval trail.
- **Automated Governance Checks:** Create a scenario that scans all projects and checks for compliance violations (e.g., "alert if a dataset tagged as \`PII\` does not have restricted permissions").

### 6. Common Challenges and Solutions
- **Challenge:** "How can I prove that I'm compliant?"
- **Solution:** The combination of the lineage graph and the documentation in your Wiki is your proof. You can literally show an auditor the visual flow of data and the documented logic that enforces the rules.
- **Challenge:** "This seems like a lot of extra work."
- **Solution:** Building with compliance in mind from the start is much easier than trying to retrofit it later. Make tagging and documentation a standard part of your development process. The governance features in Dataiku are designed to make this as painless as possible.
`},{id:70,slug:"establishing-coding-best-practices-in-team",question:"How to get started with + establishing coding best practices in team?",answer:`
### 1. Introduction/Overview
As a team starts using code recipes (Python, SQL, R) in Dataiku, establishing a set of best practices and a consistent style guide is essential. This ensures that code is readable, maintainable, and reusable, saving time and preventing errors in the long run.

### 2. Prerequisites
- **A team of developers** working in Dataiku.
- **Agreement from the team** to adopt a common standard.

### 3. Step-by-Step Instructions: Creating and Implementing Standards

1.  **Define a Style Guide:**
    *   You don't need to reinvent the wheel. Adopt an existing, widely-used style guide.
        *   **For Python:** Use **PEP 8**. It's the universal style guide for Python code.
        *   **For SQL:** Adopt a common style, such as using uppercase for keywords (\`SELECT\`, \`FROM\`) and lowercase for identifiers, and indenting subqueries.
    *   Document this choice in your team's central **Wiki**.
2.  **Automate Style Checking (Linting):**
    *   Use a linter to automatically check for style guide violations.
    *   **For Python:** Use a tool like \`flake8\` or \`black\`.
    *   You can integrate this into your CI/CD pipeline. The pipeline can be configured to fail if the committed code does not pass the linter's checks.
3.  **Establish Code Structure Standards:**
    *   Create a template for your code recipes. A good Python recipe template includes:
        *   A docstring at the top explaining the recipe's purpose.
        *   All \`import\` statements grouped at the top.
        *   A main function that contains the core logic.
        *   Helper functions for modularity.
        *   A main execution block (\`if __name__ == '__main__':\`) that calls the main function.
    *   Document this template in your Wiki.
4.  **Promote Code Reusability:**
    *   Create a rule: "If you use the same block of code in more than two places, turn it into a function."
    *   Place these reusable functions in the project's **Library** so they can be imported into any recipe.
5.  **Implement Code Reviews:**
    *   This is the most effective way to enforce all standards.
    *   Use the **Git integration** and require all changes to be submitted via **Pull Requests**.
    *   Another developer must review the code for correctness, style, and documentation before it can be merged into the main branch.

### 4. Resources and Tools
- **PEP 8:** The official style guide for Python code.
- **Linters (\`flake8\`, \`black\`):** Automated tools for checking code style.
- **Project Libraries:** The place for reusable, shared functions.
- **Git and Pull Requests:** The framework for collaborative code review.
- **Project Wiki:** The central place to document all your team's standards.

### 5. Next Steps and Progression
- **Standardize Documentation:** Require that all functions have clear docstrings explaining their purpose, parameters, and what they return.
- **Unit Testing:** For critical, complex functions in your project library, require developers to write unit tests to ensure they work correctly.

### 6. Common Challenges and Solutions
- **Challenge:** "Team members don't want to follow the new rules."
- **Solution:** Getting buy-in is key. Hold a meeting to explain the *why* behind the standards—that they will make everyone's life easier in the long run by improving readability and reducing bugs. Automating checks with a linter in a CI/CD pipeline is the most effective way to enforce the style guide without manual arguments.
- **Challenge:** "Code reviews are slowing us down."
- **Solution:** Keep pull requests small and focused on a single feature. This makes them much faster to review. Ensure the team prioritizes reviewing each other's code quickly. The time spent on review is almost always saved later by catching bugs early.
`},{id:71,slug:"embedding-generative-ai-models-in-pipelines",question:"How to get started with + embedding Generative AI models in pipelines?",answer:`
### 1. Introduction/Overview
Generative AI models (like GPT from OpenAI or models from Hugging Face) can be incredibly powerful for tasks like text summarization, classification, and data generation. You can integrate these models into your Dataiku pipelines using a **Python recipe** to call their APIs.

### 2. Prerequisites
- **An API Key:** You need an API key from a generative AI provider (e.g., OpenAI, Hugging Face, Google AI).
- **A clear task:** Know what you want to do with the model (e.g., "summarize customer reviews," "classify support tickets").
- **An input dataset:** A dataset containing the text data you want to send to the model.
- **A Python code environment** with the necessary libraries installed (e.g., \`openai\`, \`transformers\`, \`requests\`).

### 3. Step-by-Step Instructions
1.  **Set Up Your Environment:**
    *   Create a code environment and add the required Python library (e.g., \`pip install openai\`).
    *   Store your API key securely. The best practice is to save it as a "Password" type **Project Variable** in Dataiku.
2.  **Create a Python Recipe:**
    *   Take your text dataset as input.
    *   Create a new **Python recipe**.
3.  **Write the Python Code:**
    *   **Import libraries** and get your API key from the project variables.
    *   **Read your input data** into a Pandas DataFrame.
    *   **Define a function** that takes a single text input, calls the AI model's API, and returns the result. This function should include error handling.
    *   **Apply the function** to the relevant column in your DataFrame to create a new column with the AI-generated results. This is often done with \`df.apply()\`.
    *   **Write the output** DataFrame to the recipe's output dataset.
    > \`\`\`python
    > import dataiku
    > import openai
    >
    > # Get API key from project variables
    > variables = dataiku.get_custom_variables()
    > openai.api_key = variables.get("OPENAI_API_KEY")
    >
    > # Read input data
    > df = dataiku.Dataset("input_reviews").get_dataframe()
    >
    > # Define function to call the API
    > def summarize_text(text):
    >     try:
    >         response = openai.Completion.create(...) # Your API call here
    >         return response.choices[0].text.strip()
    >     except Exception as e:
    >         return str(e)
    >
    > # Apply the function to the 'review_text' column
    > df['summary'] = df['review_text'].apply(summarize_text)
    >
    > # Write the results
    > dataiku.Dataset("reviews_with_summaries").write_with_schema(df)
    > \`\`\`
4.  **Run the Recipe:** Execute the recipe to process your data and get the results from the generative AI model.

### 4. Resources and Tools
- **Python Recipe:** The main tool for this integration.
- **Project Variables:** The secure way to manage your API keys.
- **API Documentation:** The documentation from your AI model provider (e.g., OpenAI's API reference) is essential.

### 5. Next Steps and Progression
- **Error Handling and Retries:** API calls can fail. Implement robust error handling and a retry mechanism in your function to make the pipeline more resilient.
- **Batching:** Instead of calling the API for every single row, modify your code to send data in batches, which can be more efficient and cost-effective.
- **Cost Management:** Be aware that API calls to these models cost money. Process a small sample of your data first to estimate the cost before running on a large dataset.

### 6. Common Challenges and Solutions
- **Challenge:** "The API calls are very slow."
- **Solution:** This is expected, as you are making network requests for each row. Consider using batching. Also, you can use the **Spark** execution engine with Pandas UDFs to parallelize the API calls across multiple nodes, which can dramatically speed up the process for large datasets.
- **Challenge:** "How do I handle rate limits?"
- **Solution:** Most APIs have a rate limit (a maximum number of requests per minute). Your API calling function should include a \`time.sleep()\` in its error handling logic to wait and retry if it receives a rate limit error.
`},{id:72,slug:"building-nlp-text-analytics-flows",question:"How to get started with + building NLP/text analytics flows?",answer:`
### 1. Introduction/Overview
Natural Language Processing (NLP) involves extracting meaning and structure from text data. Dataiku provides a suite of tools, from visual text preparation to integration with advanced libraries, to build powerful NLP pipelines.

### 2. Prerequisites
- **Text data:** A dataset with one or more columns containing text you want to analyze (e.g., customer reviews, support tickets, articles).
- **A clear objective:** Know what you want to achieve (e.g., sentiment analysis, topic modeling, named entity recognition).

### 3. Step-by-Step Instructions: A Standard NLP Flow

#### Step 1: Text Preparation (Visual)
1.  **Create a Prepare Recipe:** Start with your raw text dataset.
2.  **Use Text Processors:** In the Prepare recipe, select your text column and add processors from the **Text Analysis** group:
    *   **Tokenize text:** Split sentences into individual words (tokens).
    *   **Remove stop words:** Remove common, non-informative words ("the", "a", "is").
    *   **Stem or Lemmatize:** Reduce words to their root form (e.g., "running" -> "run").
    *   **Clean text:** Remove punctuation, convert to lowercase.
3.  This creates a "clean text" column ready for analysis.

#### Step 2: Feature Extraction
- **Visual Method (TF-IDF):**
    - After preparing the text, use the **Text feature extraction** recipe. It can convert your text into numerical vectors using techniques like TF-IDF, which can then be used in machine learning models.
- **Code Method (Embeddings):**
    - Use a **Python recipe** and a library like \`sentence-transformers\` or \`spaCy\` to convert your text into dense vector embeddings. This is a more modern and often more powerful approach.

#### Step 3: Analysis or Modeling
- **Sentiment Analysis:** The Prepare recipe has a built-in **Compute sentiment** processor that can classify text as positive, negative, or neutral.
- **Topic Modeling:** Use the **Topic modeling** recipe on your prepared text to discover the main themes or topics present in your documents.
- **Text Classification:** Use the numerical features you extracted (TF-IDF or embeddings) as inputs to a standard **Visual ML** classification model to categorize your text.

### 4. Resources and Tools
- **Prepare Recipe (Text Analysis):** Your starting point for visual text cleaning.
- **Text Feature Extraction Recipe:** For creating classical NLP features like TF-IDF.
- **Python Recipes with NLP Libraries:** For advanced techniques, use libraries like:
    - **NLTK/spaCy:** For robust text processing and linguistic analysis.
    - **Scikit-learn:** For text classification models.
    - **Gensim:** For topic modeling.
    - **Transformers/Sentence-Transformers:** For state-of-the-art embeddings.

### 5. Next Steps and Progression
- **Named Entity Recognition (NER):** Use a library like \`spaCy\` in a Python recipe to extract entities like people, organizations, and locations from your text.
- **Word Clouds:** In the **Charts** tab of a dataset containing tokenized words, you can create a word cloud visualization.
- **Custom Sentiment Analysis:** Train your own sentiment model if the built-in processor is not accurate enough for your specific domain.

### 6. Common Challenges and Solutions
- **Challenge:** "The built-in sentiment analysis is not accurate for my industry jargon."
- **Solution:** The built-in tools are generic. For domain-specific language, you will need to train a custom classification model. Label a sample of your data with the correct sentiment and use it to train a model in the Visual ML lab.
- **Challenge:** "My text data is in a language other than English."
- **Solution:** Many of the visual text processors support multiple languages. Check the processor settings to see if your language is available. For languages not supported visually, you will need to use a Python recipe with a multilingual NLP library.
`},{id:73,slug:"integrating-labeling-management-workflows",question:"How to get started with + integrating labeling management workflows?",answer:`
### 1. Introduction/Overview
Supervised machine learning requires labeled data, but getting high-quality labels can be a significant bottleneck. Dataiku's "Labeling" features allow you to create and manage data labeling tasks directly within your project, integrating the human-in-the-loop process seamlessly into your Flow.

### 2. Prerequisites
- **Unlabeled data:** A dataset containing the data you need to have labeled (e.g., text for classification, images for object detection).
- **A team of labelers:** The people who will be performing the manual labeling task.

### 3. Step-by-Step Instructions
1.  **Install the Labeling Plugin:** Ensure the "Visual and Interactive Labeling" plugin is installed on your Dataiku instance by an administrator.
2.  **Create a Labeling Task:**
    *   From your Flow, select your unlabeled dataset.
    *   From the right-hand panel, choose the **Labeling** recipe under "Visual recipes".
    *   Click **CREATE RECIPE**.
3.  **Configure the Task:**
    *   **Choose the labeling task type:**
        *   \`Text classification\`: Assign a category to a piece of text.
        *   \`Named entity recognition\`: Identify and tag entities in text.
        *   \`Image classification\`: Assign a category to an image.
        *   \`Object detection\`: Draw bounding boxes around objects in an image.
    *   **Define the Classes:** Enter the possible labels or categories that the labelers can choose from.
    *   **Write Instructions:** Provide clear instructions for the labelers on how to correctly apply the labels.
4.  **Assign Labelers:** In the labeling task settings, you can assign the task to specific Dataiku users or groups.
5.  **Start Labeling:**
    *   The assigned users can now open the labeling task and will be presented with a simple, optimized UI for labeling the data. They see one item at a time, apply the label, and move to the next.
6.  **Monitor Progress:** The owner of the labeling task can monitor the progress, see how many items have been labeled, and review the quality of the labels.
7.  **Use the Labeled Data:** The output of the labeling recipe is a new dataset containing the original data plus the new, human-provided labels. This dataset is now ready to be used to train a machine learning model.

### 4. Resources and Tools
- **Labeling Recipe/Plugin:** The core component for creating and managing labeling tasks.
- **Labeling UI:** The simple, dedicated interface used by the people performing the labeling.

### 5. Next Steps and Progression
- **Active Learning:** For classification tasks, you can use an "Active Learning" strategy. First, label a small amount of data and train a model. Then, use the model to predict on the rest of the unlabeled data. The active learning recipe will then prioritize showing the labelers the items the model was *least confident* about, making the labeling process much more efficient.
- **Quality Control:** Have multiple people label the same subset of data to measure inter-annotator agreement and ensure a high quality of labels.

### 6. Common Challenges and Solutions
- **Challenge:** "My labelers are confused about the task."
- **Solution:** Your instructions are not clear enough. A good labeling task requires extremely clear, unambiguous instructions with examples of edge cases. It's often worth holding a short kickoff meeting with the labelers to walk them through the task.
- **Challenge:** "Labeling thousands of items is taking too long."
- **Solution:** This is the primary challenge of supervised learning. Use an **Active Learning** workflow to focus your labeling efforts on the most informative examples, which can significantly reduce the amount of data you need to label manually.
`},{id:74,slug:"using-time‑series-modeling-in-dataiku",question:"How to get started with + using time‑series-modeling in Dataiku?",answer:`
### 1. Introduction/Overview
Time series forecasting—predicting future values based on historical data—is a common business need. Dataiku provides a dedicated **Time Series Forecasting** lab that automates many of the complex steps involved, such as feature engineering from dates, handling trends and seasonality, and training appropriate forecasting models.

### 2. Prerequisites
- **Time series data:** A dataset containing at least two columns:
    1.  A **timestamp column** that is parsed as a date.
    2.  A **numerical column** with the value you want to forecast (e.g., \`daily_sales\`).
- **A clear forecasting goal:** Know what you want to predict and how far into the future (the forecast horizon).

### 3. Step-by-Step Instructions
1.  **Prepare Your Data:**
    *   Ensure your data is in a "long" format, with one row per timestamp.
    *   Use a **Prepare** recipe to make sure your timestamp column is correctly parsed as a date data type.
2.  **Launch the Forecasting Lab:**
    *   In your Flow, select your prepared time series dataset.
    *   From the right-hand panel, click **Lab > + NEW ANALYSIS**.
    *   Choose **Forecasting**.
3.  **Configure the Task:**
    *   In the settings, select your **Timestamp column** and the **Numerical series** you want to forecast.
    *   Dataiku will analyze the time series and suggest a time step (e.g., daily, weekly).
    *   You can also add external features (e.g., promotional events) if you have them.
4.  **Design the Models:**
    *   Go to the **Design** tab. Here you can configure:
        *   **Feature Engineering:** Enable options to automatically create features like day of the week, month, holidays, and lag features.
        *   **Algorithms:** Dataiku will suggest time series models like \`Seasonal ARIMA\` and standard regression models like \`Random Forest\` (which use the engineered date features).
5.  **Train and Evaluate:**
    *   Click **Train**. Dataiku will train the models using historical data for training and a more recent period for evaluation.
    *   The results page will show a leaderboard of models ranked by metrics like **MAPE** (Mean Absolute Percentage Error) and **RMSE**. You can also see a visual plot of the model's predictions against the actual values.

### 4. Resources and Tools
- **Forecasting Analysis Lab:** The dedicated visual tool for all time series modeling tasks.
- **Time Series Metrics:** Understand metrics like MAPE to evaluate the accuracy of your forecasts.
- **Dataiku Academy:** Has specific courses on time series forecasting.

### 5. Next Steps and Progression
- **Deploy and Forecast:** Deploy your best model to the Flow. You can then use the **Forecast** recipe to generate future predictions.
- **Multivariate Forecasting:** Include other time-dependent variables (e.g., weather, holidays, competitor prices) as features to potentially improve your forecast accuracy.
- **Deep Learning for Forecasting:** For very complex time series, you can use a Python recipe with a library like \`Prophet\` or a custom LSTM model in TensorFlow/PyTorch.

### 6. Common Challenges and Solutions
- **Challenge:** "My forecasts are not accurate."
- **Solution:** Time series forecasting is difficult. First, ensure you have enough historical data. Second, experiment with feature engineering. Adding features for holidays, special events, or other external factors can significantly improve performance. Finally, try different model types.
- **Challenge:** "The lab says my time series is not regular."
- **Solution:** This means you have missing timestamps in your data (e.g., you are missing a day). Before the lab, use a **Time series resampling** recipe to fill in the missing dates and impute the corresponding values.
`},{id:75,slug:"implementing-computer-vision-pipelines",question:"How to get started with + implementing computer vision pipelines?",answer:`
### 1. Introduction/Overview
Computer Vision (CV) involves training models to "see" and interpret images. In Dataiku, you can build end-to-end CV pipelines, from managing your image data to training deep learning models and deploying them for tasks like image classification or object detection.

### 2. Prerequisites
- **Image Data:** A collection of images (\`.jpg\`, \`.png\`).
- **A CV Task:** A clear goal, such as:
    - **Image Classification:** Assign a single label to an image (e.g., "cat" or "dog").
    - **Object Detection:** Draw bounding boxes around objects in an image.
- **A Deep Learning Code Environment:** You need a Python environment with libraries like \`TensorFlow\`/\`Keras\` or \`PyTorch\`, and ideally access to a GPU for training.
- **Labeled Data (for supervised learning):** You need to know the correct label or bounding box for each training image.

### 3. Step-by-Step Instructions: A Classification Pipeline

1.  **Organize and Upload Images:**
    *   On your local machine, organize your training images into subfolders, where each subfolder name is the class label (e.g., \`/data/cats/\`, \`/data/dogs/\`).
    *   In your Dataiku project, create a **Managed Folder**.
    *   Upload your entire data directory structure into this folder.
2.  **Create a Dataset of Image Paths:**
    *   Use a Python recipe to scan the managed folder and create a dataset with two columns: \`image_path\` and \`label\`. This structured list will be used to feed the training process.
3.  **Train a Model in a Notebook:**
    *   Create a **Jupyter Notebook** using your deep learning environment.
    *   Write Python code using Keras/TensorFlow to:
        *   Read the dataset of image paths.
        *   Create a data generator that loads the images from the managed folder, resizes them, and applies augmentations.
        *   Define a convolutional neural network (CNN) architecture. You can use a pre-trained model like \`ResNet\` or \`MobileNet\` for transfer learning, which is highly recommended.
        *   Train the model using \`model.fit()\`.
4.  **Save the Trained Model:**
    *   After training, save your model's architecture and weights to a new managed folder.
5.  **Create an Inference Recipe:**
    *   To use your model, create a new Python recipe that takes a folder of new, unlabeled images as input.
    *   The recipe will load your saved model, preprocess the new images, call \`model.predict()\`, and write the predicted labels to an output dataset.

### 4. Resources and Tools
- **Managed Folders:** The standard way to store and manage image files in Dataiku.
- **Python Notebooks/Recipes:** The environment for writing your custom CV code.
- **Deep Learning Frameworks:** TensorFlow/Keras or PyTorch are the standard libraries for building CV models.
- **Labeling Plugin:** Use this to create a labeling task for object detection or if your images are not already organized by class.

### 5. Next Steps and Progression
- **GPU Training:** For any serious CV task, you must use a GPU-enabled environment. Work with your admin to configure this.
- **Object Detection:** The workflow is similar, but the labeling and model architecture (e.g., YOLO, SSD) are specific to object detection.
- **Real-time Inference:** Deploy your model inference recipe to the **API Deployer** to create a REST API that can classify a single image in real time.

### 6. Common Challenges and Solutions
- **Challenge:** "My model accuracy is low."
- **Solution:** CV models require a lot of data. If you have a small dataset, make extensive use of **image augmentation** (randomly rotating, zooming, and flipping your training images) to create more variety. Using a **pre-trained model** (transfer learning) is also one of the most effective ways to get high performance with limited data.
- **Challenge:** "Training takes days."
- **Solution:** You need a GPU. Training a deep CNN on a CPU is not practical.
`},{id:76,slug:"connecting-dataiku-to-kubernetes-clusters",question:"How to get started with + connecting Dataiku to Kubernetes clusters?",answer:`
### 1. Introduction/Overview
Kubernetes (K8s) is the industry standard for container orchestration. Dataiku can leverage Kubernetes in two main ways: by running the entire platform on K8s for scalability, or by connecting to a K8s cluster to run specific jobs in isolated, containerized environments. This guide focuses on the second case: connecting an existing Dataiku instance to a K8s cluster.

### 2. Prerequisites
- **A running Kubernetes cluster:** E.g., AWS EKS, Azure AKS, Google GKE.
- **\`kubectl\` configured:** You need command-line access to your cluster to set up the necessary permissions.
- **Dataiku Administrator Rights:** This is an administrative task.

### 3. Step-by-Step Instructions
1.  **Prepare Kubernetes (The K8s Admin's Task):**
    *   **Create a Namespace:** It's best practice to create a dedicated namespace in your cluster for Dataiku jobs (e.g., \`dataiku-jobs\`).
    *   **Create a Service Account:** Create a K8s service account within this namespace that Dataiku will use to create pods.
    *   **Grant Permissions:** Create a Role and RoleBinding to give this service account permissions to create, monitor, and delete pods within the \`dataiku-jobs\` namespace.
2.  **Configure the Connection in Dataiku (The DSS Admin's Task):**
    *   Navigate to **Administration > Containerized Execution**.
    *   Click **+ NEW CONTAINER CONFIGURATION**.
    *   **Select Kubernetes:** Choose Kubernetes as the container engine.
3.  **Provide Cluster Details:**
    *   You need to provide Dataiku with the Kubeconfig file that allows it to access the cluster. This file contains the cluster address and the credentials for the service account you created in step 1.
    *   Specify the **Namespace** to use (\`dataiku-jobs\`).
4.  **Define a Base Docker Image:**
    *   Specify a Docker image to be used for running the jobs (e.g., \`dataiku/dss-python-base:latest\`). This image must contain the necessary system libraries and Python.
5.  **Test and Save:** Save the configuration.
6.  **Using the Connection (The Developer's Task):**
    *   Now, in any Python recipe, go to the **Advanced** settings.
    *   In the **Container** dropdown, you can select the Kubernetes configuration you just created.
    *   When you run the recipe, Dataiku will now execute it in a pod on your K8s cluster instead of on the main Dataiku server.

### 4. Resources and Tools
- **\`kubectl\`:** The command-line tool for managing your Kubernetes cluster.
- **Dataiku Administration > Containerized Execution:** The UI for configuring the connection.
- **Docker Hub:** To find base images for your execution environments.

### 5. Next Steps and Progression
- **Custom Images:** Build and use your own custom Docker images with specific libraries or dependencies pre-installed for your jobs.
- **Resource Profiles:** Create different container configurations with different resource requests (CPU, memory), allowing users to select a "small," "medium," or "large" container for their job.
- **GPU Jobs:** Configure a container environment that uses a base image with GPU drivers and targets a node pool in your K8s cluster that has GPU hardware, enabling deep learning workloads.

### 6. Common Challenges and Solutions
- **Challenge:** "The connection from Dataiku to Kubernetes fails."
- **Solution:** This is a network or authentication issue. Ensure the Kubeconfig file is correct. From the Dataiku server's command line, try to run a \`kubectl\` command to see if it can reach the cluster's API server. Firewalls often block this access.
- **Challenge:** "My job is submitted but it's stuck in a 'Pending' state in Kubernetes."
- **Solution:** This means the K8s scheduler cannot find a node with enough available resources (CPU or memory) to run your job's pod. You either need to add more nodes to your cluster or reduce the resource requests in your container configuration in Dataiku.
`},{id:77,slug:"using-dataiku-apis-for-advanced-control",question:"How to get started with + using Dataiku APIs for advanced control?",answer:`
### 1. Introduction/Overview
Dataiku provides two powerful APIs for programmatic control: the **Python API**, used *inside* Dataiku (in recipes and notebooks) to interact with the project, and the **REST API**, used *outside* Dataiku to automate and integrate it with other systems. Mastering these APIs unlocks the highest level of automation and advanced functionality.

### 2. Prerequisites
- **A clear goal:** Know what you want to automate.
- **Programming skills:** Python for the Python API, and any language that can make HTTP requests (or \`curl\`) for the REST API.
- **An API Key:** For the REST API, you'll need to generate a key in your user profile.

### 3. Step-by-Step Instructions

#### Getting Started with the Python API (Inside Dataiku)
1.  **Where to use it:** In a Python recipe, Jupyter notebook, or a scenario's Python step.
2.  **What it's for:** Interacting with objects in your project.
3.  **Example: Reading a dataset and getting its metadata.**
    > \`\`\`python
    > import dataiku
    >
    > # Get a handle on a dataset
    > dataset = dataiku.Dataset("my_dataset")
    >
    > # Read its data into a Pandas DataFrame
    > df = dataset.get_dataframe()
    >
    > # Get its metadata (row count, schema)
    > metadata = dataset.get_metadata()
    > print(f"Number of rows: {metadata['metrics']['recordsCount']}")
    > \`\`\`
4.  **Key Concept:** The \`dataiku\` library is automatically available. You use it to get handles on projects, datasets, models, etc., and then call methods on those objects.

#### Getting Started with the REST API (Outside Dataiku)
1.  **Where to use it:** From an external system like a CI/CD tool (Jenkins), a scheduler (Airflow), or your own application.
2.  **What it's for:** Triggering actions in Dataiku, like running a scenario or exporting a project.
3.  **Example: Triggering a scenario with \`curl\`.**
    > \`\`\`bash
    > # The -u flag provides the API key as the username with a blank password
    > curl -X POST -u 'YOUR_API_KEY:' 'https://dss.mycompany.com/public/api/projects/MYPROJ/scenarios/run_daily/run'
    > \`\`\`
4.  **Key Concept:** You make standard HTTP requests (GET, POST, PUT, DELETE) to specific endpoints that represent Dataiku objects and actions.

### 4. Resources and Tools
- **Dataiku Python API Documentation:** This is your primary reference. It's available from the Help menu in Dataiku.
- **Dataiku REST API Documentation:** Also available from the Help menu. It provides an interactive (Swagger) UI to explore all available endpoints.
- **\`curl\` and Postman:** Essential tools for testing and debugging your REST API calls.

### 5. Next Steps and Progression
- **Python API:** Write scenarios that programmatically build parts of your flow, check data quality, and deploy models based on custom logic.
- **REST API:** Build a full CI/CD pipeline that uses the REST API to update a project from Git, run tests, create a bundle, and deploy it to production.
- **Combining Them:** A common pattern is to have an external system use the REST API to trigger a scenario, and that scenario then uses the Python API to perform its complex, programmatic logic.

### 6. Common Challenges and Solutions
- **Challenge (Python API):** "I don't know what methods are available for an object."
- **Solution:** Use the \`help()\` function in a notebook (e.g., \`help(dataset)\`) or refer to the official Python API documentation, which is comprehensive.
- **Challenge (REST API):** "I'm getting a 401 Unauthorized or 403 Forbidden error."
- **Solution:** This is a permissions issue. Check that your API key is correct and that it has been granted the necessary permissions (e.g., "Run scenarios on project MYPROJ") in the API key settings.
`},{id:78,slug:"adding-custom-plugins-to-dss",question:"How to get started with + adding custom plugins to DSS?",answer:`
### 1. Introduction/Overview
Plugins are packages that extend Dataiku's core functionality. They can add new visual recipes, dataset connectors, processors, and more. You can install pre-built plugins from the Dataiku marketplace or even develop your own to encapsulate reusable, domain-specific logic for your organization.

### 2. Prerequisites
- **Dataiku Administrator rights:** Installing plugins is a global action that requires admin privileges.
- **For Development:** A local or "developer" instance of Dataiku where you have filesystem access.

### 3. Step-by-Step Instructions

#### Part 1: Installing a Pre-built Plugin from the Store
1.  **Navigate to the Plugin Store:** As an admin, go to **Administration > Plugins**.
2.  **Browse the Store:** Click on the **Store** tab. Here you will find a catalog of plugins built by Dataiku and the community.
3.  **Install a Plugin:**
    *   Find a plugin you want (e.g., the "Geospatial" plugin or the "Tableau Hyper Export" plugin).
    *   Click on it and click the **Install** button.
4.  **Build the Code Environment:** After installation, the plugin will need to build its own code environment with its specific Python or R dependencies. Click the **Build** button.
5.  **Start Using It:** Once the environment is built, the plugin's components (e.g., new visual recipes) will now be available for all users in the "+ Recipe" menu in their projects.

#### Part 2: Developing Your Own Custom Plugin (High-Level)
1.  **Enable Dev Mode:** On your developer Dataiku instance, you enable "dev mode," which allows you to create a new plugin folder directly in the Dataiku installation directory.
2.  **Define the Components:** A plugin is a collection of folders and JSON configuration files.
    *   To create a new visual recipe, you would create a \`recipe.json\` file to define its UI (inputs, outputs, user parameters) and a \`recipe.py\` file for the Python backend logic.
    *   To create a new dataset connector, you would create the necessary Python classes that handle connecting to the source and reading the data.
3.  **Test and Iterate:** You can develop and test the plugin live on your dev instance.
4.  **Package for Distribution:** Once complete, you can zip the plugin's folder. This \`.zip\` file can then be installed on other Dataiku instances, just like a plugin from the store.

### 4. Resources and Tools
- **Plugin Store:** The marketplace for finding and installing existing plugins.
- **Dataiku Developer Guide:** The official documentation contains detailed, step-by-step tutorials on how to develop your own custom plugins.
- **Git:** The source code for many official and community plugins is available on GitHub, which can be a great learning resource.

### 5. Next Steps and Progression
- **Share Plugins Internally:** Package your custom plugins and set up an internal plugin store for your organization, allowing teams to easily share and reuse powerful, governed components.
- **Contribute to the Community:** If you build a generic, useful plugin, consider open-sourcing it and contributing it to the public plugin store.

### 6. Common Challenges and Solutions
- **Challenge:** "After installing a plugin, I can't find the new recipe."
- **Solution:** First, ensure you have successfully built the plugin's code environment in the Administration section. Second, you may need to restart the Dataiku backend for the new components to be fully registered.
- **Challenge (Development):** "Creating a plugin seems very complicated."
- **Solution:** Start simple. The developer documentation has a "hello world" tutorial. Try creating a very simple recipe with one input, one output, and a single user parameter. This will teach you the basic structure and concepts.
`},{id:79,slug:"deploying-dataiku-rest-endpoints",question:"How to get started with + deploying Dataiku REST endpoints?",answer:`
### 1. Introduction/Overview
Deploying a REST endpoint allows you to expose your Dataiku models or functions as a real-time API service. This means external applications can send a single request (e.g., with a customer's data) and get an immediate prediction back. This is handled by a separate component of the Dataiku platform called the **API Deployer**.

### 2. Prerequisites
- **A "Saved Model" or a Python function:** You need an artifact to deploy. This is usually a trained model from your Flow, but can also be a Python function.
- **Access to an API Deployer instance:** The API Deployer is a separate service from the main Dataiku design node. Your administrator needs to have set one up.
- **Permissions:** You need to have deployment permissions on the API Deployer.

### 3. Step-by-Step Instructions
1.  **From your Project, Deploy to the API Deployer:**
    *   Go to your project's Flow and select your **Saved Model**.
    *   From the right-hand panel, click **API Designer**.
    *   Click **+ CREATE YOUR FIRST API SERVICE**.
    *   Give your service a name and create a new endpoint. Configure it to use your Saved Model.
2.  **Deploy the Service:**
    *   Once you have created the service and endpoint in the designer, click the **Deploy** button.
    *   The API Deployer will package up your model and deploy it as a live, running service.
3.  **Test the Live Endpoint:**
    *   Navigate to the API Deployer UI. You will see your newly deployed service.
    *   Click on it. The UI provides a testing panel where you can enter sample data (as a JSON object) and see the real-time prediction from the live model.
4.  **Integrate with Applications:**
    *   The API Deployer UI also provides the endpoint's URL and code snippets in various languages (\`curl\`, Python, etc.).
    *   Give this URL and an API key to your application developers so they can integrate the prediction service into their application.

### 4. Resources and Tools
- **API Designer:** The UI within a project for creating the definition of your API service.
- **API Deployer:** The separate, production-grade service that runs, monitors, and scales your live API endpoints.
- **Saved Model:** The versioned artifact that gets packaged and deployed.

### 5. Next Steps and Progression
- **Versioning:** You can deploy multiple versions of your model to the same endpoint. The API Deployer allows you to manage these versions and control which one is active.
- **Champion/Challenger (A/B Testing):** You can split traffic between two different model versions to compare their live performance before fully rolling out a new version.
- **Monitoring and Scaling:** The API Deployer provides monitoring dashboards for your endpoints, showing latency, traffic volume, and error rates. If running on Kubernetes, you can autoscale the number of replicas to handle high traffic loads.

### 6. Common Challenges and Solutions
- **Challenge:** "The deployment is failing."
- **Solution:** Check the deployment logs in the API Deployer. A common issue is that the code environment used by the model is not present on the API node. The API node needs to have all the same package dependencies as the design node where the model was trained.
- **Challenge:** "The live API is returning an error."
- **Solution:** Check the logs of the API service in the API Deployer UI. This will show the full traceback of any errors that occurred during prediction. A common issue is that the JSON sent by the client application does not match the schema the model is expecting.
`},{id:80,slug:"building-interactive-dataiku-apps",question:"How to get started with + building interactive Dataiku apps?",answer:`
### 1. Introduction/Overview
Dataiku Apps are standalone, interactive web applications that you can build within your project. They provide a simple UI for business users, allowing them to interact with your data pipelines and models without needing to see the complex underlying Flow. This is a powerful way to deliver insights and "productionize" your analytics work for a non-technical audience.

### 2. Prerequisites
- **A clear purpose for the app:** What do you want the user to be able to do? (e.g., "enter a customer ID and see their predicted churn score," "upload a file and have it cleaned automatically").
- **An underlying Dataiku pipeline:** The app is a "front-end" to a pipeline you have already built in the Flow.
- **Basic web development knowledge (for some app types):** Knowledge of HTML or a Python web framework can be helpful.

### 3. Step-by-Step Instructions: Choosing an App Type
Dataiku offers several types of web apps. To start, go to your project's top navigation bar and click **... > Webapps**. Then click **+ NEW WEBAPP**.

#### 1. For Simple, Guided Workflows: Standard Webapp
- **What it is:** A simple app where you can arrange "slides" with inputs (like forms for user input) and outputs (like charts or datasets).
- **How to start:**
    1.  Choose **Standard** as the app type.
    2.  Use the editor to add slides and widgets. For example, add a form for the user to enter a variable, then a button that triggers a scenario, and finally a slide that displays the resulting dataset or chart.
- **Best for:** Creating simple, guided "wizard-like" interfaces.

#### 2. For Data-Rich Dashboards: Dash/Plotly or Bokeh
- **What it is:** These options let you build a webapp using popular Python libraries for creating interactive dashboards.
- **How to start:**
    1.  Choose **Dash/Plotly** as the app type.
    2.  Write Python code in the editor that uses the \`dash\` library to define the layout and interactivity of your app. Your Python code can read Dataiku datasets to populate the charts.
- **Best for:** Creating highly interactive, custom dashboards that go beyond the standard Dataiku dashboard capabilities.

#### 3. For Rapid Prototyping: Streamlit
- **What it is:** Streamlit is a Python library that makes it incredibly fast to build simple web apps. Dataiku has a native integration.
- **How to start:**
    1.  Choose **Streamlit** as the app type.
    2.  Write a simple Python script using \`streamlit\` commands (\`st.write\`, \`st.button\`, \`st.dataframe\`).
- **Best for:** Quickly creating simple data exploration tools or prototypes.

### 4. Resources and Tools
- **Webapp Editor:** The Dataiku interface for building and configuring your app.
- **Python Framework Documentation:** The official docs for Dash, Bokeh, or Streamlit are essential when building code-based apps.
- **Tutorials and Sample Projects:** Explore the Dataiku gallery for examples of webapps.

### 5. Next Steps and Progression
- **Backend Logic:** The power of Dataiku apps comes from their ability to trigger scenarios. A button in the app can run a whole data pipeline in the background and then display the result.
- **User Permissions:** Share your app with specific user groups. They will be able to use the app without needing to see or understand the underlying project Flow.

### 6. Common Challenges and Solutions
- **Challenge:** "My Python-based app isn't working."
- **Solution:** Use the "Logs" tab in the webapp editor to see any error messages from your Python backend code. Common issues are missing library imports or incorrect logic for reading data.
- **Challenge:** "How do I pass user input from the app to my pipeline?"
- **Solution:** The user input from a form in the app can be saved as a **project variable**. Your scenario can then use this variable to control its execution (e.g., in a filter or a recipe parameter).
`},{id:81,slug:"collaborating-with-business-analysts-and-data-scientists",question:"How to get started with + collaborating with business analysts and data scientists?",answer:`
### 1. Introduction/Overview
Dataiku is fundamentally a collaborative platform, designed to bridge the gap between different roles. Effective collaboration between Business Analysts (BAs), who understand the business context, and Data Scientists (DSs), who have the technical skills, is the key to building impactful data projects.

### 2. Prerequisites
- **A shared project:** A Dataiku project where all team members are added as contributors.
- **A common goal:** Everyone should be aligned on the business problem the project is trying to solve.

### 3. Step-by-Step Instructions: A Collaborative Workflow

1.  **Shared Understanding (The Kickoff):**
    *   Start the project with a meeting between BAs and DSs.
    *   The BA's role is to clearly define the **business problem**, the available data sources, and the desired outcome.
    *   The DS's role is to translate this into a potential **technical plan** (e.g., "This sounds like a classification problem; we will need to join these three tables").
    *   Document this initial plan in the **Project Wiki**.
2.  **Parallel Work in the Flow:**
    *   The visual Flow is the shared workspace. Different roles can work on different parts of the flow simultaneously.
    *   **BA Task:** A BA can use their domain knowledge to work on visual **Prepare** recipes, creating rules to clean the data and define business logic.
    *   **DS Task:** A DS can work on a more technical part of the flow, such as writing a **Python recipe** to call an API or building a predictive model in the **Visual ML Lab**.
3.  **Communication and Handoffs:**
    *   **Use the Discussions Feature:** This is crucial. On any dataset or recipe, you can have a conversation. A BA can @-mention a DS on a dataset and ask, "I've finished cleaning the customer data. Can you check if it's ready for modeling?"
    *   **Use Descriptions:** Both roles should be disciplined about adding clear descriptions to the objects they create. A DS should be able to understand what a BA's recipe does just by reading its description.
4.  **Review and Iteration:**
    *   **Dashboards:** The DS can build a model and share the results (like feature importance) on a **Dashboard**. The BA can then review these results and provide feedback based on their business knowledge ("It's interesting that customer tenure is so important; that matches our intuition").

### 4. Resources and Tools
- **The Visual Flow:** The shared language that both roles can understand.
- **Discussions and Comments:** The primary tool for contextual, asynchronous communication.
- **Project Wiki:** The single source of truth for the project's goals and documentation.
- **Dashboards:** The tool for sharing results and insights between roles.

### 5. Next Steps and Progression
- **Code Reviews:** If using Git, a BA can be included in pull requests as an optional reviewer to ensure the code's logic aligns with the business requirements.
- **Paired Sessions:** Schedule regular sessions where a BA and a DS can sit together and work on a part of the flow, combining their skills in real-time.

### 6. Common Challenges and Solutions
- **Challenge:** "The BA and DS are not speaking the same language."
- **Solution:** The visual Flow helps bridge this gap. Encourage them to "speak through the Flow." The BA can build a visual prototype of the logic they want, and the DS can then refine or optimize it.
- **Challenge:** "The data scientist built a model that is technically accurate but doesn't solve the real business problem."
- **Solution:** This happens when there is not enough collaboration at the beginning and throughout the project. The BA must be involved in reviewing the model's results and providing feedback to ensure it is aligned with the business goal. Regular check-ins are essential.
`},{id:82,slug:"translating-business-needs-into-dss-pipelines",question:"How to get started with + translating business needs into DSS pipelines?",answer:`
### 1. Introduction/Overview
The most successful data projects are those that directly address a real business need. The skill of translating a business request into a functional, technical pipeline in Dataiku is crucial for any data professional. This is a process of deconstruction and mapping.

### 2. Prerequisites
- **A clear business request:** A well-defined problem from a stakeholder (e.g., "We need to identify customers who are likely to churn," "I need a weekly report of our top-selling products").
- **Access to business stakeholders** for clarifying questions.

### 3. Step-by-Step Instructions: The Translation Process

1.  **Deconstruct the Business Request (The "5 Whys"):**
    *   Start by asking questions to get to the core of the request.
    *   **What is the ultimate goal?** (e.g., "To reduce churn by 5%").
    *   **Who is the audience?** (e.g., "The marketing team").
    *   **What data do we need?** (e.g., "Customer demographics, purchase history, website activity").
    *   **What transformations are needed?** (e.g., "Define 'active customer', calculate 'days since last purchase'").
    *   **What is the final output?** (e.g., "A dashboard with a list of at-risk customers").
    *   Document these answers in the **Project Wiki**.
2.  **Map Requirements to Dataiku Components:**
    *   Now, translate each part of your deconstructed plan into a Dataiku object or action.
    *   "Get customer demographics" -> **Create a SQL Dataset** from the \`customers\` table.
    *   "Get purchase history" -> **Create a SQL Dataset** from the \`orders\` table.
    *   "Define 'active customer'" -> Add a **Formula** step in a **Prepare recipe**.
    *   "Join the two datasets" -> Use a **Join recipe**.
    *   "Build a predictive model" -> Use the **Visual Analysis Lab**.
    *   "Create a list of at-risk customers" -> Use a **Score recipe**.
    *   "Share the list on a dashboard" -> Create a **Dashboard** with a table view of the scored dataset.
3.  **Storyboard the Flow:**
    *   Before you build, draw a simple diagram (even on a whiteboard) of how these recipes and datasets will connect. This will become the blueprint for your Flow. Use **Flow Zones** to represent the major stages you identified (e.g., Ingestion, Preparation, Modeling, Output).
4.  **Build the Pipeline:** Following your blueprint, build the pipeline in Dataiku, creating each dataset and recipe in the logical order.

### 4. Resources and Tools
- **The Project Wiki:** Your primary tool for documenting the business requirements and your technical translation plan.
- **The Visual Flow:** The canvas where you implement your plan.
- **Visual Recipes:** The building blocks for most business logic transformations.

### 5. Next Steps and Progression
- **Build a Prototype:** Quickly build a simplified version of the pipeline to show to stakeholders. This allows you to get early feedback and ensure your interpretation of their needs is correct before you invest a lot of time.
- **Iterate:** Data projects are rarely linear. Be prepared to go back and refine your understanding of the business needs as you explore the data and share initial results.

### 6. Common Challenges and Solutions
- **Challenge:** "The business requirements are vague or keep changing."
- **Solution:** This is normal. Use an agile approach. Build the pipeline in small, iterative steps and have regular check-ins with your stakeholders. Showing them a working prototype is the best way to clarify vague requirements.
- **Challenge:** "The data we need doesn't exist or is in poor condition."
- **Solution:** This is a key finding of the translation process. You must communicate this back to the stakeholders immediately. The project may need to be redefined, or a separate project may need to be started to acquire or clean the necessary data.
`},{id:83,slug:"reviewing-code-and-mentoring-junior-workers",question:"How to get started with + reviewing code and mentoring junior workers?",answer:`
### 1. Introduction/Overview
Reviewing the work of junior team members is one of the most important roles for a senior developer or SME. It's not just about catching errors; it's about teaching best practices, ensuring consistency, and helping them grow their skills. In Dataiku, this involves reviewing both visual flows and code recipes.

### 2. Prerequisites
- **A junior team member** who is actively developing in Dataiku.
- **Established team standards:** You should have a set of best practices for naming, documentation, and code style for them to follow.
- **A collaborative mindset:** The goal is to be helpful and constructive, not just critical.

### 3. Step-by-Step Instructions: A Review and Mentoring Process

#### 1. Set Up a Review Cadence
- **For Visual Flows:** Schedule a regular, short (e.g., 30-minute) session to do a "Flow walkthrough." Have the junior developer share their screen and explain the pipeline they've built.
- **For Code Recipes (Python/SQL):** Use a formal **Pull Request (PR)** process if you are using Git. Require all code changes to be submitted as a PR that you must review and approve.

#### 2. What to Look for in a Visual Flow Review
- **Clarity and Organization:** Is the Flow well-organized with **Flow Zones**? Is it easy to understand the data's journey?
- **Naming Conventions:** Are datasets and recipes named according to your team's standards?
- **Documentation:** Does every recipe and dataset have a clear **description**?
- **Efficiency:** Are they using the right recipes? For example, are they using the in-memory engine on a very large dataset where they should be using Spark or SQL push-down?
- **Logic:** Open a few key **Prepare** recipes. Are the transformation steps logical and easy to follow?

#### 3. What to Look for in a Code Review (Pull Request)
- **Correctness:** Does the code do what it's supposed to do?
- **Style:** Does the code follow your team's style guide (e.g., PEP 8 for Python)?
- **Readability:** Is the code clean, with good variable names and comments where necessary?
- **Reusability:** Are they repeating code that should be moved into a reusable function in the project **Library**?
- **Error Handling:** Does the code handle potential errors gracefully (e.g., with \`try...except\` blocks)?

#### 4. Giving Constructive Feedback
- **Be specific:** Instead of "this is confusing," say "Can we rename this variable to be more descriptive?"
- **Explain the 'why':** Explain *why* a certain best practice is important (e.g., "We use project variables instead of hardcoding so that we can easily deploy this to production").
- **Ask questions:** Instead of "do this," try "Have you considered using a Group recipe here instead of Python? It might be more performant."

### 4. Resources and Tools
- **Git and Pull Requests:** The standard for formal code reviews.
- **The Visual Flow:** The canvas for reviewing pipeline architecture and logic.
- **Your Team's Wiki:** The place where all your standards should be documented so you can refer the junior dev to it.

### 5. Next Steps and Progression
- **Pair Programming:** Schedule a session to work on a task *together*. This is one of the most effective mentoring techniques.
- **Delegate Ownership:** As the junior developer grows, give them ownership of a small project or a significant part of a larger one. Being responsible for the outcome is a powerful learning experience.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't have time to review everything."
- **Solution:** You don't have to. Focus your review efforts on the most critical parts of the pipeline. Automate what you can (e.g., use a linter for code style). Trust your team members, but verify their work on key components.
- **Challenge:** "The junior developer keeps making the same mistakes."
- **Solution:** This might mean the best practice hasn't been clearly explained or documented. Take the time to explain the concept again and add it to your team's documented standards in the Wiki so they have a written reference.
`},{id:84,slug:"gathering-requirements-for-dataiku-projects",question:"How to get started with + gathering requirements for Dataiku projects?",answer:`
### 1. Introduction/Overview
A data project without clear requirements is like a ship without a rudder. The requirements gathering process is a structured conversation with business stakeholders to understand their goals, which you then translate into a technical plan. This is the most critical phase for ensuring your project will deliver real business value.

### 2. Prerequisites
- **Access to business stakeholders:** The people who have the business problem you are trying to solve.
- **A collaborative mindset and good listening skills.**

### 3. Step-by-Step Instructions: The Requirements Gathering Framework

1.  **The Kickoff Meeting: Understanding the "Why"**
    *   Schedule a meeting with the key stakeholder(s).
    *   **Goal:** To understand the high-level business objective. Don't talk about technology yet.
    *   **Key Questions to Ask:**
        *   "What business problem are you trying to solve?"
        *   "What does success look like? If this project is successful, what will be different?"
        *   "Who is the end user or audience for this project's output?"
        *   "What decisions will they make based on this project?"

2.  **The Follow-up: Understanding the "What"**
    *   After the initial meeting, you'll need to dig into the details of the data and logic.
    *   **Key Questions to Ask:**
        *   "What specific data sources do you think we will need? Where do they live?" (e.g., Salesforce, SAP, an Excel file).
        *   "Can you introduce me to the subject matter expert for each data source?"
        *   "What are the key business rules or calculations that need to be applied?" (e.g., "How do you define an 'active customer'?").
        *   "What should the final output look like? Is it a number, a list of customers, a dashboard?"

3.  **Document Everything in a Project Brief:**
    *   Create a new page in your **Project Wiki** called "Project Brief".
    *   This document is your source of truth. It should be written in clear, simple language and should contain:
        *   **Project Goal:** A one-paragraph summary.
        *   **Stakeholders:** A list of key contacts.
        *   **Data Sources:** A list of required source systems.
        *   **Business Logic:** A summary of the key calculations and rules.
        *   **Deliverables:** A description of the final output (e.g., "A dashboard showing daily sales, refreshed by 9 AM").
4.  **Get Sign-off:**
    *   Share this Project Brief with your stakeholders and ask them to review it.
    *   Make sure they agree that you have correctly captured their requirements. This alignment is crucial before you start building.

### 4. Resources and Tools
- **The Project Wiki:** The perfect place to create and maintain your requirements documents.
- **Interviewing Skills:** The ability to ask good, open-ended questions is your most important tool.

### 5. Next Steps and Progression
- **Technical Design:** Once the business requirements are signed off, you can create a separate technical design document that maps these requirements to a specific Dataiku Flow architecture.
- **Agile Approach:** Requirements can change. Treat the Project Brief as a living document. Have regular check-ins with your stakeholders to show them progress (e.g., with a prototype dashboard) and validate that you are still on the right track.

### 6. Common Challenges and Solutions
- **Challenge:** "The stakeholders don't know what they want" or "The requirements are very vague."
- **Solution:** This is very common. The best way to clarify vague requirements is to **build a prototype**. Quickly create a simple version of the final output using a sample of the data. When stakeholders see something tangible, it becomes much easier for them to give specific feedback and refine their requirements.
- **Challenge:** "Different stakeholders are giving me conflicting requirements."
- **Solution:** You have uncovered a misalignment in the business. Your role is to highlight this conflict. Bring the stakeholders together in a meeting, present the conflicting requirements, and facilitate a discussion to help them reach a consensus.
`},{id:85,slug:"writing-technical-specs-and-process-docs",question:"How to get started with + writing technical specs and process docs?",answer:`
### 1. Introduction/Overview
While a Project Brief captures the business requirements, a Technical Specification (or "Tech Spec") translates those requirements into a detailed blueprint for developers. It describes *how* the project will be built in Dataiku. Good technical documentation ensures the solution is well-planned, and it serves as a vital reference for future maintenance and development.

### 2. Prerequisites
- **A signed-off Project Brief:** You must understand the business requirements before you can design the technical solution.
- **A solid understanding of Dataiku's capabilities.**

### 3. Step-by-Step Instructions: Creating a Tech Spec in the Wiki

1.  **Create a New Wiki Page:** In your Dataiku project's **Wiki**, create a new page titled "Technical Specification".
2.  **Structure the Document:** Use a clear, structured format. Markdown headings (\`##\`, \`###\`) are great for this. A good tech spec includes the following sections:

    *   **1. Overview:**
        *   A brief summary of the technical approach.
        *   Link back to the main "Project Brief" for business context.
    *   **2. Flow Architecture:**
        *   A high-level description of the main **Flow Zones** that will be used (e.g., \`Ingestion\`, \`Preparation\`, \`Modeling\`).
        *   You can even include a screenshot of a whiteboard or diagram of the planned Flow.
    *   **3. Data Ingestion:**
        *   List each data source.
        *   For each source, specify the **connection type** (e.g., Snowflake, S3), the **specific table or file path**, and the planned **ingestion recipe**.
    *   **4. Transformation Logic:**
        *   This is the core of the spec. Describe the purpose of each key recipe in the pipeline.
        *   Example: "\`prepare_customers\`: This recipe will filter for active customers, parse the \`join_date\` column, and create a new \`customer_age\` feature using a Formula."
    *   **5. Outputs:**
        *   Describe each final deliverable.
        *   For a dataset: Specify its schema (columns and types).
        *   For a model: Specify the algorithm and target variable.
        *   For a dashboard: List the key charts and metrics it will contain.
    *   **6. Automation & Scheduling:**
        *   Describe the **scenario** that will be built, its trigger (e.g., "Time-based, daily at 6 AM"), and any alerting (reporters) that will be configured.

3.  **Review the Spec:** Before starting development, have another developer or a tech lead review your specification. This peer review can catch design flaws or suggest more efficient approaches early on.

### 4. Resources and Tools
- **Project Wiki:** The ideal location for creating and sharing your technical documentation.
- **Markdown:** The simple syntax used in the Wiki for formatting your document.
- **Diagramming Tools (optional):** Tools like Lucidchart or even PowerPoint can be used to create a visual diagram of your planned Flow architecture to include in the spec.

### 5. Next Steps and Progression
- **Living Document:** The tech spec is a living document. As you build the project, you may discover things that require you to change the design. Update the spec to reflect your final implementation.
- **Process Documentation:** After the project is built, you can create a separate "Process Document" or "Run Book" in the Wiki that explains how to run and maintain the pipeline, intended for the support team that will take it over.

### 6. Common Challenges and Solutions
- **Challenge:** "Writing a detailed spec before I've even seen the data seems impossible."
- **Solution:** You're right. The process is iterative. The initial tech spec is based on your best guess from the requirements. You will likely need to update it after your initial data exploration. The goal is to have a plan, not to predict the future perfectly.
- **Challenge:** "This feels like too much bureaucracy."
- **Solution:** For a very small, simple project, a detailed spec might be overkill. However, for any project of moderate complexity or one that will be maintained long-term, the time spent on a clear technical plan upfront will be saved many times over in reduced development confusion and easier maintenance later.
`},{id:86,slug:"participating-in-agile-scrum-development-flows",question:"How to get started with + participating in Agile/Scrum development flows?",answer:`
### 1. Introduction/Overview
Agile and Scrum are methodologies for managing projects in an iterative and collaborative way. Dataiku's flexible and visual nature fits perfectly into this workflow. Participating in an Agile process with Dataiku involves breaking down your data project into small, manageable pieces (stories) and delivering value in short cycles (sprints).

### 2. Prerequisites
- **Understanding of Agile/Scrum concepts:** Know the basics of sprints, user stories, backlogs, and daily stand-ups.
- **A project management tool:** JIRA, Trello, or a similar tool is often used to manage the backlog and sprint board.
- **A Dataiku project connected to Git.**

### 3. Step-by-Step Instructions: A Dataiku-flavored Sprint Workflow

1.  **Sprint Planning:**
    *   The Product Owner presents the highest priority **User Stories** from the backlog.
    *   **A User Story for a Dataiku project might be:** "As a marketing analyst, I want a dataset of all customers who made a purchase in the last 30 days so that I can create a targeted email campaign."
    *   The development team breaks this story down into technical **tasks** and estimates the effort.
        *   Task 1: Create a dataset for \`customers\`.
        *   Task 2: Create a dataset for \`orders\`.
        *   Task 3: Build a recipe to join them and filter for the last 30 days.

2.  **The Sprint (Development Work):**
    *   **Create a Git Branch:** Each developer should create a new feature branch in Git for the story they are working on (e.g., \`feature/marketing-30day-report\`).
    *   **Build in Dataiku:** The developer works on their branch in Dataiku, building the necessary datasets and recipes to complete the tasks.
    *   **Commit Regularly:** The developer should commit their changes to the feature branch frequently with clear messages.
    *   **Daily Stand-ups:** The team meets briefly each day to sync up on progress ("Yesterday I finished the join recipe; today I will add the date filter. I have no blockers.").

3.  **Completing a Story (Definition of Done):**
    *   When the development work is done, the developer creates a **Pull Request** in the Git provider (e.g., GitHub).
    *   This triggers a **code review** from another team member.
    *   Once approved, the feature branch is merged into the \`main\` or \`develop\` branch.
    *   The story is moved to "Done" on the sprint board.

4.  **Sprint Review:**
    *   At the end of the sprint, the team **demos** the completed user stories to the stakeholders. In Dataiku, this is easy: you can just show them the final output dataset or a simple dashboard you built from it.

### 4. Resources and Tools
- **Project Management Tool (JIRA, etc.):** To manage the backlog and sprint board.
- **Git:** The core technology for enabling parallel development on branches.
- **Dataiku's Git Integration:** The interface for branching, committing, and merging your work.

### 5. Next Steps and Progression
- **CI/CD Integration:** Connect your Git repository to a CI/CD tool. When a pull request is created, it can automatically run a "test" scenario in Dataiku to validate the changes before they are merged.
- **Dataiku's TODO list:** For very simple projects, the built-in "TODO" list on the project homepage can be used as a lightweight sprint backlog.

### 6. Common Challenges and Solutions
- **Challenge:** "Data exploration doesn't fit neatly into a two-week sprint."
- **Solution:** This is a common issue. Address this by having a "Spike" or "Research" story. The goal of a spike is not to deliver a finished product, but to investigate a problem and produce a recommendation. The output of the spike would be a better understanding that allows you to create well-defined development stories for the next sprint.
- **Challenge:** "How do we handle a long-running job that takes more than a sprint?"
- **Solution:** Break the pipeline down into smaller, deliverable chunks. The goal for the first sprint might be just to successfully ingest the raw data. The next sprint could focus on cleaning it, and so on. Each part delivers some value and can be demonstrated.
`},{id:87,slug:"communicating-progress-to-non‑technical-stakeholders",question:"How to get started with + communicating progress to non‑technical-stakeholders?",answer:`
### 1. Introduction/Overview
Effectively communicating with non-technical stakeholders is a critical soft skill for any data professional. They don't need to know about the complexities of your Python code or SQL joins; they need to understand how the project is progressing towards solving their business problem. The key is to use visuals, speak their language, and focus on business value.

### 2. Prerequisites
- **A Dataiku project in development.**
- **Regularly scheduled meetings or check-ins** with your stakeholders (e.g., a weekly demo).

### 3. Step-by-Step Instructions: A Communication Toolkit

#### 1. Use Dashboards as Your Primary Tool
- **Why:** Dashboards are visual, interactive, and easy to understand.
- **How:** From the very beginning of the project, create a simple **Dashboard**. As you build out your pipeline, publish key insights and charts to this dashboard. Even a simple table showing the first few rows of a cleaned dataset is more effective than talking about abstract transformations.
- **In the meeting:** Share your screen and walk them through the dashboard. Explain what each chart means in business terms.

#### 2. Focus on Outcomes, Not Activities
- **Don't say:** "This week, I wrote a complex Python recipe to parse the nested JSON from the API."
- **Do say:** "This week, we successfully connected to the product system, and as you can see on this chart, we can now report on product categories."

#### 3. Create a Simple Project "ReadMe"
- **Why:** To provide a high-level summary that stakeholders can refer to at any time.
- **How:** On your project's homepage, use a **Text** tile or the **Wiki** to create a simple summary that includes:
    - The project goal in plain English.
    - A link to the main output dashboard.
    - The names of the key project contacts.

#### 4. Use the Visual Flow (Carefully)
- **Why:** The Flow can be a powerful tool to show the overall process, but it can also be overwhelming.
- **How:** If you want to show the Flow, make sure it is extremely clean and well-organized with **Flow Zones**. Collapse the zones so you are only showing a high-level, five-step diagram (e.g., \`Ingestion -> Preparation -> Modeling -> Scoring -> Output\`). This communicates the architecture without getting lost in the details.

### 4. Resources and Tools
- **Dataiku Dashboards:** Your number one communication tool.
- **The Visual Flow (with Zones):** For high-level architectural overviews.
- **The Project Wiki:** For persistent, high-level summaries.

### 5. Next Steps and Progression
- **Build a Prototype Early:** The best way to get feedback and show progress is to build a simple, end-to-end prototype, even if the data isn't perfect. This makes the project tangible for stakeholders.
- **Dataiku Apps:** For projects that require user input, build a simple Dataiku Webapp. This provides a user-friendly interface that completely hides the underlying complexity of the Flow.

### 6. Common Challenges and Solutions
- **Challenge:** "The stakeholders are getting bogged down in technical details or asking for constant changes."
- **Solution:** You may be showing them too much detail. Steer the conversation back to the business requirements. Refer to the signed-off Project Brief. If they request a change, assess its impact on the project timeline and communicate that clearly. "That's a great idea. Adding that feature will likely push our delivery date back by a week. Should we prioritize that over the original plan?"
- **Challenge:** "We have no visible progress to show this week."
- **Solution:** This can happen during a heavy technical phase. Even if there are no new charts to show, you can still communicate progress. You could show that a data quality issue has been resolved (e.g., by showing the "before" and "after" in a Prepare recipe) or that a key technical hurdle (like connecting to a difficult data source) has been overcome.
`},{id:88,slug:"troubleshooting-live-dataiku-jobs",question:"How to get started with + troubleshooting live Dataiku jobs?",answer:`
### 1. Introduction/Overview
When an automated job fails in a live, production environment, you need a systematic process to quickly diagnose and resolve the issue. Troubleshooting in Dataiku involves reading the logs to identify the error, understanding its root cause, and applying a fix.

### 2. Prerequisites
- **A failed scenario run.**
- **Access to the project** where the failure occurred.
- **A calm and methodical mindset.**

### 3. Step-by-Step Instructions: A Troubleshooting Workflow

1.  **Start with the Alert:**
    *   Your journey will likely start with an automated failure alert (e.g., from email or Slack).
    *   The alert should contain a link to the failed job. Click it. If not, navigate to the **Scenarios** page in the relevant project and find the failed run in the "Last runs" list.
2.  **Identify the Failed Step (The "Where"):**
    *   The job log view provides a visual overview of the scenario steps. The step that failed will be clearly marked in red.
    *   This tells you *where* in the pipeline the error occurred (e.g., in the \`prepare_sales_data\` recipe).
3.  **Read the Error Message (The "What"):**
    *   Click on the failed step to view its detailed log.
    *   Scroll to the bottom of the log. The specific error message will almost always be at the end.
    *   **Read the error message carefully.** It is the single most important piece of information.
    *   **Common Error Types:**
        *   \`Connection refused\` or \`Timeout\`: A source system (like a database) was down or unreachable.
        *   \`Table not found\` or \`Column not found\`: A schema change occurred in a source system.
        *   \`NullPointerException\` or \`TypeError\`: A data quality issue (like an unexpected null value) or a bug in a code recipe.
        *   \`Permission denied\`: A permissions issue, either on a data source or an output location.
4.  **Investigate the Root Cause (The "Why"):**
    *   Based on the error message, form a hypothesis.
    *   **If it's a connection error:** Try to connect to the source system manually. Is it down?
    *   **If it's a schema change:** Open the input dataset in Dataiku and try to explore it. Does the schema match what the recipe expects?
    *   **If it's a data quality issue:** Examine the input data that caused the failure. Did a value that is normally a number suddenly contain text?
5.  **Fix and Rerun:**
    *   Apply a fix (e.g., fix the code in a recipe, wait for a source system to come back online, clean the bad data).
    *   Once you believe the issue is resolved, go back to the failed job and you can often **re-run** it from the point of failure.

### 4. Resources and Tools
- **The Job Log:** Your single most important tool for troubleshooting.
- **Scenario "Last Runs" Tab:** The starting point for finding your failed jobs.
- **The "Rerun" Button:** Allows you to restart a failed job without running the already completed steps.

### 5. Next Steps and Progression
- **Build More Resilient Pipelines:** After fixing the issue, ask yourself: "How could I have prevented this?" This might lead you to add more robust **data quality checks** or better error handling in your code recipes.
- **Create a "Run Book":** For critical production pipelines, create a document in the Wiki that lists common failure modes and their specific resolution steps.

### 6. Common Challenges and Solutions
- **Challenge:** "The error message is cryptic and I don't understand it."
- **Solution:** Copy and paste the core part of the error message into a search engine. It's very likely that someone else has encountered the same issue, and you'll find explanations on forums like Stack Overflow or the Dataiku Community.
- **Challenge:** "A job failed, but the log is empty or has no error."
- **Solution:** This is rare, but can happen if the process was killed at the operating system level (e.g., an out-of-memory error that killed the Java process). In this case, you will need to work with your Dataiku administrator to check the server-level logs.
`},{id:89,slug:"resolving-pipeline-failures-and-bottlenecks",question:"How to get started with + resolving pipeline failures and bottlenecks?",answer:`
### 1. Introduction/Overview
A robust DataOps practice involves not only building pipelines but also maintaining and optimizing them. This guide covers a two-part strategy: first, how to resolve common failures, and second, how to identify and fix performance bottlenecks.

### 2. Prerequisites
- **An existing Dataiku pipeline** that is running in production.
- **Access to the project's job logs.**

### 3. Step-by-Step Instructions

#### Part A: Resolving Failures
1.  **Categorize the Failure:** When a job fails, the first step is to read the log and categorize the error. Most failures fall into one of three buckets:
    *   **Source/External System Failure:** The pipeline failed because it couldn't connect to an external system (e.g., a database was down, an API was unresponsive).
        *   **Solution:** Verify the status of the external system. Once it's back online, you can simply **rerun** the failed job.
    *   **Data Quality Failure:** The pipeline failed because the input data changed in an unexpected way (e.g., a new unexpected value in a column, a schema change, a huge drop in row count).
        *   **Solution:** Investigate the source data. You may need to update your **Prepare recipe** to handle the new data format. **This is why automated data quality checks are so important**—they can catch these issues proactively.
    *   **Code/Logic Failure:** The pipeline failed because of a bug in a code recipe (Python/SQL) or a flaw in the logic of a visual recipe.
        *   **Solution:** This requires debugging the specific recipe. Fix the code or the recipe configuration, test it on a sample of the data, and then rerun the pipeline.
2.  **Implement Preventative Measures:** After fixing the immediate issue, think about how to prevent it in the future. Could a **data quality check** have caught the issue earlier? Does your Python code need better **error handling**?

#### Part B: Resolving Bottlenecks
1.  **Identify the Slowest Step:**
    *   Go to the **Jobs** menu and look at a successful run of your pipeline.
    *   The job's Gantt chart view will clearly show the duration of each recipe. Find the one that takes the most time. This is your bottleneck.
2.  **Optimize the Bottleneck Recipe:** The optimization strategy depends on the recipe type and where it's running.
    *   **Is it an in-memory recipe on large data?** This is the most common bottleneck.
        *   **Solution:** You must **push down the computation**. In the recipe's **Advanced** settings, change the **Execution engine** to **Spark** (if your data is on HDFS/S3) or **Run on database** (if your data is in a SQL database).
    *   **Is it an inefficient SQL or Python recipe?**
        *   **Solution:** The code itself needs to be optimized. For SQL, use \`EXPLAIN\` to analyze the query plan. For Python, profile the code to see which lines are taking the most time.
    *   **Is it a join on un-indexed keys?**
        *   **Solution:** If you are joining large tables in a database, ensure the join columns have database indexes. This can make join performance orders of magnitude faster.
3.  **Measure the Improvement:** After applying an optimization, rerun the pipeline and check the new job duration to verify that your change was effective.

### 4. Resources and Tools
- **Job Logs:** For diagnosing failures.
- **Job Inspector/Gantt Chart:** For identifying performance bottlenecks.
- **Execution Engine Setting:** The primary tool for performance optimization.

### 5. Next Steps and Progression
- **Performance Baselines:** Establish a performance baseline for your critical pipelines. Monitor the run times and set up alerts if a job takes significantly longer than expected.
- **Resource Scaling:** For cloud-based deployments, if a pipeline is still slow after optimization, you may need to scale up your compute resources (e.g., use a larger Snowflake warehouse or add more nodes to your Spark cluster).

### 6. Common Challenges and Solutions
- **Challenge:** "My pipeline fails intermittently with a connection error."
- **Solution:** This points to an unstable external system. While you should report the instability, you can also make your pipeline more resilient by adding retry logic. A Python scenario step can be used to catch the failure and automatically rerun the job a few times before finally failing and sending an alert.
- **Challenge:** "I optimized a recipe, but the whole pipeline is still slow."
- **Solution:** A pipeline is a chain, and it's only as fast as its slowest link. You may have multiple bottlenecks. Repeat the identification and optimization process until the overall pipeline performance meets your requirements.
`},{id:90,slug:"staying-updated-with-the-latest-dss-features",question:"How to get started with + staying updated with the latest DSS features?",answer:`
### 1. Introduction/Overview
Dataiku is a rapidly evolving platform, with new features, performance improvements, and plugins being released regularly. Staying updated is key to leveraging the full power of the tool and ensuring you are using the most efficient and effective methods. This requires a proactive approach to learning.

### 2. Prerequisites
- **Curiosity and a desire to learn.**
- **Access to the internet.**

### 3. Step-by-Step Instructions: Your Learning Toolkit

1.  **Read the Release Notes:**
    *   This is the most important and authoritative source of information.
    *   With every major and minor version upgrade of Dataiku DSS, the product team publishes detailed **Release Notes**.
    *   Bookmark the official Dataiku documentation site. When your instance is upgraded, take 30 minutes to read through the release notes. Pay attention to new visual recipe processors, new modeling capabilities, and performance enhancements.
2.  **Follow the Dataiku Blog and Community:**
    *   The **Dataiku Blog** often features articles that do a deep dive into new features with use cases and examples.
    *   The **Dataiku Community Forum** is a place where other users discuss new features and share their experiences. It's a great place to learn practical tips.
3.  **Explore the Dataiku Academy:**
    *   The **Dataiku Academy** is not just for beginners. They regularly add new courses and tutorials that cover advanced topics and the latest features.
    *   Check in periodically to see what new learning paths or certifications have been added.
4.  **Dedicate Time for Experimentation:**
    *   Reading is not enough. You need to practice.
    *   When a new feature that looks interesting is released, dedicate a small amount of time to try it out in your **sandbox project**.
    *   For example, if a new Prepare recipe processor is released, create a dummy dataset and experiment with its settings to see exactly how it works.
5.  **Engage with Your Internal Community:**
    *   If you are part of a larger team using Dataiku, set up a "show and tell" or "brown bag" session.
    *   Encourage team members to present a new feature they've learned or a cool trick they've discovered. This helps spread knowledge across the team.

### 4. Resources and Tools
- **Dataiku Release Notes (Official Documentation):** Your primary source of truth.
- **Dataiku Blog:** For use cases and feature deep dives.
- **Dataiku Community Forum:** For practical tips and peer discussions.
- **Dataiku Academy:** For structured learning on new and advanced features.
- **A Sandbox Project:** Your personal playground for safe experimentation.

### 5. Next Steps and Progression
- **Become a Community Contributor:** As you become more expert, start answering questions on the Dataiku Community forum. Teaching others is one of the best ways to deepen your own understanding.
- **Beta Programs:** Keep an eye out for opportunities to participate in beta testing for upcoming Dataiku versions. This gives you a sneak peek at new features and allows you to provide direct feedback to the product team.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't have time to keep up."
- **Solution:** You don't need to learn every single new feature. Focus on the ones that are most relevant to the types of projects you work on. A quick scan of the release note headlines is often enough to identify the 1-2 features that will be most impactful for you. Schedule just a small amount of "learning time" into your calendar each month.
- **Challenge:** "My company is on an old version of Dataiku, so I can't use the new features."
- **Solution:** Use this as an opportunity to build a business case for upgrading. Identify new features in the latest versions that could solve a specific pain point or deliver significant value for your team (e.g., a performance improvement that would save compute costs, or a new connector that would simplify a data ingestion pipeline). Present this case to your platform administrators.
`},{id:91,slug:"navigating-the-dataiku-academy-learning-paths",question:"How to get started with + navigating the Dataiku Academy learning paths?",answer:`
### 1. Introduction/Overview
The Dataiku Academy is the official, free online learning platform for Dataiku DSS. It provides a comprehensive set of structured **Learning Paths** designed for different user roles and skill levels. Navigating these paths effectively is the fastest way for a new user to get up to speed and for an experienced user to master advanced topics.

### 2. Prerequisites
- **A web browser and an internet connection.**
- **An account on the Dataiku Academy website.**

### 3. Step-by-Step Instructions: Your Academy Roadmap

1.  **Create Your Account:** Go to the Dataiku Academy website and register for a free account.
2.  **Choose Your Learning Path:** The Academy is organized by role-based Learning Paths. For a new developer, the recommended sequence is:
    *   **Level 1: Core Designer:** Start here. This is essential for everyone. It covers the fundamental concepts of the platform: the Flow, datasets, visual recipes (Prepare, Join, Group), and basic charting.
    *   **Level 2: Advanced Designer:** Once you are comfortable with the basics, this path covers more advanced topics like complex data preparation, performance optimization, and scenario automation.
    *   **Level 3: Developer:** This path is for users who will be writing code. It covers the Python and SQL APIs, code recipes, and Git integration.
    *   **Level 4: MLOps Practitioner:** This path focuses on productionizing machine learning, covering topics like model deployment, monitoring, and automation.
3.  **Engage with the Content:**
    *   Each learning path consists of a series of courses.
    *   Each course is made up of short **videos**, followed by hands-on **tutorials** and **quizzes**.
    *   **Don't just watch the videos.** The most important part is the hands-on tutorial. Download the provided starting project and perform the steps yourself in your own Dataiku instance.
4.  **Earn Certifications:**
    *   After completing a major learning path (like Core Designer), you will be prepared to take the corresponding **certification exam**.
    *   These certifications are a great way to validate your skills and demonstrate your proficiency to employers.

### 4. Resources and Tools
- **Learning Paths:** The main, structured curriculum.
- **Course Catalog:** You can also browse individual courses if you want to learn about a specific topic without following a full path.
- **Hands-on Tutorials:** The practical exercises that are key to real learning.

### 5. Next Steps and Progression
- **Specialize:** After completing the main paths, explore specialized topics like "Time Series Forecasting" or "Computer Vision."
- **Keep Learning:** The Academy is updated regularly with new content that aligns with new versions of Dataiku DSS. Check back periodically for new courses.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't have a Dataiku instance to do the hands-on tutorials."
- **Solution:** You can sign up for the **Dataiku Online Free Edition**, which provides a cloud-based Dataiku instance perfect for learning and completing the Academy tutorials.
- **Challenge:** "I'm an experienced user. Do I really need to do the Core Designer path?"
- **Solution:** Yes. Even experienced users often find they have small gaps in their foundational knowledge. Spending a few hours to quickly go through the Core Designer path is highly recommended to ensure you have a solid understanding of the core principles before moving to advanced topics. It also familiarizes you with the terminology used in certifications.
`},{id:92,slug:"using-the-dataiku-developer-guide-and-api-docs",question:"How to get started with + using the Dataiku Developer guide and API docs?",answer:`
### 1. Introduction/Overview
While the Dataiku Academy is for structured learning, the official **Documentation** is your comprehensive reference manual. For developers, the two most important parts of the documentation are the **Developer Guide**, which contains "how-to" articles on advanced topics, and the **API Docs**, which are the detailed reference for every available API function.

### 2. Prerequisites
- **A specific question or goal:** The documentation is best used as a reference when you are trying to accomplish a specific task (e.g., "How do I use a specific Python API function?").
- **A web browser.**

### 3. Step-by-Step Instructions: Using the Documentation Effectively

#### Part 1: The Developer Guide (for "How-to" Knowledge)
- **When to use it:** When you want to learn how to perform an advanced task, like creating a custom plugin, setting up a complex deployment, or using the REST API for CI/CD.
- **How to access:**
    1.  Go to the official Dataiku documentation website.
    2.  Navigate to the "Developer Guide" section.
- **How to use:** Browse the articles by topic. For example, if you want to create your own visual recipe, you would read the section on "Plugins" which has a step-by-step tutorial.

#### Part 2: The API Docs (for Specific Function Reference)
- **When to use it:** When you are writing code (in a Python recipe or an external script) and need to know the exact name of a function, its required parameters, and what it returns.
- **How to access:**
    1.  From within your Dataiku instance, click the **Help (?)** menu in the top right corner.
    2.  Select **Python API reference** or **REST API documentation**.
- **How to use (Python API):**
    *   The documentation is organized by class (e.g., \`DSSProject\`, \`DSSDataset\`).
    *   Use the search bar to find what you're looking for. For example, if you want to know how to get the row count of a dataset, search for "row count". This will lead you to the \`get_metadata()\` method of the \`DSSDataset\` class.
- **How to use (REST API):**
    *   The REST API doc is an interactive Swagger UI. It lists all available endpoints. You can click on an endpoint to see its required parameters and even test it live from your browser.

### 4. Resources and Tools
- **Search Bar:** The search function in the documentation is very powerful and is your best starting point.
- **Code Examples:** The documentation is full of code snippets that you can copy and adapt for your own use.

### 5. Next Steps and Progression
- **Keep it Open:** When you are doing development work in Dataiku, it's a good practice to always have a browser tab open with the relevant API documentation.
- **Contribute:** The Dataiku documentation is often hosted on a public platform. If you find an error or have a suggestion for improvement, you can contribute back.

### 6. Common Challenges and Solutions
- **Challenge:** "The documentation is overwhelming; I don't know where to start."
- **Solution:** Don't try to read it like a book. Use it as a reference. Start with a specific goal (e.g., "I want to change a project variable with Python"). Then, search the documentation for keywords like "project variable" to find the relevant function.
- **Challenge:** "The example code in the documentation doesn't work."
- **Solution:** This is rare, but can happen. First, double-check that your Dataiku instance version matches the documentation version you are looking at. API functions can change between versions. Second, make sure you have adapted the example correctly for your own project (e.g., by changing the placeholder project keys and dataset names to your actual ones). If it still doesn't work, asking a question on the Dataiku Community with your code snippet is a good next step.
`},{id:93,slug:"joining-the-dataiku-community-forums",question:"How to get started with + joining the Dataiku Community forums?",answer:`
### 1. Introduction/Overview
The Dataiku Community is a vibrant online forum where Dataiku users, employees, and partners from around the world gather to ask questions, share solutions, and discuss best practices. It is one of the most valuable resources for troubleshooting and learning advanced, practical techniques.

### 2. Prerequisites
- **A web browser.**
- **A willingness to learn and participate.**

### 3. Step-by-Step Instructions: Getting Involved

1.  **Create an Account:**
    *   Navigate to the Dataiku Community website.
    *   Sign up for a free account. You can often link it to your existing Dataiku Academy account.
2.  **Start by Searching:**
    *   Before you post a new question, always use the search bar.
    *   It is highly likely that someone else has already asked a similar question. Reading existing threads is an incredibly efficient way to solve your problem.
    *   Use specific keywords from your error message or the feature you are working with for the best results.
3.  **Asking a Good Question:**
    *   If you can't find an answer, it's time to post a new question. To get a good answer, you must ask a good question.
    *   **Use a clear, descriptive title:** Bad title: "Help!". Good title: "How to resolve 'Permission Denied' error in S3 Export Recipe?".
    *   **Provide context:** Explain what you are trying to achieve.
    *   **Describe what you've already tried:** This shows you've made an effort and saves people from suggesting things you've already done.
    *   **Include specifics:** Share the relevant parts of your code, the full error message, and screenshots of your recipe configuration. Be sure to anonymize any sensitive data.
4.  **Participate and Give Back:**
    *   As you become more experienced, start reading new questions in the forum.
    *   If you see a question you know the answer to, post a helpful reply. Helping others is a great way to solidify your own knowledge.
    *   "Like" helpful answers to give recognition to the people who provided them.

### 4. Resources and Tools
- **The Search Bar:** Your most important tool.
- **Code Formatting:** When you post code, use the code formatting tools in the forum editor to make it readable.
- **User Groups:** The community has special interest groups you can join, for example, for specific industries or regions.

### 5. Next Steps and Progression
- **Become a Regular:** Make it a habit to check the community once a week to see what new topics are being discussed.
- **Earn Badges:** The community has a gamification system with badges for asking good questions, providing accepted answers, and other contributions.
- **Connect with Experts:** The forum is a great way to interact directly with Dataiku experts and seasoned developers.

### 6. Common Challenges and Solutions
- **Challenge:** "I posted a question, but nobody answered."
- **Solution:** This usually means your question was not clear enough. Review your post. Is the title descriptive? Did you provide enough context and share your error messages? You can edit your question to add more detail. A question with a clear, reproducible example is much more likely to get an answer.
- **Challenge:** "I'm hesitant to post because I'm a beginner."
- **Solution:** Don't be! The Dataiku Community is very welcoming to beginners. There are no "stupid" questions. Clearly stating that you are new to the platform will help others provide answers that are tailored to your level.
`},{id:94,slug:"earning-dataiku-core-designer-certification",question:"How to get started with + earning Dataiku Core Designer certification?",answer:`
### 1. Introduction/Overview
The Dataiku Core Designer Certification is the foundational credential that validates your understanding of the essential concepts and capabilities of Dataiku DSS. Earning it demonstrates to your employer and the industry that you have a solid grasp of building and managing data projects on the platform.

### 2. Prerequisites
- **Completion of the Core Designer Learning Path** in the Dataiku Academy.
- **Hands-on experience:** You should have personally built several basic flows in a Dataiku instance.
- **Access to a Dataiku instance** for practicing.

### 3. Step-by-Step Instructions: Your Path to Certification

1.  **Master the "Core Designer" Learning Path:**
    *   This is not optional. The certification exam is based *directly* on the content of this learning path in the **Dataiku Academy**.
    *   Go through all the courses in the path, from "Basics" to "Visual Recipes" to "Dashboards".
    *   **Crucially, do the hands-on tutorials.** Don't just watch the videos. The practical experience is what makes the knowledge stick.
2.  **Review the Key Concepts:**
    *   The exam covers the entire data pipeline lifecycle. Make sure you are very comfortable with:
        *   The structure of the **Flow**.
        *   Creating **Datasets** from different sources (especially files and databases).
        *   The main **Visual Recipes**: Prepare, Join, Group, Stack, and Split.
        *   The most common processors in the **Prepare** recipe.
        *   The difference between **Scenarios** and **Dashboards**.
3.  **Take the Practice Exam:**
    *   The Dataiku Academy provides a practice exam. Take it under exam-like conditions.
    *   This will give you a feel for the format of the questions and highlight any areas where you are weak.
4.  **Study Your Weak Areas:**
    *   Based on your practice exam results, go back to the specific Academy courses that cover the topics you struggled with. Re-watch the videos and redo the tutorials for those sections.
5.  **Register and Take the Exam:**
    *   Once you are consistently scoring well on the practice exam and feel confident, you can register for the official certification exam.
    *   The exam is online and proctored. Find a quiet time and place to take it. Read each question carefully.

### 4. Resources and Tools
- **Dataiku Academy Core Designer Learning Path:** Your primary study material.
- **The Practice Exam:** The best tool for assessing your readiness.
- **Your own Sandbox Dataiku Instance:** For practicing the hands-on skills.

### 5. Next Steps and Progression
- **Share Your Credential:** Once you pass, you will receive a digital badge. Add it to your LinkedIn profile and resume.
- **Pursue Advanced Certifications:** After the Core Designer, you can move on to more advanced certifications like the "Advanced Designer," "Developer," or "ML Practitioner."

### 6. Common Challenges and Solutions
- **Challenge:** "I failed the exam."
- **Solution:** Don't be discouraged! Many people don't pass on the first try. The exam results will give you a breakdown of your score by topic. Use this feedback to focus your studying on your weakest areas. Go back to the Academy, practice more, and then retake the exam when you feel ready.
- **Challenge:** "The questions were tricky."
- **Solution:** The exam is designed to test a real understanding, not just memorization. The questions are often scenario-based ("A user wants to do X, which recipe should they use?"). This is why the hands-on practice is so important. You need to have actually *used* the tools to be able to answer these questions confidently.
`},{id:95,slug:"experimenting-in-a-sandbox-dss-instance",question:"How to get started with + experimenting in a sandbox DSS instance?",answer:`
### 1. Introduction/Overview
A sandbox is a safe, isolated environment where you can experiment, learn, and build without any risk of impacting production data or pipelines. Having access to a sandbox Dataiku instance is the single most effective way to learn the platform, try out new features, and hone your skills.

### 2. Prerequisites
- **Access to a sandbox instance:** This could be:
    - **Dataiku Online Free Edition:** A cloud instance provided by Dataiku, perfect for individuals.
    - **A local installation:** You can install a free edition on your own machine.
    - **A company-provided sandbox:** Many companies provide a non-production "sandbox" or "dev" instance for their employees.

### 3. Step-by-Step Instructions: Making the Most of Your Sandbox

1.  **Embrace the "Break It" Mentality:**
    *   The most important rule of a sandbox is that there are no consequences. Don't be afraid to try something you're unsure about. The worst that can happen is you delete the project and start over. This freedom is essential for learning.
2.  **Recreate Academy Tutorials:**
    *   Instead of just watching the Dataiku Academy videos, perform the hands-on tutorials in your own sandbox. This active learning is far more effective.
3.  **Take on a Personal Project:**
    *   Find a public dataset that interests you (from Kaggle, government websites, etc.).
    *   Create a new project and try to solve a problem end-to-end. For example: "Can I predict house prices from this dataset?" or "Can I build a dashboard showing global COVID-19 trends?"
4.  **Explore Every Button and Menu:**
    *   Be curious. When you're in a recipe, open up the processor library and just browse. What does the "GeoIP lookup" processor do? What are the different join types? Click on everything.
5.  **Test New Features:**
    *   When a new version of Dataiku is released, use your sandbox to try out the new features mentioned in the release notes. This is the best way to stay up-to-date.
6.  **Try to Replicate Production Issues:**
    *   If you encounter a strange bug or a performance bottleneck in a production project, try to recreate a simplified version of the problem in your sandbox. This allows you to debug and test solutions safely without touching the live pipeline.

### 4. Resources and Tools
- **Dataiku Online Free Edition:** The easiest way to get a personal sandbox.
- **Public Datasets (Kaggle, etc.):** Provide interesting data for your personal projects.
- **The "Duplicate Project" Feature:** If you create a useful template or starting point, you can easily duplicate it to start new experiments.

### 5. Next Steps and Progression
- **Build a Portfolio:** The personal projects you build in your sandbox can become a portfolio that showcases your skills to current or future employers.
- **Develop Reusable Components:** Use your sandbox to develop reusable components (like a standard cleaning flow or a custom Python library) that you can later export and use in your real work projects.

### 6. Common Challenges and Solutions
- **Challenge:** "My sandbox instance is slow or running out of memory."
- **Solution:** Free editions and small local instances have limited resources. Be mindful of the size of the data you are working with. Work with samples of large datasets. If a job crashes, it's a good learning experience in understanding the resource limitations of in-memory processing.
- **Challenge:** "I don't know what to build."
- **Solution:** Start with a question you are genuinely curious about. "I wonder what the most common words are in my favorite author's books?" or "Can I analyze the nutritional information of my favorite foods?" A personal interest is the best motivator for a learning project.
`},{id:96,slug:"running-example-projects-tutorial-flows",question:"How to get started with + running example projects/tutorial flows?",answer:`
### 1. Introduction/Overview
Dataiku DSS comes bundled with several pre-built example projects that showcase best practices and demonstrate how to solve common use cases like customer churn prediction. Running and deconstructing these projects is one of the best ways to learn how a professional, end-to-end data project is structured.

### 2. Prerequisites
- **Access to a Dataiku instance.**

### 3. Step-by-Step Instructions

1.  **Create a Project from a Sample:**
    *   From the Dataiku homepage, click **+ NEW PROJECT**.
    *   Choose **Sample projects / Tutorials**.
    *   You will see a list of available samples. A great one to start with is **Customer Churn Prediction**. Select it and click **CREATE**.
2.  **Explore the Project:**
    *   Dataiku will create a new project, complete with datasets, a well-organized Flow, a pre-trained machine learning model, and a dashboard.
3.  **Deconstruct the Flow:**
    *   Open the **Flow**. Notice how it is organized with clear **Flow Zones** (\`Data preparation\`, \`Feature Engineering\`, \`Modeling\`, etc.).
    *   Click on the recipes. Open them up to see how they are configured. For example, open the \`prepare_customers\` recipe to see the data cleaning steps that were applied.
4.  **Examine the Machine Learning Model:**
    *   Find the saved model in the Flow (the green diamond).
    *   Double-click it to open it. Explore the **Results** page to see its performance metrics, feature importances, and other details.
5.  **View the Dashboard:**
    *   Go to the **Dashboards** section and view the pre-built dashboard. See how the charts and metrics are used to present the project's results.
6.  **Run the Scenarios:**
    *   Go to the **Scenarios** page. The project has scenarios for tasks like retraining the model.
    *   Run the main scenario to see how it rebuilds the entire pipeline from start to finish.

### 4. Resources and Tools
- **The Sample Projects List:** Available from the "New Project" menu.
- **The Project Wiki:** The sample projects themselves are well-documented, with a Wiki that explains the business problem and the approach taken.

### 5. Next Steps and Progression
- **Modify the Project:** Since this is your own copy of the sample, don't be afraid to change it. Try adding a new transformation step to a Prepare recipe, or train a different type of model in the lab and see if you can beat the performance of the pre-trained one.
- **Rebuild it From Scratch:** For a real challenge, try to rebuild the entire sample project yourself from scratch in a new, blank project. This will test your understanding of every step.

### 6. Common Challenges and Solutions
- **Challenge:** "The sample project seems overwhelming."
- **Solution:** Don't try to understand everything at once. Go through it one piece at a time. Start with the Flow. Follow the data from the first input dataset to the first recipe. Understand that single step completely before moving to the next. The project is designed as a learning tool, so take your time.
- **Challenge:** "A scenario in the sample project fails to run."
- **Solution:** This can sometimes happen if the sample project has dependencies that are not perfectly aligned with your instance's version. Check the scenario log for errors. This itself is a good learning experience in troubleshooting. If you get stuck, you can ask for help on the Dataiku Community forums, mentioning the specific sample project you are using.
`},{id:97,slug:"benchmarking-performance-on-sandbox-datasets",question:"How to get started with + benchmarking performance on sandbox datasets?",answer:`
### 1. Introduction/Overview
Benchmarking is the process of comparing the performance of different methods for accomplishing the same task. In a Dataiku sandbox, you can run simple benchmarks to understand the performance implications of your design choices. This helps you build an intuition for what makes a pipeline fast or slow.

### 2. Prerequisites
- **A sandbox Dataiku instance.**
- **A moderately large dataset:** You need a dataset that is large enough for performance differences to be noticeable (e.g., at least a few hundred megabytes or a few million rows).
- **A specific transformation task to benchmark** (e.g., a join, a complex aggregation).

### 3. Step-by-Step Instructions: A Simple Benchmark

Let's benchmark the performance of an in-memory join vs. a database join.

1.  **Set Up the Data:**
    *   Upload your large dataset (\`dataset_A\`) to Dataiku.
    *   Create a second, smaller dataset (\`dataset_B\`).
    *   Use an **Export** recipe to write both of these datasets into tables in a SQL database that Dataiku is connected to. You now have two versions of your data: one as "file-based" datasets and one as "SQL" datasets.
2.  **Benchmark the In-Memory Join:**
    *   In your Flow, select the *file-based* version of \`dataset_A\`.
    *   Create a **Join** recipe and join it with the *file-based* \`dataset_B\`.
    *   In the recipe's **Advanced** settings, ensure the **Execution engine** is set to **In-Memory**.
    *   Run the recipe. Go to the **Jobs** menu and note the exact duration of this recipe.
3.  **Benchmark the Database Join (Push-down):**
    *   Go back to the Flow. Now, select the *SQL* version of \`dataset_A\`.
    *   Create a **Join** recipe and join it with the *SQL* \`dataset_B\`.
    *   In the recipe's **Advanced** settings, ensure the **Execution engine** is set to **Run on database (SQL)**.
    *   Run this recipe. Go to the **Jobs** menu and note its duration.
4.  **Compare the Results:**
    *   Compare the two durations. You will almost certainly find that the database join (push-down) was significantly faster than the in-memory join. This provides a concrete, hands-on understanding of the importance of push-down execution.

### 4. Resources and Tools
- **The Job Inspector:** The tool for getting the precise duration of a recipe run.
- **Execution Engine Setting:** The key variable you will change in your benchmarks.

### 5. Next Steps and Progression
- **Benchmark Other Operations:** You can use the same pattern to benchmark other things:
    - **File Formats:** Compare the read speed of a large CSV dataset vs. the same dataset saved as Parquet.
    - **Recipes:** Compare the performance of a **Group** recipe vs. accomplishing the same aggregation in a **Python** recipe with Pandas.
    - **Spark vs. In-Memory:** If you have Spark configured, compare the performance of a Spark recipe vs. an in-memory one.

### 6. Common Challenges and Solutions
- **Challenge:** "The performance difference isn't very big."
- **Solution:** Your test dataset is likely too small. For small data, the overhead of sending the job to a database or Spark can sometimes make it seem slower. Benchmarking is most meaningful on datasets that are large enough to represent a real-world processing challenge.
- **Challenge:** "How do I ensure a fair test?"
- **Solution:** For a more scientific benchmark, try to run each version of the recipe multiple times to account for any random fluctuations in server or database load. Also, be sure to clear the caches between runs if necessary.
`},{id:98,slug:"running-pocs-combining-dataiku-and-ml-frameworks",question:"How to get started with + running POCs combining Dataiku and ML frameworks?",answer:`
### 1. Introduction/Overview
A Proof of Concept (POC) is a small-scale project designed to test the feasibility of an idea. Running a POC to combine Dataiku with an external Machine Learning framework (like Hugging Face Transformers, spaCy, or Prophet) is a great way to explore new capabilities and demonstrate their value before committing to a full-scale project.

### 2. Prerequisites
- **A specific idea to test:** A clear, focused question, e.g., "Can we use a pre-trained model from Hugging Face to classify our support tickets?"
- **A sandbox Dataiku instance.**
- **A sample of your data.**
- **Knowledge of the external ML framework.**

### 3. Step-by-Step Instructions: Building a POC

1.  **Define a Narrow, Measurable Goal:**
    *   Don't try to boil the ocean. A good POC has a very tight scope.
    *   Bad POC goal: "Integrate AI."
    *   Good POC goal: "Use the \`spaCy\` library in a Python recipe to perform Named Entity Recognition on 100 sample news articles and output a dataset of the extracted entities."
2.  **Set Up the Environment:**
    *   In your sandbox project, create a new **Code Environment**.
    *   Add the necessary Python library for the ML framework you want to test (e.g., \`spacy\`, \`transformers\`, \`prophet\`).
    *   Download any required pre-trained models.
3.  **Implement in a Notebook First:**
    *   A **Jupyter Notebook** is the perfect place for a POC. It's interactive and great for experimentation.
    *   Create a new notebook using your new code environment.
    *   Write the code to:
        1.  Load your sample data from a Dataiku dataset.
        2.  Load the pre-trained model from the external framework.
        3.  Apply the model to your data.
        4.  Print or display the results.
4.  **Document the Results:**
    *   Your notebook itself is the primary deliverable of the POC.
    *   Use **Markdown cells** to clearly explain what you did, show the code, display the output, and write a summary of your findings.
    *   **Key finding:** Did it work? How well did it perform on your sample data? What were the challenges?
5.  **Present the POC:**
    *   Share your notebook with stakeholders. Walk them through the results.
    *   Based on the success of the POC, you can now make an informed decision about whether to proceed with a full project.

### 4. Resources and Tools
- **Jupyter Notebooks:** The ideal environment for rapid, iterative POC development.
- **Code Environments:** For managing the dependencies of the external ML frameworks.
- **Markdown Cells:** For documenting your process and results within the notebook.

### 5. Next Steps and Progression
- **"Productionize" the POC:** If the POC is successful, the next step is to turn the experimental notebook code into a robust, reusable **Python recipe** as part of a real data pipeline.
- **Benchmark Performance:** As part of the POC, you might want to do some simple performance tests to estimate how long it would take to run the process on your full dataset.

### 6. Common Challenges and Solutions
- **Challenge:** "Installing the external library is difficult."
- **Solution:** Some ML frameworks have complex dependencies. Carefully read their installation instructions. It may take some trial and error to get the code environment configured correctly. This is valuable learning in itself.
- **Challenge:** "The results of the POC are not very good."
- **Solution:** That's still a successful POC! The goal of a POC is to determine feasibility. Finding out that an idea *doesn't* work is a valuable result that saves you from investing in a full project that is doomed to fail. Document why it didn't work and move on to the next idea.
`},{id:99,slug:"evaluating-dataiku-for-specific-business-cases",question:"How to get started with + evaluating Dataiku for specific business cases?",answer:`
### 1. Introduction/Overview
When your organization is considering adopting a new platform like Dataiku, it's crucial to evaluate it against a real, specific business case. This process, often called a Proof of Value (POV) or a pilot project, goes beyond generic demos and tests the platform's ability to solve *your* actual problems with *your* actual data.

### 2. Prerequisites
- **A well-defined business case:** A real, high-value business problem that you want to solve (e.g., "Improve our sales forecast accuracy," "Automate our manual weekly reporting process").
- **Access to the relevant data:** You need a sample of the real data required for the business case.
- **A trial or sandbox Dataiku instance.**
- **A dedicated team:** A small, cross-functional team (including a business stakeholder and a technical user) dedicated to the evaluation.

### 3. Step-by-Step Instructions: Running an Evaluation

1.  **Define Success Criteria Upfront:**
    *   Before you start, agree with all stakeholders on what a successful evaluation would look like.
    *   These criteria should be specific and measurable.
    *   **Examples:**
        *   "The final pipeline must run in under 1 hour, compared to the 8 hours it takes manually."
        *   "The predictive model must achieve an accuracy of at least 85%."
        *   "The business analyst on the team must be able to build a new visualization on the final dashboard without technical help."
2.  **Build an End-to-End Solution:**
    *   Using your real data, build a complete, end-to-end pipeline in Dataiku to solve the business case.
    *   This should include data ingestion, preparation, analysis or modeling, and a final output (like a dashboard or an export).
    *   **Don't cut corners.** Try to build it as you would in a real production scenario.
3.  **Involve the Whole Team:**
    *   The evaluation should not be done just by a technical expert.
    *   Have the **business analyst** use the visual recipes.
    *   Have the **data scientist** use the ML features.
    *   Have the **business stakeholder** interact with the final dashboard.
    *   This tests the platform's suitability for all the different user personas involved.
4.  **Measure Against Success Criteria:**
    *   At the end of the evaluation period (e.g., 2-4 weeks), formally measure your results against the success criteria you defined in step 1.
    *   Did you achieve the performance goals? Was the model accurate enough? Was it easy for the business user to work with?
5.  **Present the Results:**
    *   Create a final presentation or report for the decision-makers.
    *   This should include a demo of the solution you built in Dataiku, the quantitative results based on your success criteria, and qualitative feedback from the evaluation team.

### 4. Resources and Tools
- **A Sandbox/Trial Instance:** Essential for performing the evaluation.
- **Your Own Data:** Using your real data is non-negotiable for a meaningful evaluation.
- **A Project Plan:** A simple plan with a timeline and defined success criteria.

### 5. Next Steps and Progression
- **Develop a Business Case:** The results of the successful evaluation become the core of your business case for purchasing or expanding the use of Dataiku. You now have concrete data on the value it can provide.
- **Productionize the Pilot:** The project you built for the evaluation can often become the first production pipeline once the platform is officially adopted.

### 6. Common Challenges and Solutions
- **Challenge:** "We can't get access to the real data for security reasons."
- **Solution:** This is a common hurdle. You will need to work with your security and data governance teams. Explain that the evaluation can be done in an isolated sandbox environment. If necessary, you may need to use an anonymized or sanitized version of the data, but it should still be representative of the real data's structure and complexity.
- **Challenge:** "The business case is too big to build in a short evaluation period."
- **Solution:** You have chosen too broad a scope. A good evaluation project should be small enough to be completed in a few weeks. Narrow the scope to a single, critical piece of the larger problem. The goal is to prove the *value* and *capability*, not to boil the ocean.
`},{id:100,slug:"building-your-personal-dataiku-learning-portfolio",question:"How to get started with + building your personal Dataiku learning portfolio?",answer:`
### 1. Introduction/Overview
A portfolio of projects is the most powerful way to showcase your skills to a current or potential employer. For a Dataiku developer, this means having a collection of well-built, well-documented projects that demonstrate your expertise across the platform. Building this portfolio is a continuous process of learning and creating.

### 2. Prerequisites
- **A personal sandbox Dataiku instance:** Dataiku Online Free Edition is perfect for this.
- **Curiosity and an interest in data.**

### 3. Step-by-Step Instructions: Building Your Portfolio

1.  **Find Interesting Public Datasets:**
    *   You need data to build projects. Look for datasets that genuinely interest you.
    *   **Great sources:**
        *   Kaggle Datasets
        *   Google Dataset Search
        *   Your national government's open data portal (e.g., data.gov in the US).
        *   Data Is Plural newsletter.
2.  **Create a Themed Project:**
    *   Choose a dataset and define a clear goal for a project.
    *   **Examples:**
        *   "Analyze NYC Airbnb data to find the factors that drive price." (A regression project).
        *   "Analyze movie data to predict a film's genre based on its plot summary." (An NLP classification project).
        *   "Build a dashboard to visualize global energy consumption trends." (A data visualization project).
3.  **Build a High-Quality, End-to-End Flow:**
    *   Don't just create a messy notebook. Build a clean, professional-looking project that demonstrates best practices.
    *   Use **Flow Zones** to organize your pipeline.
    *   Use **visual recipes** for data preparation.
    *   Add **Python or SQL recipes** if needed to show your coding skills.
    *   If it's a modeling project, use the **Visual ML Lab** and deploy a model.
    *   Create a final **Dashboard** to present your results.
4.  **Document Your Project Meticulously:**
    *   This is as important as the pipeline itself.
    *   Use the **Project Wiki** to write a detailed "ReadMe" explaining the project's goal, your approach, and your key findings.
    *   Add clear **descriptions** to every single dataset and recipe in your Flow.
    *   Your goal is for someone to be able to open the project and understand what you did and why without you having to be there to explain it.
5.  **Create a Portfolio of 3-5 Projects:**
    *   You don't need dozens of projects. A portfolio of 3-5 high-quality, diverse projects is far more impressive. Try to have projects that showcase different skills (e.g., one on data cleaning, one on machine learning, one on dashboarding).

### 4. Resources and Tools
- **Dataiku Online Free Edition:** Your personal portfolio-building studio.
- **Public Dataset Repositories:** Your source of raw materials.
- **GitHub (Optional):** You can link your Dataiku portfolio projects to a public GitHub repository to also showcase your Git skills.

### 5. Next Steps and Progression
- **Presenting Your Portfolio:**
    *   In a job interview, you can say, "I can actually show you an example." Share your screen and walk the interviewer through one of your portfolio projects. This is incredibly powerful.
    *   You can also take screenshots of your Flows and Dashboards to include in a personal website or an enhanced resume.
- **Keep it Updated:** As you learn new skills in Dataiku, create new projects that demonstrate them.

### 6. Common Challenges and Solutions
- **Challenge:** "I can't use my work projects in my portfolio due to company confidentiality."
- **Solution:** You are correct. You must **never** use confidential company data or projects in a personal portfolio. All portfolio projects must be built from scratch by you, using publicly available data. This is not a limitation; it's an opportunity to explore topics you are passionate about outside of work.
- **Challenge:** "My project is just a copy of a tutorial."
- **Solution:** A portfolio should showcase your own skills and thinking. While tutorials are great for learning, your portfolio projects should go a step further. Take the dataset from a tutorial, but then ask your *own* questions of it. Build a different kind of model. Create a new visualization. Add your own unique spin to make the project yours.
`}]},74928:(e,t,a)=>{"use strict";a.d(t,{ab:()=>o});let o=[{id:301,slug:"when-to-use-prepare-vs-python-recipe",question:"How to get started with understanding when to use a Prepare recipe vs. a Python recipe?",answer:`
### 1. Introduction/Overview
Choosing between a visual Prepare recipe and a code-based Python recipe is a fundamental architectural decision in Dataiku. The general principle is **"visual first."** This guide helps you understand the decision framework, enabling you to build pipelines that are both powerful and maintainable.

### 2. Prerequisites
- **A clear transformation goal:** Know what you need to do to the data.
- **Familiarity with the Prepare recipe's processors.**
- **Basic Python and Pandas knowledge** (if considering the Python recipe).

### 3. Step-by-Step Instructions
1.  **Default to the Prepare Recipe:** Always start by trying to implement your logic in a Prepare recipe. Its visual nature makes it transparent and easy for others to understand.
2.  **Identify Your Task's Category:**
    *   **Data Cleaning:** (e.g., handling nulls, filtering rows, standardizing text). Use **Prepare**.
    *   **Feature Engineering:** (e.g., creating new columns with formulas, parsing dates, splitting strings). Use **Prepare**.
    *   **Complex Custom Logic:** (e.g., iterative calculations, custom algorithms, complex conditional logic). Use **Python**.
    *   **External Integration:** (e.g., calling an external REST API, using a specific library not in Dataiku). Use **Python**.
3.  **Evaluate Complexity:** If your Prepare recipe script grows to over 30-40 steps, consider if a Python recipe would be more concise and readable for that specific, complex part of the logic.

### 4. Resources and Tools
- **Prepare Recipe Processor Library:** Your first stop for any transformation.
- **Python Recipe:** Your tool for ultimate flexibility.
- **Team's Coding Standards:** A guide for when and how to write code.

### 5. Next Steps and Progression
- **Hybrid Approach:** A common pattern is to use a Prepare recipe for 90% of the cleaning and then a Python recipe for one specific, complex step. The output of the Python recipe can then be fed into another visual recipe.
- **Custom Processors:** For reusable logic, a senior developer can create a custom processor in Python that can then be used visually within a Prepare recipe.

### 6. Common Challenges and Solutions
- **Challenge:** "My logic is possible in Prepare, but it takes many steps."
- **Solution:** This is a judgment call. If the logic is clearer and more maintainable as a 10-line Python function than as 20 complex visual steps, then Python is the right choice.
- **Challenge:** "Which is more performant?"
- **Solution:** A Prepare recipe running on a database (SQL) or Spark engine will almost always be more performant than a Python recipe processing large data in-memory. Performance should be a key factor in your decision.
`},{id:302,slug:"parsing-dates-when-dataiku-misdetects-format",question:"How to get started with parsing dates when Dataiku misdetects format?",answer:`
### 1. Introduction/Overview
Dates often come in non-standard string formats, and Dataiku's automatic detection might not guess the format correctly. Manually specifying the date format in a Prepare recipe is a fundamental data cleaning skill.

### 2. Prerequisites
- **A dataset with a date column** that is currently a "string" data type.
- **Knowledge of the date format** used in the string (e.g., is it \`Month-Day-Year\` or \`Day/Month/Year hh:mm:ss\`?).

### 3. Step-by-Step Instructions
1.  **Open a Prepare Recipe:** Start a new Prepare recipe on your dataset.
2.  **Select the "Parse date" Processor:** Click the header of your string-based date column. From the processor list, choose **Parse date**.
3.  **Manually Specify the Format:**
    *   The processor will try to auto-detect the format. If it fails or is incorrect, uncheck the "auto-detect" option.
    *   An input field will appear. Here, you must type the format that matches your data.
    *   **Example:** If your data looks like "25-Dec-2023", the format is \`dd-MMM-yyyy\`.
    *   **Example:** If your data is "01/15/2024 14:30", the format is \`MM/dd/yyyy HH:mm\`.
4.  **Preview and Run:** The preview pane will show if the parsing was successful. If the output column looks correct, run the recipe.

### 4. Resources and Tools
- **"Parse date" Processor:** The primary tool for this task.
- **Date Format Cheatsheet:** In the "Parse date" processor UI, there is often a link to a help page that shows all the possible format codes (\`yyyy\`, \`MM\`, \`dd\`, \`HH\`, etc.). This is an essential reference.

### 5. Next Steps and Progression
- **Handling Multiple Formats:** If a single column contains dates in multiple different formats, you may need to use a more advanced approach with multiple "Parse date" steps, each filtering for a specific pattern.
- **Time Zone Handling:** After parsing a date, you can use other processors to handle time zone conversions if necessary.

### 6. Common Challenges and Solutions
- **Challenge:** "The processor creates all null values."
- **Solution:** This means the format you specified does not match the data. Check your format string very carefully for typos. Even a small difference (like a hyphen vs. a slash) will cause the parsing to fail.
- **Challenge:** "Some dates parse correctly, but others don't."
- **Solution:** Your column likely contains mixed formats. Use a **Filter** step to isolate the rows that are failing and inspect their format. You may need a separate "Parse date" step just for them.
`},{id:303,slug:"handling-nulls-when-find-replace-fails",question:"How to get started with handling nulls when the Find & Replace processor fails?",answer:`
### 1. Introduction/Overview
A "null" value in a database or dataset is a special state representing the absence of data. It is not the same as an empty string ("") or the string "null". The "Find & Replace" processor works on string values and therefore cannot find or replace nulls. This guide shows you the correct tools for handling nulls.

### 2. Prerequisites
- **A dataset with missing (null) values.**
- **A clear strategy** for what to do with the nulls (e.g., remove them, fill them).

### 3. Step-by-Step Instructions
1.  **Open a Prepare Recipe** on your dataset. Dataiku's data quality bars will immediately show you which columns contain nulls.
2.  **Choose the Correct Processor based on your goal:**
    *   **To Remove Rows with Nulls:**
        *   Click the column header.
        *   Select the processor: **Remove rows where value is empty**.
    *   **To Fill Nulls with a Specific Value (Imputation):**
        *   Click the column header.
        *   Select the processor: **Impute missing values**.
        *   You can then choose to fill with the mean, median, a constant value (like 0 or "Unknown"), etc.
    *   **To Filter for Null or Non-Null Rows:**
        *   Add a **Filter** step.
        *   In the filter options, choose the column and select the condition **is empty** or **is not empty**.

### 4. Resources and Tools
- **The Data Quality Bar:** Your first indicator of null values.
- **Correct Processors:** \`Impute missing values\`, \`Remove rows where value is empty\`, \`Filter\`.
- **Incorrect Processor:** \`Find & Replace\`.

### 5. Next Steps and Progression
- **Conditional Logic with Nulls:** In a **Formula** step, you can use functions like \`isNULL(column)\` or \`isBlank(column)\` to create conditional logic that behaves differently for null values.
- **Indicator Columns:** Use the "Create indicator for missing values" processor to create a new binary column that flags whether the original value was missing before you fill it in.

### 6. Common Challenges and Solutions
- **Challenge:** "I used 'Impute' but the nulls are still there."
- **Solution:** Make sure you clicked the "Run" button on the recipe. The changes are only applied to the output dataset when the recipe is executed.
- **Challenge:** "My column contains both nulls and empty strings. How do I handle both?"
- **Solution:** You can chain processors. First, use a "Find & Replace" to replace empty strings with nulls. Then, use the "Impute missing values" processor to handle all the nulls consistently.
`},{id:304,slug:"splitting-multi-value-columns",question:"How to get started with splitting multi-value columns using Split recipe?",answer:`
### 1. Introduction/Overview
It's common to find data where a single text field contains multiple values separated by a delimiter (e.g., "action,comedy,drama"). To analyze this properly, you need to split this string into separate columns or rows. The **Split column** processor in a Prepare recipe is the perfect tool for this.

### 2. Prerequisites
- **A dataset with a multi-value column.**
- **Knowledge of the delimiter** used to separate the values (e.g., a comma, a semicolon, a pipe character).

### 3. Step-by-Step Instructions
1.  **Open a Prepare Recipe** on your dataset.
2.  **Select the "Split column" Processor:** Click the header of the multi-value column and choose **Split column**.
3.  **Configure the Splitting:**
    *   **Delimiter:** Enter the character that separates your values (e.g., \`,\`).
    *   **Output Format:** You have a choice:
        *   **Split into columns:** This will create new columns named \`your_column_1\`, \`your_column_2\`, etc. This is good if you always have a fixed number of values.
        *   **Split into rows:** This will "unfold" the data, creating a new row for each value. This is often more flexible for analysis and is the recommended approach.
4.  **Preview and Run:** The preview pane will show the result. If it looks correct, run the recipe.

### 4. Resources and Tools
- **Prepare Recipe:** The home of the splitting tool.
- **"Split column" Processor:** The specific tool for this task.

### 5. Next Steps and Progression
- **Post-Split Cleaning:** After splitting, you will likely need to add more steps to trim whitespace from the new values.
- **One-Hot Encoding:** If you split into columns, you can then use the **Dummify** processor to one-hot encode these categories for use in machine learning models.
- **Aggregation:** If you split into rows, you can now use a **Group** recipe to perform aggregations on the individual values (e.g., "count how many times each genre appears").

### 6. Common Challenges and Solutions
- **Challenge:** "The split is not working as expected."
- **Solution:** Double-check your delimiter. Is it just a comma, or is it a comma followed by a space? You can specify multi-character delimiters.
- **Challenge:** "My data has a variable number of values in each row."
- **Solution:** In this case, you should almost always use the **Split into rows** option. Splitting into columns will result in many empty columns for the rows that have fewer values.
`},{id:305,slug:"configuring-fuzzy-join-when-exact-match-fails",question:"How to get started with configuring fuzzy join when exact match fails?",answer:`
### 1. Introduction/Overview
When you need to join two datasets but the join keys have typos or slight variations (e.g., "My Company, Inc." vs. "My Company"), a standard join will fail. The **Fuzzy Join** recipe allows you to join these records based on string similarity, which is a powerful technique for data reconciliation.

### 2. Prerequisites
- **Two datasets to join** with "messy" but similar key columns.
- **An understanding of what makes a "good match"** in your business context.

### 3. Step-by-Step Instructions
1.  **Select the Fuzzy Join Recipe:** In your Flow, select your "left" dataset and from the right-hand panel, find and choose the **Fuzzy Join** recipe. Select your "right" dataset as the second input.
2.  **Select Key Columns:** In the recipe settings, choose the text columns from each dataset that you want to join on.
3.  **Choose a Similarity Metric:** This is the most important setting. It's the algorithm used to compare the strings.
    *   **Levenshtein:** Good for short strings and typos.
    *   **Jaccard:** Good for comparing sets of words where the order might be different.
    *   Experiment to see which works best for your data.
4.  **Set the Similarity Threshold:** Choose a value between 0 and 1. This defines how "similar" two strings must be to be considered a match. A higher value (e.g., 0.9) means a stricter match, while a lower value (e.g., 0.7) is more lenient.
5.  **Review Matches:** The preview pane will show you the matches found and their similarity scores. Adjust the threshold and metric until you are satisfied with the quality of the matches.
6.  **Run the Recipe:** Execute the recipe to generate the joined dataset.

### 4. Resources and Tools
- **Fuzzy Join Recipe:** The dedicated visual tool for this task.
- **Prepare Recipe:** It is highly recommended to use a Prepare recipe on your key columns *before* the fuzzy join to clean them (e.g., convert to lowercase, remove punctuation). This significantly improves match quality.

### 5. Next Steps and Progression
- **Manual Review:** The output of a fuzzy join should often be reviewed by a human to confirm the accuracy of the matches, especially in critical applications.
- **Combining with Exact Joins:** You can perform an exact join first to handle the easy matches, and then use a fuzzy join only on the records that did not match.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm getting too many bad matches."
- **Solution:** Your similarity threshold is too low. Increase it to require a stronger match.
- **Challenge:** "It's missing matches that are obviously correct."
- **Solution:** Your threshold may be too high, or you have not sufficiently cleaned the key columns before the join. Pre-processing is key.
`},{id:306,slug:"grouping-by-multiple-keys-correctly",question:"How to get started with grouping by multiple keys correctly in Group recipe?",answer:`
### 1. Introduction/Overview
Grouping (or aggregating) data is a fundamental data manipulation task. The **Group** recipe in Dataiku allows you to calculate summary statistics (like sum, count, average) for different segments of your data. A common requirement is to group by a combination of several columns at once.

### 2. Prerequisites
- **A dataset ready for aggregation.**
- **A clear understanding of what you want to calculate** (e.g., "the total sales per country per month").

### 3. Step-by-Step Instructions
1.  **Select the Group Recipe:** In your Flow, select your dataset and choose the **Group** recipe from the right-hand panel.
2.  **Define Your Group Keys:**
    *   This is the crucial step. In the "Group by" section, you can add multiple columns.
    *   For the goal "total sales per country per month," you would add both the \`country\` column and the \`month\` column to the "Group by" keys.
3.  **Define Your Aggregations:**
    *   For all the other columns you want to compute, you must specify an aggregation function.
    *   For our example, you would select the \`sales\` column and choose the **Sum** aggregation.
4.  **Preview and Run:** The output preview will show the aggregated data, with one row for each unique combination of country and month, along with their total sales. Run the recipe to generate the output.

### 4. Resources and Tools
- **Group Recipe:** The primary visual tool for all aggregation tasks.
- **SQL:** The Group recipe is the visual equivalent of a SQL \`GROUP BY\` clause with aggregate functions.

### 5. Next Steps and Progression
- **Multiple Aggregations:** You can compute multiple aggregations at once. For example, you could calculate the **Sum** of sales and the **Average** of sales in the same recipe.
- **Post-Aggregation Calculations:** Add a **Prepare** recipe after the Group recipe to perform further calculations on your aggregated data (e.g., calculate the "average transaction value" by dividing the sum of sales by the count of orders).

### 6. Common Challenges and Solutions
- **Challenge:** "My output has fewer columns than I expected."
- **Solution:** Any column from your input that is not either a group key or included in an aggregation will be dropped from the output. Make sure you have defined an aggregation for every column you want to keep.
- **Challenge:** "The numbers in my aggregation seem wrong."
- **Solution:** Double-check your group keys. Have you included all the necessary keys to define the correct level of granularity? Also, ensure you have chosen the correct aggregation function (e.g., Sum vs. Average).
`},{id:307,slug:"getting-unexpected-results-in-window-recipe",question:"How to get started with getting unexpected results in a Window recipe frame?",answer:`
### 1. Introduction/Overview
A **Window** recipe performs calculations across a sliding set of rows related to the current row. It's powerful for tasks like calculating running totals or moving averages. Unexpected results almost always stem from two things: incorrect partitioning or incorrect ordering.

### 2. Prerequisites
- **A dataset for window calculations** (e.g., time-series data).
- **A clear goal** (e.g., "calculate a 7-day moving average of sales for each store").

### 3. Step-by-Step Instructions
1.  **Select the Window Recipe:** Choose your input dataset and select the **Window** recipe.
2.  **Define the Partition (Optional but common):**
    *   If you want the calculation to restart for different groups, define a **Partition**.
    *   For our example ("...for each store"), you would partition by the \`store_id\` column.
3.  **Define the Order (CRITICAL):**
    *   You **must** specify the order of the rows for the window function to work correctly.
    *   For our example, you must **Order by** the \`date\` column in ascending order. This is the most common mistake.
4.  **Define the Window Frame:**
    *   This defines which rows (relative to the current row) are included in the calculation.
    *   For a 7-day moving average, the frame would be: **6 preceding rows** to **0 following rows** (a total of 7 rows).
5.  **Define the Aggregation:**
    *   Choose the column to aggregate (e.g., \`sales\`) and the function (**Average**).

### 4. Resources and Tools
- **Window Recipe:** The dedicated visual recipe for window functions.
- **SQL Window Functions:** This recipe is the visual equivalent of SQL's powerful \`OVER (PARTITION BY ... ORDER BY ...)\` clause.

### 5. Next Steps and Progression
- **Running Totals:** To calculate a running total, set the window frame to be "unbounded preceding" to the "current row".
- **Lag/Lead Functions:** Use the "Lag" or "Lead" aggregations to get a value from a previous or future row, which is useful for calculating period-over-period changes.

### 6. Common Challenges and Solutions
- **Challenge:** "My running total is not correct."
- **Solution:** **You forgot to set the Order.** The most common error is failing to define the ordering of the rows. Without an explicit order, the concept of "preceding" rows is meaningless.
- **Challenge:** "My moving average is being calculated across all stores at once."
- **Solution:** **You forgot to set the Partition.** If you don't partition by \`store_id\`, the recipe will treat the entire dataset as a single group.
`},{id:308,slug:"interpreting-top-n-recipe-with-duplicates",question:"How to get started with interpreting Top N recipe when ranking duplicates?",answer:`
### 1. Introduction/Overview
The **Top N** recipe is used to filter your dataset to keep only the top or bottom records based on the values in a specific column. A common point of confusion is how the recipe handles ties (duplicate values in the ordering column). Understanding the different ranking strategies is key to getting the expected output.

### 2. Prerequisites
- **A dataset to rank.**
- **A clear goal** (e.g., "find the top 10 best-selling products").

### 3. Step-by-Step Instructions
1.  **Select the Top N Recipe:** Choose your input dataset and select the **Top N** recipe.
2.  **Configure the Ranking:**
    *   **Order by:** Select the column to rank by (e.g., \`sales_total\`). Choose **Descending** for "Top N".
    *   **Retrieve:** Select "Top" and enter the number you want (e.g., 10).
3.  **Configure the Ranking Strategy (The Key Step):**
    *   This setting controls how ties are handled.
    *   **"Rows with same value have same rank (dense)" (Standard Competition Ranking):** This is usually what people expect. If two products are tied for 3rd place, they both get rank 3, and the next product gets rank 4.
    *   **"No duplicate ranks (sequential)":** If two products are tied, they will get different ranks based on their original order in the data.
4.  **Decide How to Handle Ties:**
    *   After choosing a ranking strategy, you must decide what to do if multiple rows have the same rank within the Top N.
    *   **"Retrieve all rows":** If you ask for the Top 10, but 3 products are tied for 10th place, this option will return all 3, resulting in 12 total rows.
    *   **"Select first N rows":** This will strictly return only 10 rows, potentially cutting off some of the tied records.

### 4. Resources and Tools
- **Top N Recipe:** The dedicated visual tool for this purpose.
- **Window Recipe:** For more complex ranking scenarios (e.g., ranking within groups), you can use the Window recipe to first compute a rank, and then use a Filter recipe on the rank column.

### 5. Next Steps and Progression
- **Grouped Ranking:** In the Top N recipe, you can also define "Per" groups. This allows you to find the "Top 10 products *per country*," for example.

### 6. Common Challenges and Solutions
- **Challenge:** "I asked for the Top 10, but I got 15 rows back."
- **Solution:** You have chosen the "Retrieve all rows" option for handling ties. This is expected behavior. If you need exactly 10 rows, change the setting to "Select first N rows".
- **Challenge:** "The ranking seems arbitrary when values are the same."
- **Solution:** You are likely using the "No duplicate ranks" strategy. Change this to the "dense" ranking strategy to ensure items with the same value get the same rank.
`},{id:309,slug:"stacking-datasets-and-preserving-schema",question:"How to get started with stacking datasets and preserving schema with Stack recipe?",answer:`
### 1. Introduction/Overview
The **Stack** recipe is used to append datasets on top of each other, similar to a \`UNION ALL\` in SQL. For the stack to work correctly, the schemas (column names and types) of the input datasets need to be aligned. This guide explains how to manage schemas during a stack.

### 2. Prerequisites
- **Two or more datasets** that you want to append. They should represent the same kind of data (e.g., sales data from two different regions).

### 3. Step-by-Step Instructions
1.  **Select the Stack Recipe:** Choose one of your input datasets, and from the right-hand panel, select the **Stack with...** recipe. Add your other dataset(s) as inputs.
2.  **Review the Automatic Schema Matching:**
    *   In the Stack recipe settings, Dataiku will automatically try to align the columns based on their names.
    *   The preview will show the combined schema. Columns that exist in all datasets and have the same name will be matched perfectly.
3.  **Manually Adjust Schema (If Necessary):**
    *   If your column names are inconsistent (e.g., \`cust_id\` in one dataset and \`customer_id\` in another), Dataiku may not match them correctly.
    *   In the schema mapping table, you can manually drag and drop columns to align them. You can drag the \`cust_id\` column and drop it onto the \`customer_id\` column to tell Dataiku they are the same thing.
4.  **Handle Data Types:** The recipe will also try to find a common data type. If there are conflicts (e.g., a column is a string in one dataset and a number in another), you may need to go back and use a **Prepare recipe** on one of the inputs to fix the type before stacking.
5.  **Run the Recipe:** Once the schema is aligned correctly, run the recipe to create the final, stacked dataset.

### 4. Resources and Tools
- **Stack Recipe:** The primary visual tool for appending data.
- **Schema Mapping UI:** The interface within the Stack recipe for manually aligning columns.
- **Prepare Recipe:** An essential pre-processing tool to clean up schemas *before* you stack.

### 5. Next Steps and Progression
- **Adding a Source Column:** It's often a good practice to add a column that indicates the original source of each row before you stack the datasets. This allows you to trace the data back to its origin.
- **Stacking Many Files:** You can point a single dataset to a folder of files. If the files have the same structure, Dataiku will automatically stack them, which can be simpler than using a Stack recipe on many individual datasets.

### 6. Common Challenges and Solutions
- **Challenge:** "The output has extra columns I didn't expect."
- **Solution:** This happens when column names are not identical. For example, a small typo (\`sales\` vs. \`sale\`) will cause Dataiku to treat them as two separate columns. You must either fix the name in the source or manually align them in the Stack recipe's schema mapping view.
- **Challenge:** "My stacked data is a mess of different data types."
- **Solution:** You must enforce a consistent schema *before* stacking. Use a Prepare recipe on each input to ensure the column types are the same.
`},{id:310,slug:"syncing-two-datasets-with-sync-recipe",question:"How to get started with syncing two datasets with Sync recipe and matching granular keys?",answer:`
### 1. Introduction/Overview
The **Sync recipe** has a very specific purpose that is often misunderstood by beginners. It is **not** for joining or merging two different datasets based on a key. Its purpose is to synchronize the *entire content* of one dataset to a different storage location. Think of it as a "copy/paste" for datasets.

### 2. Prerequisites
- **A source dataset** you want to copy.
- **A target storage location,** such as a different database or a cloud storage location.

### 3. Step-by-Step Instructions
1.  **Understand the Use Case:** The primary use case for Sync is to move data between different systems. For example, you might perform all your transformations on data in a Snowflake data warehouse, and then use a Sync recipe to copy the final, aggregated result table to a simpler PostgreSQL database that feeds a web application.
2.  **Select the Sync Recipe:** In your Flow, select the source dataset you want to copy. From the right-hand panel, choose the **Sync** recipe.
3.  **Configure the Output:**
    *   The "Output" of the Sync recipe is not a new dataset in your Flow, but rather the configuration of the destination.
    *   You will need to change the "Connection" for the output to point to your target storage system (e.g., your PostgreSQL connection).
    *   You can also specify the name of the new table to be created in the destination.
4.  **Run the Recipe:** When you run the Sync recipe, Dataiku will read the data from the source dataset and write it to the destination, ensuring the schema and data are identical.

### 4. Resources and Tools
- **Sync Recipe:** The dedicated tool for copying datasets between storage locations.
- **Join Recipe:** The tool you should use if you are trying to combine two datasets based on a key.

### 5. Next Steps and Progression
- **Partitioned Sync:** You can use the Sync recipe on partitioned datasets to copy only the latest partitions, which is efficient for incremental updates.
- **Automated Replication:** You can use a scenario to run a Sync recipe on a schedule, creating an automated replication job between two systems.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm trying to join two datasets, but the Sync recipe isn't working."
- **Solution:** You are using the wrong tool for the job. The Sync recipe does not have any concept of a "join key". You need to use the **Join recipe** to combine two datasets based on matching values in their columns.
- **Challenge:** "The sync to my database is failing."
- **Solution:** This is likely a permissions issue on the *destination*. The credentials Dataiku is using for the target connection need to have permissions to \`CREATE TABLE\` and \`INSERT\` data in the target database.
`},{id:311,slug:"reordering-columns-and-maintaining-order",question:"How to get started with reordering columns and maintaining order?",answer:`
### 1. Introduction/Overview
The order of columns in a dataset can be important for readability or for systems that expect a specific schema order. While some recipes might change column order, the definitive way to set and maintain a specific order is by using a **Prepare recipe**.

### 2. Prerequisites
- **A dataset** whose columns you want to reorder.
- **A clear idea of the desired final column order.**

### 3. Step-by-Step Instructions
1.  **Open a Prepare Recipe:** Create a new **Prepare** recipe on your dataset.
2.  **Reorder via Drag-and-Drop:**
    *   In the main data table view, you can simply click on a column header and drag it left or right to a new position. This is the quickest way to reorder a few columns.
3.  **Reorder via the Columns View:**
    *   For more complex reordering, click the **Columns** button at the top right of the recipe editor (it looks like \`[#]\`).
    *   This opens a list of all your columns. You can drag and drop the column names in this list to reorder them. You can also select multiple columns and move them together.
4.  **Run the Recipe:** After arranging the columns in your desired order, run the recipe. The output dataset will now have the new, enforced column order. This order will be maintained by downstream recipes unless they explicitly change it.

### 4. Resources and Tools
- **Prepare Recipe:** The primary tool for this task.
- **The Columns View:** A useful interface for managing the order and names of many columns at once.
- **The Sort Recipe:** **Do not use the Sort recipe for this.** The Sort recipe reorders *rows* based on column values; it does not reorder columns.

### 5. Next Steps and Progression
- **Standardized Schemas:** For critical output datasets, it's a good practice to have a final Prepare recipe whose only job is to select the necessary columns and put them in the correct, standard order before exporting the data.
- **Scripting:** In a Python recipe, you can also reorder columns by providing a list of column names in the desired order when you select from your DataFrame: \`df = df[['col_C', 'col_A', 'col_B']]\`.

### 6. Common Challenges and Solutions
- **Challenge:** "I reordered my columns, but the next recipe in the Flow changed the order again."
- **Solution:** Some recipes, particularly the Join recipe, can change column order (e.g., by bringing in new columns at the end). To guarantee the final order, you should place your "reordering" Prepare recipe as the very last step in your pipeline, right before the final output dataset.
- **Challenge:** "It's tedious to reorder many columns."
- **Solution:** Use the "Columns" view in the Prepare recipe. You can quickly sort the columns alphabetically by name and then make minor manual adjustments, which is often faster than dragging them one by one.
`},{id:312,slug:"applying-pivot-recipe-and-preventing-missing-columns",question:"How to get started with applying Pivot recipe and preventing missing columns?",answer:`
### 1. Introduction/Overview
The **Pivot** recipe is a powerful tool for reshaping data from a "long" to a "wide" format. A common challenge is that if a category you expect to become a column is not present in the input data for a specific run, that column will be missing from the output, which can break downstream processes. This guide explains how to handle this.

### 2. Prerequisites
- **A dataset in "long" format** (e.g., with columns for \`Date\`, \`Category\`, \`Value\`).
- **Knowledge of all the possible categories** that should become columns.

### 3. Step-by-Step Instructions
1.  **The Problem:** Imagine you are pivoting on a \`Category\` column that has values "A", "B", and "C". If, on one day, your input data only contains "A" and "B", the Pivot recipe output will only have columns for "A" and "B". The "C" column will be missing.
2.  **The Solution: Use a "Scaffold" Dataset.**
    *   **Create a Scaffold:** Create a separate dataset (e.g., from an inline table) that contains a single column with *all possible* values for your category column. For our example, this "scaffold" would have three rows: "A", "B", and "C".
    *   **Join Before Pivoting:** Before your Pivot recipe, use a **Join** recipe to perform a **Full Outer Join** between your main data and this scaffold dataset. This ensures that all categories are present in the data, even if they have null values for a particular run.
3.  **Perform the Pivot:**
    *   Now, apply the **Pivot** recipe to the output of the join.
    *   Because the data now contains rows for every possible category (even if some are null), the Pivot recipe will reliably create a column for every category every time it runs.
    *   You will likely need to use a **Prepare** recipe after the pivot to fill the nulls created by the join with zeros.

### 4. Resources and Tools
- **Pivot Recipe:** The core reshaping tool.
- **Join Recipe:** The key tool for creating the "scaffold" structure.
- **Prepare Recipe:** Used for post-pivot cleanup (e.g., filling nulls).

### 5. Next Steps and Progression
- **Automate the Scaffold:** Instead of a static inline table, you can create a flow that generates the scaffold dataset by taking the distinct values of the category column from the *entire historical dataset*.

### 6. Common Challenges and Solutions
- **Challenge:** "This seems like a lot of extra work."
- **Solution:** It is an extra step, but it is essential for creating robust, production-grade pipelines. If a downstream process (like a BI tool or a model) is expecting a fixed set of columns, a missing column will cause the entire pipeline to fail. The scaffold method prevents this.
- **Challenge:** "My pivot is creating too many columns."
- **Solution:** This means the column you are pivoting has too many unique values. You may need to use a Prepare recipe to group or clean up the categories before you pivot.
`},{id:313,slug:"filtering-vs-sampling-confusion",question:"How to get started with filtering vs sampling confusion in Sample/Filter recipe?",answer:`
### 1. Introduction/Overview
The **Sample/Filter** recipe in Dataiku contains two distinct functionalities: **Sampling** and **Filtering**. While they both reduce the number of rows in your dataset, they have fundamentally different purposes. This guide clarifies when to use each.

### 2. Prerequisites
- **A dataset** you want to reduce in size.
- **A clear goal:** Are you trying to select a random subset, or are you trying to select rows based on a specific rule?

### 3. Step-by-Step Instructions

#### When to use FILTER:
- **Purpose:** To select a subset of rows based on a **deterministic rule or condition**.
- **How it works:** You write a condition (e.g., \`country == 'USA'\` or \`price > 100\`). The recipe keeps only the rows that satisfy this condition. The same filter on the same data will always produce the exact same output.
- **Use Cases:**
    - Removing invalid or irrelevant data.
    - Focusing your analysis on a specific segment (e.g., a particular region or time frame).
    - Splitting data into different streams for separate processing.

#### When to use SAMPLE:
- **Purpose:** To select a **random subset** of your data.
- **How it works:** You specify a percentage (e.g., 10%) or a fixed number of rows. The recipe will then randomly select that many rows from your dataset. Running the same sample again may produce a different set of rows (unless you fix the random seed).
- **Use Cases:**
    - **Prototyping:** When working with a very large dataset, you can create a smaller random sample to build and test your flow quickly before running it on the full data.
    - **Machine Learning:** Creating training and test sets.
    - **Statistical Analysis:** Ensuring that a sample is representative of the whole population.

### 4. Resources and Tools
- **Sample/Filter Recipe:** The visual recipe that contains both functionalities.
- **Prepare Recipe:** The "Filter" processor in the Prepare recipe provides the same functionality as the filter in the dedicated recipe and is often more convenient to use as part of a larger cleaning script.

### 5. Next Steps and Progression
- **Stratified Sampling:** In the Sample recipe, you can use stratified sampling to ensure that the random sample maintains the same distribution of a key categorical column (e.g., the same percentage of male/female customers) as the original dataset.

### 6. Common Challenges and Solutions
- **Challenge:** "I need to split my data into a training set and a testing set."
- **Solution:** Use the **Split** recipe. This single recipe can divide your data into two or more sets (e.g., an 80% training set and a 20% testing set) and is the standard tool for this machine learning task.
- **Challenge:** "I want to get the first 100 rows of my dataset."
- **Solution:** Use the "Filter" part of the recipe and choose the "First N rows" option. This is a deterministic selection, not a random sample.
`},{id:314,slug:"deduplicating-rows-with-distinct-recipe",question:"How to get started with deduplicating rows using Distinct recipe but missing duplicates?",answer:`
### 1. Introduction/Overview
The **Distinct** recipe is designed to remove rows that are **100% identical** across all columns. A common mistake is to try to use it to deduplicate a dataset based on a specific key (like a customer ID), where other columns might be different. This guide explains why that doesn't work and shows the correct tool for the job.

### 2. Prerequisites
- **A dataset with duplicate records.**
- **A clear understanding of what defines a duplicate** in your context.

### 3. Step-by-Step Instructions

#### The Problem with the Distinct Recipe
- Imagine you have two rows for the same customer, but they have different transaction dates.
- \`[customer_id: 123, transaction_date: 2023-01-05]\`
- \`[customer_id: 123, transaction_date: 2023-02-10]\`
- The **Distinct recipe will not remove either of these rows**, because they are not completely identical (the dates are different).

#### The Correct Solution: The Group Recipe
- **Goal:** To keep only one record per unique value of a specific column (the key).
- **How:**
    1.  Select your dataset and choose the **Group** recipe.
    2.  **Group by:** In the "Key" section, select the column that defines your unique entity (e.g., \`customer_id\`).
    3.  **Aggregations:** For all the other columns you want to keep, you must tell Dataiku how to pick one value from the group.
        *   To simply keep the values from the *first* row encountered for each customer, add each other column (like \`transaction_date\`) and choose the **First** aggregation.
        *   To be more intentional, you could choose the **Max** of the \`transaction_date\` to keep the most recent record for each customer.
    4.  Run the recipe. The output will have exactly one row per unique \`customer_id\`.

### 4. Resources and Tools
- **Distinct Recipe:** Only for removing full-row duplicates.
- **Group Recipe:** The correct, powerful tool for deduplicating based on a key.

### 5. Next Steps and Progression
- **Deduplicating on Multiple Keys:** You can add multiple columns to the "Group by" section to deduplicate on a composite key (e.g., keep one record per \`customer_id\` and \`product_id\` combination).

### 6. Common Challenges and Solutions
- **Challenge:** "The Group recipe dropped some of my columns."
- **Solution:** You forgot to specify an aggregation for them. Any column not in the "Group by" key must have an aggregation function (like "First", "Last", "Sum", "Max") defined for it to be included in the output.
- **Challenge:** "How do I know which row the 'First' aggregation is keeping?"
- **Solution:** The "first" row depends on the default ordering of your data. If you need to keep a specific row (e.g., the one with the latest date), you should first use a **Sort** recipe to order your data by date descending, and *then* use the Group recipe with the "First" aggregation.
`},{id:315,slug:"chaining-multiple-visual-recipes",question:"How to get started with chaining multiple visual recipes while maintaining lineage?",answer:`
### 1. Introduction/Overview
Chaining recipes is the fundamental process of building a data pipeline in Dataiku. It involves connecting the output of one recipe to the input of another, creating a sequence of transformations. Dataiku automatically manages the connections and visualizes the entire chain as a lineage graph in your Flow.

### 2. Prerequisites
- **A Dataiku project with at least one dataset.**
- **A multi-step transformation plan** (e.g., "First, I need to clean the data, then join it with another table, then aggregate it.").

### 3. Step-by-Step Instructions
1.  **Create the First Link:**
    *   Start with your raw dataset (e.g., \`raw_sales\`).
    *   Select it and add your first recipe, for example, a **Prepare** recipe to clean it.
    *   Run the recipe. This creates a new dataset (e.g., \`sales_cleaned\`). Your chain is now \`[raw] -> (prepare) -> [cleaned]\`.
2.  **Add the Second Link:**
    *   **Crucially, select the output of the previous step.** Click on the \`sales_cleaned\` dataset.
    *   From the right-hand panel, choose your next recipe, for example, a **Join** recipe to combine it with a \`customers\` dataset.
    *   Run the recipe. This creates a new \`sales_joined_customers\` dataset.
3.  **Continue the Chain:**
    *   Select the \`sales_joined_customers\` dataset.
    *   Add a **Group** recipe to aggregate the data.
    *   Run the recipe to create your final \`sales_aggregated\` dataset.
4.  **View the Lineage:** Your Flow now clearly shows the entire chain: \`[raw] -> (prepare) -> [cleaned] -> (join) -> [joined] -> (group) -> [aggregated]\`. This visual lineage is created automatically.

### 4. Resources and Tools
- **The Flow:** The canvas where you build and see your recipe chains.
- **The Actions Panel:** The right-hand menu you use to select new recipes to add to the chain.

### 5. Next Steps and Progression
- **Branching:** A single dataset can be the input for multiple recipes, creating branches in your flow.
- **Organization:** For long chains, use **Flow Zones** to group logical segments (e.g., a "Cleaning" zone and a "Reporting" zone).
- **Automation:** Once your chain is complete, create a **Scenario** to build the final dataset (\`sales_aggregated\`) on a schedule. You only need to build the last item; Dataiku will automatically run the entire upstream chain.

### 6. Common Challenges and Solutions
- **Challenge:** "I connected my new recipe to the raw data by mistake."
- **Solution:** Open the recipe. In its settings, go to the "Input/Output" tab. You can click to change the input and select the correct intermediate dataset.
- **Challenge:** "My Flow is a mess."
- **Solution:** Use the **Arrange** button to automatically tidy the layout. For complex flows, manually dragging items to create a clean, left-to-right visual progression is a good practice.
`},{id:316,slug:"setting-up-a-python-recipe-environment",question:"How to get started with setting up a Python recipe environment (libraries, dependencies)?",answer:`
### 1. Introduction/Overview
A code environment is an isolated space that contains a specific version of Python and a specific set of libraries (packages). Using environments is essential for reproducibility and avoiding dependency conflicts. This guide explains how to create and manage them. This is typically an administrative task.

### 2. Prerequisites
- **Administrator rights** on the Dataiku instance.
- **Knowledge of the Python packages** required for your project (e.g., \`pandas\`, \`scikit-learn\`, \`requests\`).

### 3. Step-by-Step Instructions
1.  **Navigate to Code Environments:** As an admin, go to **Administration > Code Envs**.
2.  **Create a New Environment:**
    *   Click **+ NEW PYTHON ENV**.
    *   Give it a clear, descriptive name (e.g., \`python-3-9-geospatial\`).
    *   Choose the Python version to use.
3.  **Add Required Packages:**
    *   In the "Packages to install" section, click **Add**.
    *   Enter the name of the package you need as it appears on PyPI (e.g., \`pandas\`).
    *   **Best Practice:** Specify an exact version for reproducibility (e.g., \`pandas==1.4.2\`).
4.  **Save and Build:**
    *   Click **Save and update**. Dataiku will now create the isolated environment and install all the specified packages using a tool like \`pip\`. This may take a few minutes.
5.  **Use the Environment in a Project (Developer Task):**
    *   In a project, when you create a Python recipe or notebook, you can select which code environment to use.
    *   In a recipe's **Advanced** settings, choose your newly created environment from the "Code Env" dropdown.

### 4. Resources and Tools
- **Administration > Code Envs:** The central management UI for all code environments.
- **PyPI (pypi.org):** The public repository where you can find the names and versions of Python packages.

### 5. Next Steps and Progression
- **Standardized Environments:** Create a set of standard, pre-approved code environments for your organization to use (e.g., a "General Purpose" one with common data science libraries).
- **Exporting Environments:** You can export the definition of a code environment as a JSON file. This can be checked into Git to version control your dependencies.
- **GPU Environments:** For deep learning, you can create environments that include GPU-enabled libraries like TensorFlow and are configured to run on GPU-powered machines.

### 6. Common Challenges and Solutions
- **Challenge:** "The environment build fails."
- **Solution:** This is often due to dependency conflicts, where two packages require different versions of the same underlying library. You may need to experiment with the package versions in your list to find a compatible set. Reading the build log will provide detailed error messages.
- **Challenge:** "I installed a package, but my recipe can't find it."
- **Solution:** Make sure your recipe is actually configured to use the correct code environment. Check the recipe's "Advanced" settings.
`},{id:317,slug:"importing-pandas-in-python-recipes",question:"How to get started with importing pandas in Python recipes without runtime errors?",answer:`
### 1. Introduction/Overview
Pandas is the workhorse library for data manipulation in Python, and it's the primary way you'll interact with data inside a Dataiku Python recipe. A runtime error when importing pandas usually indicates a problem with the code environment configuration.

### 2. Prerequisites
- **A Dataiku project with a Python recipe.**
- **A code environment** associated with the project.

### 3. Step-by-Step Instructions
1.  **Check the Code Environment:**
    *   The error \`ModuleNotFoundError: No module named 'pandas'\` means the Python environment your recipe is using does not have pandas installed.
2.  **Navigate to the Environment Settings:**
    *   In your project, go to **Settings > Code Env**. Note the name of the Python environment being used.
    *   An administrator must then go to **Administration > Code Envs** and select that environment.
3.  **Ensure Pandas is in the Package List:**
    *   In the environment's "Packages to install" section, make sure \`pandas\` is listed.
    *   If it's not, add it. It's best practice to pin the version (e.g., \`pandas==1.4.2\`).
    *   If it is listed but you still get an error, the environment may be corrupted. Try clicking "Rebuild environment".
4.  **Write the Import Statement Correctly:**
    *   At the top of your Python recipe, the standard convention is to import pandas with the alias \`pd\`.
    > \`import pandas as pd\`
5.  **Use the Dataiku API to get a DataFrame:**
    *   Remember that the correct way to get your data into pandas is via the Dataiku API, which handles all the connection and parsing for you.
    > \`\`\`python
    > import dataiku
    > import pandas as pd
    > 
    > # Read the input dataset into a pandas DataFrame
    > my_dataset = dataiku.Dataset("your_input_name")
    > my_df = my_dataset.get_dataframe()
    > \`\`\`
### 4. Resources and Tools
- **Code Environment Package List:** The single source of truth for what libraries are installed.
- **The \`import\` statement:** The standard Python syntax for loading a library.

### 5. Next Steps and Progression
- **Managing Versions:** Be mindful of which version of pandas your environment uses, as functions and behaviors can change between major versions. Pinning the version in your code environment settings is crucial for reproducibility.
- **Optimizing Performance:** For very large datasets that don't fit in memory, loading them into a single pandas DataFrame will cause errors. You should switch to using iterators or a PySpark recipe.

### 6. Common Challenges and Solutions
- **Challenge:** "The recipe fails with the import error."
- **Solution:** 99% of the time, this is purely an environment issue. The package is either not listed in the environment's requirements, or the environment is not correctly built. Contact your Dataiku administrator to verify the environment's configuration.
- **Challenge:** "The import works, but a pandas function fails."
- **Solution:** This is likely a code error, not an import error. It could also be a versioning issue, where your code was written for a different version of pandas than the one installed in your environment.
`},{id:318,slug:"debugging-python-recipes",question:"How to get started with debugging Python recipes when exceptions occur in execution?",answer:`
### 1. Introduction/Overview
When a Python recipe fails, Dataiku provides detailed logs to help you find and fix the problem. Debugging is a systematic process of reading the error, understanding it, and using various techniques to isolate and resolve the issue in your code.

### 2. Prerequisites
- **A failed Python recipe run.**
- **Access to the job log.**

### 3. Step-by-Step Instructions
1.  **Go to the Job Log:**
    *   From the "Jobs" menu, find the failed job and click on it.
    *   Click on the red, failed Python recipe step to open its log.
2.  **Read the Traceback:**
    *   Scroll to the very bottom of the log. Python provides a "traceback," which is a detailed report of the error.
    *   **Read it from the bottom up.** The last line will tell you the type of error (e.g., \`TypeError\`, \`KeyError\`, \`ValueError\`).
    *   The lines above will show you the exact file and line number in your recipe code where the error occurred.
3.  **Form a Hypothesis:** Based on the error type and the line of code, try to understand the problem.
    *   \`KeyError\`: You tried to access a dictionary key or DataFrame column that doesn't exist. Check for typos.
    *   \`TypeError\`: You tried to perform an operation on the wrong type of data (e.g., adding a string to a number).
    *   \`IndentationError\`: Your Python code has incorrect indentation.
4.  **Use "Print" Debugging:**
    *   The simplest way to debug is to add \`print()\` statements to your recipe code to inspect the state of variables.
    *   Print the \`df.info()\` or \`df.head()\` of your DataFrame at various points to see how it's being transformed.
    *   Rerun the recipe and check the new log to see the output of your print statements.
5.  **Test Interactively in a Notebook:**
    *   For complex bugs, copy the recipe code and the failing data into a Jupyter notebook.
    *   In the notebook, you can run the code line by line, inspect variables at every step, and quickly iterate until you find the solution. Once fixed, copy the corrected code back into the recipe.

### 4. Resources and Tools
- **The Job Log and Traceback:** Your most important source of information.
- **\`print()\` statements:** A simple but effective debugging tool.
- **Jupyter Notebooks:** An interactive environment for more complex debugging.

### 5. Next Steps and Progression
- **Error Handling:** Instead of letting your recipe crash, add \`try...except\` blocks to your code to catch expected errors and handle them gracefully (e.g., by logging a warning or skipping a bad row).
- **Unit Testing:** For critical library functions, write formal unit tests to ensure they behave as expected with different inputs.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't understand the error message."
- **Solution:** Copy and paste the error message (e.g., \`TypeError: unsupported operand type(s) for +: 'int' and 'str'\`) into a search engine. You will find countless explanations and examples on sites like Stack Overflow.
- **Challenge:** "The code works on a sample but fails on the full dataset."
- **Solution:** This almost always means there is a data quality issue in the full dataset that was not present in your sample (e.g., an unexpected null value or a string in a numerical column). Your code needs to be made more robust to handle these edge cases.
`},{id:319,slug:"accessing-dataiku-datasets-via-api",question:"How to get started with accessing Dataiku datasets via DSS API in code recipes?",answer:`
### 1. Introduction/Overview
The Dataiku Python API is the bridge between your custom code and the Dataiku Flow. It provides a simple, high-level way to read data from your input datasets into common Python data structures like Pandas DataFrames, and to write your results back to output datasets.

### 2. Prerequisites
- **A Python recipe** with at least one input and one output dataset configured.
- **Basic knowledge of Pandas DataFrames.**

### 3. Step-by-Step Instructions
1.  **Import the Library:** The first line in your recipe should always be to import the Dataiku library.
    > \`import dataiku\`
2.  **Get a Handle on Your Input Dataset:** Use the \`dataiku.Dataset()\` function, passing the *name* of your input dataset (as seen in the Flow). This creates a dataset object.
    > \`input_dataset = dataiku.Dataset("my_raw_data")\`
3.  **Read the Data into a Pandas DataFrame:** The most common action is to read the entire dataset into a Pandas DataFrame using the \`.get_dataframe()\` method.
    > \`df = input_dataset.get_dataframe()\`
4.  **Perform Your Transformations:** Now you can use standard Pandas code to manipulate this DataFrame.
5.  **Get a Handle on Your Output Dataset:** Similar to the input, create an object for your output dataset.
    > \`output_dataset = dataiku.Dataset("my_prepared_data")\`
6.  **Write the DataFrame to the Output:** Use the \`.write_with_schema()\` method to save your transformed DataFrame to the output dataset. Dataiku will handle creating the table and defining the schema.
    > \`output_dataset.write_with_schema(final_df)\`

### 4. Resources and Tools
- **Dataiku Python API documentation:** The official reference, accessible from the Help menu in DSS. It details all the available classes and methods.
- **Code Recipe Boilerplate:** When you create a new Python recipe, Dataiku automatically provides this boilerplate code, so you just need to fill in your logic.

### 5. Next Steps and Progression
- **Iterating Over Rows:** For very large datasets that don't fit in memory, don't use \`.get_dataframe()\`. Instead, use \`input_dataset.iter_rows()\` to process the data row by row with low memory usage.
- **Reading and Writing to/from Other Systems:** The API can also interact with SQL databases, cloud storage, and more, allowing you to build complex data movement pipelines in code.
- **Managing Multiple Inputs/Outputs:** A single recipe can read from multiple inputs and write to multiple outputs. Just create a separate dataset object for each.

### 6. Common Challenges and Solutions
- **Challenge:** "I get an error: 'Dataset not found'."
- **Solution:** You have a typo in the dataset name inside the \`dataiku.Dataset("...")\` call. The name must exactly match the dataset name in the Flow. It is case-sensitive.
- **Challenge:** "The recipe is using a lot of memory."
- **Solution:** You are using \`.get_dataframe()\` on a dataset that is too large. You must refactor your code to use an iterator or switch to a PySpark recipe for distributed processing.
`},{id:320,slug:"writing-sql-recipes-with-special-chars-in-names",question:"How to get started with writing SQL recipes when field names contain spaces or special chars?",answer:`
### 1. Introduction/Overview
While it's a best practice to have column names without spaces or special characters, you will inevitably encounter them in source data. To query these columns in a SQL recipe, you must use your database's specific "quoting" characters to tell the SQL parser to treat the name as a single identifier.

### 2. Prerequisites
- **A SQL dataset** that has columns with spaces or special characters (e.g., "Customer Name", "sales-amount").
- **Knowledge of your database's specific SQL dialect.**

### 3. Step-by-Step Instructions
1.  **Identify the Quoting Character for Your Database:** Different SQL databases use different characters for quoting identifiers.
    *   **Standard SQL (PostgreSQL, Snowflake, Oracle):** Double quotes (\`"\`).
    *   **SQL Server:** Square brackets (\`[]\`).
    *   **MySQL:** Backticks (\\\`\\\`).
2.  **Write the SQL Query:** In your SQL recipe, when you refer to a column with a special character, enclose it in the appropriate quoting characters.

    *   **Example for Snowflake/PostgreSQL:**
        > \`\`\`sql
        > SELECT
        >   "Customer Name",
        >   "sales-amount" * 1.1 AS "amount_incl_tax"
        > FROM my_input_dataset;
        > \`\`\`
    *   **Example for SQL Server:**
        > \`\`\`sql
        > SELECT
        >   [Customer Name],
        >   [sales-amount] * 1.1 AS [amount_incl_tax]
        > FROM my_input_dataset;
        > \`\`\`
3.  **Validate and Run:** Use the "Validate" button to ensure your syntax is correct for your database.

### 4. Resources and Tools
- **SQL Recipe Editor:** Your workspace for writing the query.
- **Your Database's SQL Documentation:** The definitive source for its syntax rules.

### 5. Next Steps and Progression
- **Rename Columns:** The best long-term solution is to add a **Prepare recipe** early in your flow to rename the problematic columns to a standard format (e.g., rename "Customer Name" to \`customer_name\`). This makes all downstream querying much simpler.
- **Use Aliases:** You can use table aliases to make your queries cleaner. \`SELECT t1."Customer Name" FROM my_table t1 ...\`

### 6. Common Challenges and Solutions
- **Challenge:** "I'm still getting a syntax error."
- **Solution:** Double-check that you are using the correct quoting character for your specific database. Also, ensure that the quoting is consistent. If you quote an identifier in the \`SELECT\` clause, you must also quote it if you use it in the \`WHERE\` or \`GROUP BY\` clause.
- **Challenge:** "Why can't I use single quotes (' ')?"
- **Solution:** In SQL, single quotes are used exclusively for defining string literals (e.g., \`WHERE country = 'USA'\`). They cannot be used for quoting column or table names.
`},{id:321,slug:"handling-large-data-in-memory-errors",question:"How to get started with handling large data in-memory causing out-of-memory errors?",answer:`
### 1. Introduction/Overview
"Out of Memory" (OOM) errors are a common challenge when working with large datasets. They happen when you try to load more data into a server's RAM than it can hold. The solution is not to simply add more memory, but to change your processing strategy to avoid loading all the data at once. This involves "pushing down" computation to a more powerful engine.

### 2. Prerequisites
- **A large dataset** (e.g., many gigabytes or tens of millions of rows).
- **A recipe that is failing** with an OOM error.

### 3. Step-by-Step Instructions: The Optimization Framework
1.  **Identify the Failing Recipe:** The job log will clearly show which recipe failed. It will almost always be a recipe running with the **"In-Memory"** execution engine.
2.  **Choose the Correct Engine (Push Down):** The goal is to move the work from the Dataiku server's memory to a system designed for large-scale processing.
    *   **If your data is in a SQL Database (Snowflake, BigQuery, etc.):**
        *   **Solution:** Open the failing recipe, go to **Advanced** settings, and change the **Execution engine** to **Run on database (SQL)**. This is the most common and effective solution.
    *   **If your data is on a distributed filesystem (S3, HDFS, etc.):**
        *   **Solution:** Change the execution engine to **Spark**. This will distribute the processing across a cluster.
    *   **If you *must* use a Python recipe:**
        *   **Solution:** You cannot use \`dataset.get_dataframe()\` which loads everything. You must refactor your code to use \`dataset.iter_rows()\` to process the data in a streaming fashion, one row at a time, with very low memory usage.

### 4. Resources and Tools
- **The Execution Engine Dropdown:** Your most powerful tool for solving OOM errors.
- **The Dataiku Python API's \`.iter_rows()\` method:** The tool for memory-efficient processing in Python recipes.
- **The Job Log:** The place where you will see the OOM error message.

### 5. Next Steps and Progression
- **Proactive Design:** When starting a new project, if you know the data will be large, design your flow from the beginning to use push-down execution. Don't wait for it to fail.
- **Resource Allocation:** If even a Spark or database job fails with a memory error, you may need to allocate more resources to that engine (e.g., use a larger Snowflake warehouse or increase the executor memory in Spark). This is an administrative task.

### 6. Common Challenges and Solutions
- **Challenge:** "The 'Run on database' option is not available."
- **Solution:** This means your input and output datasets for that recipe are not on the same database connection. You may need to use a **Sync recipe** first to move your data into the database before you can process it there.
- **Challenge:** "My Python recipe is too complex to rewrite with an iterator."
- **Solution:** Consider using a **PySpark recipe**. The Spark DataFrame API is very similar to Pandas and allows you to perform complex transformations in a distributed way.
`},{id:322,slug:"correcting-python-syntax-errors",question:"How to get started with correcting indentation or syntax errors in Python code?",answer:`
### 1. Introduction/Overview
Python syntax errors, especially \`IndentationError\`, are common for beginners. These errors prevent your code from running at all. Learning to read the error message and use your code editor's features is the key to fixing them quickly.

### 2. Prerequisites
- **A Python recipe or notebook** with a syntax error.
- **The error message** from the job log or notebook output.

### 3. Step-by-Step Instructions
1.  **Read the Error Message Carefully:**
    *   Python's error messages are very helpful. For a syntax error, it will usually show you the exact line (and sometimes point to the specific character) where it got confused.
    *   **Example \`IndentationError: expected an indented block\`:** This means you have a line with a colon (\`:\`) like an \`if\` statement or \`for\` loop, and the very next line is not indented.
2.  **Understand Python's Indentation:**
    *   Unlike many other languages, Python uses whitespace (indentation) to define code blocks. It is not optional.
    *   The standard is to use **4 spaces** for each level of indentation.
    *   All lines within the same block must have the exact same level of indentation.
3.  **Use Your Editor's Features:**
    *   The Dataiku code recipe editor has syntax highlighting, which can help you spot issues.
    *   **Mixing Tabs and Spaces:** This is a common, invisible cause of indentation errors. Configure your editor to **use spaces instead of tabs** to prevent this.
4.  **Use a Linter:**
    *   A linter is a tool that automatically checks your code for style and syntax errors.
    *   Integrating a linter like \`flake8\` into your workflow can catch these errors before you even run the code.

### 4. Resources and Tools
- **The Python Traceback:** The error message is your best guide.
- **A good text editor:** Configure it to show whitespace characters and use spaces for indentation.
- **A Linter (\`flake8\`, \`black\`):** Automated tools to help enforce correct syntax and style.

### 5. Next Steps and Progression
- **Develop Muscle Memory:** Over time, you will develop a feel for correct Python indentation.
- **Review Code:** Reading well-formatted Python code is a great way to internalize the correct style.

### 6. Common Challenges and Solutions
- **Challenge:** "I fixed the indentation on that line, but now I get an error on the next line."
- **Solution:** This often happens when you fix one indentation level, but the following lines are still at the wrong level. Remember that all lines in a block must be aligned. You may need to fix the indentation for the entire block of code.
- **Challenge:** "I'm getting a generic \`SyntaxError: invalid syntax\`."
- **Solution:** This is a more general error. Look closely at the line indicated. Common causes include a missing colon (\`:\`) at the end of an \`if\`, \`for\`, or \`def\` statement, or an unclosed parenthesis or quote.
`},{id:323,slug:"integrating-scikit‑learn-pipelines-in-python-recipes",question:"How to get started with integrating scikit‑learn pipelines in Python recipes?",answer:`
### 1. Introduction/Overview
Scikit-learn's \`Pipeline\` object is a powerful tool for chaining together multiple data preprocessing and modeling steps into a single, reusable component. You can easily integrate these pipelines into a Dataiku Python recipe to create a clean, robust, and reproducible machine learning workflow.

### 2. Prerequisites
- **Knowledge of the Scikit-learn \`Pipeline\` API.**
- **A prepared dataset** for modeling.
- **A code environment** with \`scikit-learn\` installed.

### 3. Step-by-Step Instructions
1.  **Create a Python Recipe:** Set up a Python recipe that takes your prepared data as input and has an output for the predictions and a managed folder for the saved model.
2.  **Write the Pipeline Definition:** In your Python recipe, import the necessary modules (\`Pipeline\`, transformers, and your chosen estimator). Define the steps of your pipeline.
    > \`\`\`python
    > from sklearn.pipeline import Pipeline
    > from sklearn.preprocessing import StandardScaler
    > from sklearn.impute import SimpleImputer
    > from sklearn.linear_model import LogisticRegression
    >
    > # Define the pipeline
    > my_pipeline = Pipeline(steps=[
    >     ('imputer', SimpleImputer(strategy='median')),
    >     ('scaler', StandardScaler()),
    >     ('classifier', LogisticRegression())
    > ])
    > \`\`\`
3.  **Train the Pipeline:**
    *   Read your data into a Pandas DataFrame.
    *   Split your data into features (X) and target (y).
    *   Train the entire pipeline with a single \`.fit()\` call.
    > \`my_pipeline.fit(X_train, y_train)\`
4.  **Save the Trained Pipeline:**
    *   The entire trained pipeline object can be saved. Use a library like \`joblib\` to serialize the object.
    *   Save the serialized file to a Dataiku **Managed Folder**.
5.  **Use the Pipeline for Predictions:**
    *   In a separate "scoring" recipe, load the saved pipeline from the managed folder.
    *   Use \`loaded_pipeline.predict(new_data)\` to make predictions.

### 4. Resources and Tools
- **Scikit-learn Documentation:** The official guide for the \`Pipeline\` object.
- **Python Recipe:** The environment for your code.
- **Managed Folder:** The correct place to store your saved model/pipeline artifacts.
- **\`joblib\` library:** The standard way to save and load Scikit-learn objects.

### 5. Next Steps and Progression
- **ColumnTransformer:** Use \`ColumnTransformer\` within your pipeline to apply different preprocessing steps to different columns (e.g., scale numerical features and one-hot encode categorical features).
- **GridSearchCV:** Wrap your pipeline in a \`GridSearchCV\` object to automatically search for the best hyperparameters for all steps in your pipeline.
- **Dataiku's Visual ML:** For many standard use cases, Dataiku's visual machine learning lab automatically creates and manages these pipelines for you. Use a custom Python recipe when you need a level of customization that the visual tools don't offer.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm getting a 'data leakage' warning."
- **Solution:** This can happen if you fit your preprocessors (like an imputer or scaler) on the entire dataset before splitting into train and test sets. A key benefit of the Scikit-learn Pipeline is that it prevents this. It correctly fits the transformers on only the training data during the \`.fit()\` call.
- **Challenge:** "How do I see what the pipeline is doing?"
- **Solution:** You can access the individual steps of a fitted pipeline by name. For example, \`my_pipeline.named_steps['classifier'].coef_\` would let you inspect the coefficients of the trained logistic regression model.
`},{id:324,slug:"connecting-to-external-apis-securely",question:"How to get started with connecting to external APIs securely in code recipes?",answer:`
### 1. Introduction/Overview
Connecting to external APIs is a common task for enriching data or automating processes. Doing this securely is critical. The most important rule is to **never hardcode credentials** like API keys directly in your code. Dataiku provides a secure way to manage these secrets using Project Variables.

### 2. Prerequisites
- **API Documentation:** You need the URL and authentication details for the external API.
- **An API Key** or other credential from the service provider.
- **A project in Dataiku.**

### 3. Step-by-Step Instructions
1.  **Store the Secret in Project Variables:**
    *   In your project, go to **... > Variables**.
    *   Click **Edit** and **+ ADD VARIABLE**.
    *   Give the variable a name (e.g., \`MY_SERVICE_API_KEY\`).
    *   **Crucially, set the type to "Password".** This will encrypt the secret and hide its value from being displayed in the UI or logs.
    *   Paste your API key into the value field and save.
2.  **Create a Python Recipe:** This is where you will write your API call logic.
3.  **Retrieve the Secret in Your Code:**
    *   In your Python recipe, use the Dataiku API to get the secret from the project variables.
    > \`\`\`python
    > import dataiku
    > variables = dataiku.get_custom_variables()
    > api_key = variables.get("MY_SERVICE_API_KEY")
    > \`\`\`
4.  **Make the API Call:** Use a library like \`requests\`. Pass your retrieved API key in the request headers, as required by the API's documentation.
    > \`\`\`python
    > import requests
    > headers = {
    >     "Authorization": f"Bearer {api_key}"
    > }
    > response = requests.get("https://api.externalservice.com/data", headers=headers)
    > \`\`\`
### 4. Resources and Tools
- **Project Variables (Password type):** The secure, standard way to manage secrets in Dataiku.
- **Python \`requests\` library:** The standard library for making HTTP calls.
- **API Documentation** from the external service provider.

### 5. Next Steps and Progression
- **Central Secrets Management:** For enterprise-level security, a Dataiku administrator can integrate the instance with a dedicated secrets vault like HashiCorp Vault or Azure Key Vault. Your code can then retrieve secrets directly from this central vault.
- **Error Handling:** Wrap your API calls in \`try...except\` blocks to handle network errors or authentication failures gracefully.

### 6. Common Challenges and Solutions
- **Challenge:** "I've committed my project to Git, is the API key exposed?"
- **Solution:** No. The *values* of your project variables are not committed to Git, only their names. This is a key security feature. The values are stored securely within the Dataiku instance itself.
- **Challenge:** "I'm getting a 401 or 403 authentication error."
- **Solution:** This means your key is invalid or you are not presenting it correctly. Double-check that you copied the key correctly. Read the API's documentation carefully to see the exact format required for the Authorization header.
`},{id:325,slug:"parameterizing-code-recipes-with-variables",question:"How to get started with parameterizing code recipes using project variables?",answer:`
### 1. Introduction/Overview
Hardcoding values like thresholds, file paths, or filter conditions directly in your recipes makes them inflexible and difficult to maintain. **Project Variables** allow you to externalize these parameters, so you can change the behavior of a recipe without editing its code.

### 2. Prerequisites
- **A code recipe (Python or SQL)** with a hardcoded value you want to parameterize.
- **A Dataiku project.**

### 3. Step-by-Step Instructions
1.  **Create a Project Variable:**
    *   Go to your project's **... > Variables** page.
    *   Click **Edit** and **+ ADD VARIABLE**.
    *   Give it a name (e.g., \`price_threshold\`) and a value (e.g., \`100.0\`). Save your changes.
2.  **Use the Variable in a SQL Recipe:**
    *   In a SQL recipe, you can directly reference the variable using the \`\${variable_name}\` syntax. Dataiku performs a simple text substitution before executing the query.
    > \`\`\`sql
    > SELECT * FROM my_input WHERE price > \${price_threshold}
    > \`\`\`
3.  **Use the Variable in a Python Recipe:**
    *   **Method A (Substitution):** You can use the same \`\${...}\` syntax for simple cases.
        > \`threshold = \${price_threshold}\`
    *   **Method B (API - Recommended):** A more robust method is to fetch the variables as a dictionary using the Dataiku API. This is better because it avoids syntax errors and makes it clear that the values are coming from an external configuration.
    > \`\`\`python
    > import dataiku
    > variables = dataiku.get_custom_variables()
    > threshold = float(variables.get('price_threshold')) # All variables are strings, so cast if needed
    > filtered_df = df[df['price'] > threshold]
    > \`\`\`
### 4. Resources and Tools
- **Project Variables page:** The central place to define and manage your parameters.
- **The \`\${...}\` syntax:** For simple substitution in SQL and Python.
- **The \`dataiku.get_custom_variables()\` function:** The robust API method for Python.

### 5. Next Steps and Progression
- **Scenario Overrides:** The real power of variables comes from automation. In a **Scenario**, you can set different values for your variables for a specific run. This allows you to run the same recipe with different parameters (e.g., run the pipeline for different regions by changing a \`region_filter\` variable).
- **Environment Promotion:** Use variables for things that change between environments, like database names. When you deploy your project from dev to prod, you can easily update the variable values without touching your code.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm getting a syntax error in my SQL recipe."
- **Solution:** If your variable is a string, you need to remember to put quotes around it in your SQL query: \`WHERE country = '\${country_name}'\`.
- **Challenge:** "I'm getting a \`TypeError\` in Python."
- **Solution:** When you retrieve variables using the Dataiku API, they are always returned as strings. If you need to use it as a number, you must explicitly cast it: \`my_number = int(variables.get('my_number_var'))\`.
`},{id:326,slug:"committing-code-recipes-to-git",question:"How to get started with committing code recipes to Git and handling diff conflicts?",answer:`
### 1. Introduction/Overview
Using Git to version control your Dataiku project is a best practice for collaboration and reproducibility. When you make a change to a code recipe, you should commit it to your Git repository with a clear message explaining the change. This creates a full audit trail of your work.

### 2. Prerequisites
- **Your Dataiku project must be linked to a Git repository.**
- **You have made a change** to a Python or SQL recipe.

### 3. Step-by-Step Instructions: The Commit Workflow
1.  **Navigate to the Git Page:** After saving your changes to the recipe, click the **Git** icon in your project's top navigation bar.
2.  **Review Your Changes (Diff):**
    *   Dataiku will show your modified recipe in the "Changes" panel.
    *   Click on it. Dataiku will display a "diff" view, highlighting the exact lines you have added (in green) or removed (in red). Review this to make sure the changes are what you expect.
3.  **Stage Your Changes:**
    *   Select the checkbox next to the recipe you want to commit. This moves it from "Changes" to the "Staged changes" area.
4.  **Write a Commit Message:**
    *   In the text box at the bottom, write a clear and concise message describing your change.
    *   **Bad message:** "updates"
    *   **Good message:** "Fix bug in sales tax calculation for EU region"
5.  **Commit and Push:**
    *   Click the **Commit** button. This saves the change to your *local* Git repository on the Dataiku server.
    *   Click the **Push** button to send your commit to the *remote* repository (e.g., GitHub), making it visible to your teammates.

### 4. Resources and Tools
- **Dataiku's Git Page:** Your UI for staging, committing, pushing, and pulling.
- **The "Diff" Viewer:** The tool for reviewing your changes before you commit.
- **Your Git Provider's Website (GitHub, etc.):** Where you can view the full commit history and create pull requests.

### 5. Next Steps and Progression
- **Pull Requests:** Instead of pushing directly to the main branch, push your changes to a feature branch. Then, create a **Pull Request** on GitHub. This allows a teammate to review your code and provide feedback before it's merged.
- **Handling Merge Conflicts:** If you and a teammate change the same lines in the same recipe, you will get a merge conflict when you try to pull their changes. Dataiku provides a visual merge tool to help you resolve these conflicts by choosing which version of the code to keep.

### 6. Common Challenges and Solutions
- **Challenge:** "I committed something by mistake."
- **Solution:** If you haven't pushed it yet, you can use the Git "Revert" feature in Dataiku to undo the commit. If you've already pushed, the standard Git practice is to create a new commit that reverts the changes from the previous one.
- **Challenge:** "The diff is hard to read."
-   **Solution:** This can happen with large changes. It's a best practice to make small, frequent commits, each focused on a single logical change. This makes the history much easier to read and review.
`},{id:327,slug:"using-r-recipes-with-missing-packages",question:"How to get started with using R recipes when R packages are not installed?",answer:`
### 1. Introduction/Overview
Similar to Python, R recipes in Dataiku rely on **code environments** to manage dependencies. If you try to use a package in your R code that isn't installed in the recipe's environment, you will get a "package not found" error. The solution is to add the missing package to the environment's configuration.

### 2. Prerequisites
- **R configured on your Dataiku instance** (an admin task).
- **An R recipe** where you want to use a specific package (e.g., \`dplyr\`).
- **Administrator rights** to modify code environments.

### 3. Step-by-Step Instructions
1.  **Identify the Error:** When you run your R recipe, the job log will show an error like: \`Error in library(dplyr) : there is no package called 'dplyr'\`. This clearly tells you what is missing.
2.  **Find the Code Environment:** In your project, go to **Settings > Code Env** and find the name of the R environment your project is using.
3.  **Add the Package (Admin Task):**
    *   An administrator must go to **Administration > Code Envs** and select the R environment.
    *   In the "Packages to install" section, click **Add**.
    *   Enter the name of the package as it appears on CRAN (the R package repository), for example, \`dplyr\`.
    *   You can also specify a version if needed.
4.  **Save and Build:** Click **Save and update**. Dataiku will now install the R package and its dependencies into the isolated environment.
5.  **Rerun the Recipe:** Once the environment has finished building, you can go back to your R recipe and run it again. The \`library(dplyr)\` call should now succeed.

### 4. Resources and Tools
- **Administration > Code Envs:** The central UI for managing R package dependencies.
- **CRAN (The Comprehensive R Archive Network):** The public repository for finding R packages.
- **The Job Log:** The place where you will find the error message indicating a missing package.

### 5. Next Steps and Progression
- **Standard Environments:** Create a standard, shared R environment for your organization that includes the most commonly used packages (\`tidyverse\`, \`data.table\`, etc.) so that developers don't have to request new packages as often.
- **Version Management:** For reproducibility, it's a good practice to specify the versions of the R packages in your code environment, just as you would for Python.

### 6. Common Challenges and Solutions
- **Challenge:** "The package installation is failing."
- **Solution:** Some R packages have system-level dependencies (e.g., they need a specific library installed on the operating system). The build log will provide details. You may need to work with your Dataiku administrator to install these system dependencies on the server before the R package can be installed.
- **Challenge:** "I added the package, but the recipe still can't find it."
- **Solution:** Double-check that your recipe is actually configured to use the code environment that you modified. Go to the recipe's **Advanced** settings and verify the "Code Env" selection.
`},{id:328,slug:"breaking-long-sql-queries",question:"How to get started with breaking long SQL queries by proper line continuation?",answer:`
### 1. Introduction/Overview
Writing your entire SQL query as one long line is bad for readability and maintainability. SQL is very flexible with whitespace, and you should use this to your advantage by breaking your query into logical, indented sections. This makes it much easier for you and your teammates to understand and debug.

### 2. Prerequisites
- **A long SQL query** in a Dataiku SQL recipe.

### 3. Step-by-Step Instructions: A Style Guide for Readability
There is no single "correct" style, but the following is a very common and highly readable convention.

1.  **One Clause Per Line:** Start each major SQL clause (\`SELECT\`, \`FROM\`, \`WHERE\`, \`GROUP BY\`, \`ORDER BY\`) on a new line.
2.  **Indent Field Lists:** If your \`SELECT\` statement has many columns, put each column on its own line, indented under the \`SELECT\` keyword.
3.  **Align Keywords:** Align all the major clause keywords vertically.
4.  **Use Comments:** Use SQL comments (\`--\` for a single line, \`/* ... */\` for a block) to explain complex parts of your logic.

#### Example
- **Bad, Unreadable Query:**
> \`SELECT customer_id, name, COUNT(order_id) FROM customers c JOIN orders o ON c.id = o.customer_id WHERE c.country = 'USA' GROUP BY c.customer_id, c.name ORDER BY COUNT(order_id) DESC;\`

- **Good, Readable Query:**
> \`\`\`sql
> -- Calculate the total number of orders for each US-based customer
> SELECT
>     c.customer_id,
>     c.name,
>     COUNT(o.order_id) AS number_of_orders
> FROM
>     customers c
> LEFT JOIN
>     orders o ON c.customer_id = o.customer_id
> WHERE
>     c.country = 'USA'
> GROUP BY
>     c.customer_id,
>     c.name
> ORDER BY
>     number_of_orders DESC;
> \`\`\`

### 4. Resources and Tools
- **The SQL Recipe Editor:** Your workspace for writing and formatting your queries.
- **SQL Formatters:** There are many online tools and editor plugins that can automatically format your SQL code according to a standard style.

### 5. Next Steps and Progression
- **Common Table Expressions (CTEs):** For very complex queries, use CTEs (\`WITH\` clauses) to break the logic down into named, sequential steps. This is the best way to make a highly complex query understandable. Each CTE can act like a temporary, intermediate recipe.

### 6. Common Challenges and Solutions
- **Challenge:** "Does all this extra whitespace affect performance?"
- **Solution:** No. The database's SQL parser ignores whitespace like spaces, tabs, and newlines. Formatting has zero impact on performance; its only purpose is human readability.
- **Challenge:** "My team uses a different style."
- **Solution:** Consistency is more important than any single style. The best style guide is the one your team agrees on and uses consistently. Document your chosen style in your team's Wiki.
`},{id:329,slug:"testing-code-logic-in-notebooks",question:"How to get started with testing code logic locally using notebooks before recipe run?",answer:`
### 1. Introduction/Overview
Before you create a formal, production-ready recipe, it's often useful to experiment with your code in a more interactive environment. **Jupyter Notebooks** in Dataiku are the perfect tool for this. They allow you to write and run code in small chunks (cells), see the output immediately, and iteratively develop your logic.

### 2. Prerequisites
- **A data analysis or transformation idea** that involves code.
- **A dataset** to test your logic on.
- **A configured code environment** (Python or R).

### 3. Step-by-Step Instructions: The Notebook-to-Recipe Workflow
1.  **Create a New Notebook:**
    *   In your project, go to the **Notebooks** tab.
    *   Click **+ New Notebook** and choose your language (e.g., Python).
    *   Make sure to select the correct code environment.
2.  **Load a Sample of Your Data:**
    *   In the first cell of the notebook, use the Dataiku API to read your input dataset.
    *   **Crucially, only read a sample.** You don't need the full dataset for development. This makes your interactive tests much faster.
    > \`\`\`python
    > import dataiku
    > df = dataiku.Dataset("my_large_dataset").get_dataframe(sampling='head', limit=10000)
    > \`\`\`
3.  **Develop and Test Interactively:**
    *   In subsequent cells, write your code one logical step at a time.
    *   Run each cell to see the intermediate results. Use \`print()\`, \`df.head()\`, or plotting libraries to inspect your data at each stage.
    *   This interactive loop of writing code, running it, and seeing the output is what makes notebooks so powerful for development.
4.  **"Productionize" into a Recipe:**
    *   Once you are confident that your code is correct and complete, it's time to move it into a recipe.
    *   Create a new **Python recipe**.
    *   Copy the finalized code from your notebook cells into the recipe's editor.
    *   Remove the sampling from the data loading step to ensure the recipe runs on the full dataset.
    *   Add code to write the final DataFrame to the recipe's output.

### 4. Resources and Tools
- **Jupyter Notebooks:** Your interactive development environment.
- **The \`.get_dataframe()\` sampling parameters:** Key for fast iteration on large data.
- **Python Recipes:** The final, production-ready home for your code.

### 5. Next Steps and Progression
- **Document as You Go:** Use Markdown cells in your notebook to document your thought process. When you're done, the notebook serves as a record of how you developed the final recipe logic.
- **Share Notebooks:** You can share your notebooks with colleagues for them to review your exploratory analysis.

### 6. Common Challenges and Solutions
- **Challenge:** "My code works in the notebook but fails when I move it to a recipe."
- **Solution:** This is often an environment or data issue. First, double-check that the recipe is using the exact same code environment as the notebook. Second, remember that the recipe runs on the *full* dataset. Your code might be failing on an edge case or a data quality issue that was not present in your small sample.
- **Challenge:** "My notebook is becoming a mess of unordered cells."
- **Solution:** It's a good practice to periodically use the "Restart Kernel and Run All Cells" command. This runs your entire notebook from top to bottom, ensuring that the logic is sequential and reproducible.
`},{id:330,slug:"capturing-execution-logs-from-code-recipes",question:"How to get started with capturing execution logs from code recipes for troubleshooting?",answer:`
### 1. Introduction/Overview
When a code recipe runs as part of an automated scenario, you can't see the output directly. The **Job Log** is the primary way to understand what happened during the execution. Capturing custom log messages is essential for debugging failures and monitoring the recipe's behavior.

### 2. Prerequisites
- **A Python or R code recipe.**
- **A need to record information** during the recipe's execution.

### 3. Step-by-Step Instructions

#### Method 1: Using Basic \`print()\` Statements (Simple & Effective)
- **What it is:** The simplest way to log information. Anything you \`print()\` in your recipe will be captured and displayed in the job log.
- **How to use:**
    *   Sprinkle \`print()\` statements throughout your code to track its progress.
    *   Print the shape of a DataFrame, the value of a key variable, or a simple status message.
    > \`\`\`python
    > print("Starting recipe execution...")
    > df = input_dataset.get_dataframe()
    > print(f"Input dataframe has {df.shape[0]} rows.")
    > # ... your logic here ...
    > print("Recipe execution finished successfully.")
    > \`\`\`
- **How to view:** Run the recipe. Go to the **Jobs** menu, find the run, and open the log for the recipe step. You will see all your printed messages.

#### Method 2: Using Python's \`logging\` Module (More Advanced)
- **What it is:** Python's built-in logging module offers more control, including different log levels (DEBUG, INFO, WARNING, ERROR).
- **How to use:**
    *   Import the \`logging\` module.
    *   Use the different logging functions to record messages with different severity levels.
    > \`\`\`python
    > import logging
    > logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    >
    > logging.info("This is an informational message.")
    > logging.warning("This is a warning message.")
    > \`\`\`
- **Why it's useful:** It allows you to configure the log verbosity. In development, you might show all DEBUG messages, but in production, you might only log INFO and higher.

### 4. Resources and Tools
- **The Job Log:** The place where all your logs are stored and viewed.
- **The \`print()\` function:** The simplest logging tool.
- **Python's \`logging\` module:** For more structured, level-based logging.

### 5. Next Steps and Progression
- **Custom Log Dataset:** For critical pipelines, you can create a dedicated log dataset. Your Python recipe can write structured log entries (e.g., with columns for timestamp, message, status) to this dataset in "append" mode. This creates an audit trail that you can analyze and build dashboards on.
- **Error Handling:** In a \`try...except\` block, use \`logging.error()\` to capture the details of any exceptions that occur.

### 6. Common Challenges and Solutions
- **Challenge:** "My logs are too noisy; I can't find anything."
- **Solution:** You are printing too much. Be strategic about what you log. Log key milestones, variable states, and errors. Using the \`logging\` module can help you filter for only the most important messages.
- **Challenge:** "Where do the logs go?"
- **Solution:** They are automatically captured by Dataiku. Every time you run a recipe, a job is created. Find that job in the "Jobs" menu, and you will find the log attached to the recipe step within that job.
`},{id:331,slug:"organizing-datasets-into-flow-zones",question:"How to get started with organizing datasets into Flow Zones?",answer:`
### 1. Introduction/Overview
As a project grows, your Flow can become a complex web of datasets and recipes. **Flow Zones** are the essential tool for managing this complexity. They are visual containers that allow you to group related items, creating a clean, high-level, and understandable project architecture.

### 2. Prerequisites
- **A Dataiku project with a Flow** that has more than a few items.
- **A logical understanding of your pipeline's stages.**

### 3. Step-by-Step Instructions
1.  **Plan Your Zone Structure:** Before you start, think about the logical stages of your pipeline. A very common and effective structure is based on data maturity:
    *   **Zone 1: Raw Data / Ingestion:** Where all your source datasets live.
    *   **Zone 2: Data Preparation:** Where cleaning, joining, and transformation happens.
    *   **Zone 3: Feature Engineering / Modeling:** For machine learning projects.
    *   **Zone 4: Final Outputs:** For the final, aggregated datasets ready for consumption.
2.  **Create Your First Zone:**
    *   In the Flow, right-click on an empty part of the canvas.
    *   Select **Create Flow Zone**.
    *   Give it a name. It's a best practice to number them to enforce order (e.g., \`1_Data_Ingestion\`).
3.  **Move Items into the Zone:**
    *   Select the datasets and recipes that belong in that zone by holding down the \`Shift\` key and clicking on them.
    *   Drag the entire group of selected items and drop them anywhere inside the new zone's boundary.
4.  **Repeat for All Stages:** Create the other zones (\`2_Data_Preparation\`, etc.) and move the appropriate items into them.
5.  **Collapse the Zones:** Click the minus (\`-\`) icon on a zone to collapse it. This hides the internal details and shows only the connections between zones. This gives you a clean, high-level view of your entire project architecture.

### 4. Resources and Tools
- **The Flow canvas:** The interface for creating and managing zones.
- **Project Wiki:** Document your standard Flow Zone structure here so all team members build their projects in a consistent way.

### 5. Next Steps and Progression
- **Nested Zones:** You can even create Flow Zones inside other Flow Zones for more granular organization in very complex projects.
- **Communication:** Use the collapsed Flow Zone view when presenting your project to stakeholders. It allows you to explain the overall process without getting lost in the technical details.

### 6. Common Challenges and Solutions
- **Challenge:** "My Flow still looks messy even with zones."
- **Solution:** Manually arrange the zones on the canvas to create a clean, left-to-right flow of data. You can also use the "Arrange" button *inside* a zone to automatically tidy up the items within it.
- **Challenge:** "An item seems to belong in two different zones."
- **Solution:** An item can only be in one zone. A dataset's zone is determined by the recipe that creates it. Place it in the zone that best represents its stage of maturity.
`},{id:332,slug:"renaming-datasets-without-breaking-recipes",question:"How to get started with renaming datasets without breaking recipes?",answer:`
### 1. Introduction/Overview
As projects evolve, you may need to rename a dataset to be more descriptive or to conform to new naming conventions. In many traditional coding environments, this would be a risky operation requiring a manual "find and replace" across many scripts. In Dataiku, this process is safe and easy, as the platform automatically manages these dependencies.

### 2. Prerequisites
- **A dataset in your Flow** that you wish to rename.

### 3. Step-by-Step Instructions
1.  **Locate the Dataset in the Flow:** Find the dataset you want to rename.
2.  **Open the Summary Panel:** Click on the dataset. The summary panel will open on the right.
3.  **Click the Name to Edit:** The name of the dataset at the top of the summary panel is editable.
    *   Click on the name.
    *   Type the new name for your dataset.
    *   Press **Enter**.
4.  **Observe the Automatic Update:** Dataiku will automatically update all the recipes that use this dataset as an input or output. You do not need to manually go into each recipe and change the name. The lineage and all connections are preserved.

### 4. Resources and Tools
- **The Summary Panel:** The side panel in the Flow view where you can see and edit an object's properties.

### 5. Next Steps and Progression
- **Standardize Naming:** Use this feature to enforce your team's naming conventions. If you see a poorly named dataset, you can confidently rename it to something clearer.
- **Renaming Recipes:** The same process applies to renaming recipes. Just click on the recipe, and edit its name in the summary panel. Dataiku will maintain the connections.

### 6. Common Challenges and Solutions
- **Challenge:** "I renamed a dataset, but a code recipe failed with 'Dataset not found'."
- **Solution:** This can happen if your code recipe refers to the dataset by its name as a hardcoded string.
    *   **Bad Code:** \`my_dataset = dataiku.Dataset("old_dataset_name")\`
    *   **The Fix:** You must update the string in your code to reflect the new name.
    *   **Best Practice:** A better way is to use the Dataiku API to get the inputs and outputs programmatically without hardcoding names, which makes the recipe more robust to renaming.
    > \`\`\`python
    > # In a recipe with one input and one output
    > input_name = dataiku.get_recipe_config()['inputs']['main']['items'][0]['ref']
    > input_dataset = dataiku.Dataset(input_name)
    > \`\`\`
- **Challenge:** "I can't edit the name."
- **Solution:** You may not have the necessary permissions. You typically need "Contributor" or "Administrator" rights on the project to rename objects.
`},{id:333,slug:"visual-clutter-in-flow",question:"How to get started with visual clutter in Flow and grouping logical zones?",answer:`
### 1. Introduction/Overview
A visually cluttered Flow with tangled lines is difficult to understand and maintain. The primary tools for managing this clutter and creating a clean, professional-looking pipeline are **Flow Zones** for grouping and the **Arrange** button for automatic layout.

### 2. Prerequisites
- **A messy, cluttered Dataiku Flow.**

### 3. Step-by-Step Instructions
1.  **Group with Flow Zones:**
    *   First, impose a high-level structure on your flow.
    *   Create logical **Flow Zones** for the major stages of your project (e.g., \`1_Ingestion\`, \`2_Preparation\`, \`3_Modeling\`).
    *   Drag and drop the related items into these zones.
2.  **Arrange Items Within a Zone:**
    *   After moving items into a zone, they might still be messy inside it.
    *   Click the "three dots" menu on the Flow Zone itself and select **Arrange zone items**. Dataiku will automatically lay out the items inside that zone for better readability.
3.  **Arrange the Entire Flow:**
    *   You can also use the global **Arrange** button (usually in the bottom right of the Flow view) to tidy up all items in your Flow that are not inside a zone.
4.  **Collapse Zones for a High-Level View:**
    *   Once your zones are organized, **collapse** them by clicking the minus (\`-\`) icon.
    *   This hides all the internal complexity and shows you a clean, simple diagram of how your major stages connect to each other.
5.  **Manual Tweaking:**
    *   While the automatic arrangement is good, you should still manually drag your zones and key datasets to create a clear, left-to-right visual narrative of your data's journey.

### 4. Resources and Tools
- **Flow Zones:** The most powerful tool for grouping and managing complexity.
- **The "Arrange" button:** For automatic, one-click layout cleaning.

### 5. Next Steps and Progression
- **Standardized Layouts:** Establish a standard Flow Zone structure for your team to ensure all projects are organized in a consistent and predictable way.
- **Presentations:** Use the collapsed zone view when presenting your pipeline to stakeholders. It provides a perfect high-level overview without getting bogged down in technical details.

### 6. Common Challenges and Solutions
- **Challenge:** "The automatic 'Arrange' button made my layout worse."
- **Solution:** This can sometimes happen in very complex flows. Use it as a starting point. After the automatic arrange, you can manually drag individual items to fine-tune the layout to your preference.
- **Challenge:** "The connection lines between my collapsed zones are crossing and look confusing."
- **Solution:** You may need to re-think which datasets are the "outputs" of a zone. Try to have a single, primary dataset that exits a zone and connects to the next one. This simplifies the connections between zones.
`},{id:334,slug:"troubleshooting-broken-links-from-deleted-datasets",question:"How to get started with troubleshooting broken links due to deleted datasets?",answer:`
### 1. Introduction/Overview
In Dataiku, a "broken link" occurs when a recipe's required input dataset is missing or has been deleted. The Flow clearly visualizes this problem, making it easy to identify and fix.

### 2. Prerequisites
- **A recipe in the Flow** that is showing an error for a missing input.

### 3. Step-by-Step Instructions
1.  **Identify the Broken Link:**
    *   In the Flow, the recipe with the missing input will often be highlighted in red.
    *   The link between the missing dataset (which may appear as a red, ghosted item) and the recipe will be broken.
2.  **Understand the Cause:**
    *   The most common reason is that the input dataset was accidentally deleted.
    *   Another possibility is that the project was imported from another instance, and the input dataset (which might come from a different project) was not imported with it.
3.  **Fix the Link:** You have two main options:
    *   **Option A: Re-create the Missing Dataset.** If the dataset was deleted by mistake, you may need to rebuild it. Find the upstream recipe that was supposed to create it and run it. If the raw source dataset was deleted, you will need to re-import it.
    *   **Option B: Re-wire the Recipe to a Different Input.** If the deleted dataset was obsolete and has been replaced by a new one, you can edit the recipe to use the new input.
        1.  Open the recipe with the broken link.
        2.  Go to the **Input/Output** tab.
        3.  You will see the missing input listed in red. Click on it and choose **Change input**.
        4.  Select the new, correct dataset to use as the input.
        5.  Save the recipe. The link in the Flow will now be fixed.

### 4. Resources and Tools
- **The Visual Flow:** It clearly highlights the broken dependencies.
- **The Input/Output Tab:** The settings page inside a recipe where you can change its inputs and outputs.

### 5. Next Steps and Progression
- **Dependency Checking:** Before deleting any dataset, always use the **View downstream dependencies** feature (by right-clicking on the dataset). This will show you exactly what will break if you delete it, preventing the problem in the first place.
- **Git History:** If you are using Git and are unsure why a dataset was deleted, you can check the commit history to find the commit where the deletion happened and who did it.

### 6. Common Challenges and Solutions
- **Challenge:** "I deleted a dataset and can't get it back."
- **Solution:** If the project is not on Git and you have no backups, you may have to rebuild the dataset from scratch. This highlights the importance of using version control and having a backup strategy for critical projects.
- **Challenge:** "I fixed the input, but now the recipe fails with a schema error."
- **Solution:** The new input dataset you wired up does not have the same schema (column names and types) as the original one. You will need to edit the recipe's code or transformation steps to work with the new schema.
`},{id:335,slug:"viewing-dataset-lineage",question:"How to get started with viewing dataset lineage and recipe dependencies clearly?",answer:`
### 1. Introduction/Overview
Data lineage is the "family tree" of your data. It answers the question, "Where did this data come from and how was it made?". Dataiku automatically captures and visualizes lineage at both a high level (the Flow) and a very detailed level (column-level). Understanding how to read this lineage is essential for debugging, auditing, and impact analysis.

### 2. Prerequisites
- **A Dataiku project** with a flow of connected recipes and datasets.

### 3. Step-by-Step Instructions

#### Method 1: The Flow (High-Level Lineage)
1.  **Understand the Flow:** The Flow itself is a lineage diagram. It shows the dependencies between recipes and datasets.
2.  **View Upstream Dependencies:**
    *   To see how a specific dataset was created, right-click on it in the Flow.
    *   Select **View upstream dependencies**.
    *   Dataiku will highlight the entire chain of recipes and datasets that were used to produce it, all the way back to the raw sources.
3.  **View Downstream Dependencies (Impact Analysis):**
    *   To see what would be affected by a change, right-click on a dataset.
    *   Select **View downstream dependencies**.
    *   Dataiku will highlight every recipe, model, and dashboard that depends on this dataset.

#### Method 2: The Lineage Tab (Column-Level Lineage)
1.  **Open a Dataset:** In your Flow, open any dataset that is the output of a recipe.
2.  **Navigate to the Lineage Tab:** Click the **Lineage** tab in the top menu.
3.  **Select a Column:** On the right side of the screen, you will see the list of columns in your current dataset. Click on one of them.
4.  **Trace the Column's Origin:** The main panel will now display a detailed graph. This graph shows you exactly which columns from which source datasets were used, and which specific transformations were applied, to create the final column you selected.

### 4. Resources and Tools
- **The Flow view:** Your tool for high-level dependency and impact analysis.
- **The Lineage Tab:** Your tool for detailed, column-by-column auditing and debugging.

### 5. Next Steps and Progression
- **Debugging:** When you find an error in a specific column, the column-level lineage is the fastest way to trace the problem back to the recipe that created it.
- **Auditing and Compliance:** Use screenshots of the column-level lineage graph as definitive proof of data provenance for auditors.
- **Understanding Complex Logic:** Use the lineage graph to deconstruct and understand complex data pipelines built by others.

### 6. Common Challenges and Solutions
- **Challenge:** "My column-level lineage seems incomplete or broken."
- **Solution:** This almost always happens when a code recipe (Python/R/SQL) accesses data without formally declaring it as an input. To ensure full lineage, your code must use the Dataiku APIs to read and write data, and all sources must be added as formal inputs to the recipe.
`},{id:336,slug:"refreshing-schema-metadata",question:"How to get started with refreshing schema metadata when source changes?",answer:`
### 1. Introduction/Overview
When the schema of an external data source changes (e.g., a new column is added or a column is renamed in a source database table), you need to update Dataiku's metadata to reflect this change. This ensures that your pipelines don't fail due to unexpected schema mismatches.

### 2. Prerequisites
- **A dataset in Dataiku** that connects to an external source (like a SQL database or a cloud file).
- **A known change in the source schema.**

### 3. Step-by-Step Instructions
1.  **Navigate to the Dataset:** Open the dataset whose source schema has changed.
2.  **Go to the Settings Tab:** Click on the **Settings** tab.
3.  **Check for Schema Mismatches:**
    *   Dataiku often proactively checks for schema changes. You may see a warning message indicating that the schema is out of sync.
    *   In the "Schema" section, you can review the columns that Dataiku currently "thinks" the dataset has.
4.  **Refresh the Schema:**
    *   There is typically a button labeled **Check Now** or **Refresh schema from source**.
    *   Click this button. Dataiku will reconnect to the external source, re-read its schema, and show you a "diff" view comparing the old schema with the new one.
5.  **Confirm the Changes:**
    *   Review the changes. Dataiku will show you which columns have been added, deleted, or have had their types changed.
    *   If you are happy with the changes, click the **Update Schema** or **Save** button. Dataiku's metadata for this dataset is now up-to-date.

### 4. Resources and Tools
- **The Dataset Settings page:** The central place for managing a dataset's schema.
- **The "Check Schema" or "Refresh Schema" button:** The tool for triggering a re-scan of the source.

### 5. Next Steps and Progression
- **Propagate Schema Changes:** After updating a source dataset's schema, you will likely need to update your downstream recipes to account for the changes (e.g., to use a new column or handle a deleted one). Dataiku often provides a "Propagate schema changes" utility to help with this.
- **Automated Schema Checks:** For critical pipelines, you could create a scenario with a Python step that uses the API to programmatically check for schema changes and sends an alert if an unexpected change is detected.

### 6. Common Challenges and Solutions
- **Challenge:** "I refreshed the schema, but now my downstream recipe is failing."
- **Solution:** This is expected. If a column was renamed or deleted from the source, any recipe that was using that column will now fail with a "column not found" error. You must edit the recipe to either remove the reference to the old column or update it to use the new column name.
- **Challenge:** "The schema check is very slow."
- **Solution:** This can happen if the source is a folder containing a very large number of files. Dataiku may need to read the headers of many files to infer the schema.
`},{id:337,slug:"tracking-downstream-effects",question:"How to get started with tracking downstream effects when modifying upstream logic?",answer:`
### 1. Introduction/Overview
Before you make a change to any dataset or recipe in a Dataiku Flow, it is critical to understand the potential impact on all downstream processes. This "impact analysis" prevents you from making a small change that inadvertently breaks a critical report or model. Dataiku's automatic lineage makes this easy and visual.

### 2. Prerequisites
- **A Dataiku project** with a connected Flow.
- **A plan to modify** an "upstream" object (a dataset or recipe early in the flow).

### 3. Step-by-Step Instructions
1.  **Locate the Object to be Changed:** In your Flow, find the dataset or recipe you are planning to modify.
2.  **Right-Click and Select "View downstream dependencies":** This is the key action.
3.  **Analyze the Highlighted Path:**
    *   Dataiku will instantly highlight every single object in the Flow that depends on the item you selected.
    *   This includes all downstream recipes, datasets, machine learning models, dashboards, and webapps.
    *   The highlighted path shows you the complete "blast radius" of your potential change.
4.  **Review the Impacted Objects:**
    *   Look at the highlighted items. Are any of them critical?
    *   For example, does your change affect the input to a production model or the final dataset used by an executive dashboard?
5.  **Plan Your Changes and Communication:**
    *   Now that you understand the impact, you can plan accordingly.
    *   You will need to test all the highlighted downstream objects after you make your change.
    *   You may need to communicate with the owners of the impacted dashboards or models to let them know about the upcoming change.

### 4. Resources and Tools
- **The "View downstream dependencies" feature:** Your primary tool for impact analysis. It is accessible via a right-click on any object in the Flow.
- **The Lineage Tab:** For a more detailed view, you can use the lineage tab to see how a change to a specific *column* might affect downstream columns.

### 5. Next Steps and Progression
- **Making it a Habit:** Make impact analysis a mandatory first step before any modification. It should become a natural reflex for any Dataiku developer.
- **Change Management Process:** For critical production projects, use this feature as part of a formal change management process. A developer might need to include a screenshot of the downstream dependencies in their change request ticket to show they have assessed the impact.

### 6. Common Challenges and Solutions
- **Challenge:** "The downstream dependency path is huge. I don't know what all these things are."
- **Solution:** This indicates the project may be poorly documented. This is where good descriptions on all objects are essential. Hover over the highlighted items to read their descriptions and understand their purpose. You may need to talk to other team members who own the downstream components.
- **Challenge:** "Does this feature work for code recipes?"
- **Solution:** Yes, as long as the code recipe correctly declares its inputs and outputs using the Dataiku API. If it does, it will be part of the lineage graph like any other recipe.
`},{id:338,slug:"resolving-circular-dependencies",question:"How to get started with resolving circular dependencies in Flow?",answer:`
### 1. Introduction/Overview
A circular dependency (or cycle) is a situation where A depends on B, and B also depends on A. This creates an impossible loop. Dataiku Flows are required to be **Directed Acyclic Graphs (DAGs)**, meaning these cycles are not allowed. Dataiku's UI will prevent you from creating them and will show a clear error if one exists.

### 2. Prerequisites
- **An understanding of dependencies:** A recipe depends on its inputs to create an output.

### 3. Step-by-Step Instructions
1.  **How a Cycle is Created:**
    *   Imagine you have: \`Dataset_A -> Recipe_1 -> Dataset_B\`.
    *   Now, you try to create a new recipe (\`Recipe_2\`) that takes \`Dataset_B\` as input and tries to write back to \`Dataset_A\`.
    *   This creates a cycle: \`A -> 1 -> B -> 2 -> A\`.
2.  **Identifying the Cycle:**
    *   Dataiku will not let you create this. When you try to configure \`Recipe_2\` to write to \`Dataset_A\`, it will show an error message like "Circular dependency detected."
    *   The Flow will often draw the problematic dependency with a red, dashed line.
3.  **Resolving the Cycle:**
    *   You must rethink your logic. A cycle means your pipeline's logic is fundamentally flawed.
    *   You cannot have a dataset that is both an ancestor and a descendant of another dataset.
    *   **The solution is almost always to create a new dataset.** Instead of having \`Recipe_2\` write back to \`Dataset_A\`, it should write to a new dataset, for example, \`Dataset_A_updated\`.
    *   Your new, valid flow would be: \`A -> 1 -> B -> 2 -> A_updated\`. This is a linear, acyclic graph.

### 4. Resources and Tools
- **The Dataiku Flow Validator:** The built-in mechanism that detects and prevents cycles.
- **Logical Flow Design:** The best tool is to plan your flow logically before you build, ensuring a clear, one-way progression of data.

### 5. Next Steps and Progression
- **Refactoring:** If you inherit a project with a very tangled flow, you may need to refactor it to make the data dependencies clearer and more linear, which will help prevent accidental attempts to create cycles.
- **Understanding Partitions:** With partitioned datasets, you can have a recipe that reads from partition N-1 of a dataset to build partition N of the same dataset. This is a special, valid case of self-dependency that Dataiku handles correctly.

### 6. Common Challenges and Solutions
- **Challenge:** "I really need to update the original dataset with new calculations."
- **Solution:** You can't have a recipe that reads from and writes to the exact same dataset. The standard pattern is to read from \`Dataset_A\`, apply your logic, and write the result to \`Dataset_A_prepared\`. Then, all downstream processes should use the new, prepared version.
- **Challenge:** "Dataiku is not letting me connect the recipe, and I don't understand why."
- **Solution:** Stop and trace the lineage carefully. Look at the inputs and outputs. You have almost certainly created a logical loop. Whiteboard the flow to see the dependency graph more clearly.
`},{id:339,slug:"archiving-stale-datasets-safely",question:"How to get started with archiving stale datasets safely without losing lineage?",answer:`
### 1. Introduction/Overview
Over time, projects can accumulate datasets that are no longer actively used. These "stale" datasets can clutter your Flow and consume storage space. Archiving is the process of safely saving the data and then removing the dataset from the active flow.

### 2. Prerequisites
- **A Dataiku project** with potentially stale or obsolete datasets.
- **A designated long-term storage location** (e.g., a specific folder in S3 or a network drive).
- **Project administrator rights** to delete datasets.

### 3. Step-by-Step Instructions
1.  **Identify Stale Datasets:**
    *   A dataset is stale if it is no longer used by any downstream recipe, model, or dashboard.
    *   **Use Impact Analysis:** Right-click on the dataset in the Flow and select **View downstream dependencies**. If there are none, it is a candidate for archiving.
2.  **Export the Data for Archive:**
    *   Before deleting, save the data.
    *   Select the dataset and use an **Export recipe**.
    *   Configure the recipe to export the data to your designated long-term archive location. It's a good practice to use an efficient storage format like Parquet and include a date in the filename.
3.  **Export the Schema:**
    *   Open the dataset, go to **Settings > Schema**, and use the option to download the schema as a JSON file. Save this in your archive folder alongside the data.
4.  **Delete the Dataset from the Flow:**
    *   Once you have safely exported the data and schema, you can delete the dataset from your project.
    *   Right-click on the dataset and choose **Delete**. Confirm the deletion. This will remove the dataset from the Flow and delete its data from Dataiku's active storage.
5.  **Document the Archive:**
    *   In your project's **Wiki**, create a page called "Archive Log."
    *   For each dataset you archive, add an entry noting its original name, the date it was archived, and the path to where the data is now stored.

### 4. Resources and Tools
- **"View downstream dependencies" feature:** Your safety check before deletion.
- **Export Recipe:** The tool for creating the data archive.
- **Project Wiki:** The place to document what has been archived and where it can be found.

### 5. Next Steps and Progression
- **Automated Archiving:** You could create a scenario with a Python step that periodically scans the project, identifies stale datasets based on their last build time and lack of dependencies, and then automatically runs the export and deletion process.
- **Data Retention Policies:** Implement an enterprise data retention policy that defines how long different types of data should be kept in active storage before being archived.

### 6. Common Challenges and Solutions
- **Challenge:** "I deleted a dataset that I needed."
- **Solution:** If you have archived the data and schema, you can re-import it. If not, you may need to restore it from a Dataiku backup. This emphasizes the importance of the impact analysis and export steps. Never delete without saving first.
- **Challenge:** "How is archiving different from just clearing the data?"
- **Solution:** "Clearing" a dataset removes its data but leaves the dataset object in the Flow. It is still an active part of the pipeline. "Archiving" involves exporting the data and then deleting the dataset object from the Flow entirely, as it is no longer part of the active pipeline.
`},{id:340,slug:"multi-user-collaboration-flow-conflicts",question:"How to get started with multi-user collaboration causing Flow conflicts?",answer:`
### 1. Introduction/Overview
When multiple developers work on the same Dataiku project simultaneously, they can sometimes overwrite each other's changes. The solution to this is to use a structured, collaborative workflow based on **Git integration**, which is designed specifically for managing concurrent development.

### 2. Prerequisites
- **A Dataiku project linked to a Git repository.**
- **A team of two or more developers.**
- **All team members should be familiar with basic Git concepts** (branch, commit, push, pull, merge).

### 3. Step-by-Step Instructions: A Safe Collaborative Workflow
1.  **Never Work Directly on the \`main\` Branch:** The \`main\` branch should represent the stable, production-ready version of the project. It should be protected.
2.  **Create a Feature Branch:**
    *   Before starting a new task, each developer must create their own **branch** from the latest version of \`main\`.
    *   In Dataiku's Git page, use the "Create branch" feature. Give the branch a descriptive name (e.g., \`feature/JIRA-123-new-sales-report\`).
3.  **Work in Isolation on Your Branch:**
    *   Each developer now works on their own branch. Any changes they make (editing recipes, creating datasets) are isolated to their branch and will not affect their teammates.
    *   They should commit and push their changes to their feature branch regularly.
4.  **Update Your Branch with Changes from \`main\`:**
    *   Periodically, developers should pull the latest changes from the shared \`main\` branch into their feature branch. This is done by using the **Pull** button on the Git page.
    *   This helps to integrate changes from other team members early and often, reducing the chance of large merge conflicts later.
5.  **Create a Pull Request for Review:**
    *   When a developer has finished their feature, they go to the Git provider's website (e.g., GitHub) and create a **Pull Request (PR)**.
    *   A PR is a formal request to merge the changes from their feature branch into the \`main\` branch.
6.  **Code Review and Merge:**
    *   Another team member must review the PR. They can see all the changes and provide comments.
    *   Once the PR is approved, the feature branch is merged into \`main\`. The new feature is now part of the stable project.

### 4. Resources and Tools
- **Dataiku's Git Integration:** The UI for branching and committing.
- **Your Git Provider (GitHub, GitLab, etc.):** The platform for managing pull requests and code reviews.
- **A defined branching strategy** (like GitFlow).

### 5. Next Steps and Progression
- **Handling Merge Conflicts:** If two developers edit the same recipe, a merge conflict can occur. Dataiku provides a visual merge tool to help you resolve these conflicts by choosing which version of the code to keep.

### 6. Common Challenges and Solutions
- **Challenge:** "Two developers edited the same visual recipe, and the merge is a mess."
- **Solution:** This is the primary challenge of visual tool collaboration. The best solution is to **break down tasks** so that two people are not working on the exact same recipe at the same time. If they must, they should communicate frequently.
- **Challenge:** "This seems like a lot of process."
- **Solution:** It is, but it is the industry-standard process for safe, scalable, collaborative software and data development. It prevents developers from overwriting work and ensures that all changes are reviewed before they go into production.
`},{id:341,slug:"building-a-basic-scenario",question:"How to get started with building a basic Scenario to schedule recipe execution?",answer:`
### 1. Introduction/Overview
A **Scenario** is Dataiku's tool for automation and orchestration. It allows you to define a sequence of actions (like running a pipeline) and then schedule it to run automatically. This is how you move a project from manual development to automated production.

### 2. Prerequisites
- **A Dataiku Flow** that you want to automate. For example, a flow that produces a final dataset.
- **A clear automation goal** (e.g., "I want to rebuild my final report every day").

### 3. Step-by-Step Instructions
1.  **Navigate to the Scenarios Page:** In your project's top menu bar, click **Scenarios**.
2.  **Create a New Scenario:** Click the **+ NEW SCENARIO** button and give it a clear, descriptive name (e.g., \`Build_Daily_Sales_Report\`).
3.  **Define the Steps (The "What"):**
    *   Go to the **Steps** tab. This is where you tell the scenario what to do.
    *   Click **+ ADD STEP**. The most common step is **Build / Train**.
    *   In the step's configuration, select the **final output dataset** of your flow. You only need to select the last item; Dataiku will automatically build all of its upstream dependencies.
4.  **Define the Trigger (The "When"):**
    *   Go to the **Settings** tab.
    *   Click **+ ADD TRIGGER**. The most common type is **Time-based**.
    *   Configure the schedule (e.g., "Daily" at "02:00").
    *   Enable the trigger using the toggle switch.
5.  **Define the Alerts (The "What If"):**
    *   Go to the **Reporters** tab.
    *   Click **+ ADD REPORTER**. Select **Mail**.
    *   Configure it to send an email **On failure** to your team.
6.  **Activate and Save:** Ensure the main toggle at the top of the scenario is set to **Active**, and then **Save** your changes. The scenario is now live and will run on the schedule you defined.

### 4. Resources and Tools
- **The Scenarios Page:** Your central hub for all project automation.
- **The Step Library:** The list of actions your scenario can perform.
- **Triggers and Reporters:** The tools for scheduling and alerting.

### 5. Next Steps and Progression
- **Manual Run:** Before you leave it to run on a schedule, it's a good practice to click the **Run** button to test the scenario manually.
- **Chained Scenarios:** You can have one scenario trigger another upon completion, allowing for more complex orchestrations.
- **Data Quality Gates:** Add a "Run checks" step after your build step to ensure the data is valid before the scenario finishes successfully.

### 6. Common Challenges and Solutions
- **Challenge:** "My scenario didn't run."
- **Solution:** Check the basics first: Is the main scenario toggle on? Is the trigger's toggle on? Check the "Last runs" tab for any error messages that might have prevented it from starting.
- **Challenge:** "My scenario builds the whole flow every time and is slow."
- **Solution:** In the "Build" step settings, change the build mode from the default to **Smart rebuild**. This will intelligently only rebuild the parts of the flow that have changed, which is much more efficient.
`},{id:342,slug:"setting-triggers-based-on-dataset-update",question:"How to get started with setting triggers based on dataset update timing?",answer:`
### 1. Introduction/Overview
In addition to time-based schedules, Dataiku scenarios can be launched by **event-based triggers**. The most common event is a change in a dataset. This allows you to create reactive pipelines that automatically run whenever new data becomes available.

### 2. Prerequisites
- **A Dataiku Scenario.**
- **An input dataset** whose update should trigger the pipeline (e.g., a dataset connected to a folder where new files are dropped daily).

### 3. Step-by-Step Instructions
1.  **Navigate to Scenario Settings:** Open the scenario you want to trigger. Go to the **Settings** tab.
2.  **Add a New Trigger:** In the "Triggers" section, click **+ ADD TRIGGER**.
3.  **Select "Dataset change":** Choose this option from the trigger type list.
4.  **Configure the Trigger:**
    *   **Dataset:** Select the specific dataset that should be watched for changes.
    *   **Trigger when:** You can choose to trigger the scenario when the dataset's data changes, its schema changes, or both. For most ingestion workflows, you will choose "Data changes".
5.  **Enable and Save:**
    *   Enable the trigger using the toggle switch next to it.
    *   **Save** the scenario.
6.  **How it Works:** The scenario is now active. Dataiku will periodically check the status of the selected dataset. As soon as it detects that the dataset has been updated (e.g., a new partition has appeared, or an upstream job has rebuilt it), it will automatically launch your scenario.

### 4. Resources and Tools
- **The "Dataset change" Trigger:** The primary tool for creating event-driven workflows.
- **Scenarios:** The orchestration engine where the trigger is configured.

### 5. Next Steps and Progression
- **File-based Triggers:** This pattern is very powerful when combined with datasets that point to folders in cloud storage (S3, GCS, ADLS) or SFTP servers. When an external process drops a new file in the folder, your Dataiku scenario will automatically wake up and process it.
- **Chaining Scenarios:** A common pattern is to have one scenario (e.g., \`process_data\`) be triggered by the completion of an upstream dataset, which in turn was built by another scenario (e.g., \`ingest_data\`).

### 6. Common Challenges and Solutions
- **Challenge:** "My trigger is firing too often."
- **Solution:** The trigger will fire every single time the source dataset is modified. If the upstream process is very "chatty" and rebuilds the dataset frequently, your scenario will also run frequently. You may need to adjust the logic of the upstream process or add a "quiet period" condition to your trigger.
- **Challenge:** "The trigger didn't fire even though the data changed."
- **Solution:** Dataiku's check for changes is not instantaneous; it runs on a periodic background task. There may be a delay of a few minutes between the data changing and the scenario being triggered. Also, verify that the dataset was actually modified in a way that Dataiku detects (e.g., a new partition appeared).
`},{id:343,slug:"configuring-retries-on-failure",question:"How to get started with configuring retries on failure without infinite loops?",answer:`
### 1. Introduction/Overview
Transient failures, like a temporary network hiccup or a database deadlock, can cause a scenario to fail. Instead of requiring manual intervention, you can build an automatic retry mechanism. While Dataiku doesn't have a simple "retry N times" button, you can implement this logic reliably using a **Python scenario step**.

### 2. Prerequisites
- **A scenario** that you want to make more resilient.
- **Basic Python skills** and familiarity with the Dataiku API.

### 3. Step-by-Step Instructions: The "Wrapper" Scenario Pattern
1.  **Isolate the Action:** Your main scenario (let's call it \`build_my_report\`) should contain only the build steps.
2.  **Create a "Wrapper" Scenario:** Create a new, separate scenario called \`wrapper_retry_build\`. This scenario's only purpose is to call the main scenario and handle the retries.
3.  **Add a Python Step:** In the wrapper scenario, add a single step of type **Execute Python code**.
4.  **Write the Retry Script:**
    *   The script will use a \`for\` loop to try running the main scenario a fixed number of times.
    *   It uses a \`try...except\` block to catch any failures.
    *   If the main scenario succeeds, the loop breaks. If it fails, it waits for a moment and tries again.
    *   A counter is essential to prevent infinite loops.
    > \`\`\`python
    > import dataiku
    > import time
    > 
    > MAX_RETRIES = 3
    > RETRY_DELAY_SECONDS = 60 # Wait 1 minute between retries
    > 
    > scenario_to_run = dataiku.api_client().get_project("MY_PROJECT").get_scenario("build_my_report")
    > 
    > for i in range(MAX_RETRIES):
    >     try:
    >         print(f"Attempt {i+1} of {MAX_RETRIES}...")
    >         job = scenario_to_run.run_and_wait() # run_and_wait is key
    >         
    >         if job.get_info()["result"] == "SUCCESS":
    >             print("Scenario succeeded.")
    >             dataiku.scenario.set_scenario_outcome(True, "Successfully completed.")
    >             break
    >         else:
    >             # This case handles if the job runs but ends in a FAILED state
    >             print("Scenario ran but failed.")
    >     except Exception as e:
    >         print(f"An exception occurred: {e}")
    >     
    >     # If not the last attempt, wait before retrying
    >     if i < MAX_RETRIES - 1:
    >         print(f"Waiting {RETRY_DELAY_SECONDS} seconds before next retry.")
    >         time.sleep(RETRY_DELAY_SECONDS)
    > else: # This 'else' belongs to the 'for' loop
    >     # This block executes only if the loop completed without a 'break'
    >     dataiku.scenario.set_scenario_outcome(False, f"Scenario failed after {MAX_RETRIES} attempts.")
    > \`\`\`
5.  **Schedule the Wrapper:** You now schedule the \`wrapper_retry_build\` scenario, not the original one.

### 4. Resources and Tools
- **Python Scenario Step:** The environment for your custom orchestration logic.
- **Dataiku Python API:** Specifically the functions for running scenarios (\`scenario.run_and_wait()\`) and setting outcomes (\`set_scenario_outcome()\`).
- **Python's \`time\` module:** For adding delays between retries.

### 5. Next Steps and Progression
- **Exponential Backoff:** Make your script smarter by increasing the delay between retries (e.g., 1 min, then 5 mins, then 15 mins).
- **Conditional Retries:** Analyze the error message inside the \`except\` block. You could choose to only retry on specific, known transient errors (like "Connection timeout") but fail immediately for fatal errors ("Table not found").

### 6. Common Challenges and Solutions
- **Challenge:** "This is too complicated for a simple retry."
- **Solution:** This is an advanced pattern. It should only be used for critical production scenarios where you expect occasional, recoverable failures. For most scenarios, a simple failure alert that notifies a human to rerun it is sufficient.
- **Challenge:** "I created an infinite loop."
- **Solution:** The \`for i in range(MAX_RETRIES)\` structure is your primary safety mechanism. It guarantees the loop cannot run more than \`MAX_RETRIES\` times.
`},{id:344,slug:"handling-global-variables-in-scenarios",question:"How to get started with handling global variables within Scenarios effectively?",answer:`
### 1. Introduction/Overview
Project Variables act as global parameters for your project. Scenarios provide two powerful ways to interact with them: you can **retrieve** their values to control scenario logic, and you can **override** their values for a specific run, which is key for creating dynamic, reusable pipelines.

### 2. Prerequisites
- **A Dataiku project** with pre-defined Project Variables (e.g., \`region\`, \`start_date\`).
- **A Scenario.**

### 3. Step-by-Step Instructions

#### Method 1: Retrieving Variables in a Python Step
- **Use Case:** Making a decision in your scenario based on a project's configuration.
- **How:**
    1.  Add a **Execute Python code** step to your scenario.
    2.  Use the \`dataiku.get_custom_variables()\` API call to fetch all variables as a dictionary.
    > \`\`\`python
    > import dataiku
    > variables = dataiku.get_custom_variables()
    > target_region = variables.get('region')
    > 
    > print(f"Running pipeline for region: {target_region}")
    > # ... add logic based on the region ...
    > \`\`\`
#### Method 2: Overriding Variables for a Scenario Run
- **Use Case:** Running the same flow with different parameters without changing the project itself. For example, running a monthly report for January, then February, then March.
- **How:**
    1.  Open your scenario.
    2.  In the "Steps" tab, find the "Build" step. You can often set parameters here.
    3.  A more powerful way is to define **Run-as-you-go parameters** for the scenario itself. In the scenario's "Settings" tab, you can define parameters that will override the project variables only for the duration of this scenario's run.
    4.  When triggering a scenario via the API, you can also pass in a JSON payload with new variable values.

### 4. Resources and Tools
- **Project Variables:** The source of your parameters.
- **Python Scenario Step:** The tool for programmatically retrieving and acting on variables.
- **Scenario Parameters / API Payload:** The mechanisms for overriding variables for a specific run.

### 5. Next Steps and Progression
- **Looping with Variables:** A common pattern is to have a scenario loop. In each iteration, a Python step updates a project variable (e.g., \`run_date\`), and then a "Build" step runs the parameterized flow using that new date.
- **Dynamic Configuration:** Use variables to control every aspect of your flow, from filter conditions to source filenames to database connections.

### 6. Common Challenges and Solutions
- **Challenge:** "I updated the variable in my Python step, but the next 'Build' step in the scenario didn't use the new value."
- **Solution:** You need to use the correct API call. The \`dataiku.get_custom_variables()\` call reads the variables. To *set* them, you must use the full API client:
    > \`\`\`python
    > client = dataiku.api_client()
    > project = client.get_project("MY_PROJECT")
    > current_vars = project.get_variables()
    > current_vars['standard']['my_variable'] = "new_value"
    > project.set_variables(current_vars)
    > \`\`\`
    This updates the variable for all subsequent steps in the scenario.
`},{id:345,slug:"setting-up-email-alerts",question:"How to get started with setting up email alerts when a step fails?",answer:`
### 1. Introduction/Overview
Email alerts are a fundamental part of monitoring automated pipelines. They ensure that you are notified immediately when a job fails, allowing you to investigate and resolve the issue quickly. In Dataiku, this is configured using **Reporters** in a Scenario.

### 2. Prerequisites
- **A Scenario** that you want to monitor.
- **Mail Server Configuration:** A Dataiku administrator must have configured the connection to your company's mail server in **Administration > Settings**.

### 3. Step-by-Step Instructions
1.  **Open Your Scenario:** Navigate to the **Scenarios** page in your project and select the scenario you want to add alerts to.
2.  **Go to the "Reporters" Tab:** This is the dedicated section for all notifications.
3.  **Add a New Reporter:** Click **+ ADD REPORTER**.
4.  **Select "Mail":** Choose this from the channel dropdown list.
5.  **Configure the Reporter:**
    *   **Name:** Give it a clear name, like "Failure Alert to Dev Team".
    *   **Run condition:** This is the most important setting. For failure alerts, choose **On failure**.
    *   **Recipients:** Enter the email address or distribution list that should receive the alert.
    *   **Subject and Body:** Customize the message. It is **critical** to include context variables to make the alert actionable.
        *   **Good Subject:** \`Dataiku Job FAILED: \${projectKey} - \${scenarioName}\`
        *   **Good Body:** Include details and, most importantly, a direct link to the logs: \`The job log is available here: \${jobURL}\`
6.  **Save:** Save the scenario. The reporter is now live. The next time this scenario fails, the configured email will be sent automatically.

### 4. Resources and Tools
- **Reporters Tab:** The UI for configuring all alerts.
- **Built-in Variables:** Use variables like \`\${jobURL}\` to make your alerts useful. A full list is available in the editor.

### 5. Next Steps and Progression
- **Multiple Reporters:** You can add multiple reporters. For example, a detailed technical alert on failure, and a simple high-level summary on success.
- **Slack/Teams Integration:** If your team uses a chat application, configure a Slack or Teams reporter for more immediate notifications.
- **Attach Reports:** You can configure a reporter to attach a dashboard export (as a PDF) or a dataset (as a CSV) to the email.

### 6. Common Challenges and Solutions
- **Challenge:** "We are not receiving the emails."
- **Solution:** First, double-check that a scenario has actually failed. Second, verify with your Dataiku administrator that the instance's mail server is configured correctly and that there are no firewalls blocking the emails.
- **Challenge:** "We get the email, but we don't know what to do."
- **Solution:** The alert message is not actionable enough. You must include the \`\${jobURL}\` variable. This provides a direct link to the job log, which is the starting point for any investigation.
`},{id:346,slug:"passing-parameters-into-scenario-jobs",question:"How to get started with passing parameters into Scenario jobs correctly in Python?",answer:`
### 1. Introduction/Overview
Passing parameters into a scenario run allows you to create highly dynamic and reusable automation. This is most powerfully done when triggering a scenario from an external system via the REST API. The external system can include a JSON payload with values that will override the project variables for that specific run.

### 2. Prerequisites
- **A parameterized Dataiku project:** The project should use Project Variables to control its behavior (e.g., in recipe filters).
- **A Scenario** that builds the parameterized flow.
- **A tool to call the REST API** (like \`curl\` or a Python script).

### 3. Step-by-Step Instructions
1.  **Identify the API Endpoint:** The endpoint for running a scenario is:
    > \`YOUR_DSS_URL/public/api/projects/PROJECT_KEY/scenarios/SCENARIO_ID/run\`
2.  **Construct the JSON Payload:**
    *   Create a JSON object that specifies the variables you want to override.
    *   The structure should be: \`{"variables": {"standard": {"variable_name": "new_value"}}}\`.
    > \`\`\`json
    > {
    >   "variables": {
    >     "standard": {
    >       "region": "EMEA",
    >       "start_date": "2023-01-01"
    >     }
    >   }
    > }
    > \`\`\`
3.  **Make the API Call with the Payload:**
    *   You will make an **HTTP POST** request to the endpoint.
    *   The JSON payload is sent as the body of the request.
    *   **Example using \`curl\`:**
    > \`\`\`bash
    > API_KEY="your_api_key"
    > PAYLOAD_JSON='{"variables":{"standard":{"region":"EMEA"}}}'
    >
    > curl -X POST -u "\${API_KEY}:" \\
    >      -H "Content-Type: application/json" \\
    >      -d "\${PAYLOAD_JSON}" \\
    >      "https://dss.mycompany.com/public/api/projects/MYPROJ/scenarios/run_report/run"
    > \`\`\`
4.  **Execution:** When Dataiku receives this request, it will start the "run_report" scenario. For this specific run, the value of the project variable \`region\` will be temporarily set to "EMEA", and any recipe that uses \`\${region}\` will use this new value.

### 4. Resources and Tools
- **Dataiku REST API Documentation:** It provides details on the "run scenario" endpoint and the expected payload format.
- **\`.json\` file:** For complex payloads, it's easier to write the JSON in a file and pass it to curl (\`-d @payload.json\`).
- **Python \`requests\` library:** The ideal tool for making these API calls from a controlling application.

### 5. Next Steps and Progression
- **Dynamic Parameter Calculation:** The controlling application can dynamically calculate the parameters before making the API call. For example, it could calculate yesterday's date and pass that as the \`run_date\` parameter.
- **Orchestration:** This is the core pattern for integrating Dataiku with external orchestrators like Airflow. The Airflow DAG can calculate the parameters and then use an operator to make the API call to Dataiku.

### 6. Common Challenges and Solutions
- **Challenge:** "The scenario ran, but it didn't use the parameters I sent."
- **Solution:** Check the syntax of your JSON payload very carefully. It must match the exact structure Dataiku expects. Also, verify that your content-type header is set to \`application/json\`.
- **Challenge:** "How do I do this from a Python script?"
- **Solution:**
    > \`\`\`python
    > import requests
    > payload = {"variables": {"standard": {"region": "EMEA"}}}
    > response = requests.post(url, auth=(api_key, ""), json=payload)
    > \`\`\`
`},{id:347,slug:"organizing-scenario-steps",question:"How to get started with organizing scenario steps and dependencies visually?",answer:`
### 1. Introduction/Overview
A scenario is a sequence of steps that run in order. Organizing these steps logically is key to creating automation that is easy to understand, debug, and maintain. The best practice is to keep scenarios focused on a single business purpose and to name steps clearly.

### 2. Prerequisites
- **A Dataiku Scenario.**
- **A multi-step process** you need to automate (e.g., build data, run quality checks, train a model, send a report).

### 3. Step-by-Step Instructions
1.  **Give Your Scenario a Clear Name:** The scenario name should describe its business goal (e.g., \`Retrain_Churn_Model_Weekly\`, not \`Scenario_1\`).
2.  **Add Steps in Logical Order:**
    *   Scenario steps execute sequentially from top to bottom. Add them in the order they need to run.
    *   **Example Order:**
        *   Step 1: Build the training dataset.
        *   Step 2: Run data quality checks on the training data.
        *   Step 3: Train the model.
        *   Step 4: Evaluate the new model.
        *   Step 5: Send a report with the results.
3.  **Give Each Step a Descriptive Name:**
    *   Just like the scenario name, the name of each step is important documentation.
    *   Instead of the default "Build / Train", rename it to "Build Final Training Data". Instead of "Run python code", rename it to "Deploy Model if Better".
4.  **Use the "Continue on error" Setting Carefully:**
    *   By default, if a step fails, the entire scenario stops.
    *   You can check the "Continue on error" box for a step if you want the scenario to proceed even if that specific step fails. This should be used sparingly, for non-critical steps like an optional cleanup task.

### 4. Resources and Tools
- **The Scenario "Steps" Tab:** Your main workspace for organizing your automation logic.
- **The Step Library:** The list of all available actions you can add to your sequence.

### 5. Next Steps and Progression
- **Conditional Logic:** For more complex dependencies, use a **Python step** to implement if/else logic. The script can check a condition and then programmatically trigger different build steps based on the outcome.
- **Chaining Scenarios:** For very complex workflows, break them down into multiple, simpler scenarios. One "master" scenario can then be responsible for triggering the other scenarios in the correct order. This is excellent for modularity.

### 6. Common Challenges and Solutions
- **Challenge:** "My scenario has 20 steps and is hard to follow."
- **Solution:** Your scenario is likely doing too many different things. Consider breaking it down into smaller, more focused scenarios. For example, have one scenario for data ingestion and another for reporting. You can then chain them together.
- **Challenge:** "A step failed, but the scenario was still marked as a 'SUCCESS'."
- **Solution:** You have likely checked the "Continue on error" box for that failing step. This tells the scenario to ignore the failure. Uncheck this box for any step that is critical to the pipeline's success.
`},{id:348,slug:"executing-conditional-steps",question:"How to get started with executing conditional steps depending on dataset checks?",answer:`
### 1. Introduction/Overview
Standard scenario steps run sequentially. To execute steps conditionally (i.e., an "if/then" logic), you need to use a **Python scenario step**. This gives you the full power of Python to check any condition—such as the result of a data quality check—and then programmatically decide which part of your flow to execute next.

### 2. Prerequisites
- **A Dataiku Scenario.**
- **A condition to check** (e.g., "does my input dataset have more than 1000 rows?").
- **A basic understanding of the Dataiku Python API.**

### 3. Step-by-Step Instructions
1.  **Build Your Different Logical Paths:** In your Flow, build out the different recipe chains you might want to execute. For example, a "standard" processing flow and a separate "alerting" flow.
2.  **Add a Python Step for the Condition:**
    *   In your scenario, add a new step of type **Execute Python code**. This step will act as your "if" statement.
3.  **Write the Conditional Script:**
    *   In the Python script, use the Dataiku API to get the information you need to check your condition.
    *   Then, use a standard Python \`if...else\` block to decide which job to run.
    *   **Example: Run a build only if the input dataset is not empty.**
    > \`\`\`python
    > import dataiku
    >
    > client = dataiku.api_client()
    > project = client.get_project("MY_PROJECT")
    >
    > # Get the row count of the input dataset
    > input_dataset = project.get_dataset("my_input")
    > row_count = input_dataset.get_metadata()["metrics"]["recordsCount"]
    >
    > # The conditional logic
    > if row_count > 0:
    >     print(f"Input has {row_count} rows. Running main pipeline.")
    >     project.get_dataset("final_output").build()
    > else:
    >     print("Input dataset is empty. Skipping build and sending alert.")
    >     # You could add code here to trigger an alerting scenario instead
    > \`\`\`
4.  **Place the Step Correctly:** This Python step should come *before* any of the steps it is controlling. The rest of the pipeline logic is now contained within this single Python step.

### 4. Resources and Tools
- **Python Scenario Step:** Your environment for writing the custom conditional logic.
- **Dataiku Python API Documentation:** Essential for learning how to get dataset metrics, project variables, and trigger jobs programmatically.

### 5. Next Steps and Progression
- **Checking Data Quality Check Results:** You can have a scenario step that runs data quality checks, and then a subsequent Python step can use the API to get the results of those checks and make a decision.
- **Complex Branching:** Your Python script can implement complex \`if/elif/else\` chains to orchestrate very sophisticated, multi-path workflows.

### 6. Common Challenges and Solutions
- **Challenge:** "This seems complicated. Can't I just use a visual 'if' step?"
- **Solution:** Dataiku does not have a visual "if" step in scenarios. The Python step is the designated tool for this kind of custom control flow. While it requires code, it provides enormous flexibility.
- **Challenge:** "How do I get the outcome of a data quality check in Python?"
- **Solution:** After running a "Run checks" step, you can use \`dataset.get_last_checks_results()\` in your Python step. This will return a detailed object that you can inspect to see which checks passed or failed.
`},{id:349,slug:"exporting-logs-from-scenarios",question:"How to get started with exporting logs from Scenarios for debugging?",answer:`
### 1. Introduction/Overview
When a scenario fails, its log is the most important piece of information for debugging. Dataiku provides several ways to access and export these logs, from downloading them directly from the UI to forwarding them automatically to an external logging system.

### 2. Prerequisites
- **A scenario that has been run at least once.**

### 3. Step-by-Step Instructions

#### Method 1: Manual Download from the UI (Most Common)
1.  **Navigate to the Scenario Run:**
    *   In your project, go to **Scenarios**.
    *   Click on the "Last runs" tab for your scenario.
    *   Find the specific run you want to investigate and click on it. This will take you to the main Job Log page for that run.
2.  **View the Step Log:**
    *   In the job log, you will see the list of steps. Click on the specific recipe or code step whose log you want to view.
3.  **Download the Log:**
    *   In the log viewer pane, there is usually a **Download** button.
    *   Clicking this will save the full log for that step as a \`.txt\` file, which you can then analyze offline or share with a colleague.

#### Method 2: Automatic Export via Reporters
1.  **Configure a Reporter:** In your scenario, go to the **Reporters** tab.
2.  **Attach the Log to an Email:**
    *   Create a **Mail** reporter that triggers **On failure**.
    *   In the reporter's configuration, you can often find an option to attach the job log to the email. This is not always available or recommended for very large logs.
    *   **Better Practice:** Instead of attaching the log, include the \`\${jobURL}\` variable in the email body. This gives the recipient a direct, one-click link to view the log in the Dataiku UI.

#### Method 3: Forwarding to an External System (Admin Task)
1.  **Configure Log Shipping:** A Dataiku administrator can configure the instance to automatically ship all its logs (including scenario logs) to an external logging platform like Splunk, Datadog, or an ELK stack.
2.  **Analyze Externally:** This allows for long-term storage, advanced searching, and correlation of Dataiku logs with logs from other applications.

### 4. Resources and Tools
- **The Job Log UI:** The primary place to view and download logs.
- **Reporters:** For automatically sending notifications with links to logs.
- **External Logging Platforms (Splunk, etc.):** For enterprise-grade, centralized log management.

### 5. Next Steps and Progression
- **Programmatic Access:** Using the Dataiku Python API, you can get a handle on a job object and then call methods to retrieve the log contents programmatically, allowing you to build custom monitoring and alerting scripts.

### 6. Common Challenges and Solutions
- **Challenge:** "The log file is huge and hard to read."
- **Solution:** Use the search function (\`Ctrl+F\`) in your text editor to search for keywords like "ERROR", "Exception", or "failed". When troubleshooting, it's often best to start at the bottom of the log and work your way up.
- **Challenge:** "I don't have access to the global log settings."
- **Solution:** Forwarding logs to an external system is a global, administrative setting. You will need to work with your platform administrators to have this set up.
`},{id:350,slug:"scheduling-incremental-runs",question:"How to get started with scheduling incremental runs without redundant full refresh?",answer:`
### 1. Introduction/Overview
For large, time-based datasets, rebuilding the entire table every day is inefficient and slow. An incremental run, which only processes the newest data, is the standard best practice. In Dataiku, the easiest and most robust way to achieve this is with **partitioned datasets**.

### 2. Prerequisites
- **A large, time-based dataset** (e.g., daily transaction logs).
- **Your dataset must be partitioned**, typically by day. This is configured in the dataset's **Settings > Partitioning** tab.

### 3. Step-by-Step Instructions
1.  **Ensure Your Flow is Partitioned:**
    *   Start by partitioning your raw input dataset (e.g., by day).
    *   When you build recipes downstream from this, Dataiku will automatically propagate the partitioning. Your entire flow should be partitioned.
2.  **Create a Scenario:** Go to **Scenarios** and create a new scenario to automate the job.
3.  **Configure the Build Step:**
    *   Add a **Build / Train** step and select your final, partitioned output dataset.
4.  **Specify Partitions to Build (The Key Step):**
    *   In the build step's configuration, you will see a field for **Partitions to build**.
    *   Do not leave this blank (which defaults to "ALL").
    *   Instead, enter a dynamic identifier. The most common one is **LATEST**.
    *   This single keyword tells Dataiku: "Find the latest available partition in the source datasets, and run the entire flow to build only the corresponding partition in the final output."
5.  **Schedule the Scenario:**
    *   Go to the **Settings > Triggers** tab and add a **Time-based** trigger to run daily.
    *   Each day, this scenario will wake up, see the new "LATEST" day's worth of data, and process only that slice through your entire pipeline.

### 4. Resources and Tools
- **Dataset Partitioning:** The core feature that enables incremental runs.
- **Scenario Build Step:** The place where you specify the dynamic partition (\`LATEST\`) to build.
- **Job Inspector:** You can view the job log to confirm that the run only processed a single partition, not the whole dataset.

### 5. Next Steps and Progression
- **Backfilling:** To build all the historical partitions for the first time, launch a manual build from the Flow. The build dialog will allow you to specify a date range (e.g., "build all partitions from 2022-01-01 to yesterday").
- **Handling Late-Arriving Data:** You can use more advanced partition identifiers, like \`LATEST-1\` to rebuild yesterday's partition, or specify a date range to rebuild the last N days.
- **Non-Partitioned Sources:** If your source is a single large table without partitions, you must implement a manual incremental load using a "high-water mark" approach with SQL and project variables.

### 6. Common Challenges and Solutions
- **Challenge:** "My incremental job is rebuilding the whole dataset."
- **Solution:** You have misconfigured the "Build" step in your scenario. You must ensure the "Partitions to build" field is set to \`LATEST\` or another dynamic identifier. Leaving it blank defaults to building "ALL" partitions.
- **Challenge:** "What if I need to rebuild a specific day from the past?"
- **Solution:** Do not change the scenario. Go to the Flow, click on the final dataset, click the "Build" button, and manually specify the single partition (date) that you want to rebuild.
`},{id:351,slug:"installing-new-plugins",question:"How to get started with installing new plugins to access custom recipes?",answer:`
### 1. Introduction/Overview
Plugins are add-ons that extend Dataiku's core functionality. They can provide new dataset connectors, visual recipes, processors for the Prepare recipe, and more. Installing plugins from the Dataiku store allows you to easily add new capabilities to your instance. This is an administrative task.

### 2. Prerequisites
- **Dataiku Administrator rights.**
- **An idea of the functionality you are missing.**

### 3. Step-by-Step Instructions
1.  **Navigate to the Plugins Page:** As an administrator, go to **Administration** (the nine-dots icon) and select **Plugins**.
2.  **Browse the Plugin Store:**
    *   Click on the **Store** tab.
    *   Here you will find a catalog of plugins built and maintained by Dataiku, as well as contributions from the user community.
    *   You can search for plugins or browse by category (e.g., "Connectors", "Code recipes").
3.  **Install the Plugin:**
    *   Find a plugin that meets your needs (e.g., the "Geospatial" plugin for map-based analysis, or the "Tableau Hyper Export" plugin).
    *   Click on the plugin to see its details and documentation.
    *   Click the **Install** button.
4.  **Build the Plugin's Environment:**
    *   After installing, the plugin will appear in your "Installed" list.
    *   Most plugins have their own Python or R code environments with specific library dependencies. You must build this environment.
    *   Click the **Build** button next to the plugin's code environment. This may take a few minutes.
5.  **Start Using the New Features:**
    *   Once the environment is built, the features from the plugin are now available to all users on the instance.
    *   For example, if you installed a plugin with a new visual recipe, users will now see that recipe in the "+ Recipe" menu in their projects.

### 4. Resources and Tools
- **Administration > Plugins:** The central UI for managing all plugins.
- **The Plugin Store:** The marketplace for discovering new functionality.

### 5. Next Steps and Progression
- **Developing Your Own Plugins:** For advanced use cases, you can develop your own custom plugins to encapsulate reusable logic specific to your company. This allows you to turn a complex Python script into a simple, reusable visual recipe for others to use.
- **Plugin Updates:** Periodically check the Plugin store for updates to the plugins you have installed to get the latest features and bug fixes.

### 6. Common Challenges and Solutions
- **Challenge:** "I installed a plugin, but I can't find its features."
- **Solution:** The most common reason is that you forgot to **build the plugin's code environment**. Go back to the Plugins page and ensure the environment is successfully built. In some rare cases, you may need to restart the Dataiku backend for the new components to be fully registered.
- **Challenge:** "The plugin environment fails to build."
- **Solution:** This can happen if the plugin has complex dependencies or conflicts with other libraries. Check the build log for detailed error messages. You may need to contact the plugin's developer (often via a GitHub repository linked from the plugin page) or Dataiku support for help.
`},{id:352,slug:"using-a-plugin-to-wrap-code",question:"How to get started with using a plugin to wrap your own code visually?",answer:`
### 1. Introduction/Overview
Developing a custom plugin is an advanced technique that allows you to make your own code accessible as a visual, reusable component in the Dataiku UI. This is the ultimate way to enable self-service and scale your team's best practices, turning a complex Python script into a simple visual recipe that anyone can use.

### 2. Prerequisites
- **A piece of reusable Python or R code** that you want to share.
- **Administrator access to a development Dataiku instance,** including filesystem access.
- **Familiarity with JSON.**

### 3. Step-by-Step Instructions (High-Level)
Developing a plugin is a detailed process, but here is the conceptual workflow for creating a custom visual recipe.

1.  **Enable Developer Mode:** On your dev instance, an admin must enable "dev mode" to allow you to create plugin files.
2.  **Create the Plugin Folder:** On the server's filesystem, in the Dataiku installation directory, create a new folder for your plugin (e.g., \`my-company-plugin\`).
3.  **Define the UI in \`recipe.json\`:**
    *   Inside your plugin, create a folder for your new recipe. In it, create a file named \`recipe.json\`.
    *   This JSON file defines what the user will see in the UI: the recipe's name, its icon, how many inputs and outputs it has, and any custom fields (like text boxes or dropdowns) you want to present to the user.
4.  **Write the Backend in \`recipe.py\`:**
    *   Create a corresponding \`recipe.py\` file.
    *   This script contains the Python class that defines the recipe's execution logic.
    *   It will have methods to read from the inputs, get the values of the custom parameters the user entered in the UI, perform your core logic, and write to the outputs.
5.  **Test and Iterate:** As you save changes to these files, you can refresh the Dataiku UI to see your visual recipe appear. You can test it on sample data and debug your Python code just like a regular recipe.
6.  **Package the Plugin:** Once development is complete, zip the entire plugin folder. This \`.zip\` file is your distributable plugin, ready to be installed on a production instance by an administrator.

### 4. Resources and Tools
- **The Dataiku Developer Guide:** The official documentation provides a detailed, step-by-step tutorial on creating your first plugin. This is an essential resource.
- **Source Code of Existing Plugins:** Many official plugins are open-source on GitHub. Reading their code is an excellent way to learn advanced techniques.

### 5. Next Steps and Progression
- **Custom Processors:** You can also create custom processors that will appear in the Prepare recipe, allowing users to apply your custom logic as a single step in a visual data cleaning flow.
- **Custom Charts:** Develop new, custom chart types for Dataiku's visualization engine.

### 6. Common Challenges and Solutions
- **Challenge:** "This is too complex."
- **Solution:** It is an advanced topic. Start with the "hello world" plugin tutorial in the developer guide. This will walk you through creating the simplest possible recipe, which teaches you the fundamental file structure and concepts.
- **Challenge:** "My recipe UI doesn't look right or is giving an error."
- **Solution:** The \`recipe.json\` file is very sensitive to syntax errors. Use an online JSON validator to check your file. Carefully read the developer guide for the correct syntax for defining different UI components.
`},{id:353,slug:"generating-visual-recipes-via-ai-sql-assistant",question:"How to get started with generating visual recipes via AI SQL assistant?",answer:`
### 1. Introduction/Overview
The AI-assisted features in Dataiku can significantly accelerate development by generating code or recipe steps from natural language prompts. This guide focuses on using the AI Assistant to generate SQL queries, which you can then use or convert into visual recipes.

### 2. Prerequisites
- **A Dataiku instance with AI features enabled and configured** by an administrator.
- **SQL datasets** in your Flow.
- **A clear question** you want to ask of your data in plain English.

### 3. Step-by-Step Instructions
1.  **Open a SQL Recipe:** The primary interface for the SQL assistant is within a SQL recipe. Create a new **SQL recipe** with your tables as input.
2.  **Launch the AI Assistant:** Look for a button or icon to "Generate with AI" or similar.
3.  **Write a Natural Language Prompt:**
    *   In the prompt box, write a clear, specific instruction.
    *   **Bad prompt:** "join data"
    *   **Good prompt:** "Write a SQL query that does a left join from the 'customers' table to the 'orders' table on the 'customer_id' column, and then counts the number of orders for each customer."
4.  **Review and Use the Generated SQL:**
    *   The AI will generate a SQL query based on your prompt.
    *   **This code is a suggestion, not a final answer.** You must read it and verify that it is correct and matches your intent.
    *   You can then use this SQL directly in your recipe.
5.  **Convert to Visual Recipes (Optional):**
    *   While there is no direct "SQL to Visual" conversion, you can now manually create the equivalent visual recipes (e.g., a Join recipe followed by a Group recipe) based on the logic provided by the generated SQL. This can be faster than figuring out the visual steps from scratch.

### 4. Resources and Tools
- **The AI Assistant feature** within the SQL recipe editor.
- **Prompt Engineering Skills:** The quality of the output depends heavily on the quality of your input prompt. Be specific.

### 5. Next Steps and Progression
- **AI-Powered Prepare Recipe:** Some versions of Dataiku have an AI assistant directly in the Prepare recipe, which can suggest entire sequences of cleaning steps based on a high-level goal.
- **Code Explanation:** Use the AI features to explain a complex piece of existing SQL or Python code in natural language.

### 6. Common Challenges and Solutions
- **Challenge:** "The generated SQL is wrong or inefficient."
- **Solution:** This is expected from time to time. AI is a powerful assistant, not an infallible expert. You must use your own knowledge to review and correct the generated code. If it's incorrect, try rephrasing your prompt to be more specific.
- **Challenge:** "The AI feature is not available for me."
- **Solution:** These features are part of Dataiku's AI offerings and may require specific licenses and administrator configuration. Contact your platform administrator to see if they can be enabled.
`},{id:354,slug:"enabling-natural-language-recipe-generation",question:"How to get started with enabling natural language recipe generation in paid edition?",answer:`
### 1. Introduction/Overview
Dataiku's Generative AI capabilities can translate natural language prompts into recipes and code, significantly speeding up development. Enabling these features is an administrative task that involves configuring a connection to a backend Large Language Model (LLM) service.

### 2. Prerequisites
- **A Dataiku instance with the appropriate license** for Generative AI features.
- **An API key for an LLM service,** such as OpenAI, Azure OpenAI, or Google's Vertex AI.
- **Dataiku Administrator rights.**

### 3. Step-by-Step Instructions (Admin Task)
1.  **Navigate to Administration Settings:** Go to **Administration > Settings**.
2.  **Find the LLM Configuration Section:** Look for a section related to "Generative AI" or "LLMs".
3.  **Create a New LLM Connection:**
    *   Click to add a new connection.
    *   Select your provider (e.g., **OpenAI**).
    *   You will need to provide your **API key** and potentially other details like the specific model to use (e.g., \`gpt-4\`).
4.  **Configure Permissions:**
    *   Once the connection is created, you need to grant permission to user groups to use this LLM connection for the AI-assisted features.
5.  **How Users Access the Feature:**
    *   Once enabled, users will see new "Generate with AI" buttons or options appear in various parts of the UI, such as the Prepare recipe, code recipes, and charts.
    *   They can then type a prompt (e.g., "remove rows where the country is USA and the price is less than 100"), and the AI will suggest the corresponding recipe steps.

### 4. Resources and Tools
- **Administration > Settings:** The location for the LLM connection configuration.
- **An LLM API Key:** The credential needed to connect to the backend AI service.

### 5. Next Steps and Progression
- **Using the Feature:** Train your users on how to write effective prompts to get the most out of the feature.
- **Monitoring Usage and Cost:** Keep an eye on the usage of the underlying LLM API, as it is often a metered service that incurs costs.

### 6. Common Challenges and Solutions
- **Challenge:** "The AI feature is not working or is greyed out."
- **Solution:** This means the administrator has not configured the LLM connection or has not granted your user group permission to use it. Contact your platform administrator.
- **Challenge:** "The generated recipe steps are not correct."
- **Solution:** The quality of the suggestions depends on the LLM and the prompt. Users must always review the AI-generated steps to ensure they are correct before applying them. It's an assistant, not a replacement for a developer.
`},{id:355,slug:"prompting-ai-to-explain-recipe-logic",question:"How to get started with prompting generative AI to explain existing recipe logic?",answer:`
### 1. Introduction/Overview
One of the powerful applications of Generative AI in Dataiku is code and recipe explanation. When you encounter a complex piece of code or a long visual recipe built by someone else, you can use the AI assistant to generate a natural language summary of what it does. This is incredibly useful for onboarding, debugging, and documentation.

### 2. Prerequisites
- **A Dataiku instance with AI features enabled** and connected to a Large Language Model (LLM).
- **A complex recipe or code snippet** you want to understand.

### 3. Step-by-Step Instructions
1.  **Navigate to the Code or Recipe:** Open the Python recipe, SQL recipe, or visual Prepare recipe that you want to have explained.
2.  **Find the "Explain" Feature:** Look for a button or menu option labeled **Explain with AI** or similar.
3.  **Generate the Explanation:**
    *   Click the button. Dataiku will send the code or the definition of the visual recipe steps to the configured LLM.
    *   The LLM will process the logic and generate a natural language summary.
    *   This explanation will appear in a side panel or dialog box.
4.  **Review the Explanation:**
    *   Read the AI-generated summary. It should describe the purpose of the code, what it takes as input, the key transformation steps it performs, and what it produces as output.
5.  **Use the Explanation:**
    *   You can use this generated text as a starting point for your own documentation. Copy the explanation and paste it into the recipe's **Description** field to help future developers understand it.

### 4. Resources and Tools
- **The "Explain with AI" feature** within the recipe editors.
- **A configured LLM connection** (an admin task).

### 5. Next Steps and Progression
- **Explaining Other Objects:** This feature is often available for more than just recipes. You can use it to explain machine learning model results, charts, and more.
- **Code Refactoring:** After getting an explanation of a complex script, you can ask the AI Assistant to refactor it to make it more efficient or readable.

### 6. Common Challenges and Solutions
- **Challenge:** "The explanation is too generic or misses some nuance."
- **Solution:** The quality of the explanation depends on the power of the underlying LLM. For very complex or domain-specific logic, the AI might miss some of the subtle business context. The explanation should be treated as a very good starting point for understanding, but it may still require a human developer to fully grasp all the details.
- **Challenge:** "Is my code being sent to an external company?"
- **Solution:** Yes. This feature works by sending your code to an external LLM provider (like OpenAI or Google). Your company's administrator should have configured this with your security and legal teams' approval. For sensitive code, ensure you are using a provider that meets your company's data privacy standards (e.g., an Azure OpenAI instance that does not store prompts).
`},{id:356,slug:"customizing-ai-prompts-to-get-accurate-sql-code",question:"How to get started with customizing AI prompts to get accurate SQL code?",answer:`
### 1. Introduction/Overview
When using Dataiku's AI Assistant to generate SQL, the quality of the generated code is directly proportional to the quality of your prompt. "Prompt engineering" is the skill of writing clear, specific, and context-rich instructions to get the most accurate and efficient results from the language model.

### 2. Prerequisites
- **A Dataiku instance with the AI Assistant enabled.**
- **A clear analytical question** you want to answer with SQL.

### 3. Step-by-Step Instructions: Principles of Good Prompting
1.  **Be Specific and Unambiguous:**
    *   **Bad Prompt:** "Join customers and orders."
    *   **Good Prompt:** "Write a SQL query that performs a **left join** from the **customers** table to the **orders** table, using the **customer_id** column in both tables as the join key."
2.  **Provide Context and Constraints:**
    *   Tell the AI about your data and any rules.
    *   **Bad Prompt:** "Find top customers."
    *   **Good Prompt:** "Find the top 10 customer names by total sales amount. The sales amount is in the 'price' column of the orders table. Only include orders from the year 2023."
3.  **Specify the Desired Output:**
    *   Tell the AI exactly which columns you want in the final output and what they should be named.
    *   **Good Prompt:** "...The final output should have two columns: 'customer_name' and 'total_sales'."
4.  **Iterate and Refine:**
    *   Don't expect the perfect query on the first try.
    *   If the AI generates something that's not quite right, modify your prompt to give it more specific guidance. For example, if it uses the wrong aggregate function, you can add: "...and calculate the **sum** of the price, not the average."

### 4. Resources and Tools
- **The AI Assistant Prompting Interface.**
- **Your database schema:** Knowing the exact table and column names is crucial for writing a good prompt.

### 5. Next Steps and Progression
- **Few-Shot Prompting:** You can provide an example in your prompt.
    > "/* I need to find active users. For example: SELECT name FROM users WHERE status = 'active' */. Now, write a similar query to find inactive users."
- **Requesting Specific Dialects:** You can ask the AI to generate code for a specific SQL dialect, e.g., "Write this query using Snowflake SQL syntax."

### 6. Common Challenges and Solutions
- **Challenge:** "The AI is 'hallucinating' and using columns that don't exist."
- **Solution:** Your prompt was likely not specific enough. You must provide the exact column names you want it to use. For example, "...join on the \`customers.id\` column and the \`orders.customer_fk\` column."
- **Challenge:** "The query works, but it's very inefficient."
- **Solution:** The AI may not always choose the most optimal query plan. You can try to guide it by adding constraints to your prompt, such as "Use a window function instead of a self-join to calculate the running total." However, for deep performance tuning, you will likely need to rely on your own SQL expertise to refine the AI-generated code.
`},{id:357,slug:"avoiding-over-dependence-on-ai-assistants",question:"How to get started with avoiding over-dependence on AI assistants when wrong?",answer:`
### 1. Introduction/Overview
AI Assistants are powerful tools for accelerating development, but they are not infallible. They can make mistakes, generate inefficient code, or misunderstand context. Avoiding over-dependence means treating the AI as a knowledgeable junior partner, not as an ultimate authority. You must always remain the expert in the loop.

### 2. Prerequisites
- **Access to AI-assisted features** in Dataiku.
- **A critical mindset.**

### 3. Step-by-Step Instructions: A Framework for Responsible AI Use
1.  **Treat AI Output as a Suggestion, Not a Command:**
    *   Never blindly copy and paste AI-generated code or accept AI-suggested recipe steps without understanding them.
    *   The AI's output is a first draft or a starting point, not a finished product.
2.  **Always Review and Verify:**
    *   **Read the code:** Go through the AI-generated code line by line. Do you understand what each line does?
    *   **Check the logic:** Does the code correctly implement your business logic?
    *   **Test the output:** Run the code or recipe on a sample of your data and check the results. Do they make sense? Are there any unexpected edge cases?
3.  **Maintain Your Own Skills:**
    *   Don't let the AI assistant cause your own skills to atrophy. Continue to learn and practice the underlying technologies (Python, SQL, Dataiku's features).
    *   When the AI generates a solution, take the time to understand *why* it works. This is a learning opportunity.
4.  **Know When Not to Use AI:**
    *   For very simple tasks that you know how to do, it's often faster to just do them yourself than to write a prompt and review the output.
    *   For highly sensitive or critical production code, the level of human review required should be extremely high.

### 4. Resources and Tools
- **Your own brain:** Critical thinking is your most important tool.
- **The Dataiku documentation and your own knowledge:** Use these to validate the AI's suggestions.

### 5. Next Steps and Progression
- **Use AI for Brainstorming:** AI can be excellent for exploring different ways to solve a problem. You can ask it, "What are three different ways to calculate a running total in SQL?" and then use your expertise to choose the best approach.
- **Use AI for Learning:** If the AI generates a function or a technique you've never seen before, use it as a prompt to go and learn more about that technique from the official documentation.

### 6. Common Challenges and Solutions
- **Challenge:** "The AI-generated code looks right, but it gives the wrong answer."
- **Solution:** This is why testing is non-negotiable. The code might be syntactically correct but logically flawed for your specific business context. This highlights the fact that the AI does not understand your business; you do.
- **Challenge:** "It's faster to just let the AI do it."
- **Solution:** It may be faster in the short term, but it's much slower in the long term when you have to debug a subtle error in production caused by un-reviewed AI code. The time spent on review is an investment in quality and stability.
`},{id:358,slug:"verifying-ai-generated-logic",question:"How to get started with verifying AI-generated logic via manual review?",answer:`
### 1. Introduction/Overview
AI-generated code and recipes are powerful accelerators, but they must be treated with caution. Verifying the logic before you use it is a critical, non-negotiable step to ensure correctness, security, and efficiency. This manual review process is your quality gate.

### 2. Prerequisites
- **AI-generated output:** A piece of code (SQL, Python) or a set of visual recipe steps suggested by Dataiku's AI Assistant.
- **Understanding of the business goal.**

### 3. Step-by-Step Instructions: A Verification Checklist
Before accepting any AI suggestion, go through this checklist:

1.  **Does it Solve the Right Problem?**
    *   Read the generated code or steps. Does the overall logic match your original intent and the business requirement? It's possible the AI misunderstood your prompt.
2.  **Is the Code Correct and Safe?**
    *   **For Code:** Go through it line by line. Are there any subtle bugs? Does it handle edge cases (like null values) correctly? Does it contain any insecure patterns (like being vulnerable to SQL injection, though this is rare)?
    *   **For Visual Recipes:** Click on each suggested step. Are the parameters correct? Is the filter condition right? Is the join type correct?
3.  **Is it Efficient?**
    *   The AI might generate code that works but is very inefficient.
    *   For SQL, does the query plan look reasonable? For Python, is it using vectorized operations where possible instead of slow loops?
4.  **Does it Adhere to Your Team's Standards?**
    *   Does the generated code follow your team's style guide and naming conventions?
    *   You will likely need to refactor the code to match your standards.
5.  **Does it Work on Real Data?**
    *   Apply the recipe or run the code on a representative sample of your data.
    *   Inspect the output carefully. Does it look correct? Calculate a few examples by hand to verify the results.

### 4. Resources and Tools
- **Your expertise:** Your own knowledge of your data and your business logic is the most important verification tool.
- **Dataiku's Preview Pane:** Instantly see the results of visual recipe steps.
- **Jupyter Notebooks:** A good place to paste and test AI-generated code snippets on sample data before putting them into a recipe.

### 5. Next Steps and Progression
- **Refine the Prompt:** If the AI's first attempt is not good, don't just fix the code. Go back and refine your prompt to be more specific. This helps the AI learn and provides better results next time.
- **Peer Review:** For critical pieces of logic, even if they were AI-generated, they should still go through your team's standard peer review process. A second set of human eyes is an invaluable safety check.

### 6. Common Challenges and Solutions
- **Challenge:** "The AI code is too complex for me to understand."
- **Solution:** This is a red flag. Do not use code that you do not understand. You can ask the AI assistant to "explain this code," which can help. If it's still too complex, it's better to discard it and write a simpler solution that you do understand and can maintain.
- **Challenge:** "Verifying the code takes almost as long as writing it myself."
- **Solution:** This can be true, especially for experienced developers. The primary benefit of AI assistance is often not in writing the final production code, but in quickly prototyping, brainstorming different approaches, and learning new techniques.
`},{id:359,slug:"integrating-plugin-recipes-into-your-flow",question:"How to get started with integrating plugin recipes into your flow for modularity?",answer:`
### 1. Introduction/Overview
Plugins extend Dataiku's capabilities with new components, the most common of which are **visual recipes**. Once a plugin is installed by an administrator, its recipes become available to all users and can be integrated into a Flow just like any standard, built-in recipe. This allows you to use powerful, pre-packaged logic for tasks like geospatial analysis or connecting to specific services.

### 2. Prerequisites
- **A plugin installed** on your Dataiku instance by an administrator.
- **An input dataset** suitable for the plugin recipe.

### 3. Step-by-Step Instructions
1.  **Find the Plugin Recipe:**
    *   In your Flow, click the **+ RECIPE** button in the top menu.
    *   The menu that appears will now contain the new recipes from your installed plugin, often grouped under their own heading. You can also use the search bar to find the recipe by name.
2.  **Select the Recipe:**
    *   Click on the plugin recipe you want to use (e.g., "Geocode" from the Geospatial plugin).
3.  **Configure the Inputs and Outputs:**
    *   Just like a standard recipe, you will be prompted to select your input dataset(s).
    *   Dataiku will propose an output dataset name. Click **CREATE RECIPE**.
4.  **Use the Custom UI:**
    *   The recipe settings page will now open. This UI is custom-defined by the plugin.
    *   Fill in the required parameters. For the Geocode recipe, for example, you would select the column containing the addresses you want to geocode.
5.  **Run and Use the Output:**
    *   Run the recipe. It will execute the plugin's underlying code and generate the output dataset.
    *   You can now chain other recipes to this output, fully integrating the plugin's functionality into your Flow.

### 4. Resources and Tools
- **The "+ RECIPE" Menu:** Where you will find all available recipes, including those from plugins.
- **Plugin Documentation:** Good plugins have documentation that explains what each recipe does and how to configure its parameters. This is often accessible from the plugin's page in the Administration section.

### 5. Next Steps and Progression
- **Chaining Plugin Recipes:** You can create powerful pipelines by chaining multiple plugin recipes together or by combining them with standard Dataiku recipes.
- **Explore the Plugin Store:** Periodically browse the plugin store in the Administration section to discover new tools that could help with your projects.

### 6. Common Challenges and Solutions
- **Challenge:** "I can't find the recipe from a plugin I just installed."
- **Solution:** After an admin installs a plugin, they must also **build the plugin's code environment**. If this step was missed, the recipes won't be available. Contact your administrator to ensure the plugin is fully installed and its environment is built.
- **Challenge:** "The plugin recipe is failing with a strange error."
- **Solution:** Since plugins are add-ons, their quality can vary. Check the job log for the error message. If you can't solve it, the best place to get help is often the plugin's dedicated support channel, which is usually a GitHub repository linked from the plugin's store page.
`},{id:360,slug:"debugging-plugin-recipe-failures",question:"How to get started with debugging plugin recipe failures and exceptions?",answer:`
### 1. Introduction/Overview
When a recipe from a plugin fails, debugging it can be slightly different from a standard recipe because the underlying code is not immediately visible. However, the troubleshooting process is similar: start with the log to identify the error and then investigate the cause.

### 2. Prerequisites
- **A failed job** that involves a plugin recipe.
- **Access to the job log.**

### 3. Step-by-Step Instructions
1.  **Read the Job Log:**
    *   Navigate to the failed job and open the log for the specific plugin recipe step.
    *   Scroll to the bottom to find the error message and the Python or R traceback. This is your most important clue.
2.  **Categorize the Error:**
    *   **Configuration Error:** The error message might indicate that you have configured the recipe incorrectly. For example, "API Key is missing" or "Input column 'address' not found."
        *   **Solution:** Go back to the recipe's settings page and carefully review all the parameters you entered.
    *   **Data Error:** The error might be caused by the data itself. For example, a geocoding recipe might fail if it encounters an address string that it cannot parse.
        *   **Solution:** Isolate the failing row(s) and try to understand why they are problematic. You may need to add a Prepare recipe to clean the data *before* it enters the plugin recipe.
    *   **Bug in the Plugin:** The error might be a genuine bug in the plugin's code. The traceback will point to a line in the plugin's source code.
        *   **Solution:** This is more difficult to solve. See the "Next Steps" below.
3.  **Check the Plugin's Documentation:**
    *   Good plugins have documentation that may include a section on common errors or troubleshooting. Find the plugin in the **Administration > Plugins** section to see if it has a link to its documentation.

### 4. Resources and Tools
- **The Job Log:** Your primary source for the error message.
- **The Plugin's Documentation:** Your guide to correct configuration.
- **The Plugin's GitHub Repository:** The place to report bugs and get support from the developer.

### 5. Next Steps and Progression
- **Viewing the Source Code:** If you suspect a bug, a Dataiku administrator can view the plugin's source code directly from the filesystem of the Dataiku server. This can help in understanding the logic and confirming the issue.
- **Reporting an Issue:** The best place to report a bug in a plugin is via the "Issues" tab on its GitHub repository (if it's open source). Provide a clear description of the problem, the full error log, and a way to reproduce the issue.

### 6. Common Challenges and Solutions
- **Challenge:** "The error message is generic and not helpful."
- **Solution:** Increase the log verbosity. In the failed job's log view, you can sometimes find an option to rerun the job with more detailed logging, which might provide more clues.
- **Challenge:** "I don't have access to the plugin's source code or documentation."
- **Solution:** Contact your Dataiku administrator. They can access the plugin's files and find more information about it from the central Plugins page.
`},{id:361,slug:"creating-charts-from-datasets",question:"How to get started with creating charts from datasets and customizing axes?",answer:`
### 1. Introduction/Overview
Charts are the primary way to visually explore your data and present findings. Dataiku has a powerful, drag-and-drop charting engine that allows you to create a wide variety of visualizations without writing any code.

### 2. Prerequisites
- **A dataset** in your project that you want to visualize.
- **A question** you want to answer with your data (e.g., "What are the sales per category?", "How have sales trended over time?").

### 3. Step-by-Step Instructions
1.  **Open the Charts Tab:** In your Flow, open the dataset you want to visualize. Click on the **Charts** tab.
2.  **Select a Chart Type:** On the left side, you'll see a list of available chart types (Bar, Line, Pie, Scatter, etc.). Click on the one that best suits your question. For "sales per category," a **Bar Chart** is a good choice.
3.  **Drag and Drop to Configure:**
    *   The chart editor has "bins" for different chart dimensions, like X-axis, Y-axis, and groups.
    *   A list of your dataset's columns is on the left.
    *   **Drag** the column you want on the X-axis (e.g., \`product_category\`) and **drop** it into the "X" bin.
    *   **Drag** the column you want on the Y-axis (e.g., \`sales\`) and **drop** it into the "Y" bin.
    *   Dataiku will automatically aggregate the Y-axis value (e.g., as a SUM). You can click on it to change the aggregation to Average, Count, etc.
4.  **Customize the Axes and Appearance:**
    *   Click on the **Format** tab in the chart editor.
    *   Here you can change chart titles, axis labels, colors, and many other visual properties.
5.  **Save the Chart:** Once you are happy with your chart, give it a name and click the **Save** button. This makes it available to be added to a dashboard.

### 4. Resources and Tools
- **The Charts Tab:** Your main workspace for creating all visualizations.
- **The Drag-and-Drop Interface:** The intuitive way to build and configure charts.
- **The Format Tab:** The panel for customizing the look and feel of your chart.

### 5. Next Steps and Progression
- **Create More Chart Types:** Experiment with other chart types. Use a **Line Chart** to show trends over time. Use a **Scatter Plot** to see the relationship between two numerical variables.
- **Add to a Dashboard:** Go to the **Dashboards** section of your project and add your saved chart as a new tile to create a report.
- **Interactive Filters:** On a dashboard, you can add filters that apply to your charts, allowing users to explore the data interactively.

### 6. Common Challenges and Solutions
- **Challenge:** "My chart is showing a single bar, but I expected multiple bars."
- **Solution:** Check the aggregation on your Y-axis. You may also need to use a "Group by" dimension. For example, to see sales by category over time, you would put "Date" on the X-axis, "Sales" on the Y-axis, and "Category" in the "Group by" (color) bin.
- **Challenge:** "My numerical column is being treated as a category."
- **Solution:** Your column's data type might be incorrect. Go back to the dataset and use a Prepare recipe to ensure the column is correctly parsed as a number. In the chart editor, you can also sometimes change how a column is treated (e.g., "treat as numerical" vs. "treat as categorical").
`},{id:362,slug:"building-dashboards",question:"How to get started with building dashboards and arranging visuals coherently?",answer:`
### 1. Introduction/Overview
A dashboard is a collection of charts, metrics, and other insights, arranged on a single screen to tell a story or monitor a process. Building a dashboard in Dataiku is a simple, drag-and-drop process of arranging pre-built "tiles" of content.

### 2. Prerequisites
- **Insights to display:** You must first create the content you want to show. This means you should have already saved some **Charts** from your datasets and computed some **Metrics** on their Status tabs.
- **A narrative or goal** for the dashboard.

### 3. Step-by-Step Instructions
1.  **Navigate to the Dashboards Page:** In your project's top menu bar, click on **Dashboards**.
2.  **Create a New Dashboard:** Click the **+ NEW DASHBOARD** button. Give it a name.
3.  **Add Your First Tile:**
    *   Your new dashboard is a blank canvas. In "Edit" mode, click **+ ADD A TILE**.
    *   A dialog will appear. Choose the type of content you want to add. For example, select **Chart**.
    *   A list of all the charts you have saved in the project will appear. Select the one you want to add.
4.  **Add More Tiles:** Continue adding tiles for all your content.
    *   **Metric tiles:** To show key performance indicators (KPIs).
    *   **Text tiles:** To add titles, section headers, and explanatory comments.
    *   **Dataset views:** To show a preview of a table.
5.  **Arrange the Tiles:**
    *   Once the tiles are on the dashboard, you can **drag and drop** them to arrange their position.
    *   You can **resize** them by dragging their bottom-right corner.
    *   A good layout tells a story. Place the most important KPIs at the top, followed by summary charts, and then more detailed views.
6.  **Save and View:** Click the **Save** button. Then switch from "Edit" mode to "View" mode to see how the final dashboard will look to your audience.

### 4. Resources and Tools
- **The Dashboards Page:** Your canvas for building reports.
- **The "Add a Tile" dialog:** Your tool for adding content.
- **Text Tiles:** An essential tool for providing context and turning a collection of charts into a coherent report.

### 5. Next Steps and Progression
- **Interactive Filters:** Add dashboard-level filters. Go to the "Filters" panel in edit mode to add a filter (e.g., on a date column or a category). This will allow viewers to interact with the dashboard and slice all the data at once.
- **Automation:** Create a **Scenario** to automatically rebuild the datasets that feed your dashboard and then refresh the dashboard's cache, ensuring the information is always up-to-date.
- **Distribution:** Use a scenario **Reporter** to email a PDF export of the dashboard to stakeholders on a schedule.

### 6. Common Challenges and Solutions
- **Challenge:** "My dashboard is slow to load."
- **Solution:** The data for the charts is cached. If the underlying datasets are very large, the initial computation can be slow. Ensure your data pipelines are optimized. You can also manually trigger a refresh of the dashboard caches from the scenario.
- **Challenge:** "My dashboard looks cluttered and confusing."
- **Solution:** You are probably trying to show too much information at once. A good dashboard should be focused on a specific purpose. Use multiple dashboards for different topics. Use whitespace and text headers to create clear visual separation between sections.
`},{id:363,slug:"refreshing-cached-charts",question:"How to get started with refreshing cached charts when datasets update?",answer:`
### 1. Introduction/Overview
For performance reasons, the data used to render charts on a Dataiku dashboard is **cached**. This means that if the underlying dataset is updated, the chart will not automatically show the new data. You must explicitly tell Dataiku to refresh this cache. This is typically done as part of an automated scenario.

### 2. Prerequisites
- **A Dataiku dashboard** with charts.
- **A scenario** that rebuilds the datasets used by those charts.

### 3. Step-by-Step Instructions
1.  **Understand the Caching Mechanism:** When you view a dashboard, Dataiku doesn't re-query the full dataset every time. It uses a pre-computed cache of the chart's data. This makes loading fast, but it can become stale.
2.  **Locate Your Automation Scenario:** Open the scenario that is responsible for updating the data for your dashboard (e.g., your daily data ingestion and processing pipeline).
3.  **Add the "Refresh Caches" Step:**
    *   In the scenario's **Steps** tab, go to the very end of the sequence.
    *   After the step that builds the final datasets, click **+ ADD STEP**.
    *   From the step library, choose **Update dashboard caches**.
4.  **Configure the Step:**
    *   In the step's settings, you can choose to refresh all dashboards in the project or select specific ones.
5.  **Run the Scenario:** Now, when your scenario runs, it will follow this sequence:
    1.  Build the fresh data.
    2.  Once the data is built, it will re-compute the cached data required for your dashboard charts.
    3.  When a user next loads the dashboard, they will see the latest data.

### 4. Resources and Tools
- **"Update dashboard caches" Scenario Step:** The specific tool for this automation task.
- **The Job Log:** You can see the "Update caches" step in your scenario log to confirm that it ran.

### 5. Next Steps and Progression
- **Manual Refresh:** If you need to refresh a dashboard immediately without running the whole scenario, you can go to the **Dashboards** page, click the "three dots" menu on your dashboard, and choose **Refresh Caches**.
- **Conditional Refresh:** You could put the cache refresh step in a separate scenario and have it triggered by the completion of your main build scenario.

### 6. Common Challenges and Solutions
- **Challenge:** "My dashboard data is still stale even after my scenario ran."
- **Solution:** The most likely reason is that the steps in your scenario are in the wrong order. You **must** build the datasets *first*, and only then run the "Update dashboard caches" step. If you refresh the caches before the data is rebuilt, you are just re-caching the old data.
- **Challenge:** "The cache refresh step is taking a long time."
- **Solution:** This can happen if your dashboard has many complex charts built on very large datasets. Ensure the underlying data pipelines are optimized. For very large dashboards, you may need to schedule the refresh during off-peak hours.
`},{id:364,slug:"controlling-dashboard-access-permissions",question:"How to get started with controlling access permissions for dashboards?",answer:`
### 1. Introduction/Overview
Controlling who can view or edit a dashboard is a key governance requirement. In Dataiku, dashboard permissions are not set on the dashboard itself, but are **inherited** from the **project** that contains the dashboard. This means you control access by managing the project's permissions.

### 2. Prerequisites
- **A Dataiku project** containing a dashboard.
- **Project administrator rights** to manage permissions.
- **User groups** defined by a Dataiku administrator.

### 3. Step-by-Step Instructions
1.  **Navigate to Project Permissions:** In your project, go to **Settings** (the gear icon) and then to the **Permissions** tab.
2.  **Add a Group:** Click **+ ADD GROUP** and select the group of users you want to give access to (e.g., \`Finance_Analysts\`).
3.  **Assign the Correct Permission Level:**
    *   **To allow users to VIEW the dashboard:** Grant the group **Reader** permissions. This will let them see the dashboard and interact with filters, but they cannot edit it or see the underlying Flow. This is the standard permission level for business consumers.
    *   **To allow users to EDIT the dashboard:** Grant the group **Contributor** or **Administrator** permissions. This will let them add, remove, and configure tiles on the dashboard. This is for developers.
4.  **Save the Changes:** The permissions are applied immediately. Users in the group you added will now be able to see the project and its dashboards in their workspace.

### 4. Resources and Tools
- **Project Settings > Permissions:** The single place to manage access control for all objects within a project, including dashboards.
- **User Groups:** The mechanism for applying permissions to roles rather than individuals.

### 5. Next Steps and Progression
- **Sharing a Direct Link:** Once a user has permission, you can send them a direct URL to the dashboard. When they open it, they will see the dashboard content without needing to navigate through the project structure.
- **Isolating Dashboards:** If you need to give a user access to *only one specific dashboard* and nothing else, you must use a different project structure. Create a separate, dedicated "dashboard-only" project. Share the final, necessary data into it with a Sync recipe, and build the dashboard there. Then, grant the user "Reader" access to only that isolated project.

### 6. Common Challenges and Solutions
- **Challenge:** "I gave a group 'Reader' access, but they say they can't see the dashboard."
- **Solution:** The user might not be logged in, or they may not be a member of the group you think they are. Another possibility is that the project is inside a "Project Folder" that the user does not have permission to view.
- **Challenge:** "A user with 'Reader' access can see the dashboard but the charts are empty."
- **Solution:** This is a more complex permissions issue. The user has permission to see the dashboard, but they may not have permission to use the underlying **Data Connection** that the datasets are built on. Connection-level permissions are managed separately by a Dataiku administrator.
`},{id:365,slug:"scheduling-pdf-or-excel-reports-from-dashboards",question:"How to get started with scheduling PDF or Excel reports from dashboards?",answer:`
### 1. Introduction/Overview
Automating the distribution of reports is a common business requirement. Dataiku allows you to schedule a recurring job that will export a dashboard as a PDF (or individual charts as images/Excel files) and automatically email it to a list of stakeholders.

### 2. Prerequisites
- **A Dataiku dashboard** that you want to export.
- **A scenario** to run the automation.
- **Mail server configured** on the Dataiku instance (admin task).

### 3. Step-by-Step Instructions
1.  **Create an Automation Scenario:** Go to **Scenarios** and create a new scenario (e.g., \`Email_Weekly_Sales_Dashboard\`).
2.  **Add a Build Step:**
    *   The first step in your scenario should be to ensure the dashboard's data is up-to-date.
    *   Add a **Build / Train** step that builds all the datasets used by the dashboard.
    *   Follow this with a **Update dashboard caches** step.
3.  **Add a Reporter Step:**
    *   Go to the **Reporters** tab of the scenario.
    *   Click **+ ADD REPORTER** and select **Mail**.
4.  **Configure the Email and Attachment:**
    *   **Run condition:** Set this to **On success** or **On completion**.
    *   **Recipients:** Enter the email distribution list.
    *   **Attachments:** This is the key part.
        *   Click **+ Add an attachment**.
        *   Choose the **Type:** **Dashboard**.
        *   Select your dashboard from the dropdown list.
        *   Choose the **Export format:** **PDF (all tabs)** is common for a full report. You can also choose to export individual charts as images.
5.  **Schedule the Scenario:**
    *   Go back to the **Settings** tab and add a **Time-based** trigger to run the scenario on your desired schedule (e.g., every Monday at 8 AM).
    *   Activate the scenario.

### 4. Resources and Tools
- **Scenarios and Reporters:** The core automation and distribution framework.
- **Dashboard Export Formats:** PDF, PNG, or exporting chart data as Excel/CSV.

### 5. Next Steps and Progression
- **Exporting Datasets:** If stakeholders want the raw data, you can have the reporter attach a dataset as a CSV file instead of a dashboard PDF.
- **Custom Filenames:** The exported file will have a default name. If you need a custom name, you can use a Python step with the API to perform the export, which gives you more control over the filename.
- **Conditional Reporting:** Use a Python step to check a condition, and only have the scenario send the report if the condition is met (e.g., "only send the report if total sales were above a certain threshold").

### 6. Common Challenges and Solutions
- **Challenge:** "The PDF export of my dashboard looks poorly formatted."
- **Solution:** The PDF rendering is sensitive to the dashboard's layout. You may need to design your dashboard with the PDF export in mind. Use standard slide sizes (like 16:9), avoid very long scrolling pages, and ensure tiles are neatly aligned within the grid.
- **Challenge:** "The attachment is too large and the email is being rejected."
- **Solution:** Dashboards with many high-resolution charts can create large PDFs. Instead of attaching the file, a better approach is to use a scenario step to export the dashboard to a shared location (like SharePoint or S3) and then have the email reporter send a *link* to the file.
`},{id:366,slug:"embedding-dashboards-in-external-tools",question:"How to get started with embedding dashboards in external tools or portals?",answer:`
### 1. Introduction/Overview
You can embed a Dataiku dashboard or a single chart into another web-based application, such as a company intranet, a Confluence page, or another BI tool. This is done using a standard web technology called an **iframe**.

### 2. Prerequisites
- **A Dataiku dashboard** or chart that you want to embed.
- **An external web page** that supports embedding HTML iframes.
- **Authentication Considerations:** The user viewing the external page must also have an active Dataiku session and permission to view the dashboard.

### 3. Step-by-Step Instructions
1.  **Navigate to your Dashboard:** Open the dashboard you want to embed.
2.  **Find the "Share" Option:** In the top right corner of the dashboard, click the **Share** button.
3.  **Get the Embed Code:**
    *   In the sharing dialog, go to the **Embed** tab.
    *   Dataiku will provide a snippet of HTML code. It will look like: \`<iframe src="https://YOUR_DSS_URL/public/embedded-dashboard/..."></iframe>\`.
    *   Click the button to **Copy** this code snippet.
4.  **Paste into the External Tool:**
    *   Go to your external application (e.g., a Confluence page editor or a SharePoint web part).
    *   Find the option to insert or embed "HTML" or "Web Content".
    *   Paste the iframe code you copied from Dataiku.
5.  **Save and View:** Save the external page. When you view it, you should now see your Dataiku dashboard rendered inside a frame within the page.

### 4. Resources and Tools
- **The Share/Embed feature:** The tool in Dataiku that generates the necessary iframe code.
- **Iframe HTML element:** The standard web technology that makes embedding possible.

### 5. Next Steps and Progression
- **Single Sign-On (SSO):** For a seamless user experience in a corporate environment, your administrator should set up SSO between Dataiku and your other company applications. This means that if a user is logged into the main company portal, they will be automatically logged into Dataiku, and the embedded dashboard will appear without requiring a separate login.
- **Passing Filters via URL:** For advanced use cases, it is possible to pass filter values from the parent page into the embedded Dataiku dashboard by modifying the \`src\` URL in the iframe. This requires custom JavaScript development.

### 6. Common Challenges and Solutions
- **Challenge:** "The embedded frame shows a Dataiku login screen instead of the dashboard."
- **Solution:** This is the most common issue. It means the user viewing the page is not logged into Dataiku or does not have permission to see that specific dashboard. Embedding does not bypass Dataiku's security. The user must have a valid session and "Reader" access to the project.
- **Challenge:** "The embedded dashboard doesn't fit well or has double scrollbars."
- **Solution:** You may need to adjust the \`width\` and \`height\` attributes of the iframe code snippet to better fit the layout of your external page.
`},{id:367,slug:"adding-kpi-cards",question:"How to get started with adding KPI cards using statistical summaries?",answer:`
### 1. Introduction/Overview
KPI (Key Performance Indicator) cards are dashboard widgets that display a single, important number in a large, prominent way. They are perfect for showing high-level summary metrics. In Dataiku, you create these by first computing a **Metric** on a dataset and then adding that metric to a dashboard.

### 2. Prerequisites
- **A dataset containing your KPI value.** This is often a dataset with just a single row and a single column containing the number you want to display (e.g., "Total Sales this Month").
- **A Dataiku dashboard.**

### 3. Step-by-Step Instructions

#### Part 1: Create the Metric
1.  **Calculate the KPI:** Use a recipe (like **Group**) to create a dataset that contains your KPI value. Let's call it \`kpi_data\`.
2.  **Navigate to the Status Tab:** Open the \`kpi_data\` dataset and go to the **Status** tab.
3.  **Add a New Metric:**
    *   Click on the **Metrics** section.
    *   Click **+ ADD METRIC**.
    *   Choose the "Column statistics" metric.
    *   Select the column that holds your KPI number. For the aggregation, choose **Value of first row**.
    *   Give the metric a clear, descriptive name (e.g., \`Metric_Total_Sales\`).
    *   Click **SAVE AND COMPUTE**.

#### Part 2: Add the Metric to the Dashboard
1.  **Open Your Dashboard:** Go to the **Dashboards** page and open your dashboard in "Edit" mode.
2.  **Add a Metric Tile:**
    *   Click **+ ADD A TILE**.
    *   Choose the **Metric** tile type.
3.  **Configure the Tile:**
    *   **Source:** Choose "Dataset Metric".
    *   **Dataset:** Select your \`kpi_data\` dataset.
    *   **Metric:** Select the \`Metric_Total_Sales\` metric you just created.
    *   You can then customize the tile's appearance, adding a label, changing the color, etc.
4.  **Arrange and Save:** Place the KPI card prominently on your dashboard (usually at the top) and save your changes.

### 4. Resources and Tools
- **The Status Tab > Metrics:** The UI for defining and computing the KPI values.
- **The Metric Tile:** The specific dashboard widget for displaying a single number.

### 5. Next Steps and Progression
- **Automation:** Create a **Scenario** that rebuilds your \`kpi_data\` dataset and then runs an "Update Metrics" step to keep the KPI value fresh.
- **Trend Indication:** In the metric tile's settings, you can compare the current value to a historical one (e.g., from a different partition) to show a "sparkline" or a red/green arrow indicating the trend.
- **Alerting on KPIs:** In a scenario, you can add a "Run checks" step that checks if your KPI metric has crossed a certain threshold and send an alert if it has.

### 6. Common Challenges and Solutions
- **Challenge:** "My KPI card is showing the wrong number or is blank."
- **Solution:** Check the metric's configuration. Did you select the correct column and the correct aggregation ("Value of first row")? Also, ensure that you have computed the metric on the dataset's Status tab.
- **Challenge:** "The number is not updating."
- **Solution:** Your automation scenario needs to have a specific **Update metrics** step after the step that rebuilds the data. Without this, the metric value will not be re-calculated.
`},{id:368,slug:"resolving-missing-chart-errors",question:"How to get started with resolving missing chart errors due to incompatible field types?",answer:`
### 1. Introduction/Overview
A common issue when creating charts is that the chart fails to render or shows an error. This is often because you are trying to use a column with an incorrect data type for a specific chart axis. For example, trying to plot a "string" on a numerical axis. Resolving this involves correcting the data type in a Prepare recipe.

### 2. Prerequisites
- **A Dataiku chart** that is showing an error or behaving unexpectedly.
- **A dataset** that feeds the chart.

### 3. Step-by-Step Instructions
1.  **Identify the Problematic Column:** Look at your chart's configuration. Which column is causing the issue?
    *   **Example 1:** You are trying to create a line chart with a "date" column on the X-axis, but the chart is treating each date as a separate category instead of a continuous timeline.
    *   **Example 2:** You are trying to create a bar chart with a "price" column on the Y-axis, but you can only "count" it, not "sum" or "average" it.
2.  **Check the Column's Data Type:**
    *   Go back to the dataset used by the chart.
    *   Look at the column header. Dataiku shows an icon indicating the type (\`A\` for string, \`#\` for number, a calendar for date).
    *   In the examples above, you would likely see that your "date" column is a string, and your "price" column is also a string.
3.  **Fix the Data Type with a Prepare Recipe:**
    *   You need to insert a **Prepare recipe** upstream of the dataset your chart is using.
    *   **For the date column:** Use the **Parse date** processor to convert the string into a proper date type.
    *   **For the price column:** Use the **Parse to decimal number** processor. You may also need to first use a "Find & Replace" processor to remove currency symbols or commas.
4.  **Rebuild and Refresh:**
    *   Run the new Prepare recipe.
    *   Go back to your chart. You may need to refresh it or rebuild it. The chart should now recognize the correct data types and render properly.

### 4. Resources and Tools
- **The Data Type Icons:** Quickly see the type of a column in the dataset view.
- **Prepare Recipe:** Your primary tool for fixing data types.
- **"Parse date" and "Parse to..." processors:** The specific tools for data type conversion.

### 5. Next Steps and Progression
- **Proactive Cleaning:** It's a best practice to always parse and set the correct data types for all your columns as one of the first steps in your data preparation flow. This prevents charting and modeling issues later on.
- **Analyze Tool:** Use the "Analyze" feature on a column to get a detailed view of its current type, format, and any invalid values.

### 6. Common Challenges and Solutions
- **Challenge:** "The 'Parse date' processor failed."
- **Solution:** The date format of your string does not match what Dataiku expects. You need to manually specify the correct format in the processor's settings.
- **Challenge:** "I fixed the type, but the chart is still wrong."
- **Solution:** You may need to explicitly tell the chart how to treat the column. In the chart editor, you can sometimes click on a column in a bin and change its interpretation (e.g., "treat as numerical"). You may also need to clear the chart's cache.
`},{id:369,slug:"exporting-visuals-to-power-bi-or-tableau",question:"How to get started with exporting visuals to Power BI or Tableau properly?",answer:`
### 1. Introduction/Overview
While you can embed Dataiku charts into external tools, the most robust and performant way to integrate with dedicated BI tools like Power BI or Tableau is to export the clean, prepared **data**, not the visual itself. You use Dataiku for the heavy data preparation and then use the BI tool's native capabilities for visualization.

### 2. Prerequisites
- **A final, prepared dataset** in Dataiku, ready for visualization.
- **A shared database** that both Dataiku and your BI tool can connect to (e.g., Snowflake, SQL Server, BigQuery).
- **A configured connection** to this database in Dataiku.

### 3. Step-by-Step Instructions
1.  **Do the ETL in Dataiku:** Use a Dataiku Flow to perform all your complex data ingestion, cleaning, joining, and aggregation. The final output should be a dataset that is perfectly shaped for your dashboard—ideally a single, aggregated summary table.
2.  **Use an Export Recipe:**
    *   In your Flow, select this final summary dataset.
    *   Choose the **Export** recipe from the right-hand panel.
3.  **Export to the Shared Database:**
    *   Configure the Export recipe to write to your shared SQL database connection.
    *   Give the destination table a clear name, like \`POWER_BI_SALES_AGGREGATED\`.
    *   Set the write mode to "Overwrite" to ensure the data is refreshed each time.
    *   Run the recipe.
4.  **Connect Your BI Tool to the Database:**
    *   Open Power BI or Tableau.
    *   Create a new data source connection, pointing to your shared SQL database.
    *   Import the \`POWER_BI_SALES_AGGREGATED\` table.
5.  **Build Your Visuals Natively:** Now, use the powerful native visualization capabilities of your BI tool to build your charts and dashboards from this clean, pre-processed data.

### 4. Resources and Tools
- **The Export Recipe:** The key Dataiku tool for this workflow.
- **A SQL Database:** The critical "bridge" between the two platforms.
- **Your BI Tool's Data Connector.**

### 5. Next Steps and Progression
- **Automate the Data Refresh:** In Dataiku, create a **Scenario** that runs your entire pipeline, ending with the Export recipe. Schedule this to run daily. This will automatically update the table that your Power BI or Tableau dashboard reads from. In your BI tool, you can also configure its data source to refresh on a schedule.
- **Tableau Hyper Export:** For Tableau users, Dataiku provides a specific plugin that allows you to export directly to Tableau's high-performance \`.hyper\` file format, which can then be published to Tableau Server.

### 6. Common Challenges and Solutions
- **Challenge:** "My BI dashboard is slow."
- **Solution:** You are trying to do too much work in the BI tool. **This is the most common anti-pattern.** Do not export raw, un-aggregated data. Perform all the heavy joins and aggregations in Dataiku, leveraging its push-down capabilities. The BI tool should receive a small, clean summary table. Its job is visualization, not complex ETL.
- **Challenge:** "The export to the database failed with a permissions error."
- **Solution:** The database user account that Dataiku is using needs \`CREATE TABLE\` and \`INSERT\` permissions in the target schema. Contact your database administrator to have these permissions granted.
`},{id:370,slug:"troubleshooting-dashboard-layout-issues",question:"How to get started with troubleshooting layout issues across screen sizes?",answer:`
### 1. Introduction/Overview
Creating a dashboard that looks good on different screen sizes and resolutions can be challenging. Dataiku dashboards use a grid-based layout system. Understanding how this grid reflows and using best practices for arrangement can help you build clean, responsive reports.

### 2. Prerequisites
- **A Dataiku dashboard** with several tiles.
- **Access to different screen sizes** for testing (or you can simply resize your browser window).

### 3. Step-by-Step Instructions
1.  **Understand the Grid System:**
    *   When you are in "Edit" mode on a dashboard, you can see the underlying grid. All tiles snap to this grid.
    *   The grid has a fixed number of columns (usually 12 or 24). When the screen gets narrower, the tiles will "reflow"—they may stack vertically instead of sitting side-by-side.
2.  **Design for a Standard Width:**
    *   Design your dashboard primarily for a standard desktop monitor width.
    *   Use the "Fit to width" option in the dashboard settings to make the content scale.
3.  **Group and Align:**
    *   Keep related charts and metrics together.
    *   Try to align the tops and bottoms of adjacent tiles to create clean horizontal lines.
    *   Use **Text** tiles as headers to create clear visual sections. This helps guide the viewer's eye.
4.  **Use Containers (Advanced):**
    *   For very complex layouts, you can use "Container" tiles. A container is a tile that can have its own internal grid, allowing you to nest layouts.
5.  **Test for Responsiveness:**
    *   After arranging your dashboard, exit "Edit" mode.
    *   Slowly make your browser window narrower and observe how the tiles reflow. Do they stack in a logical order? Or does the layout become jumbled?
    *   Go back to "Edit" mode and adjust the tile placement and size to improve the reflow behavior.

### 4. Resources and Tools
- **The Dashboard Grid:** Your guide for aligning tiles.
- **Your Browser:** The best tool for testing responsiveness by resizing the window.
- **Text Tiles:** An essential tool for creating visual structure and separation.

### 5. Next Steps and Progression
- **Design for PDF/Export:** If you know a dashboard will be regularly exported to PDF, design it with a fixed page size in mind (e.g., a 16:9 slide). Avoid very long, scrolling dashboards, as they don't export well to a paged format.
- **Create Separate Views:** For a truly optimized experience on mobile, you might consider creating a separate, simplified version of the dashboard with fewer tiles, specifically designed for a narrow screen.

### 6. Common Challenges and Solutions
- **Challenge:** "My tiles are overlapping or leaving weird gaps."
- **Solution:** This usually means the tiles are not perfectly snapped to the grid. In "Edit" mode, try nudging the tiles until they snap cleanly into the grid cells. Ensure there are no single-grid-unit gaps between them.
- **Challenge:** "When the screen gets smaller, the order of the stacked tiles is illogical."
- **Solution:** The stacking order is determined by the tile's position from top-to-bottom and left-to-right. To control the order, you may need to adjust the vertical and horizontal placement of your tiles in the main desktop layout.
`},{id:371,slug:"setting-roles-and-permissions",question:"How to get started with setting roles and permissions for team collaboration?",answer:`
### 1. Introduction/Overview
Setting up roles and permissions is fundamental to secure and governed collaboration in Dataiku. The platform uses a Role-Based Access Control (RBAC) model, where you assign permissions to **groups** of users, and those permissions dictate what they can do within a specific **project**.

### 2. Prerequisites
- **Clearly defined user roles** in your organization (e.g., Business Analyst, Data Scientist, Developer, Project Admin).
- **A Dataiku instance** where an administrator has created user groups corresponding to these roles.
- **Project administrator rights** to manage permissions for a project.

### 3. Step-by-Step Instructions
1.  **Understand the Core Concepts:**
    *   **Users** are individual people.
    *   **Groups** are collections of users (e.g., the "Finance Team" group).
    *   **Permissions** (like "Reader" or "Contributor") are assigned to a group *on a specific project*.
2.  **Navigate to Project Permissions:** In the project you want to secure, go to **Settings > Permissions**.
3.  **Add Groups to the Project:**
    *   Click **+ ADD GROUP**.
    *   Select the group you want to grant access to (e.g., \`data_scientists_team\`).
4.  **Assign the Appropriate Role (Permission Level):**
    *   For the group you just added, choose a permission level from the dropdown. The key roles are:
        *   **Reader:** Can view everything (datasets, flows, dashboards) but cannot edit or run anything. **Best for:** Business consumers.
        *   **Contributor:** Can do most things, including creating and editing recipes and datasets. **Best for:** The core development team.
        *   **Administrator:** Can do everything a contributor can, plus manage project settings and permissions. **Best for:** The project owner or tech lead.
5.  **Save and Verify:** Save your changes. The permissions are effective immediately. Ask a user from the group to check if they can now see and access the project with the correct rights.

### 4. Resources and Tools
- **Project Settings > Permissions:** The UI for managing who can do what in your project.
- **Administration > Security > Groups:** The admin-level UI for creating the user groups.

### 5. Next Steps and Progression
- **Principle of Least Privilege:** Always grant the minimum permission level required for a user to do their job. Don't make everyone an administrator.
- **Connection-Level Permissions:** Remember that access to data also depends on permissions granted to the underlying data **Connections**, which is managed by a global administrator.
- **Regular Audits:** Periodically review the permissions on your critical projects to ensure they are up-to-date and that people who have left the team have been removed.

### 6. Common Challenges and Solutions
- **Challenge:** "How do I give a user access to only one dashboard but not the rest of the project?"
- **Solution:** Dataiku's security model is primarily at the project level. The standard solution is to create a **separate project** just for that dashboard. Use a Sync recipe to share the final data to the new dashboard project, and then grant the user "Reader" access to only that new, isolated project.
- **Challenge:** "A user with 'Contributor' rights can't use a specific database."
- **Solution:** The user's group needs to be granted "Read" and/or "Write" access on the specific **Data Connection** in the global Administration settings. Project-level permissions and connection-level permissions work together.
`},{id:372,slug:"applying-read-write-datasets-access",question:"How to get started with applying read/write datasets access appropriately?",answer:`
### 1. Introduction/Overview
In Dataiku, a user's ability to read or write to a dataset is not controlled on the individual dataset itself. Instead, it is determined by two layers of permissions: their **project-level role** and the permissions on the underlying **data connection**. Understanding this two-level system is key to managing data access correctly.

### 2. Prerequisites
- **A Dataiku project** with datasets.
- **Understanding of Dataiku's permission model** (Users, Groups, Project Roles).

### 3. Step-by-Step Instructions: The Two Layers of Access

#### Layer 1: Project Role
- **What it is:** The user's permission level on the project containing the dataset.
- **How it works:**
    *   A user with a **Reader** role on a project can *view* the data in any dataset within that project but cannot run any recipe that would *write* to or modify a dataset.
    *   A user with a **Contributor** role can both read from and write to datasets within the project (by running recipes).
- **Where to configure:** **Project > Settings > Permissions**.

#### Layer 2: Connection Permissions
- **What it is:** The permissions granted to a user's group on the underlying data connection (e.g., the connection to your Snowflake or S3 data source). This is managed by a global administrator.
- **How it works:**
    *   Even if a user is a "Contributor" on a project, they cannot write data to a Snowflake dataset if their user group has not been granted "Write" access on the Snowflake connection itself.
    *   This provides a crucial, global safety gate. The connection-level permissions override the project-level ones.
- **Where to configure:** **Administration > Connections > (Your Connection) > Security**.

### 4. Resources and Tools
- **Project Permissions:** Controls what a user can do *within* a project.
- **Connection Permissions:** Controls what a user can do *on an external storage system* via Dataiku.

### 5. Next Steps and Progression
- **Scenario Execution:** When a scenario runs, it typically runs with the permissions of the user who started it or a dedicated service account. You must ensure this user has the necessary read/write permissions on both the project and the connections.
- **Auditing:** Use the project timeline and global audit logs to track who is reading from and writing to key datasets.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm a Contributor on the project, but my Export recipe to our database is failing with a permissions error."
- **Solution:** This is a classic example of the two-level system. You have permission to create and run the recipe in the project, but your user group lacks "Write" permission on the target database connection. You need to ask a Dataiku administrator to grant your group write access to that connection.
- **Challenge:** "I want to make a specific dataset read-only for everyone."
- **Solution:** The best way to do this is to place that dataset in its own dedicated, "locked" project. Grant all other developers only "Reader" access to this project. They can then import this certified, read-only dataset into their own projects to use as an input.
`},{id:373,slug:"versioning-analysis-artifacts-in-git",question:"How to get started with versioning analysis artifacts in Git?",answer:`
### 1. Introduction/Overview
Versioning your analysis artifacts—your recipes, notebooks, and pipeline definitions—is a critical practice for reproducibility and collaboration. Dataiku's native Git integration allows you to connect your entire project to a Git repository, providing a robust system for tracking changes, reviewing code, and managing different versions of your work.

### 2. Prerequisites
- **A Dataiku project.**
- **An empty Git repository** on a provider like GitHub or GitLab.
- **Git configured** on the Dataiku server.

### 3. Step-by-Step Instructions
1.  **Connect the Project to Git:**
    *   In your project, go to **Settings > Git**.
    *   Click **Convert to Git project** and provide the URL of your remote repository.
2.  **Understand What is Versioned:**
    *   When you commit, Dataiku saves the *definition* of all your project artifacts. This includes:
        *   The code in your Python and SQL recipes.
        *   The sequence of steps in your visual Prepare recipes.
        *   The code and outputs in your Jupyter notebooks.
        *   The structure of your Flow.
        *   The configuration of your charts and dashboards.
    *   **It does not version the actual data in your datasets.**
3.  **Adopt a Git Workflow:**
    *   **Branch:** Before starting new work, create a new **branch**.
    *   **Commit:** As you make changes, make small, frequent **commits** with clear messages. Use the Git page in Dataiku to stage and commit your changes.
    *   **Push:** Regularly **push** your branch to the remote repository to back it up and share it.
    *   **Pull Request:** When your feature is complete, create a **Pull Request** on your Git provider's website. This is a formal request for a teammate to review your changes before they are merged into the main branch.

### 4. Resources and Tools
- **Dataiku's Git Integration:** The UI for managing your Git workflow from within Dataiku.
- **Git Branching:** The mechanism for working on changes in isolation.
- **Pull Requests:** The framework for collaborative code review.

### 5. Next Steps and Progression
- **CI/CD Integration:** Use the Git commits and pull requests to trigger automated CI/CD pipelines for testing and deployment.
- **Visual Diffs:** When reviewing a pull request or a merge conflict, Dataiku provides a "visual diff" tool that shows you the changes to visual recipes and flows, not just code.

### 6. Common Challenges and Solutions
- **Challenge:** "I made a change to a notebook, but the output in the commit looks like a huge, unreadable JSON file."
- **Solution:** This is how notebook files (\`.ipynb\`) are structured. The diff can be hard to read on GitHub. Some browser extensions and tools are available to render notebook diffs more cleanly. The key is to make small changes and write very clear commit messages to explain what changed.
- **Challenge:** "We have a merge conflict on a visual recipe."
- **Solution:** This happens when two people edit the same recipe on different branches. Dataiku's visual merge tool will show you the two conflicting versions. You will have to manually choose which version to keep or combine the logic from both into a new version. This highlights the importance of good communication and breaking down tasks to avoid concurrent editing of the same object.
`},{id:374,slug:"documenting-recipes-with-in-flow-annotations",question:"How to get started with documenting recipes with in-flow annotations?",answer:`
### 1. Introduction/Overview
Effective documentation is key to maintainable pipelines. "In-flow annotations" refer to the practice of adding descriptions and context directly onto the objects in your Dataiku Flow. This makes your pipeline self-documenting and easy for others to understand at a glance.

### 2. Prerequisites
- **A Dataiku Flow.**
- **A commitment from the team** to document their work.

### 3. Step-by-Step Instructions

#### 1. Use Descriptions on Everything
- **This is the most important habit.** Every object in the Flow (datasets, recipes, models) has a "Description" field.
- **How:** Click on any recipe. In the summary panel on the right, there is a field for a description. Write a clear, one-sentence summary of what the recipe does.
- **Example:** For a Join recipe, a good description is: "Joins customer data with transaction data to create a unified view."
- **Benefit:** This description appears when anyone hovers their mouse over the recipe in the Flow, providing immediate context.

#### 2. Use Clear, Standardized Names
- The name of the recipe itself is a form of documentation.
- **Bad Name:** \`Prepare_1\`
- **Good Name:** \`prepare_customers_for_modeling\`
- Use a consistent naming convention that your team agrees on.

#### 3. Use Text Boxes for High-Level Comments
- You can add annotations directly to the Flow canvas.
- **How:** Right-click on an empty area of the Flow and select **Add text box**.
- **Benefit:** Use these text boxes to create headers for different sections of your Flow or to add a more detailed explanation for a complex series of recipes.

### 4. Resources and Tools
- **The Description Field:** Your primary tool for in-flow documentation.
- **Text Boxes:** For adding larger comments and section headers to the Flow canvas.
- **Flow Zones:** While not just for documentation, organizing your flow into clearly named zones is a powerful way to document the high-level architecture.

### 5. Next Steps and Progression
- **Make it a Standard:** Include "All recipes must have a description" as a mandatory item in your team's "definition of done" for any development task.
- **Review for Documentation:** During peer reviews (e.g., in a Pull Request), check not only the code but also the quality of the descriptions and annotations.

### 6. Common Challenges and Solutions
- **Challenge:** "It takes too long to document everything."
- **Solution:** It doesn't have to be a novel. A single, well-written sentence is often enough. The time it takes to write that sentence is minimal compared to the time a future developer will waste trying to understand an undocumented recipe.
- **Challenge:** "My descriptions get out of date when the recipe changes."
- **Solution:** Make updating the description part of the change process. When you modify a recipe's logic, take the extra 10 seconds to update its description to match.
`},{id:375,slug:"attaching-metadata-to-datasets-for-lineage",question:"How to get started with attaching metadata to datasets for lineage tracking?",answer:`
### 1. Introduction/Overview
Data lineage in Dataiku is tracked automatically. However, you can significantly enrich this lineage and improve governance by attaching business and operational metadata to your datasets. This provides crucial context that makes the lineage graph more meaningful.

### 2. Prerequisites
- **A dataset** in your Dataiku Flow.
- **Information about the dataset's origin, purpose, and quality.**

### 3. Step-by-Step Instructions
1.  **Open the Dataset and Go to Summary:** Click on the dataset in your Flow to open its summary panel on the right.
2.  **Add a Clear Description:**
    *   This is the most important piece of metadata. In the "Description" field, write a clear sentence explaining what the dataset represents.
    *   **Example:** "Raw customer data from the Salesforce CRM, updated daily."
    *   This description will appear in the lineage graph, providing immediate context.
3.  **Use Tags for Classification:**
    *   In the "Tags" section, add keywords to classify the dataset. This is essential for governance and search.
    *   **Example Tags:** \`Source:Salesforce\`, \`Sensitivity:PII\`, \`Status:Raw\`, \`Owner:SalesTeam\`.
4.  **Use Custom Metadata for Structured Info:**
    *   For more structured metadata, click the "+ Add metadata" button.
    *   This allows you to add key-value pairs.
    *   **Example:** Key: \`Source_Table\`, Value: \`dbo.customers\`. Key: \`Refresh_Frequency\`, Value: \`Daily\`.
5.  **Review the Enriched Lineage:** Now, when you view the lineage graph for a downstream dataset, you can click on any ancestor dataset and see all the rich metadata you've attached, giving you a complete picture of its origin and context.

### 4. Resources and Tools
- **The Dataset Summary Panel:** The UI for adding all types of metadata.
- **Tags:** For searchable, keyword-based classification.
- **Custom Metadata:** For structured key-value information.
- **The Lineage Graph:** Where your enriched metadata provides context to the technical lineage.

### 5. Next Steps and Progression
- **Establish a Metadata Standard:** Create a company-wide policy on what metadata (which tags and custom properties) is mandatory for different types of datasets.
- **Automate Metadata Tagging:** Use a Python script in a scenario to automatically scan datasets and apply tags based on rules (e.g., if a column name is 'email', add the 'PII' tag).
- **Data Catalog:** This metadata feeds directly into Dataiku's Data Catalog, making all your data assets searchable and discoverable based on their business context.

### 6. Common Challenges and Solutions
- **Challenge:** "It's too much manual work to tag everything."
- **Solution:** Focus on what's important. Start by ensuring your critical, "golden" source datasets have rich metadata. The value of this metadata will propagate downstream through the lineage.
- **Challenge:** "What's the difference between tags and custom metadata?"
- **Solution:** Use tags for simple, non-unique classifications (many datasets can have the 'PII' tag). Use custom metadata for specific, structured attributes where you have a clear key and a single value (a dataset has only one 'Refresh_Frequency').
`},{id:376,slug:"applying-project-level-tags",question:"How to get started with applying project-level tags and description consistently?",answer:`
### 1. Introduction/Overview
Just as you add metadata to datasets, you should also add metadata to your projects. Applying a consistent set of tags and a clear description at the project level is essential for managing, finding, and governing projects in a large, multi-user Dataiku instance.

### 2. Prerequisites
- **A Dataiku project.**
- **A standard taxonomy** for project tags defined by your organization.

### 3. Step-by-Step Instructions
1.  **Navigate to the Project Homepage:** Open the project you want to annotate.
2.  **Edit the Project Description:**
    *   The project description is displayed prominently at the top of the project homepage.
    *   Click on it to edit.
    *   Write a clear, one-to-two sentence summary of the project's business purpose.
    *   **Example:** "This project analyzes customer churn and contains the production model for predicting at-risk customers."
3.  **Add Project Tags:**
    *   On the right side of the project homepage, you will see a panel for "Project Tags".
    *   Click to add tags. These tags should classify the entire project.
    *   **Example Tags:** \`Domain:Marketing\`, \`Status:Production\`, \`Criticality:High\`.
4.  **Save Your Changes.**

### 4. Resources and Tools
- **The Project Homepage:** The UI for editing the project-level description and tags.
- **The Dataiku Homepage (Projects List):** This is where your project-level metadata becomes powerful. You can search and filter your list of all projects based on their tags, making it easy to find what you're looking for.

### 5. Next Steps and Progression
- **Enforce Consistency:** Make it a mandatory step in your project creation process for the project owner to add the standard set of tags and a clear description.
- **Project Governance:** A Dataiku administrator can write a script using the API that periodically scans all projects and generates a report of projects that are missing required tags or descriptions.
- **Project Folders:** Use project folders on the Dataiku homepage to group related projects (e.g., a "Finance" folder for all finance-related projects).

### 6. Common Challenges and Solutions
- **Challenge:** "We have hundreds of projects, and none of them have tags."
- **Solution:** You will need to undertake a one-time cleanup effort. This can be done programmatically. A script using the Dataiku API can loop through all projects, and based on their name or other properties, apply a default set of tags to get you started.
- **Challenge:** "What's the difference between project tags and dataset tags?"
- **Solution:** Project tags classify the entire project (e.g., its business domain). Dataset tags are more granular and classify the specific data within the project (e.g., its source or sensitivity). A project tagged as \`Domain:Finance\` might contain datasets tagged as \`Source:SAP\` and \`Source:Salesforce\`.
`},{id:377,slug:"exporting-project-documentation",question:"How to get started with exporting project documentation for reviews?",answer:`
### 1. Introduction/Overview
When you need to share your project's documentation with external stakeholders, auditors, or team members who may not have full access to Dataiku, you need a way to export it. Dataiku provides several methods for exporting different types of documentation.

### 2. Prerequisites
- **A well-documented Dataiku project.** This means you have already added descriptions to objects, created a Wiki, etc.

### 3. Step-by-Step Instructions

#### Method 1: Exporting the Wiki
- **What it is:** Exports the entire project Wiki as a set of HTML files.
- **How:**
    1.  Go to the **Wiki** in your project.
    2.  Click the "three dots" menu and select **Export**.
    3.  This will download a \`.zip\` file containing the HTML of all your Wiki pages, which can be shared and viewed in a browser.

#### Method 2: Exporting Dashboards
- **What it is:** Exports a dashboard as a PDF or image file.
- **How:**
    1.  Open the **Dashboard** you want to export.
    2.  Click the "three dots" menu and select **Export**.
    3.  You can choose to export as PDF or PNG.

#### Method 3: Exporting the Entire Project (for technical review)
- **What it is:** Creates a complete, portable bundle of your entire project.
- **How:**
    1.  From the project homepage, click the **...** menu and select **Export**.
    2.  This downloads a \`.zip\` file.
- **Why it's useful:** This bundle can be imported into another Dataiku instance, allowing another developer to see your entire project, including all recipes, the Flow, and all documentation, exactly as you built it.

#### Method 4: Automated Documentation Generation (Advanced)
- **What it is:** Writing a script to extract all metadata and create a custom document.
- **How:**
    1.  Create a **Python recipe**.
    2.  Use the Dataiku API to get handles on all your project's objects (datasets, recipes).
    3.  Loop through the objects, get their metadata (name, description, tags, schema), and write this information to a file (e.g., a Markdown or HTML file).
    4.  Save the final document to a managed folder.

### 4. Resources and Tools
- **The Export features** on the Wiki and Dashboards.
- **The Project Bundle export feature.**
- **The Dataiku Python API** for advanced, custom documentation generation.

### 5. Next Steps and Progression
- **Scenario Automation:** You can create a scenario that automatically runs your documentation-generation script and emails the result, creating a self-updating documentation process.
- **Static Site Generator:** Your script could generate a set of Markdown files that can be used as the source for a static website generator like Jekyll or Hugo, creating a professional-looking documentation website for your project.

### 6. Common Challenges and Solutions
- **Challenge:** "The PDF export of my dashboard doesn't look right."
- **Solution:** You need to design your dashboard with the export in mind. Use fixed-size layouts (like 16:9) and avoid long, scrolling pages.
- **Challenge:** "The project bundle \`.zip\` file is huge."
- **Solution:** When you export a project, you have the option to include the data from your datasets or not. For sharing documentation and logic, you should choose to export **without** the data to keep the file size small.
`},{id:378,slug:"enforcing-governance-policies-in-dataset-access",question:"How to get started with enforcing governance policies in dataset access?",answer:`
### 1. Introduction/Overview
Data governance in Dataiku is about controlling who can access what data and under what conditions. This is enforced through a multi-layered security model that combines project-level permissions, connection-level permissions, and clear metadata tagging to ensure your company's policies are followed.

### 2. Prerequisites
- **A clear data governance policy** (e.g., "Only the Finance team can see raw financial data," "PII data must be masked for general analysts").
- **A Dataiku instance** with configured user groups.
- **Administrator rights** to manage permissions.

### 3. Step-by-Step Instructions: A Governance Framework

1.  **Tag Your Data:**
    *   The foundation of governance is knowing what you have. Go through your key datasets and apply **Tags** to classify them.
    *   **Example:** Tag datasets with \`Sensitivity:PII\`, \`Sensitivity:Confidential\`, or \`Sensitivity:Public\`.
2.  **Isolate by Project:**
    *   The primary mechanism for access control is the **project**.
    *   Place highly sensitive data in its own dedicated, highly restricted project. For example, a \`RAW_FINANCIAL_DATA\` project.
    *   Only grant access (even "Reader" access) to this project to the small number of users who are authorized to see the raw data (e.g., the \`Finance_Admins\` group).
3.  **Control Connection Access (Admin Task):**
    *   An administrator must go to **Administration > Connections**.
    *   For connections to sensitive systems, they must configure the "Security" tab to only allow specific groups to read from or write to that system. This is a powerful global control.
4.  **Provide Sanitized Versions:**
    *   In a separate, less-restricted project, create a pipeline that reads from the sensitive project (this requires a service account with special permissions).
    *   This pipeline's job is to create a sanitized version of the data (e.g., with PII columns masked or removed).
    *   Grant wider access to this "clean" project, allowing general analysts to use the safe, anonymized data without ever seeing the raw sensitive information.

### 4. Resources and Tools
- **Tags:** For classifying data sensitivity.
- **Project-level Permissions:** The primary tool for restricting access.
- **Connection-level Permissions:** A global safety gate controlled by administrators.
- **Prepare Recipes:** For creating the sanitized or masked versions of datasets.

### 5. Next Steps and Progression
- **Automated Governance Checks:** Create a scenario with a Python script that uses the API to scan all projects. The script can check for policy violations (e.g., "alert if a dataset tagged as \`PII\` is in a project that is open to all users") and send a report to the governance team.
- **Formal Sign-offs:** For regulated industries, use Dataiku's features for formal project sign-offs to create an auditable trail of who approved the pipeline and its security model.

### 6. Common Challenges and Solutions
- **Challenge:** "A user can't access a dataset even though they are a Contributor on the project."
- **Solution:** This is likely a Connection permission issue. The user's group needs to be granted access to the underlying data connection in the global Administration settings.
- **Challenge:** "This project-level security model seems too coarse."
- **Solution:** It is by design. If you need to enforce different permissions on different datasets, the correct pattern is to separate them into different projects with different permission sets. This makes the security model explicit and easy to audit.
`},{id:379,slug:"auditing-recipe-changes",question:"How to get started with auditing recipe changes via project history logs?",answer:`
### 1. Introduction/Overview
Auditing is the process of reviewing a history of changes to understand who changed what, and when. This is essential for compliance, debugging, and accountability. Dataiku automatically logs all changes to recipes, and you can access this audit trail through the Project Timeline or, more powerfully, through Git integration.

### 2. Prerequisites
- **A Dataiku project** where recipes have been modified.

### 3. Step-by-Step Instructions

#### Method 1: Using the Project Timeline (Simple Audit)
1.  **Navigate to the Timeline:** In your project, go to the **...** menu in the top navigation bar and select **Timeline**.
2.  **Filter for Recipe Changes:**
    *   The Timeline shows a chronological feed of all project events.
    *   Use the filter bar at the top to narrow down the view. You can filter by the user who made the change, the type of object (e.g., "Recipe"), or a date range.
3.  **Review the Log:** The log will show entries like "User 'John Doe' modified recipe 'prepare_customers' at 2023-10-27 10:30 AM". While this tells you who and when, it doesn't show the exact change.

#### Method 2: Using Git History (Detailed Audit)
- **This is the gold standard for auditing recipe changes.**
1.  **Ensure Project is on Git:** Your project must be linked to a Git repository. All changes must be committed.
2.  **Go to Your Git Provider:** Navigate to your project's repository on GitHub, GitLab, etc.
3.  **View the Commit History:** Go to the "Commits" section. This shows a detailed list of every change.
4.  **Inspect a Commit:** Click on a specific commit to see the "diff".
    *   **For Code Recipes (Python/SQL):** The diff will show the exact lines of code that were added, removed, or changed.
    *   **For Visual Recipes:** The diff will show changes to the underlying JSON file that defines the recipe's steps. This can be technical, but it provides a complete and precise record of the change.
    *   Dataiku's own diff viewer on the Git page is optimized to show these visual recipe changes in a more human-readable format.

### 4. Resources and Tools
- **Project Timeline:** For a high-level, user-friendly audit trail.
- **Git and your Git Provider:** For a detailed, line-by-line, and immutable audit trail.
- **Pull Requests:** The review process of a pull request is itself a form of audit, creating a record of who reviewed and approved a change.

### 5. Next Steps and Progression
- **Enforce Good Commit Messages:** A Git history is only as good as its commit messages. Mandate that your team writes clear messages explaining the *why* behind each change.
- **Webhook to JIRA:** You can set up a webhook to automatically add a comment to a JIRA ticket every time a commit is made that references that ticket number, linking your development work directly to your project management audit trail.

### 6. Common Challenges and Solutions
- **Challenge:** "The Project Timeline doesn't give me enough detail."
- **Solution:** You are correct. The Timeline is for high-level tracking. For detailed, code-level auditing, you must use Git integration.
- **Challenge:** "I don't understand the JSON diff for a visual recipe."
- **Solution:** Use the diff viewer inside Dataiku's Git page. It is designed to interpret this JSON and show the changes in a more understandable way (e.g., "Step 'Filter' was modified").
`},{id:380,slug:"aligning-usage-with-compliance-standards",question:"How to get started with aligning usage with organizational compliance standards?",answer:`
### 1. Introduction/Overview
Aligning your Dataiku usage with your organization's compliance standards (whether internal policies or external regulations like GDPR) is a critical responsibility. It requires a proactive approach, using Dataiku's governance features to implement controls and document your processes to prove adherence.

### 2. Prerequisites
- **A clear understanding of the compliance standards** you must meet. This requires close collaboration with your organization's legal, security, and compliance teams.
- **A Dataiku project** that handles data covered by these standards.

### 3. Step-by-Step Instructions: A Compliance Checklist
1.  **Data Classification:**
    *   **Action:** Work with the compliance team to define a data classification policy (e.g., \`Public\`, \`Internal\`, \`Confidential\`, \`PII\`).
    *   **Implementation:** Use Dataiku **Tags** to apply these classifications to all your datasets.
2.  **Access Control:**
    *   **Action:** Apply the "Principle of Least Privilege."
    *   **Implementation:** Use **Project-level Permissions** to ensure only authorized groups can access projects containing sensitive data. Use **Connection-level Permissions** as a global safeguard.
3.  **Data Minimization and Retention:**
    *   **Action:** Only process the data you need, and delete it when it's no longer required.
    *   **Implementation:** Use **Prepare recipes** to select only necessary columns. Use automated **Scenarios** with Python steps to delete old data partitions according to your company's retention policy.
4.  **Privacy Protection:**
    *   **Action:** Anonymize or mask sensitive data where appropriate.
    *   **Implementation:** Use **Prepare recipes** with hashing functions or regex replacements to create sanitized versions of datasets for wider use.
5.  **Auditability and Provenance:**
    *   **Action:** Be able to prove where your data came from and how it was transformed.
    *   **Implementation:** Rely on Dataiku's automatic **Lineage** graphs. Use screenshots of column-level lineage as proof for auditors. Use **Git integration** to maintain an immutable audit trail of all changes to your pipeline's logic.
6.  **Documentation:**
    *   **Action:** Document your compliance controls.
    *   **Implementation:** In your project's **Wiki**, create a "Compliance" page. For each relevant clause of the standard, explain the specific technical control you have implemented in Dataiku to meet it.

### 4. Resources and Tools
- **Dataiku's Governance Features:** Tags, Permissions, Lineage, and the Wiki are your primary tools.
- **Collaboration:** Regular meetings with your compliance team are essential.

### 5. Next Steps and Progression
- **Automated Governance Checks:** Create a "governance" scenario that uses the API to scan projects for compliance violations (e.g., "alert if a dataset tagged as \`PII\` is in a public project") and sends an alert.
- **Formal Sign-offs:** For regulated industries, use Dataiku's formal sign-off features to create an auditable approval trail for your pipeline's design and deployment.

### 6. Common Challenges and Solutions
- **Challenge:** "The compliance rules are complex and hard to translate into technical controls."
- **Solution:** This requires close partnership with your compliance team. You bring the knowledge of what's technically possible in Dataiku; they bring the knowledge of the legal requirements. Work together to design a solution that is both compliant and practical.
- **Challenge:** "This seems like it will slow down our development."
- **Solution:** Building with compliance in mind from the start is much more efficient than having to fix a non-compliant pipeline after an audit. Integrating these steps into your standard development workflow makes them a normal part of building high-quality, trustworthy data products.
`},{id:381,slug:"profiling-long-running-recipes",question:"How to get started with profiling long-running recipes for performance issues?",answer:`
### 1. Introduction/Overview
When a data pipeline is slow, the first step is to profile it to find the bottleneck. Profiling means identifying which specific recipe or step is consuming the most time. Dataiku's **Job Inspector** provides a visual Gantt chart that makes it easy to spot these long-running recipes.

### 2. Prerequisites
- **A slow-running scenario** or job in Dataiku.
- **Access to the "Jobs" menu** in your project.

### 3. Step-by-Step Instructions
1.  **Find a Recent Run:** Go to the **Jobs** menu and find a recent execution of the slow pipeline. Click on it to open the Job Inspector.
2.  **Analyze the Gantt Chart:**
    *   The main view of the Job Inspector is a Gantt chart. This chart shows every recipe that was run as a horizontal bar along a timeline.
    *   **The length of the bar is proportional to the time it took that recipe to run.**
    *   Visually scan the chart. Look for the longest bar(s). This is your bottleneck. This is the recipe you need to optimize.
3.  **Investigate the Slow Recipe:**
    *   Click on the long bar in the Gantt chart to see the detailed log for that specific recipe. The log might contain warnings or other clues.
    *   Go back to the Flow and open the recipe itself.
4.  **Diagnose the Root Cause:** Ask critical questions about the recipe.
    *   **Where is it running?** Check the **Advanced** settings. If the **Execution engine** is "In-Memory" and it's processing a large dataset, you've found the problem.
    *   **What is it doing?** Is it a very complex Join? Is it a Python recipe with an inefficient loop?
5.  **Apply Optimizations:** Based on your diagnosis, apply a fix. The most common fix is to change the execution engine to **Run on database (SQL)** or **Spark** to push down the computation.

### 4. Resources and Tools
- **The Job Inspector and Gantt Chart:** Your primary tool for identifying bottlenecks visually.
- **The Recipe Log:** Provides detailed execution information for a single step.
- **The Execution Engine setting:** The most powerful lever for performance optimization.

### 5. Next Steps and Progression
- **Iterative Tuning:** Performance tuning is a cycle. After you fix the biggest bottleneck, run the job again. A new recipe might now be the slowest part. Repeat the profiling process until the overall pipeline speed meets your requirements.
- **Engine-Specific Tools:** If a pushed-down SQL or Spark recipe is still slow, you need to use the native tools for that engine (like a database's \`EXPLAIN\` plan analyzer or the Spark UI) to do a deeper performance analysis.

### 6. Common Challenges and Solutions
- **Challenge:** "The whole job is slow, not just one recipe."
- **Solution:** This often points to a fundamental architectural issue. It could be that your Dataiku server is under-resourced, or that you are reading a huge amount of data from a very slow source system (e.g., over a slow network connection).
- **Challenge:** "The Gantt chart shows a lot of time between recipes."
- **Solution:** This "gap" time can represent data movement. For example, the time it takes Dataiku to write the output of one recipe to disk before the next recipe can start reading it. Using more efficient storage formats (like Parquet) and faster storage systems can help reduce this overhead.
`},{id:382,slug:"switching-visual-to-code-recipes-for-efficiency",question:"How to get started with switching visual recipes to code recipes for efficiency?",answer:`
### 1. Introduction/Overview
A common misconception is that code recipes (Python/SQL) are always more performant than visual recipes. This is often **not true**. Visual recipes are powerful because they can be pushed down to high-performance engines like Spark or a SQL database. This guide explains the correct framework for deciding when to switch to code for efficiency.

### 2. Prerequisites
- **A slow or inefficient visual recipe.**
- **An understanding of Dataiku's execution engines.**

### 3. Step-by-Step Instructions: The Decision Framework

**Scenario 1: Your data is in a SQL Database.**
- **Problem:** Your visual Prepare recipe is slow.
- **Incorrect Solution:** Re-writing it in a Python recipe using Pandas. This will be *slower*, because you will be pulling all the data out of the database into the Dataiku server's memory.
- **Correct Solution:** Keep it as a visual recipe, but go to its **Advanced** settings and change the **Execution engine** to **Run on database (SQL)**. Dataiku will translate your visual steps into a single SQL query and run it directly on the database, which is extremely efficient.

**Scenario 2: Your data is on a distributed filesystem (S3, HDFS).**
- **Problem:** Your visual Join recipe on two very large datasets is slow.
- **Incorrect Solution:** Re-writing it in a Python recipe using Pandas. This will crash with an Out of Memory error.
- **Correct Solution:** Keep it as a visual recipe and change the **Execution engine** to **Spark**. Dataiku will run the join as a distributed Spark job.

**Scenario 3: You have logic that CANNOT be expressed visually OR pushed down.**
- **Problem:** You need to use a specific Python library or perform a complex, iterative calculation that is not available in visual processors or translatable to SQL/Spark.
- **Correct Solution:** This is the correct time to switch to a **Python recipe**. You accept that the data will be processed in-memory on the Dataiku server, because your custom logic requires it. The key is to do as much of the filtering and preparation as possible in upstream visual, pushed-down recipes, so that the Python recipe receives the smallest possible dataset.

### 4. Resources and Tools
- **The Execution Engine Setting:** Your most important performance lever.
- **Code Recipes (Python/SQL/PySpark):** The tools for custom logic that cannot be pushed down.

### 5. Next Steps and Progression
- **Hybrid Approach:** The most efficient pipelines often use a hybrid approach. They use pushed-down visual recipes for the heavy lifting (filtering, joining large tables) and then use in-memory Python recipes only for the final, specific steps that require custom code.

### 6. Common Challenges and Solutions
- **Challenge:** "I re-wrote my slow visual recipe in Python and now it's even slower."
- **Solution:** You have likely fallen into the trap described in Scenario 1. You have moved from an engine-based process to an in-memory process. You should almost always try to push down a visual recipe before you consider re-writing it in Python.
`},{id:383,slug:"enabling-spark-mode",question:"How to get started with enabling Spark mode for large dataset processing?",answer:`
### 1. Introduction/Overview
For processing datasets that are too large to fit in memory, Apache Spark is the industry-standard distributed computing engine. Dataiku allows you to seamlessly switch the execution of your recipes to a Spark cluster, enabling you to process massive amounts of data in parallel.

### 2. Prerequisites
- **Dataiku integrated with a Spark cluster:** An administrator must have configured your Dataiku instance to connect to a Spark cluster (e.g., a standalone cluster, or a managed one like AWS EMR or Dataproc).
- **Data on Spark-compatible storage:** Your input datasets must be on a distributed filesystem that Spark can access, such as HDFS, S3, GCS, or ADLS.

### 3. Step-by-Step Instructions

#### For Visual Recipes (Prepare, Join, Group, etc.)
1.  **Open the Recipe:** Select a visual recipe in your Flow that is processing a large dataset.
2.  **Navigate to Advanced Settings:** In the recipe editor, click on the **Advanced** tab in the settings panel.
3.  **Change the Execution Engine:**
    *   Find the dropdown menu labeled **Execution engine**.
    *   Change the selection from the default (likely "DSS engine" or "In-Memory") to **Spark**.
4.  **Run the Recipe:** Click **Run**. Dataiku will now translate the visual steps of your recipe into an optimized Spark application and submit it to your cluster.

#### For Code Recipes
1.  **Choose a Spark Recipe Type:**
    *   When you create a new code recipe (**+ RECIPE**), choose one of the native Spark recipe types:
        *   **PySpark:** For writing custom logic using Python with the Spark DataFrame API.
        *   **SparkR:** For using R with Spark.
        *   **SparkSQL:** For writing SQL queries to be executed by the SparkSQL engine.
2.  **Write Your Spark Code:** The editor will open with a pre-configured Spark session. You can write standard Spark API code to perform your transformations.
3.  **Run the Recipe:** Running the recipe will execute your script as a Spark application on the cluster.

### 4. Resources and Tools
- **The Execution Engine Dropdown:** The key to enabling Spark for visual recipes.
- **Spark Code Recipes:** For writing custom distributed applications.
- **The Spark UI:** The web interface for your Spark cluster, which is essential for monitoring the progress and performance of the jobs submitted by Dataiku.

### 5. Next Steps and Progression
- **Performance Tuning:** In the recipe's advanced settings, you can pass custom Spark configurations to tune the job's performance (e.g., setting the number of executors, driver memory, etc.).
- **User Defined Functions (UDFs):** In PySpark, you can define custom Python functions and use them as UDFs to apply complex logic at scale on your Spark DataFrames.

### 6. Common Challenges and Solutions
- **Challenge:** "The Spark engine option is greyed out."
- **Solution:** This means your input or output datasets for the recipe are not on a Spark-compatible storage system. For example, a dataset from an uploaded CSV file lives on the local Dataiku server filesystem and cannot be directly accessed by a Spark cluster. You must first use a **Sync recipe** to move the data to a location like S3 or HDFS.
- **Challenge:** "My Spark job is failing."
- **Solution:** Debugging Spark jobs can be complex. The first step is to go to the native Spark UI, find your application, and look at the logs for the failed executors. The root cause is often found there. Common issues include Out of Memory errors or data skew.
`},{id:384,slug:"leveraging-push-down-sql",question:"How to get started with leveraging push‑down SQL instead of in-memory compute?",answer:`
### 1. Introduction/Overview
"Push-down" is the single most important performance optimization strategy in Dataiku when working with data stored in a database. It means sending the computation *to* the data, rather than pulling the data into Dataiku's memory. This leverages the power of your database and dramatically reduces data movement and processing time.

### 2. Prerequisites
- **Your data is stored in a SQL database** that Dataiku is connected to (e.g., Snowflake, Redshift, BigQuery, PostgreSQL).
- **A visual recipe** (like Prepare or Join) that is processing this data.

### 3. Step-by-Step Instructions
1.  **Identify In-Memory Processing:**
    *   Open your visual recipe.
    *   Go to the **Advanced** settings panel.
    *   Look at the **Execution engine**. If it says "DSS engine" or "In-Memory," you are not using push-down. This means Dataiku is loading the entire input dataset into its own memory to perform the transformations. This is very inefficient for large data.
2.  **Enable Push-down Execution:**
    *   Click on the **Execution engine** dropdown.
    *   Change the selection to **Run on database (SQL)**.
3.  **What Happens Now?**
    *   When you run the recipe, Dataiku will not pull any data.
    *   Instead, it will act as a "compiler," translating the visual steps of your recipe (e.g., your filters, formulas, joins) into a single, complex SQL query.
    *   It then sends this SQL query to your database, which executes it using its own powerful, optimized engine. The result is then stored in a new table in the database.
4.  **Verify the Push-down:**
    *   Run the recipe. In the job log, you can often see the actual SQL query that Dataiku generated and executed on the database.

### 4. Resources and Tools
- **The Execution Engine Dropdown:** This is the on/off switch for push-down execution.
- **SQL Recipes:** For ultimate control, you can write the SQL query yourself in a SQL recipe. This also uses push-down execution.

### 5. Next Steps and Progression
- **Design for Push-down:** When building new flows with database sources, make it a habit to set the engine to "Run on database" for all your visual recipes from the start.
- **Check Compatibility:** Not all processors in the Prepare recipe can be translated to SQL. Dataiku will show a warning if a step is not compatible with the SQL engine. You may need to split your logic into multiple recipes if you have an incompatible step.

### 6. Common Challenges and Solutions
- **Challenge:** "The 'Run on database' option is not available."
- **Solution:** This means the recipe's inputs and outputs are not all from the same database connection. Push-down requires all the data to be in one place. You cannot, for example, push down a join between a Snowflake table and a CSV file.
- **Challenge:** "The pushed-down query is slow."
- **Solution:** The performance is now dependent on your database. You may need to work with a database administrator to ensure the tables are properly indexed and that the database's query optimizer is creating an efficient plan for the SQL generated by Dataiku.
`},{id:385,slug:"optimizing-join-recipes",question:"How to get started with optimizing join recipes to minimize data duplication?",answer:`
### 1. Introduction/Overview
The **Join** recipe is a fundamental tool, but if used incorrectly on large datasets, it can be a major source of performance bottlenecks and logical errors. Optimizing joins involves choosing the right execution engine and ensuring the join keys are clean and unique.

### 2. Prerequisites
- **Two or more datasets** to be joined.
- **A clear understanding of the join keys** and the relationship between the tables.

### 3. Step-by-Step Instructions: Optimization Checklist

#### 1. Choose the Right Execution Engine
- **This is the most important optimization.**
- Open your Join recipe and go to the **Advanced** settings.
- If your input datasets are from a **SQL database**, set the engine to **Run on database (SQL)**.
- If your input datasets are from **S3/HDFS**, set the engine to **Spark**.
- **Avoid the "In-Memory" engine** for joins on any large dataset.

#### 2. Pre-filter and Pre-aggregate Your Inputs
- A join's performance is highly dependent on the size of the input tables.
- Before the Join recipe, add **Prepare** or **Group** recipes to filter out any unnecessary rows and columns from your input datasets. Send the smallest possible datasets into the join.

#### 3. Ensure the "Right-side" Key is Unique
- In a typical left join, if the key in your "right-hand" table is not unique, you can get a "row explosion" where the output has many more rows than the left table. This is a common logical error and performance killer.
- **Solution:** Before the Join recipe, add a **Group** recipe on your right-hand dataset. Group by the join key to ensure it is unique. You will need to decide how to aggregate the other columns (e.g., take the "First" or "Max" value).

#### 4. Select Only the Columns You Need
- In the Join recipe's settings, at the bottom, there is a "Selected Columns" panel.
- Deselect any columns from the input datasets that you do not need in the final output. This reduces the amount of data that needs to be written.

### 4. Resources and Tools
- **The Execution Engine Setting:** Your primary performance tool.
- **The Group Recipe:** Essential for deduplicating join keys.
- **The Job Inspector:** To measure the performance of your join before and after optimization.

### 5. Next Steps and Progression
- **Refactor to SQL:** For complex, multi-table joins on database data, it can be more performant to refactor the logic into a single, comprehensive **SQL recipe**. This allows the database's query optimizer to create the best possible execution plan.
- **Check Database Indexes:** If a pushed-down SQL join is still slow, work with a DBA to ensure that the join key columns are indexed in the source database.

### 6. Common Challenges and Solutions
- **Challenge:** "My join created millions of extra rows."
- **Solution:** You have a non-unique key on the right side of your join. You must use a Group recipe to deduplicate it before the join.
- **Challenge:** "My join is running out of memory."
- **Solution:** You are using the in-memory engine. You must change the execution engine to push down the computation to your database or Spark cluster.
`},{id:386,slug:"caching-intermediate-datasets",question:"How to get started with caching intermediate datasets to avoid recompute?",answer:`
### 1. Introduction/Overview
In Dataiku, you don't need to implement a special caching strategy because **every dataset in the Flow is a form of cache**. When you run a recipe, its output is physically written to storage. Dataiku's "smart rebuild" system then uses this cached, materialized result to avoid recomputing unchanged upstream parts of a pipeline.

### 2. Prerequisites
- **A Dataiku Flow** with a chain of recipes.

### 3. Step-by-Step Instructions: Understanding the Built-in Caching
1.  **Datasets are Materialized Results:**
    *   Think of each blue dataset square in your Flow as a saved, intermediate result. When you run \`Recipe_A\` to produce \`Dataset_B\`, the contents of \`Dataset_B\` are saved to disk (or a database table). This is your cache.
2.  **Smart Rebuild in Action:**
    *   Imagine a flow: \`A -> B -> C\`. You run the flow to build C.
    *   Now, you change the recipe between B and C.
    *   You ask Dataiku to build C again.
    *   Dataiku sees that dataset B and its parent recipe have not changed. It will **not** rerun the recipe that creates B. It will read the cached, materialized data directly from B and then run only the changed recipe to create C. This is the "smart rebuild."
3.  **Controlling the Cache:**
    *   **To use the cache:** In your scenario's "Build" step, use the **Smart rebuild** mode. This is the default and most efficient mode.
    *   **To ignore the cache:** If you need to force a full refresh of the entire pipeline from the very beginning, change the build mode to **Forced rebuild**.
4.  **Optimizing the Cache:**
    *   You can improve the performance of reading from the cache by changing the storage format.
    *   Open a dataset, go to **Settings**, and change the **Format** from CSV to a columnar format like **Parquet**. This is much faster for downstream analytical recipes to read.

### 4. Resources and Tools
- **The Flow:** The visual representation of your cached pipeline.
- **Build Modes (Smart vs. Forced):** Your control over how the cache is used.
- **Dataset Format Settings:** Your tool for optimizing the cache's performance.

### 5. Next Steps and Progression
- **Clearing the Cache:** To manually clear the cache for a specific dataset, open it, go to the **Actions** menu, and select **Clear data**. The dataset will become empty, and the next job will have to recompute it.
- **Sync Recipe as an Explicit Cache Point:** You can use a **Sync** recipe to create an explicit, major checkpoint in your flow. This is often used to materialize the results of a complex preparation phase before moving on to modeling.

### 6. Common Challenges and Solutions
- **Challenge:** "My job is recomputing the whole flow every time."
- **Solution:** Your scenario's build step is likely set to "Forced rebuild." Change it to "Smart rebuild" to take advantage of the caching.
- **Challenge:** "My colleague changed a recipe, but my flow is still using the old results."
- **Solution:** You need to "pull" their changes from Git and then run a build. Simply pulling the changes doesn't automatically recompute the downstream datasets. You must explicitly run a job to update the cached results.
`},{id:387,slug:"partitioning-data-to-speed-up-group-by",question:"How to get started with partitioning data to speed up group-by operations?",answer:`
### 1. Introduction/Overview
Partitioning is a powerful technique for managing and processing large datasets, especially time-series data. By splitting a large table into smaller, independent chunks (partitions), you can significantly speed up aggregations because recipes only need to read and process the specific partitions they need, rather than scanning the entire table.

### 2. Prerequisites
- **A large dataset** suitable for partitioning (i.e., it has a date or discrete category column).
- **An aggregation task** (like a Group By) that is running slowly.

### 3. Step-by-Step Instructions
1.  **Partition Your Source Dataset:**
    *   Open your large input dataset.
    *   Go to **Settings > Partitioning**.
    *   Activate partitioning. The most common strategy is to partition by a **time dimension** (e.g., by "Day" or "Month") using a date column.
2.  **Propagate the Partitioning:**
    *   When you build recipes downstream from your partitioned dataset, Dataiku will automatically make the output datasets partitioned as well. Your **Group** recipe should take a partitioned dataset as input and will produce a partitioned output.
3.  **Run the Group By on a Single Partition:**
    *   In your **Group** recipe, the logic remains the same (e.g., group by \`customer_id\`, sum \`sales\`).
    *   However, when you run the job, you can now specify to build only a single partition (e.g., only yesterday's data).
    *   The Group recipe will now only read the data for that single day, perform the aggregation, and write the result to the corresponding output partition. This is dramatically faster than running the aggregation on the entire multi-year dataset.
4.  **Automate Incremental Aggregations:**
    *   Create a **Scenario**.
    *   In the "Build" step, specify to build the **LATEST** partition of your final aggregated dataset.
    *   Schedule this to run daily. Each day, it will perform the Group By operation on only the newest day's worth of data.

### 4. Resources and Tools
- **Dataset Partitioning Settings:** The UI for defining how your data is chunked.
- **Scenario Build Step:** Where you specify which partitions to process.
- **The Job Inspector:** Use this to verify that your job only read and processed the data for a single partition.

### 5. Next Steps and Progression
- **Window Functions on Partitions:** Window functions also benefit enormously from partitioning. A running total can be calculated incrementally, only processing the latest partition and carrying over the last value from the previous one.
- **Partitioning by Category:** You can also partition on a discrete string column, like \`country\`. This would physically separate your data by country, and you could then run a job to process only the data for a specific country.

### 6. Common Challenges and Solutions
- **Challenge:** "My Group By is still slow even with partitions."
- **Solution:** Ensure you are actually running the job on a single partition. If you ask a scenario to build "ALL" partitions, it will still have to process the entire dataset (though it can do it more efficiently). Also, ensure the execution engine is set to **Spark** or **Run on database** for best performance.
- **Challenge:** "How do I get the total aggregation across all time?"
- **Solution:** You have two options. You can run a job that builds "ALL" partitions of your grouped dataset, and then add another Group recipe on top of that (with no group key) to sum up the results from all partitions. Alternatively, you can write a separate, non-partitioned pipeline for calculating global aggregates.
`},{id:388,slug:"deleting-obsolete-datasets",question:"How to get started with deleting obsolete datasets to free storage space?",answer:`
### 1. Introduction/Overview
As projects evolve, they can accumulate datasets that are temporary, outdated, or no longer used. Deleting these obsolete datasets is good housekeeping. It frees up storage space, reduces clutter in your Flow, and makes the project easier to understand. However, you must do it safely to avoid breaking your pipelines.

### 2. Prerequisites
- **A Dataiku project** that may contain unused datasets.
- **Project administrator rights** to delete objects.

### 3. Step-by-Step Instructions
1.  **Identify a Candidate for Deletion:**
    *   Look for datasets that are at the "end" of a flow—that is, no recipes use them as an input.
    *   Look for old test or debug datasets that were created during development and are no longer needed.
2.  **Perform an Impact Analysis (CRITICAL STEP):**
    *   **Never delete a dataset without checking its dependencies.**
    *   In the Flow, right-click on the dataset you want to delete.
    *   Select **View downstream dependencies**.
3.  **Review the Dependencies:**
    *   A dialog will show you every object (recipe, model, dashboard, webapp) that uses this dataset.
    *   **If this list is empty**, the dataset is truly obsolete and safe to delete.
    *   **If the list is not empty**, you must not delete the dataset, as it will break these downstream processes.
4.  **Delete the Dataset:**
    *   If and only if the downstream dependency check is clear, you can proceed.
    *   Right-click on the dataset and select **Delete**.
    *   A confirmation dialog will appear. It will warn you again if there are any dependencies you missed. Confirm the deletion.
    *   This action removes the dataset from the Flow and deletes its underlying data from storage.

### 4. Resources and Tools
- **"View downstream dependencies" feature:** Your essential safety check.
- **The Delete action:** Available from the right-click context menu.

### 5. Next Steps and Progression
- **Archiving:** If you think you *might* need the data again someday, don't just delete it. First, use an **Export recipe** to save a copy of the data to a long-term archive location (like S3). Then you can safely delete the dataset from the project.
- **Automated Cleanup:** For advanced use cases, you can write a Python scenario that uses the API to periodically scan for and delete datasets that have no dependencies and haven't been rebuilt in a long time.

### 6. Common Challenges and Solutions
- **Challenge:** "I deleted a dataset by mistake. How do I get it back?"
- **Solution:** If your project is integrated with **Git**, you can revert the commit that deleted the dataset. This will restore the dataset's *definition* to the Flow, but the data itself will be gone. You will need to re-run the upstream recipe to rebuild the data. If you don't use Git, you will have to restore the project from a backup.
- **Challenge:** "The delete option is greyed out."
- **Solution:** You do not have sufficient permissions. You must be a project administrator to delete objects.
`},{id:389,slug:"comparing-runtime-metrics-across-environments",question:"How to get started with comparing runtime metrics across environments?",answer:`
### 1. Introduction/Overview
When you deploy a project from a development environment to a production environment, you should compare its performance to ensure it runs as expected. A significant slowdown in production can indicate a configuration or resource issue. This comparison involves checking the job runtimes in both environments.

### 2. Prerequisites
- **Separate Dataiku instances** for development and production.
- **The same project deployed** on both instances.
- **Access to the "Jobs" menu** on both instances.

### 3. Step-by-Step Instructions
1.  **Run the Job on the Development Instance:**
    *   On your dev instance, run the main scenario for your project.
    *   Go to the **Jobs** menu and find the run.
    *   Record the total duration of the scenario and the duration of the key, long-running recipes. This is your baseline.
2.  **Deploy to Production:**
    *   Export your project from the dev instance as a bundle.
    *   Import the bundle into the production instance, remapping the connections to use the production data sources.
3.  **Run the Job on the Production Instance:**
    *   On the prod instance, run the same main scenario.
    *   Go to the **Jobs** menu and find the run.
    *   Record the durations for the same scenario and recipes.
4.  **Compare the Metrics:**
    *   Create a simple table comparing the runtimes from dev and prod.
    *   **Expected outcome:** The production environment is usually more powerful than the development one, so you might expect the runtimes to be faster.
    *   **Potential problem:** If the production run is significantly *slower*, this is a red flag that needs investigation.

### 4. Resources and Tools
- **The Job Inspector:** Your tool for getting the runtime metrics on both instances.
- **Project Bundles:** The mechanism for moving the project between environments.

### 5. Next Steps and Progression
- **Automated Benchmarking:** You can build this comparison into your CI/CD pipeline. The deployment script could run a test scenario on the prod instance after deployment and then use the Dataiku API to fetch the job duration and compare it against a known baseline.
- **Resource Monitoring:** If you see a performance discrepancy, the next step is to use your cloud provider's monitoring tools to compare the CPU, memory, and I/O utilization of the dev and prod environments during the job run.

### 6. Common Challenges and Solutions
- **Challenge:** "The production run is much slower. Why?"
- **Solution:** There are several possible causes:
    *   **Resource Allocation:** The production Dataiku server or its database/Spark cluster may be under-provisioned compared to the dev environment.
    *   **Configuration Mismatch:** Check that the recipe execution engines are set the same way in both environments. A recipe might be correctly pushed down to the database in dev but accidentally set to run in-memory in prod.
    *   **Data Volume:** Are you running on the same amount of data? The production data might be much larger, which would naturally lead to a longer runtime.
    *   **"Noisy Neighbors":** The production server might be busy running many other jobs at the same time, leading to resource contention.
`},{id:390,slug:"tuning-resource-config-of-dss-nodes",question:"How to get started with tuning resource config of DSS nodes for heavy loads?",answer:`
### 1. Introduction/Overview
Tuning the resource configuration of Dataiku nodes is an advanced administrative task aimed at optimizing performance, stability, and cost. It involves allocating the right amount of memory and CPU to the different services that make up a Dataiku deployment, especially when handling heavy workloads.

### 2. Prerequisites
- **Administrator access** to the Dataiku instance and its underlying infrastructure (the servers or Kubernetes cluster).
- **A deep understanding of your platform's workload** (e.g., many concurrent users, large in-memory jobs, real-time scoring).
- **Infrastructure monitoring tools** (e.g., Grafana, Datadog).

### 3. Step-by-Step Instructions: Key Tuning Areas

#### 1. The Main Backend Node (Design Node)
- **What it is:** The server that runs the main Dataiku web interface and coordinates jobs.
- **What to tune:** The main parameter is the **Java Heap Size**, which controls the memory available to the main Dataiku process.
- **How (Admin Task):**
    1.  Edit the \`install.ini\` configuration file in the Dataiku installation directory.
    2.  Increase the value for \`dss.jvm.heap.max\`.
- **Caution:** Over-allocating memory here can starve other processes. It's often better to offload heavy work to other nodes rather than scaling up the backend indefinitely.

#### 2. Job Execution Nodes (Execution Recipes)
- **What it is:** The environment where your recipes actually run.
- **How to tune:** The best practice is to use **containerized execution** with Kubernetes.
    1.  In **Administration > Containerized Execution**, an admin defines different container configurations.
    2.  Each configuration can specify different **CPU and Memory requests and limits**. This allows you to create different "t-shirt sizes" for jobs (e.g., a "high-memory" profile for a specific Python recipe).
    3.  Users then select the appropriate profile for their recipe.

#### 3. API Nodes (Real-time Scoring)
- **What it is:** The nodes that serve your deployed real-time models.
- **How to tune:**
    1.  **Vertical Scaling:** Increase the CPU and memory allocated to the API node server or pod.
    2.  **Horizontal Scaling:** This is more common. Increase the **number of replicas** (pods) of the API node to handle more concurrent prediction requests. If using Kubernetes, this can be automated with a Horizontal Pod Autoscaler (HPA).

### 4. Resources and Tools
- **Dataiku's \`install.ini\` file.**
- **The Containerized Execution settings page.**
- **Kubernetes configuration files (YAML).**
- **Infrastructure monitoring tools** to see the actual resource utilization and identify bottlenecks.

### 5. Next Steps and Progression
- **Load Testing:** Before going to production with a critical API, use a load testing tool (like JMeter or Locust) to simulate high traffic and see how the system behaves. Use the results to fine-tune your resource allocation and autoscaling rules.

### 6. Common Challenges and Solutions
- **Challenge:** "My jobs are failing with 'Out of Memory' errors."
- **Solution:** If using containerized execution, you need to increase the memory limit in the container configuration for that job type. If running in-memory on the backend, you may need to increase the Java heap size, but the better solution is to offload the job to a container or a different engine.
- **Challenge:** "How do I know what values to set for CPU and memory?"
- **Solution:** There is no magic number. It requires an iterative process. Start with a reasonable baseline, monitor the actual usage of your jobs and services, and then adjust the requests and limits based on that real-world data.
`},{id:391,slug:"using-dataiku-academy",question:"How to get started with using Dataiku Academy’s Visual Recipes free videos?",answer:`
### 1. Introduction/Overview
The Dataiku Academy is the official, free online training platform for Dataiku. The "Visual Recipes" courses are specifically designed to help you master the powerful, code-free transformation tools at the heart of the platform. This guide explains how to get the most out of this resource.

### 2. Prerequisites
- **A web browser.**
- **A free Dataiku Academy account.**
- **Access to a Dataiku instance** for hands-on practice (the free online edition is perfect).

### 3. Step-by-Step Instructions
1.  **Register for the Academy:** Go to the Dataiku Academy website and sign up.
2.  **Find the "Core Designer" Learning Path:** This is the foundational learning path for all new users. The courses on visual recipes are a key part of it.
3.  **Find the "Visual Recipes" Courses:** Within the Core Designer path, or by searching the catalog, you will find specific courses on recipes. Key courses to watch include:
    *   **Prepare Recipe Basics:** Covers filtering, formulas, and data cleaning.
    *   **Joining and Stacking Data:** Covers the Join and Stack recipes.
    *   **Grouping and Aggregating Data:** Covers the Group and Pivot recipes.
4.  **Watch the Videos:** The courses are broken down into short, focused videos (usually 2-5 minutes each) that explain a single concept or feature.
5.  **Do the Hands-On Tutorials (Most Important Step):**
    *   After each video, there is a hands-on tutorial.
    *   **Download the provided starting project** and upload it to your own Dataiku instance.
    *   Follow the tutorial's step-by-step instructions to replicate what was shown in the video. This active learning is crucial for building real skills.
6.  **Take the Quizzes:** Test your understanding with the short quizzes at the end of each course.

### 4. Resources and Tools
- **Dataiku Academy:** The learning platform itself.
- **Learning Paths:** Structured curricula to guide your learning.
- **Hands-on Tutorials:** The practical exercises that turn knowledge into skill.
- **A Sandbox Dataiku Instance:** Your personal environment for doing the tutorials.

### 5. Next Steps and Progression
- **Get Certified:** Completing the Core Designer learning path prepares you for the **Core Designer Certification exam**, which is a great way to formally validate your skills.
- **Explore Advanced Recipes:** After mastering the basics, explore the Academy's content on more advanced visual recipes like Window, Fuzzy Join, and the text analysis recipes.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm just watching the videos, but I'm not retaining the information."
- **Solution:** You must do the hands-on tutorials. Passive learning by watching is not effective. You need to actively use the recipes and click the buttons yourself for the knowledge to stick.
- **Challenge:** "I'm stuck on a tutorial step."
- **Solution:** First, re-watch the preceding video carefully. It almost always explains the concept needed for the tutorial. If you're still stuck, you can ask for help on the Dataiku Community forums, mentioning the specific Academy course and tutorial you are working on.
`},{id:392,slug:"browsing-dataikus-knowledge-base",question:"How to get started with browsing Dataiku’s Knowledge Base for specific feature issues?",answer:`
### 1. Introduction/Overview
The Dataiku Knowledge Base is a part of the official documentation that contains practical, "how-to" articles, tutorials, and solutions to common problems. It's a valuable resource for troubleshooting specific issues or learning how to implement a particular feature.

### 2. Prerequisites
- **A specific question or problem** you are trying to solve in Dataiku.
- **A web browser.**

### 3. Step-by-Step Instructions
1.  **Access the Documentation:** Navigate to the official Dataiku documentation website.
2.  **Use the Search Bar:** This is the most effective way to use the Knowledge Base.
    *   Enter specific keywords related to your issue.
    *   **Bad Search:** "join"
    *   **Good Search:** "fuzzy join address data" or "error out of memory python recipe"
3.  **Filter for Knowledge Base Articles:** The search results will include content from the reference manual, developer guides, and the Knowledge Base. You can often filter the results to show only articles from the Knowledge Base.
4.  **Read the Article:** Knowledge Base articles are typically structured as a problem/solution or a step-by-step guide. They often include screenshots and code snippets.
5.  **Apply the Solution:** Use the information from the article to solve your problem in your Dataiku project.

### 4. Resources and Tools
- **The Official Dataiku Documentation Website.**
- **The Search Bar:** Your primary tool for finding relevant articles.

### 5. Next Steps and Progression
- **Browse by Topic:** If you don't have a specific problem, you can also browse the Knowledge Base by category (e.g., "Data Preparation," "Machine Learning") to discover new tips and techniques.
- **Combine with the Community Forum:** If you can't find a solution in the Knowledge Base, the next step is to search the Dataiku Community forum. If you still can't find an answer, you can post a new question, and you might even reference the Knowledge Base article you already read.

### 6. Common Challenges and Solutions
- **Challenge:** "The solution in the article is for an older version of Dataiku."
- **Solution:** The documentation is versioned. Make sure you are viewing the documentation that corresponds to the version of Dataiku you are using. There is usually a version selector at the top of the page. While the core concepts often remain the same, UI elements and specific features can change between versions.
- **Challenge:** "I can't find an article about my specific, niche problem."
- **Solution:** The Knowledge Base covers common issues. For very specific or new problems, it's possible nobody has written an article about it yet. This is the perfect time to turn to the **Dataiku Community forum** to ask your question. Your question and its eventual solution may become the basis for a future Knowledge Base article.
`},{id:393,slug:"posting-questions-in-dataiku-community-forum",question:"How to get started with posting questions and search in Dataiku Community forum?",answer:`
### 1. Introduction/Overview
The Dataiku Community forum is a public platform where you can get help from other Dataiku users, partners, and employees. Learning how to use it effectively—by searching first and then asking good questions—is a key skill for any Dataiku developer.

### 2. Prerequisites
- **A specific, well-defined question or problem.**
- **A free Dataiku Community account.**

### 3. Step-by-Step Instructions

#### 1. Search First!
- **Before you post, always search.** It is very likely someone has had the same problem before.
- Use the search bar with specific keywords from your error message or the feature you are using.
- Reading an existing answer is the fastest way to solve your problem.

#### 2. How to Ask a Good Question
If you can't find an answer, post a new question. To get a good answer, you must ask a good question.
1.  **Choose a Clear, Descriptive Title:**
    *   **Bad:** "Help with recipe"
    *   **Good:** "How can I fix a \`KeyError\` in a Python recipe when processing JSON?"
2.  **Provide Context:** In the body of your post, explain:
    *   **What are you trying to achieve?** (Your high-level goal).
    *   **What have you already tried?** (Shows you've made an effort).
3.  **Include Specific Details:**
    *   **The full error message:** Copy and paste the entire traceback from the job log.
    *   **Screenshots:** A screenshot of your recipe's configuration is incredibly helpful.
    *   **Code Snippets:** If it's a code recipe, include the relevant parts of your code. Use the code formatting tools in the forum editor.
    *   **Anonymize your data:** Make sure your screenshots and code do not contain any sensitive company or personal information.
4.  **Be Courteous:** Remember that people are helping you for free. Be polite and thank them for their time.

### 4. Resources and Tools
- **The Community Forum Website.**
- **The Search Bar:** Your most important tool.
- **Code Formatting Tools:** To make your posts readable.

### 5. Next Steps and Progression
- **Accept an Answer:** If someone provides a solution that works, click the "Accept Solution" button. This helps future users with the same problem quickly find the correct answer.
- **Give Back:** As you gain experience, start answering questions yourself. This is a great way to solidify your knowledge and build your reputation in the community.

### 6. Common Challenges and Solutions
- **Challenge:** "I posted a question but didn't get a response."
- **Solution:** Your question was probably not clear enough. Go back and edit it. Can you add more details? Can you provide a screenshot or a code sample? The more information you provide, the more likely you are to get a helpful answer.
- **Challenge:** "I'm a beginner, and I'm afraid to ask a 'stupid' question."
- **Solution:** There are no stupid questions. The community is very welcoming to new users. It's perfectly fine to state that you are a beginner, as it helps others tailor their answers to your level.
`},{id:394,slug:"testing-sample-projects",question:"How to get started with testing sample projects to understand layout and flow?",answer:`
### 1. Introduction/Overview
Dataiku provides a set of pre-built sample projects that are designed to showcase best practices and demonstrate how to solve common business problems. Exploring and deconstructing these sample projects is one of the most effective ways to learn how a professional, well-structured Dataiku project should be built.

### 2. Prerequisites
- **Access to a Dataiku instance.**

### 3. Step-by-Step Instructions
1.  **Create a Project from a Sample:**
    *   From the Dataiku homepage, click **+ NEW PROJECT**.
    *   Select the **Sample projects / Tutorials** tab.
    *   Choose a sample that interests you. The **Customer Churn Prediction** project is an excellent, comprehensive example. Click **CREATE**.
2.  **Explore the Flow:**
    *   Open the newly created project and go to the **Flow**.
    *   **Observe the organization:** Notice the use of **Flow Zones** to group the pipeline into logical stages (\`Data preparation\`, \`Feature Engineering\`, etc.). This is a key best practice.
    *   **Trace the lineage:** Follow the data from the raw input datasets on the left to the final outputs on the right.
3.  **Deconstruct the Recipes:**
    *   Open some of the recipes to see how they are configured.
    *   Look at the **Prepare** recipes to see the sequence of data cleaning steps.
    *   Look at the **Join** and **Group** recipes to understand how the data is blended and aggregated.
4.  **Run the Pipeline:**
    *   Go to the **Jobs** menu or simply build the final datasets. Watch how Dataiku executes the entire dependency graph.
    *   This helps you understand how the different parts of the flow connect and run in the correct order.
5.  **Review the Outputs:**
    *   Explore the final datasets.
    *   Go to the **Dashboards** section to see how the project's results are presented to business stakeholders.

### 4. Resources and Tools
- **The Sample Projects library** within Dataiku.
- **The Project Wiki:** The sample projects are usually well-documented, with a Wiki that explains the project's goals and methods.

### 5. Next Steps and Progression
- **Modify and Experiment:** This is your own copy of the project, so don't be afraid to change it. Try adding a new step to a recipe, changing a parameter, or building a new chart. See how your changes affect the outcome.
- **Rebuild from Scratch:** For a true test of your understanding, try to rebuild the entire sample project yourself in a new, blank project. This forces you to understand every single step.

### 6. Common Challenges and Solutions
- **Challenge:** "The sample project is huge and overwhelming."
- **Solution:** Don't try to understand it all at once. Focus on one part at a time. Start with the first Flow Zone, "Data Ingestion," and make sure you understand every dataset and recipe in that zone before moving on to the next.
- **Challenge:** "When I run a scenario, it fails."
- **Solution:** This can occasionally happen if the sample project has dependencies that are not perfectly compatible with your Dataiku instance's version. This is a good learning opportunity. Open the job log, find the error message, and try to debug it just as you would with a real project.
`},{id:395,slug:"experimenting-in-free-edition",question:"How to get started with experimenting in a trial or free edition environment?",answer:`
### 1. Introduction/Overview
A "sandbox" is a safe environment for learning and experimentation where you don't have to worry about breaking anything important. Using a free edition of Dataiku as a personal sandbox is the single best way to build hands-on skills and explore the platform's capabilities without any risk.

### 2. Prerequisites
- **Access to a free edition:** This can be **Dataiku Online** (cloud-based) or a local installation of the **Free Edition**.

### 3. Step-by-Step Instructions: A Sandbox Mindset
1.  **Embrace Freedom and Curiosity:**
    *   The goal of a sandbox is to play. Don't be afraid to click every button and try every recipe. If you break something, you can just delete the project and start over. This freedom is essential for learning.
2.  **Follow Along with Tutorials:**
    *   Use your sandbox to do the hands-on exercises from the **Dataiku Academy**. This active learning is much more effective than just watching videos.
3.  **Start a Personal Project:**
    *   This is the best way to learn. Find a public dataset that interests you (e.g., about movies, sports, or a social topic).
    *   Define a question you want to answer and try to build a project from start to finish. For example, "Can I analyze this dataset of video game sales to see which genre is most popular?".
4.  **Test the Limits:**
    *   Try to do things you haven't done before. Can you connect to a public API? Can you build a webapp? Can you use a text analysis recipe? The sandbox is the place to try.
5.  **Replicate and Debug:**
    *   If you encounter a problem in a real work project, try to create a small, simplified version of the problem in your sandbox using dummy data. This allows you to debug and test solutions in isolation before applying them to your production pipeline.

### 4. Resources and Tools
- **Dataiku Online Free Edition:** The easiest way to get a personal, cloud-hosted sandbox.
- **Public Data Sources:** Websites like Kaggle, Awesome Public Datasets on GitHub, or data.gov are great sources of data for your personal projects.

### 5. Next Steps and Progression
- **Build a Portfolio:** The projects you build in your sandbox can become a personal portfolio to showcase your skills.
- **Test New Features:** When a new version of Dataiku is released, use your sandbox to try out the new features before they are available in your company's production environment.

### 6. Common Challenges and Solutions
- **Challenge:** "The free edition has limitations."
- **Solution:** Yes, free editions have limitations on project size, data volume, and some enterprise features. However, they contain all the core functionality you need to learn 95% of the platform's capabilities. The limitations force you to learn to work with samples of data, which is a good practice anyway.
- **Challenge:** "I don't know what kind of project to work on."
- **Solution:** Pick a topic you are personally passionate about. Your natural curiosity will drive your learning more effectively than any assigned task.
`},{id:396,slug:"following-dataiku-tips-and-tricks",question:"How to get started with following top tips & tricks emails from Dataiku?",answer:`
### 1. Introduction/Overview
Staying current with a rapidly evolving platform like Dataiku can be a challenge. Following official channels for tips, tricks, and feature updates is an easy and effective way to continuously improve your skills and learn about new, more efficient ways to work.

### 2. Prerequisites
- **A desire to continuously learn.**
- **An email address and a web browser.**

### 3. Step-by-Step Instructions: Your Information Sources
1.  **Subscribe to the Dataiku Newsletter and Blog:**
    *   Go to the official Dataiku website. They will have a blog and a newsletter you can subscribe to.
    *   The blog often features articles that do a deep dive on specific features or showcase interesting use cases from which you can learn new techniques.
2.  **Read the Release Notes:**
    *   This is the most important source. With every major version update, Dataiku publishes detailed release notes.
    *   When your company updates its Dataiku instance, take 30 minutes to scan the release notes. You will discover new features, new recipe processors, and performance improvements that you can start using immediately.
3.  **Be Active on the Dataiku Community:**
    *   Make it a habit to browse the Dataiku Community forum once a week.
    *   You will see the real-world problems other users are facing and the creative solutions they come up with. This is a great source of practical tips and tricks.
4.  **Follow Dataiku on Social Media:**
    *   Follow Dataiku on platforms like LinkedIn. They often post short videos, tips, and links to useful articles.

### 4. Resources and Tools
- **The Dataiku Website:** For the blog and newsletter.
- **The Official Documentation:** For the release notes.
- **The Dataiku Community Forum.**

### 5. Next Steps and Progression
- **Try it Yourself:** When you read about a new trick or feature, don't just file it away. Go to your sandbox project and try it out immediately. The hands-on practice is what makes the knowledge stick.
- **Share What You Learn:** If you learn a cool new trick, share it with your team. You can do this in a team chat, or by doing a short demo in a team meeting.

### 6. Common Challenges and Solutions
- **Challenge:** "It's too much information to keep up with."
- **Solution:** You don't have to read every single article in detail. Develop a habit of quickly scanning the headlines and titles. If something looks particularly relevant to a problem you are currently facing, then you can do a deep dive on that specific article.
- **Challenge:** "The tip is for a newer version of Dataiku than what my company uses."
- **Solution:** This is still useful! It gives you a preview of what's coming. You can use this knowledge to build a business case for why your company should upgrade to the latest version.
`},{id:397,slug:"leveraging-mentor-or-peer-reviews",question:"How to get started with leveraging mentor or peer reviews on beginner projects?",answer:`
### 1. Introduction/Overview
One of the fastest ways to accelerate your learning is to get feedback on your work from a more experienced peer or mentor. A review is not a test; it's a collaborative process designed to catch errors, share knowledge, and ensure that best practices are being followed.

### 2. Prerequisites
- **A completed piece of work** (e.g., a Dataiku flow, a Python recipe).
- **A mentor or peer** who is willing to review your work.
- **A mindset that is open to constructive feedback.**

### 3. Step-by-Step Instructions
1.  **Prepare for the Review:**
    *   Before you ask for a review, review your own work first.
    *   Ensure it is clean, well-organized, and documented. Add descriptions to your recipes. This shows respect for your reviewer's time.
2.  **Ask for a Review:**
    *   Reach out to your mentor or a senior teammate. Be specific in your request.
    *   **Bad request:** "Can you look at my project?"
    *   **Good request:** "I've finished the first version of the customer preparation flow. Could you please take 20 minutes to review my Prepare recipe and let me know if I'm following our team's best practices?"
3.  **Use the Right Tools for the Review:**
    *   **For Visual Flows:** A live screen-sharing session is often best. You can walk your reviewer through the flow and they can provide feedback in real-time.
    *   **For Code:** The best practice is to use a **Pull Request (PR)** if your project is on Git. This allows the reviewer to see your exact changes and leave comments on specific lines of code.
4.  **Receive the Feedback Gracefully:**
    *   Remember that feedback is about the work, not about you. Its purpose is to help you improve.
    *   Listen carefully. Ask clarifying questions. Thank the reviewer for their time and insights.
5.  **Act on the Feedback:**
    *   The review is only useful if you act on the suggestions. Go back to your project and make the recommended changes. This is how you learn and grow.

### 4. Resources and Tools
- **Git and Pull Requests:** The industry standard for formal code reviews.
- **Screen-sharing tools (Zoom, Teams):** For interactive reviews of visual flows.
- **A team culture** that values collaboration and feedback.

### 5. Next Steps and Progression
- **Become a Reviewer:** As you become more experienced, you should start reviewing the work of other junior team members. Teaching is the best way to solidify your own knowledge.
- **Review Checklists:** Your team can create a simple checklist for reviews to ensure that key things (like documentation, naming conventions, and error handling) are checked every time.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm afraid my work will be heavily criticized."
- **Solution:** A good mentor provides constructive feedback, not destructive criticism. If you feel the feedback is unfair or personal, you should speak with your team lead. However, most of the time, the feedback is well-intentioned. Try to see it as a valuable learning opportunity.
- **Challenge:** "My senior teammates are too busy to review my work."
- **Solution:** This is a management and process issue. Code/flow reviews should be considered a mandatory part of the development process for any critical project. Talk to your team lead about how to formally build review time into your project planning.
`},{id:398,slug:"building-reusable-personal-template-flows",question:"How to get started with building reusable personal template flows for practice?",answer:`
### 1. Introduction/Overview
As you build more projects in Dataiku, you will notice that you often repeat the same structural patterns. Creating a personal "template" project allows you to capture these best practices. You can then duplicate this template to kickstart any new project, saving time and ensuring consistency.

### 2. Prerequisites
- **Experience with building a few Dataiku projects.**
- **A sandbox Dataiku instance.**

### 3. Step-by-Step Instructions
1.  **Identify Your Common Patterns:**
    *   Look back at the projects you've built. What are the common elements?
    *   This often includes a standard set of **Flow Zones**, a consistent **naming convention**, and a set of common utility **Python functions**.
2.  **Create a New "Template" Project:**
    *   In your sandbox, create a new, blank project. Name it something like \`TEMPLATE - Standard Analytics Project\`.
3.  **Build the Reusable Structure:**
    *   **Flow Zones:** In the Flow, create your standard zone structure (e.g., \`1_Ingestion\`, \`2_Preparation\`, \`3_Analysis\`, \`4_Outputs\`). Leave them empty.
    *   **Wiki:** In the Wiki, create placeholder pages for your standard documentation, like "Project Brief," "Data Dictionary," and "Meeting Notes."
    *   **Code Library:** If you have common Python helper functions (e.g., for cleaning specific data types), add them to the project's **Library**.
    *   **Scenarios:** You could include template scenarios for common tasks, like "Build All" or "Run Quality Checks."
4.  **Keep it Clean:** The template project should not contain any actual data or project-specific logic. It is just the empty, well-organized shell.
5.  **Use the Template:**
    *   When you need to start a new project, instead of creating a blank one, find your template project.
    *   Click the **...** menu and choose **Duplicate project**.
    *   Give the new project a name. You now have a new project that already has your best-practice structure built in.

### 4. Resources and Tools
- **The "Duplicate project" feature:** The key to using your template.
- **Flow Zones, Wiki, and Libraries:** The components that make up a good template.

### 5. Next Steps and Progression
- **Team Templates:** Share your personal template with your team. You can collaborate to create a single, shared team template that everyone uses. This is a powerful way to enforce consistency and best practices across an entire organization.
- **Specialized Templates:** You could create different templates for different types of projects, for example, a "Template - ML Project" that includes zones for feature engineering and modeling, and a "Template - Reporting Project" that is focused on data prep and dashboards.

### 6. Common Challenges and Solutions
- **Challenge:** "My template is becoming too complex and specific."
- **Solution:** A template should be generic. It should provide the structure, not the specific implementation. If you find yourself adding project-specific recipes to your template, you are probably making it too specialized.
- **Challenge:** "How do I update the template?"
- **Solution:** Your template should be a living project. As your team develops new best practices, you should update the template project to reflect them.
`},{id:399,slug:"debugging-stuck-steps-by-cleaning-recipe-caches",question:"How to get started with debugging stuck steps by cleaning recipe caches?",answer:`
### 1. Introduction/Overview
Sometimes, a job in Dataiku can get "stuck" or fail in a strange way. A potential cause can be a corrupted or inconsistent cache for an intermediate dataset. Forcing Dataiku to re-run a recipe from scratch by clearing its output cache can often resolve these issues.

### 2. Prerequisites
- **A Dataiku job** that is failing or behaving unexpectedly.
- **A hypothesis** that the issue might be with a cached intermediate result.

### 3. Step-by-Step Instructions
1.  **Identify the Suspect Dataset:**
    *   Look at your flow and the job log. Identify the output dataset of the recipe that seems to be causing the problem.
2.  **Navigate to the Dataset:** In the Flow, click on the suspect dataset to open it.
3.  **Find the "Clear data" Action:**
    *   In the top right corner of the dataset view, click on the **Actions** menu.
    *   Select the option **Clear data**.
4.  **Confirm the Action:**
    *   A dialog will appear, warning you that this will delete the data associated with this dataset.
    *   Confirm the action. The dataset icon in the Flow will now often change to indicate that it is "empty".
5.  **Rerun the Upstream Recipe:**
    *   Now, you must re-run the recipe that produces this cleared dataset.
    *   You can do this by selecting the dataset and clicking the **Build** button.
    *   Dataiku will now re-execute the parent recipe from scratch, creating a fresh version of the data.
6.  **Rerun the Downstream Failing Job:** After the upstream data has been rebuilt, try running the original job that was failing. The fresh cache may have resolved the issue.

### 4. Resources and Tools
- **The "Clear data" Action:** The primary tool for clearing a dataset's cache.
- **The "Build" Action:** The tool for re-populating the cache.

### 5. Next Steps and Progression
- **Forced Rebuild:** A more comprehensive way to ignore all caches is to run a job with the **Forced rebuild** mode. This tells Dataiku to rerun every single recipe in the pipeline from the very beginning, ignoring all intermediate cached results. This is often a good step when debugging complex dependency issues.

### 6. Common Challenges and Solutions
- **Challenge:** "When should I use this?"
- **Solution:** This is a debugging technique, not a standard operation. You should use it when you suspect that the state of an intermediate dataset is the cause of a problem. For example, if a job failed halfway through writing to a dataset, the output might be in a corrupted, partial state. Clearing it ensures you start fresh.
- **Challenge:** "I cleared the data, but the job still fails."
- **Solution:** This means the cache was not the root cause of the problem. The issue is likely a persistent bug in your recipe logic or an issue with the source data. You need to continue with standard debugging techniques, like reading the job log and inspecting the data.
`},{id:400,slug:"tracking-common-issues-to-build-troubleshooting-checklist",question:"How to get started with tracking your common issues to build your troubleshooting checklist?",answer:`
### 1. Introduction/Overview
As you gain experience with Dataiku, you will encounter and solve the same types of problems repeatedly. Building a personal troubleshooting checklist or "cookbook" is a powerful way to accelerate your own debugging process and to help onboard new team members.

### 2. Prerequisites
- **Experience with solving at least a few problems** in Dataiku.
- **A place to store your checklist** (e.g., a personal Wiki page, a text file, a tool like Notion or OneNote).

### 3. Step-by-Step Instructions
1.  **Start a New Document:** Create your checklist document. Give it a clear title like "My Dataiku Troubleshooting Guide".
2.  **Adopt a Problem/Solution Format:** For each issue you encounter and solve, add a new entry to your checklist. A good format is:
    *   **Problem/Error Message:** Briefly describe the symptom or paste the specific error message.
    *   **Common Causes:** List the one or two most likely root causes for this error.
    *   **Resolution Steps:** Write down the exact steps you took to fix it.
3.  **Start Populating Your Checklist:** Think back to the last few problems you solved.
    *   **Example Entry 1:**
        *   **Problem:** Python recipe fails with \`ModuleNotFoundError\`.
        *   **Causes:** The required package is not in the code environment, or the recipe is not using the correct environment.
        *   **Resolution:** Check the recipe's "Advanced" settings. Go to "Administration > Code Envs" and add the missing package.
    *   **Example Entry 2:**
        *   **Problem:** A join produces duplicate rows.
        *   **Causes:** The join key is not unique in the right-hand dataset.
        *   **Resolution:** Add a "Group" recipe before the join to deduplicate the right-hand table on the key.
    *   **Example Entry 3:**
        *   **Problem:** A scenario didn't run at its scheduled time.
        *   **Causes:** The scenario's master toggle or the trigger's toggle was turned off.
        *   **Resolution:** Open the scenario and check that both switches are enabled.
4.  **Make it a Habit:** Every time you solve a new or interesting problem, take five minutes to add it to your checklist while the solution is fresh in your mind.

### 4. Resources and Tools
- **A personal note-taking tool** or a page in your project's Wiki.
- **Your own experience and memory.**

### 5. Next Steps and Progression
- **Share with Your Team:** Your personal checklist can become the foundation for a team-wide troubleshooting guide. Create a shared Wiki page where everyone on the team can contribute their own solutions.
- **Categorize the Checklist:** As your list grows, organize it into categories (e.g., "Python Recipe Errors," "Scenario Failures," "Performance Issues") to make it easier to find what you're looking for.

### 6. Common Challenges and Solutions
- **Challenge:** "I forget to write things down."
- **Solution:** This is a discipline. Try to make it part of your "definition of done" for fixing a bug. The task isn't truly complete until you've documented the solution.
- **Challenge:** "This seems like extra work."
- **Solution:** It is a small investment of time that will pay huge dividends. The next time you encounter the same error six months later, you won't have to waste time re-discovering the solution. You can just look it up in your own guide.
`}]},78335:()=>{},78963:(e,t,a)=>{"use strict";a.d(t,{Wu:()=>c,ZB:()=>l,Zp:()=>r,aR:()=>s});var o=a(37413),i=a(61120),n=a(10974);let r=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("rounded-lg border bg-card text-card-foreground shadow-sm",e),...t}));r.displayName="Card";let s=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("flex flex-col space-y-1.5 p-6",e),...t}));s.displayName="CardHeader";let l=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("text-2xl font-semibold leading-none tracking-tight",e),...t}));l.displayName="CardTitle",i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("text-sm text-muted-foreground",e),...t})).displayName="CardDescription";let c=i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("p-6 pt-0",e),...t}));c.displayName="CardContent",i.forwardRef(({className:e,...t},a)=>(0,o.jsx)("div",{ref:a,className:(0,n.cn)("flex items-center p-6 pt-0",e),...t})).displayName="CardFooter"},79737:(e,t,a)=>{"use strict";a.d(t,{Toaster:()=>o});let o=(0,a(12907).registerClientReference)(function(){throw Error("Attempted to call Toaster() from the server but Toaster is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.")},"/home/user/studio/src/components/ui/toaster.tsx","Toaster")},85407:(e,t,a)=>{"use strict";a.r(t),a.d(t,{default:()=>h,metadata:()=>d});var o=a(37413);a(61135);var i=a(10974),n=a(4536),r=a.n(n),s=a(74341);function l(){return(0,o.jsx)("header",{className:"bg-card border-b sticky top-0 z-50",children:(0,o.jsx)("div",{className:"w-full max-w-6xl mx-auto px-4 sm:px-6 lg:px-8",children:(0,o.jsx)("div",{className:"flex items-center justify-between h-16",children:(0,o.jsxs)(r(),{href:"/",className:"flex items-center gap-2 text-xl font-bold group",children:[(0,o.jsx)(s.A,{className:"w-7 h-7 text-primary transition-transform group-hover:rotate-[-5deg]"}),(0,o.jsx)("span",{className:"text-foreground",children:"DSS QuickStart Guides"})]})})})})}function c(){return(0,o.jsx)("footer",{className:"bg-card border-t mt-auto",children:(0,o.jsx)("div",{className:"w-full max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-4",children:(0,o.jsxs)("p",{className:"text-center text-sm text-muted-foreground",children:["\xa9 ",new Date().getFullYear()," DSS QuickStart Guides. All Rights Reserved."]})})})}var u=a(79737);let d={title:"DSS QuickStart Guides",description:"How to get started with Dataiku DSS - Quick questions and answers."};function h({children:e}){return(0,o.jsxs)("html",{lang:"en",className:"h-full",children:[(0,o.jsxs)("head",{children:[(0,o.jsx)("link",{rel:"preconnect",href:"https://fonts.googleapis.com"}),(0,o.jsx)("link",{rel:"preconnect",href:"https://fonts.gstatic.com",crossOrigin:"anonymous"}),(0,o.jsx)("link",{href:"https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap",rel:"stylesheet"})]}),(0,o.jsxs)("body",{className:(0,i.cn)("font-body antialiased flex flex-col min-h-screen bg-background"),children:[(0,o.jsx)(l,{}),(0,o.jsx)("main",{className:"flex-1 w-full max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8",children:e}),(0,o.jsx)(c,{}),(0,o.jsx)(u.Toaster,{})]})]})}},90571:(e,t,a)=>{"use strict";a.d(t,{ab:()=>o});let o=[{id:201,slug:"inventorying-existing-alteryx-workflows-for-migration",question:"How to get started with inventorying existing Alteryx workflows for migration?",answer:`
### 1. Introduction/Overview
Inventorying your existing Alteryx workflows is the critical first step in any migration project. It provides a comprehensive catalog of what needs to be moved, allowing you to scope, plan, and prioritize effectively. This process involves creating a detailed list of all workflows and their key attributes, which can be accomplished in a few days of focused effort.

### 2. Prerequisites
- **Access to Alteryx Server/Gallery:** To get a complete list of all workflows. If no server exists, you'll need filesystem access to where workflows are stored.
- **A spreadsheet tool:** (e.g., Google Sheets, Microsoft Excel) for creating the inventory list.
- **Access to business stakeholders:** People who can help you understand the purpose and criticality of each workflow.

### 3. Step-by-Step Instructions
1.  **Extract a List of Workflows:** Use the Alteryx Server API or Gallery admin views to export a list of all workflows. If workflows are just files, use a script to list all \`.yxmd\` and \`.yxmc\` files.
2.  **Create Your Inventory Spreadsheet:** Create a new spreadsheet with columns like: \`WorkflowName\`, \`Owner\`, \`BusinessPurpose\`, \`Criticality (1-5)\`, \`FrequencyOfUse\`, \`Complexity (1-5)\`, \`InputSources\`, \`OutputTargets\`.
3.  **Populate the Spreadsheet:** Fill in the basic information from the file list.
4.  **Interview Stakeholders:** Sit down with workflow owners and business users to fill in the qualitative columns like \`BusinessPurpose\` and \`Criticality\`. This is the most important part of the process.
5.  **Initial Complexity Assessment:** Briefly review each workflow to assign a rough complexity score. Count the number of tools as a simple proxy for complexity.

### 4. Resources and Tools
- **Alteryx Server API:** For programmatic extraction of workflow lists.
- **Spreadsheet Software:** Your primary tool for documentation.
- **Dataiku's Project Planning Checklist:** Can provide a useful framework for what to look for.

### 5. Next Steps and Progression
- **Prioritization:** Use the completed inventory to prioritize workflows for migration, focusing on high-criticality, low-complexity workflows first ("quick wins").
- **Dependency Mapping:** Use the inventory to start mapping out dependencies between workflows.

### 6. Common Challenges and Solutions
- **Challenge:** "Workflow owners are unknown or have left the company."
- **Solution:** Focus on the workflow's outputs. Identify who consumes the output files or reports. That person is your de facto stakeholder.
- **Challenge:** "The purpose of a workflow is not clear."
- **Solution:** You may need to do a deeper dive, opening the workflow and tracing the logic from inputs to outputs to reverse-engineer its purpose.
`},{id:202,slug:"mapping-alteryx-macros-and-custom-tools-to-dataiku-recipes",question:"How to get started with mapping Alteryx macros and custom tools to Dataiku recipes?",answer:`
### 1. Introduction/Overview
Alteryx macros represent reusable custom logic. Mapping these to their Dataiku equivalents is key to a successful migration. This process involves analyzing the macro's function and choosing the most appropriate Dataiku component, whether it's a visual recipe, a reusable Flow Zone, or custom code.

### 2. Prerequisites
- **Your Alteryx workflow inventory,** with macros identified.
- **Understanding of the logic** within each macro.
- **Familiarity with Dataiku's visual recipes and Python capabilities.**

### 3. Step-by-Step Instructions
1.  **Categorize Your Macros:** Review each macro and categorize its function:
    *   **Simple Data Cleansing:** (e.g., a macro that standardizes addresses).
    *   **Complex Transformation:** (e.g., a macro that performs a custom allocation calculation).
    *   **Predictive:** (e.g., a macro that runs an R script to score a model).
2.  **Map to Dataiku Equivalents:**
    *   **For Simple Macros:** These can usually be rebuilt as a **Dataiku Prepare recipe**. Save this recipe in a shared "Library" project so other flows can reuse it.
    *   **For Complex Transformations:** If the logic can be built with visual tools, create a **reusable Flow Zone** containing the set of recipes. For logic requiring code, create a **Python recipe**.
    *   **For Predictive Macros:** Rebuild the model using Dataiku's **Visual ML Lab** or write the equivalent R/Python code in a code recipe.
3.  **Document the Mapping:** Add a column to your inventory spreadsheet called \`Dataiku_Equivalent\` and document the mapping for each macro.

### 4. Resources and Tools
- **Dataiku Prepare Recipe:** The workhorse for visual transformations.
- **Flow Zones:** Excellent for visually grouping a set of reusable steps.
- **Python Recipes:** For ultimate flexibility and custom logic.
- **Shared Projects:** A Dataiku project can serve as a library of reusable components.

### 5. Next Steps and Progression
- **Create a "Component Library" Project:** Build a dedicated Dataiku project to house your reusable migrated components (shared recipes, flow zones, code libraries).
- **Refactor Logic:** Use the migration as an opportunity to improve the macro's logic. Can it be simplified or made more efficient in Dataiku?

### 6. Common Challenges and Solutions
- **Challenge:** "A macro is a 'black box' and no one knows what it does."
- **Solution:** You will have to reverse-engineer it. Open the macro in Alteryx, examine its internal tools and configuration, and trace the data flow step by step to understand and document its logic before you can rebuild it.
- **Challenge:** "The macro uses a specific R package that's not in Dataiku."
- **Solution:** In Dataiku, you can create a custom **Code Environment** and add any required R or Python package to it. This environment can then be used by your code recipes.
`},{id:203,slug:"classifying-transformation-complexity-and-dependencies",question:"How to get started with classifying transformation complexity and dependencies?",answer:`
### 1. Introduction/Overview
To plan a migration effectively, you must understand the complexity and interconnectedness of your existing pipelines. This process involves assigning a complexity score to each workflow and mapping out its dependencies on other workflows and data sources.

### 2. Prerequisites
- **A complete inventory of your Alteryx workflows.**
- **Access to view the contents of each workflow.**

### 3. Step-by-Step Instructions
1.  **Define a Complexity Scoring System:** Create a simple scoring rubric. For example:
    *   **Low (1):** < 20 tools, simple filters and joins.
    *   **Medium (3):** 20-50 tools, uses macros, some complex formulas.
    *   **High (5):** > 50 tools, uses custom scripting (R/Python), predictive tools, complex iterative logic.
2.  **Score Each Workflow:** Go through your inventory and assign a complexity score to each workflow based on your rubric.
3.  **Identify Dependencies:** For each workflow, analyze its **Input Data** and **Output Data** tools.
    *   List all external data sources (databases, files) it reads from.
    *   List all external locations it writes to.
    *   **Crucially, identify if a workflow's input is the output of another workflow.** This creates a dependency between the two.
4.  **Visualize the Dependencies:** Use a simple diagramming tool (or even a spreadsheet) to create a graph showing how your workflows connect to each other. This visual map is essential for planning migration waves.

### 4. Resources and Tools
- **Alteryx Workflow Dependencies Viewer:** A built-in Alteryx feature that can help identify file-based dependencies.
- **Diagramming tools:** (e.g., Lucidchart, Miro) for visualizing the dependency graph.

### 5. Next Steps and Progression
- **Plan Migration Waves:** Use the dependency graph to group workflows into logical migration batches. You must migrate the upstream workflows before the downstream ones that depend on them.
- **Identify Quick Wins:** A workflow that is low-complexity and has few dependencies is a great candidate for an early migration to build momentum.

### 6. Common Challenges and Solutions
- **Challenge:** "We have circular dependencies (A depends on B, and B depends on A)."
- **Solution:** This is a sign of a design flaw in the original system. The migration is a perfect opportunity to fix this. You will need to refactor the logic to break the circular dependency, for example by creating a third, consolidated workflow.
- **Challenge:** "Dependencies are not explicit; they are just files dropped in a folder."
- **Solution:** This implicit dependency is risky. In Dataiku, you will replace this with an explicit dependency by having the upstream recipe write to a managed dataset, which the downstream recipe then reads from directly. This makes the lineage clear and robust.
`},{id:204,slug:"assessing-business-critical-alteryx-pipelines",question:"How to get started with assessing business-critical Alteryx pipelines?",answer:`
### 1. Introduction/Overview
Not all data pipelines are created equal. Some are essential for day-to-day business operations, while others are less critical. Assessing the business criticality of each workflow is vital for prioritizing your migration efforts and managing risk.

### 2. Prerequisites
- **Your inventory of Alteryx workflows.**
- **Access to business stakeholders and users of the workflow outputs.**

### 3. Step-by-Step Instructions
1.  **Identify the Consumers:** For each workflow, find out who uses its output. This could be an analyst, a manager, or another automated system.
2.  **Conduct Stakeholder Interviews:** Schedule brief meetings with the consumers of the workflow outputs.
3.  **Ask Impact-focused Questions:**
    *   "What do you use this report/data for?"
    *   "What business decision does this output support?"
    *   "What would be the impact on your work if this data was delayed by a day? A week?"
    *   "Is this data used for financial reporting, regulatory compliance, or customer-facing interactions?"
4.  **Assign a Criticality Score:** Based on the answers, assign a criticality score to each workflow. Use a simple scale:
    *   **1 (Low):** Internal, non-urgent analysis.
    *   **3 (Medium):** Supports regular internal business operations.
    *   **5 (High):** Mission-critical. Used for financial reporting, regulatory compliance, or directly impacts customers. A failure has immediate, significant consequences.
5.  **Document in Your Inventory:** Add the "Criticality Score" and a "Business Impact" summary to your inventory spreadsheet.

### 4. Resources and Tools
- **Interviewing Skills:** The ability to talk to business users and understand their needs is the most important tool here.
- **Your Workflow Inventory:** The central document for capturing this information.

### 5. Next Steps and Progression
- **Prioritize Migration:** Your highest-priority migration candidates are workflows with **High Criticality** and **Low/Medium Complexity**.
- **Risk Planning:** For High Criticality workflows, you will need a more detailed validation and fallback plan during the migration cutover.

### 6. Common Challenges and Solutions
- **Challenge:** "Stakeholders say everything is high priority."
- **Solution:** This is common. Use impact-quantifying questions to differentiate. Ask them to stack rank their reports. "If you could only have one of these three reports tomorrow, which one would it be?" This forces a more realistic prioritization.
- **Challenge:** "The workflow seems important, but we can't find anyone who uses the output."
- **Solution:** This is a red flag. The workflow may be obsolete. Before migrating it, confirm with management that it is still needed. The migration process is an excellent opportunity to decommission unused pipelines.
`},{id:205,slug:"documenting-source‑to‑target-field-mappings",question:"How to get started with documenting source‑to‑target field mappings?",answer:`
### 1. Introduction/Overview
A source-to-target mapping document is a detailed blueprint that traces each field from its source, through all transformations, to its final destination. Creating this document is essential for ensuring your migrated pipeline is accurate and for providing a clear audit trail for validation and governance.

### 2. Prerequisites
- **A specific Alteryx workflow to analyze.**
- **A spreadsheet tool (Excel, Google Sheets).**

### 3. Step-by-Step Instructions
1.  **Create a Mapping Template:** Create a spreadsheet with the following columns: \`Source_Field\`, \`Source_Table\`, \`Transformation_Logic\`, \`Target_Field\`, \`Target_Table\`, \`Data_Type\`, \`Notes\`.
2.  **List All Source Fields:** Open the Alteryx workflow. For each **Input Data** tool, list all the columns it brings in, and populate the \`Source_Field\` and \`Source_Table\` columns in your spreadsheet.
3.  **Trace Each Field:** This is the detailed work. Follow each source field through the workflow tool by tool.
    *   If a **Formula** tool creates a new field, add a new row to your spreadsheet. Put the formula logic in the \`Transformation_Logic\` column.
    *   If a **Join** tool brings in a field, document the join condition.
    *   If a **Filter** tool affects the rows, note the filter condition in the \`Notes\` for the relevant fields.
4.  **Document the Target:** For each path that ends in an **Output Data** tool, fill in the \`Target_Field\` and \`Target_Table\` columns.
5.  **Review and Verify:** Walk through the completed mapping document with the workflow owner or a business user to confirm that your understanding of the logic is correct.

### 4. Resources and Tools
- **Alteryx Workflow Canvas:** Your primary reference for the transformation logic.
- **Spreadsheet Software:** The tool for creating the mapping document.
- **Dataiku Column-Level Lineage:** After migration, Dataiku's automated lineage graph will serve as a living, visual version of this document.

### 5. Next Steps and Progression
- **Validation Checklist:** This mapping document becomes a checklist for your post-migration validation. For each target field, you will need to verify that the Dataiku flow implements the documented logic correctly.
- **Data Dictionary:** The mapping document is a key input for creating a formal data dictionary for the new, migrated dataset.

### 6. Common Challenges and Solutions
- **Challenge:** "The workflow is huge and tracing every field will take forever."
- **Solution:** Prioritize. Focus on the most critical output fields first—the ones used in key reports or calculations. You can document the less important, "pass-through" fields with less detail.
- **Challenge:** "The transformation logic is in a complex, undocumented macro."
- **Solution:** You must first reverse-engineer and document the macro's logic before you can complete the source-to-target mapping for the fields it affects. This can be a significant task in itself.
`},{id:206,slug:"analyzing-performance-bottlenecks-in-legacy-etl",question:"How to get started with analyzing performance bottlenecks in legacy ETL?",answer:`
### 1. Introduction/Overview
Before migrating a slow-running ETL pipeline, it's crucial to understand *why* it's slow. Analyzing performance bottlenecks in your legacy Alteryx workflows helps you identify opportunities for optimization and ensure the new Dataiku pipeline will be more efficient.

### 2. Prerequisites
- **Access to Alteryx Server logs or workflow performance profiles.**
- **A specific, slow-running workflow to analyze.**

### 3. Step-by-Step Instructions
1.  **Gather Performance Data:** Find the logs for a recent run of the slow workflow. In Alteryx Server, you can view the job history. In Alteryx Designer, you can enable "Performance Profiling" in the workflow settings to see the time spent in each tool.
2.  **Identify the Slowest Tools:** Look through the performance data to find the tools that consume the most time. Common culprits include:
    *   **Joins:** Especially on large, un-indexed datasets.
    *   **Summarize (Group By):** Can be slow on very large data.
    *   **Custom R or Python tools:** Inefficient code can be a major bottleneck.
    *   **Data Input/Output:** Reading from or writing to slow network locations.
3.  **Analyze the Root Cause:** For each slow tool, ask why it's slow.
    *   **For a Join:** Is it pulling a huge amount of data into memory? Could this join be performed in a database instead?
    *   **For a custom script:** Is the code written efficiently? Is it processing row-by-row in a loop where a vectorized operation would be faster?
4.  **Document Optimization Opportunities:** In your migration plan for this workflow, document the identified bottlenecks and the planned optimization strategy in Dataiku. For example:
    *   **Bottleneck:** "Slow Join tool joining two large SQL tables."
    *   **Dataiku Optimization Plan:** "Replace with a Dataiku SQL recipe to push the join down to the database."

### 4. Resources and Tools
- **Alteryx Performance Profiling:** The key feature for getting tool-level performance data.
- **Alteryx Server Job Logs:** For analyzing historical run times.

### 5. Next Steps and Progression
- **Benchmarking:** After migrating and optimizing the workflow in Dataiku, run it and compare the new end-to-end time with the original Alteryx run time. This provides a clear metric of the performance improvement.
- **Architectural Changes:** The analysis might reveal architectural problems (like reading huge files over a slow network). The migration is an opportunity to fix these by moving the data to a more appropriate location, like a cloud data warehouse.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't have access to performance logs."
- **Solution:** You will have to make an educated guess. Open the workflow and look for the most computationally expensive operations (joins on large data, iterative macros, etc.). These are your most likely bottlenecks.
- **Challenge:** "The whole workflow is slow, not just one tool."
- **Solution:** This could indicate a fundamental issue with the engine. If Alteryx is processing a very large dataset in-memory, every step will be slow. The solution in Dataiku would be to switch to a distributed engine like Spark or a database engine, which will speed up the entire pipeline.
`},{id:207,slug:"identifying-alteryx-specific-logic-that-needs-rewriting",question:"How to get started with identifying Alteryx-specific logic that needs rewriting?",answer:`
### 1. Introduction/Overview
While many Alteryx tools have direct equivalents in Dataiku, some rely on Alteryx-specific functions, macros, or scripting environments that will require a full rewrite. Identifying these early is crucial for accurately estimating the migration effort.

### 2. Prerequisites
- **Your inventory of Alteryx workflows.**
- **A good understanding of both Alteryx and Dataiku capabilities.**

### 3. Step-by-Step Instructions
1.  **Scan for "Code-Heavy" Tools:** Systematically review your workflows and look for tools that involve custom code or highly specialized logic. Pay close attention to:
    *   **R Tool / Python Tool:** Any workflow using these will require a manual rewrite of the script using Dataiku's code recipes.
    *   **Custom Macros (\`.yxmc\`):** Especially iterative or batch macros, which often contain complex logic that cannot be replicated with a single visual recipe.
    *   **Predictive Tools:** Workflows using Alteryx's predictive suite will need to have the models retrained and rebuilt in Dataiku's Visual ML Lab.
2.  **Look for Complex Formula Expressions:**
    *   Examine **Formula** tools. While simple expressions are easy to translate, look for formulas that use Alteryx-specific functions that don't have a direct equivalent in Dataiku's formula language. These may need to be implemented in a Python recipe.
3.  **Identify Specialized Connectors:**
    *   Check for **Input/Output** tools that connect to proprietary or uncommon systems for which Dataiku does not have a native connector. These connections may need to be rebuilt using a generic JDBC connector or a custom Python recipe that calls the system's API.
4.  **Tag for Rewrite:** In your workflow inventory, add a column "Requires Rewrite" and flag any workflow that contains these elements. These workflows will require more effort than a simple "lift and shift."

### 4. Resources and Tools
- **Dataiku's Connector and Recipe Library:** Use this to check if a direct equivalent exists for the Alteryx functionality.
- **Dataiku Python API Documentation:** This will be essential for rewriting the logic that interacts with the Dataiku environment.

### 5. Next Steps and Progression
- **Estimate Effort:** The workflows you've flagged for a rewrite will take significantly more time. Factor this into your project timeline and resource planning.
- **Prioritize Skillsets:** Ensure you have team members with the necessary Python or R skills to handle the rewriting tasks.

### 6. Common Challenges and Solutions
- **Challenge:** "The workflow uses a complex chain of macros that call other macros."
- **Solution:** This is a high-complexity item. You will need to carefully reverse-engineer the entire chain of logic. The best approach in Dataiku might be to create a single, well-documented Python recipe that encapsulates the entire logic, or a reusable Flow Zone if parts can be built visually.
- **Challenge:** "The Python tool in Alteryx uses a library that is hard to install or has conflicts."
- **Solution:** This is where Dataiku's robust code environment management shines. You can create a dedicated code environment for this specific recipe and carefully manage the dependencies to resolve conflicts, ensuring the code can run reliably.
`},{id:208,slug:"creating-migration-timelines-and-milestones",question:"How to get started with creating migration timelines and milestones?",answer:`
### 1. Introduction/Overview
A migration project without a timeline is just a wish. Creating a realistic timeline with clear milestones is essential for managing expectations, tracking progress, and ensuring the project stays on course. This involves grouping workflows into manageable waves and estimating the effort for each.

### 2. Prerequisites
- **Your completed workflow inventory,** including complexity and criticality scores.
- **Your workflow dependency map.**
- **A project management tool** (e.g., JIRA, Asana, or even a simple spreadsheet).

### 3. Step-by-Step Instructions
1.  **Group Workflows into "Waves" or "Sprints":**
    *   Using your dependency map and inventory, group workflows into logical migration waves. A wave should be a set of workflows that can be migrated together, typically over a few weeks.
    *   **Rule 1:** An upstream workflow must be in the same wave or an earlier wave than its downstream dependencies.
    *   **Rule 2:** Start with a wave of "quick wins"—workflows with high business value but low complexity—to build momentum.
2.  **Estimate Effort for Each Workflow:**
    *   Use your complexity scores to assign a rough time estimate (in days) to migrate each workflow. Be sure to include time for development, testing, and validation.
    *   A simple rubric: Low complexity = 2 days, Medium = 5 days, High = 10+ days. Adjust based on your team's experience.
3.  **Create a Gantt Chart or Roadmap:**
    *   In your project management tool, create a timeline.
    *   List your migration waves as major phases.
    *   Within each wave, list the specific workflows to be migrated.
    *   Assign start and end dates based on your effort estimates.
4.  **Define Key Milestones:** Identify the key checkpoints in your timeline. These are not just dates; they are tangible achievements.
    *   **Milestone 1:** "All Finance workflows migrated and validated."
    *   **Milestone 2:** "First predictive model live in Dataiku."
    *   **Milestone 3:** "Alteryx Server successfully decommissioned."

### 4. Resources and Tools
- **Project Management Software:** JIRA, Asana, Monday.com, etc.
- **Gantt Chart Tools:** Can be created in most project management tools or Excel.
- **Your Workflow Inventory and Dependency Map:** The essential inputs for this process.

### 5. Next Steps and Progression
- **Track Progress:** Regularly update the timeline to reflect actual progress against your estimates.
- **Communicate:** Share the high-level roadmap with stakeholders so they know when to expect their workflows to be migrated.
- **Adjust the Plan:** Be prepared to adjust the timeline based on lessons learned from the early migration waves.

### 6. Common Challenges and Solutions
- **Challenge:** "Our time estimates are wrong."
- **Solution:** This is almost guaranteed to happen. Your initial estimates are a best guess. After the first migration wave, you will have a much better understanding of your team's velocity. Use this new knowledge to re-estimate the remaining waves. Agile planning is key.
- **Challenge:** "A key person (SME) is not available when we need them."
- **Solution:** This is a major risk. Identify your key SME dependencies during planning and schedule their time well in advance. If their availability changes, you may need to reorder your migration waves to work on flows that don't depend on them.
`},{id:209,slug:"risk‑analysis-and-fallback-planning-during-migration",question:"How to get started with risk‑analysis and fallback planning during migration?",answer:`
### 1. Introduction/Overview
For any critical system migration, hoping for the best is not a strategy. A thorough risk analysis and a documented fallback plan are essential to ensure business continuity if something goes wrong during the cutover. This process involves identifying potential problems and having a pre-planned response.

### 2. Prerequisites
- **Your inventory of workflows,** with criticality scores.
- **A defined migration schedule.**

### 3. Step-by-Step Instructions
1.  **Identify Potential Risks:** For each high-criticality workflow you are about to migrate, hold a short brainstorming session. Ask "What could go wrong?". Common risks include:
    *   **Technical Risk:** The new Dataiku flow produces incorrect results.
    *   **Performance Risk:** The new flow is much slower than the old one.
    *   **Data Risk:** The source data is corrupted on migration day.
    *   **People Risk:** The key person needed to validate the output is sick or unavailable.
2.  **Assess and Prioritize Risks:** For each risk, assess its likelihood and impact. Focus your planning on the high-likelihood, high-impact risks.
3.  **Develop a Mitigation and Fallback Plan:** For each key risk, define your plan.
    *   **Risk:** Incorrect output from the new Dataiku flow.
    *   **Mitigation:** Perform a rigorous, parallel validation for at least one full run cycle.
    *   **Fallback Plan:** "If the validation fails, we will immediately revert to using the output from the legacy Alteryx workflow. We will not provide the Dataiku output to business users. The migration team will then have 24 hours to debug and fix the issue."
4.  **Define the "Go/No-Go" Criteria:** Before the final cutover, have a clear checklist.
    *   "Did the final parallel run complete successfully?"
    *   "Did the validation checks pass?"
    *   "Is the business stakeholder available and ready to sign off?"
    *   If the answer to any of these is "No," you execute the fallback plan and postpone the cutover.
5.  **Document the Plan:** Document the fallback plan clearly and ensure everyone on the migration team and the key business stakeholders understand it.

### 4. Resources and Tools
- **Risk Register:** A simple spreadsheet to list risks, their impact, and your mitigation plan.
- **A "Cutover Runbook":** A checklist document that details the step-by-step actions to be taken during the migration event, including the steps for the fallback plan.

### 5. Next Steps and Progression
- **Rehearse the Fallback:** For extremely critical systems, you could even rehearse the fallback procedure to ensure it works smoothly.
- **Post-Mortem:** If you do have to execute a fallback, hold a post-mortem meeting to understand what went wrong and how to prevent it in the next migration attempt.

### 6. Common Challenges and Solutions
- **Challenge:** "We don't have time for this; we just need to get it done."
- **Solution:** This is a dangerous mindset. The time spent on risk planning is an insurance policy. For a non-critical workflow, the plan can be very simple. But for a mission-critical financial reporting workflow, taking a few hours to create a solid fallback plan can prevent a major business disruption.
- **Challenge:** "What does 'revert to using the legacy workflow' mean?"
- **Solution:** It means you will keep the original Alteryx workflow ready to run, even after you think you've migrated. Don't decommission it immediately. Keep it on standby for a few successful run cycles of the new Dataiku flow before finally turning it off.
`},{id:210,slug:"prioritizing-alteryx-flows-for-incremental-migration",question:"How to get started with prioritizing Alteryx flows for incremental migration?",answer:`
### 1. Introduction/Overview
Trying to migrate all your Alteryx workflows at once (a "big bang" approach) is extremely risky and likely to fail. A gradual, incremental approach is far superior. Prioritizing which workflows to migrate first is a strategic decision based on balancing business value, technical complexity, and dependencies.

### 2. Prerequisites
- **A completed workflow inventory** with scores for business criticality and technical complexity.
- **A map of dependencies** between workflows.

### 3. Step-by-Step Instructions
1.  **Create a 2x2 Prioritization Matrix:**
    *   Draw a four-quadrant matrix. The Y-axis is **Business Criticality** (Low to High). The X-axis is **Technical Complexity** (Low to High).
2.  **Plot Your Workflows:** Place each workflow from your inventory onto this matrix based on its scores.
3.  **Identify the Four Quadrants:**
    *   **Top-Left (High Criticality, Low Complexity): QUICK WINS.** These are your top priority. They deliver significant business value and are relatively easy to migrate. Success here builds momentum and confidence in the project.
    *   **Top-Right (High Criticality, High Complexity): MAJOR PROJECTS.** These are important but difficult. Tackle them after you have some successful quick wins under your belt. They will require significant planning and resources.
    *   **Bottom-Left (Low Criticality, Low Complexity): FILLERS / TRAINING.** These are good for "filler" work between major projects or as training exercises for new team members.
    *   **Bottom-Right (Low Criticality, High Complexity): QUESTION / DE-PRIORITIZE.** Question if these are worth migrating at all. The high effort may not be justified by the low business value. Consider decommissioning them.
4.  **Factor in Dependencies:** Use your dependency map to adjust the plan. An easy "quick win" might depend on a more complex upstream workflow. You must migrate the upstream one first, even if it's in a different quadrant.
5.  **Create Your Migration Roadmap:** Sequence your migration waves based on this quadrant analysis, starting with the "Quick Wins."

### 4. Resources and Tools
- **The 2x2 Prioritization Matrix:** A simple but powerful strategic planning tool.
- **Your Workflow Inventory and Dependency Map:** The data that feeds the matrix.

### 5. Next Steps and Progression
- **Communicate the Roadmap:** Share the prioritized roadmap with business stakeholders to manage their expectations about when their specific workflows will be migrated.
- **Re-evaluate Periodically:** Priorities can change. Re-visit your prioritization matrix every few months to ensure it still aligns with the business's current needs.

### 6. Common Challenges and Solutions
- **Challenge:** "All our workflows seem to be in the 'Major Projects' quadrant."
- **Solution:** This indicates a complex legacy environment. You may need to break down the "Major Projects" further. Can you migrate a simplified version of a complex workflow first, and then add the more complex features in a later phase?
- **Challenge:** "A business leader is insisting we migrate a 'Low Value / High Complexity' workflow first."
- **Solution:** Use the matrix as a communication tool. Show them visually where the workflow sits and explain the high effort and low relative value. Try to understand the real reason for their request. Perhaps there's a hidden business value you haven't captured. If not, you can use the matrix to make a data-driven case for prioritizing other workflows first.
`},{id:211,slug:"translating-alteryx-data-cleansing-steps-into-dataiku-prepare-recipes",question:"How to get started with translating Alteryx data cleansing steps into Dataiku Prepare recipes?",answer:`
### 1. Introduction/Overview
The Dataiku Prepare recipe is the direct and powerful equivalent of many of Alteryx's data cleansing tools. Translating this logic involves mapping each Alteryx tool (like Filter, Formula, or Select) to a corresponding "processor" step within a single Prepare recipe, creating a clean and auditable transformation script.

### 2. Prerequisites
- **An Alteryx workflow** that performs data cleansing.
- **An input dataset** loaded into a Dataiku project.

### 3. Step-by-Step Instructions
1.  **Create a Prepare Recipe:** In your Dataiku Flow, select your input dataset and create a new **Prepare** recipe.
2.  **Map Alteryx Tools to Processors:** Go through your Alteryx workflow tool by tool and add the equivalent processor step in your Prepare recipe.
    *   **Alteryx \`Filter\` Tool:** Use the **Filter** processor in Dataiku. You can write a similar expression to keep or remove rows.
    *   **Alteryx \`Formula\` Tool:** Use the **Formula** processor. The expression language is very similar to Alteryx's.
    *   **Alteryx \`Select\` Tool:** You can reorder, rename, or remove columns by simply clicking the column dropdown in the Prepare recipe and selecting the action.
    *   **Alteryx \`Data Cleansing\` Tool:** This single tool maps to several Dataiku processors. "Remove Nulls" maps to **Clear rows where value is empty**. "Modify Case" maps to **Convert to uppercase/lowercase**. "Remove Punctuation" maps to a **Find and Replace** processor with a regular expression.
3.  **Build a Step-by-Step Script:** Add a new step in the Prepare recipe for each tool in the Alteryx flow. This creates a sequential script that is easy to read and debug.
4.  **Review and Run:** The preview pane will show the live result of your transformations. Once you have replicated the logic, run the recipe to generate the cleaned output dataset.

### 4. Resources and Tools
- **The Dataiku Prepare Recipe:** Your primary workspace.
- **The Processor Library:** The searchable list of over 100 transformation functions available in the Prepare recipe.
- **A "Translation Cheat Sheet":** It can be helpful to create a simple document that maps your company's most commonly used Alteryx tools to their Dataiku equivalents.

### 5. Next Steps and Progression
- **Refactor and Consolidate:** In Alteryx, you might have many separate tools on the canvas. In Dataiku, you can often consolidate all of these steps into a single, clean Prepare recipe.
- **Add Comments:** Use the "Description" field on each processor step to explain its purpose, creating a well-documented transformation.

### 6. Common Challenges and Solutions
- **Challenge:** "I can't find a processor for a specific Alteryx function."
- **Solution:** First, check the **Formula** processor; Dataiku's formula language is very rich. If the logic is too complex for a formula, you will need to use a **Python recipe** to implement that specific transformation step.
- **Challenge:** "The output is not exactly the same."
- **Solution:** This requires careful debugging. Go through your Prepare recipe step-by-step and compare the intermediate result with the output of the corresponding Alteryx tool to find where the logic diverges. Pay close attention to how each tool handles nulls and data types.
`},{id:212,slug:"reproducing-alteryx-joins-using-dataiku-join-recipes-or-sql-recipes",question:"How to get started with reproducing Alteryx joins using Dataiku Join recipes or SQL recipes?",answer:`
### 1. Introduction/Overview
Joining datasets is a fundamental ETL operation. When migrating an Alteryx workflow, you can replicate its \`Join\` tools using either Dataiku's visual \`Join\` recipe or, for better performance with database sources, a \`SQL\` recipe.

### 2. Prerequisites
- **An Alteryx workflow containing one or more Join tools.**
- **The input datasets for the join,** loaded into your Dataiku project.

### 3. Step-by-Step Instructions

#### Method 1: Using the Visual Join Recipe (Most Common)
1.  **Select the "Left" Dataset:** In your Dataiku Flow, select the dataset that corresponds to the "L" input of your Alteryx Join tool.
2.  **Create a Join Recipe:** From the right-hand panel, choose the **Join with...** recipe. Select the dataset corresponding to the "R" input.
3.  **Configure the Join:**
    *   **Join Type:** Select the join type that matches the Alteryx configuration. An Alteryx "inner join" (the "J" output) is an **Inner join** in Dataiku. An Alteryx "left join" ("L" output) is a **Left join**.
    *   **Join Condition:** Click on the key column(s) in both datasets to define the join condition.
    *   **Select Columns:** In the "Selected Columns" panel at the bottom, choose which columns to keep in the output, just as you would in an Alteryx Select tool after a join.
4.  **Run the Recipe:** Execute the recipe to produce the joined dataset.

#### Method 2: Using a SQL Recipe (For Performance)
1.  **When to Use:** Use this method when both of your input datasets are tables in the same SQL database.
2.  **Create a SQL Recipe:** In your Flow, select **+ RECIPE > SQL**. Add your two database datasets as inputs.
3.  **Write the SQL:** Write a standard SQL \`SELECT\` statement with a \`JOIN\` clause to replicate the logic. You can refer to the input datasets by their Dataiku names.
    > \`\`\`sql
    > SELECT *
    > FROM left_table_prepared
    > INNER JOIN right_table_prepared
    > ON left_table_prepared.id = right_table_prepared.id;
    > \`\`\`
4.  **Run the Recipe:** This will execute the join query directly in your database, which is highly efficient.

### 4. Resources and Tools
- **Join Recipe:** The primary visual tool for joining.
- **SQL Recipe:** The tool for high-performance, push-down joins.

### 5. Next Steps and Progression
- **Handling All Alteryx Outputs:** To get the equivalent of the Alteryx "L" and "R" outputs (unmatched rows), you can either use a **Full outer join** and then filter the results, or use the **Split** recipe after a join to separate the matched and unmatched rows.
- **Fuzzy Joins:** If the Alteryx workflow uses a fuzzy match macro, use Dataiku's dedicated **Fuzzy Join** recipe.

### 6. Common Challenges and Solutions
- **Challenge:** "My join is creating duplicate rows."
- **Solution:** This is a classic join problem, not specific to any tool. It means the join key is not unique in your "right" table. Before the join, you must add a **Group** recipe to deduplicate the right-hand dataset on its key.
- **Challenge:** "The SQL recipe is faster, but I don't know SQL well."
- **Solution:** Use the visual **Join** recipe first. Then, in the recipe's settings, you can often find a button to "Convert to SQL recipe". Dataiku will automatically generate the SQL code for you, which is a great way to learn.
`},{id:213,slug:"replacing-alteryx-macros-by-dataiku-loops-and-scenario-logic",question:"How to get started with replacing Alteryx macros by Dataiku loops and scenario logic?",answer:`
### 1. Introduction/Overview
Alteryx's iterative and batch macros are used to perform looping operations. In Dataiku, this type of logic is handled not in the Flow itself, but at the orchestration layer using **Scenarios** and **Python code**. This approach allows for more powerful and explicit control over the looping process.

### 2. Prerequisites
- **An Alteryx workflow that uses an iterative or batch macro.**
- **Understanding of the looping logic** (e.g., "for each region, run this calculation").
- **Basic Python skills.**

### 3. Step-by-Step Instructions
1.  **Parameterize Your Dataiku Flow:**
    *   First, build the core logic of what happens *inside* the loop as a standard Dataiku Flow.
    *   Crucially, parameterize this flow using a **Project Variable**. For example, if the macro iterates by region, create a variable named \`region\` and use it in a filter in your flow (e.g., \`region_column == '\${region}'\`).
2.  **Create a "Controller" Scenario:** In your project, go to **Scenarios** and create a new scenario.
3.  **Add a Python Step for Looping:**
    *   In the scenario, add a new step of type **Execute Python code**.
4.  **Write the Loop Logic:** The Python script will act as the controller for the loop.
    *   First, get the list of items to loop over (e.g., a list of all unique regions from a dataset).
    *   Then, create a \`for\` loop.
    *   Inside the loop, use the Dataiku API to:
        1.  Set the value of the project variable (\`region\`).
        2.  Trigger a job to build the final output of your parameterized flow.
    > \`\`\`python
    > import dataiku
    > client = dataiku.api_client()
    > project = client.get_project("MY_PROJECT")
    > # 1. Get the list of items to loop over
    > regions_df = dataiku.Dataset("unique_regions").get_dataframe()
    > # 2. Loop
    > for region_name in regions_df['region_column']:
    >     # 3. Set the variable
    >     variables = project.get_variables()
    >     variables["standard"]["region"] = region_name
    >     project.set_variables(variables)
    >     # 4. Trigger the build
    >     project.get_dataset("final_output").build()
    > \`\`\`
5.  **Run the Scenario:** Executing this scenario will now run your flow once for each region, effectively replacing the Alteryx macro.

### 4. Resources and Tools
- **Project Variables:** The key to parameterizing your flow.
- **Python Scenario Steps:** The engine for your custom looping logic.
- **Dataiku Python API Documentation:** Essential for learning how to control variables and jobs from code.

### 5. Next Steps and Progression
- **Parallel Execution:** For better performance, you can modify the Python script to launch all the jobs concurrently instead of in a sequential loop. This requires more advanced use of the API to manage multiple jobs at once.
- **Error Handling:** Add \`try...except\` blocks to your script to handle cases where the job for a specific item (e.g., one region) fails, allowing the loop to continue with the other items.

### 6. Common Challenges and Solutions
- **Challenge:** "This seems much more complicated than an Alteryx macro."
- **Solution:** While it involves code, this pattern is more explicit and powerful. It clearly separates the core data logic (in the Flow) from the operational control logic (in the scenario). This makes the overall system easier to debug and maintain.
- **Challenge:** "How do I combine the results from all the loop iterations?"
- **Solution:** The parameterized flow will overwrite the output dataset on each iteration. To collect all results, you need to configure the output dataset to be partitioned by your loop variable (e.g., partitioned by \`region\`). Each loop iteration will then write to a different partition, and the final dataset will contain the results for all regions.
`},{id:214,slug:"reimplementing-predictive-macros-in-automl-or-code-recipes",question:"How to get started with reimplementing predictive macros in AutoML or code recipes?",answer:`
### 1. Introduction/Overview
If your Alteryx workflows contain predictive macros (e.g., using R or Python tools to train or score a model), you will need to reimplement this logic in Dataiku. Dataiku offers two primary paths: the powerful, visual **AutoML** environment for standard models, and **Code Recipes** for custom or highly specialized models.

### 2. Prerequisites
- **An Alteryx workflow with a predictive component.**
- **The training data used by the original model.**
- **Understanding of the original model** (e.g., what was the target variable, what algorithm was used).

### 3. Step-by-Step Instructions

#### Method 1: Rebuilding with AutoML (Recommended for most models)
1.  **Prepare the Data:** In your Dataiku Flow, create a clean, "model-ready" dataset, just as you would have in Alteryx.
2.  **Launch the Visual Analysis Lab:** Select your training dataset and click **Lab > + NEW ANALYSIS > Prediction**.
3.  **Select the Target and Task:** Choose your target variable. Dataiku will automatically detect if it's a classification or regression task.
4.  **Train Multiple Models:** In the **Design** tab, select a variety of algorithms (like Random Forest, Gradient Boosted Trees, etc.). Click **Train**. Dataiku's AutoML will handle feature preprocessing, model training, and hyperparameter tuning.
5.  **Evaluate and Deploy:** Analyze the results leaderboard to find the best model. If it meets your performance criteria, **Deploy** it to the Flow. This creates a "Saved Model" object.
6.  **Score New Data:** Use the **Score** recipe with your deployed model to generate predictions on new data, replicating the "scoring" part of your Alteryx workflow.

#### Method 2: Rebuilding with a Code Recipe (For custom models)
1.  **When to Use:** Use this if the original Alteryx model uses a custom algorithm, a very specific library, or complex pre-processing that can't be done visually.
2.  **Create a Python/R Recipe:** Create a code recipe that takes your training data as input.
3.  **Write the Training Script:** Copy the core R or Python logic from the Alteryx tool into the Dataiku recipe. You will need to replace the Alteryx-specific data reading/writing code with the Dataiku API equivalents.
4.  **Save the Model:** After training the model in your script, save the model artifact (e.g., a pickled file) to a **Managed Folder** in Dataiku.
5.  **Create a Scoring Recipe:** Write a second code recipe that loads your saved model from the managed folder and uses it to score new data.

### 4. Resources and Tools
- **Visual Analysis Lab (AutoML):** The primary tool for building standard predictive models.
- **Code Recipes (Python/R):** For implementing custom modeling logic.
- **Managed Folders:** The correct place to store custom model artifacts.

### 5. Next Steps and Progression
- **Automated Retraining:** Create a **Scenario** to automate the entire pipeline, from rebuilding the training data to retraining the model and deploying the new version if it performs well.
- **Monitoring:** Use Dataiku's model monitoring features to track the performance and drift of your newly migrated model over time.

### 6. Common Challenges and Solutions
- **Challenge:** "My new Dataiku model doesn't have the same performance as the old Alteryx model."
- **Solution:** This is expected. It's very difficult to perfectly replicate a model. Focus on whether the new model meets the *business requirement* for accuracy. Often, the models built with Dataiku's modern AutoML engine will outperform older models built with legacy R scripts.
- **Challenge:** "The R script from Alteryx uses many obscure packages."
- **Solution:** You will need to create a dedicated **R Code Environment** in Dataiku and add all of the required packages to its dependencies. This can sometimes be challenging if the packages are old or have conflicts.
`},{id:215,slug:"recreating-record-parsing-logic-as-python-recipes",question:"How to get started with recreating record parsing logic as Python recipes?",answer:`
### 1. Introduction/Overview
When migrating workflows that handle complex, unstructured, or non-standard text data (like log files or custom reports), Alteryx's RegEx or XML Parse tools are often used. If this parsing logic is too complex for Dataiku's visual processors, a **Python Recipe** is the perfect tool for the job, giving you the full power of Python's text manipulation and regular expression libraries.

### 2. Prerequisites
- **An Alteryx workflow that performs complex record parsing.**
- **The raw text data** loaded into a Dataiku dataset.
- **Basic Python skills,** including regular expressions (\`re\` library).

### 3. Step-by-Step Instructions
1.  **Analyze the Alteryx Logic:** Open the Alteryx workflow and carefully examine the configuration of the RegEx, XML Parse, or Formula tools being used. Understand the exact pattern or structure it is trying to extract.
2.  **Create a Python Recipe:** In Dataiku, take your raw text dataset as input and create a new **Python recipe**.
3.  **Read the Input Data:** Use the standard boilerplate code to read your input dataset into a Pandas DataFrame.
    > \`df = dataiku.Dataset("raw_logs").get_dataframe()\`
4.  **Write a Parsing Function:** In your script, define a Python function that takes a single text record (a string) as input and returns the parsed components. Use Python's \`re\` library for regular expressions or a library like \`lxml\` or \`BeautifulSoup\` for XML/HTML parsing.
    > \`\`\`python
    > import re
    > # Function to parse a single log line
    > def parse_log_line(line):
    >     match = re.search(r"(\\d{4}-\\d{2}-\\d{2}) - (\\w+): (.*)", line)
    >     if match:
    >         return match.groups()
    >     return None, None, None
    > \`\`\`
5.  **Apply the Function:** Use the \`df.apply()\` method in Pandas to apply your parsing function to the column containing the raw text. This will create new columns with the extracted data.
    > \`\`\`python
    > # Apply the function and create new columns
    > df[['date', 'level', 'message']] = df['raw_log_column'].apply(lambda x: pd.Series(parse_log_line(x)))
    > \`\`\`
6.  **Write the Output:** Write the resulting DataFrame, which now contains the new parsed columns, to the output dataset.

### 4. Resources and Tools
- **Python Recipe:** Your environment for custom code.
- **Python's \`re\` library:** The standard library for powerful regular expression matching.
- **Online RegEx Testers:** Websites like regex101.com are invaluable for building and debugging your regular expressions before putting them in your code.

### 5. Next Steps and Progression
- **Error Handling:** Make your parsing function robust. What should it do if a line doesn't match the expected pattern? It should handle this gracefully (e.g., by returning null values) instead of crashing.
- **Create a Reusable Library:** If you have parsing logic that is needed in multiple places, move your parsing function into your project's **Library** so it can be imported and reused.

### 6. Common Challenges and Solutions
- **Challenge:** "My regular expression is not working correctly."
- **Solution:** Regular expressions are powerful but tricky. Build and test your expression incrementally in an online tool like regex101.com. This allows you to see the results in real-time as you type.
- **Challenge:** "The Python recipe is very slow on my large file."
- **Solution:** Applying a complex regex function row-by-row in Pandas can be slow. For very large datasets, you may be able to get better performance by using a **PySpark recipe** and a User Defined Function (UDF) to distribute the parsing work across a Spark cluster.
`},{id:216,slug:"replicating-conditional-logic-if-case-in-dataiku-formulas",question:"How to get started with replicating conditional logic (IF/CASE) in Dataiku formulas?",answer:`
### 1. Introduction/Overview
Conditional logic is a fundamental part of data transformation, used to create new columns or flag rows based on specific criteria. Alteryx's \`IF\` and \`CASE\` statements in its Formula tool can be directly and easily replicated in Dataiku using the powerful **Formula processor** within a Prepare recipe.

### 2. Prerequisites
- **An Alteryx workflow using a Formula tool** with conditional logic.
- **The input dataset** loaded into a Dataiku project.

### 3. Step-by-Step Instructions
1.  **Create a Prepare Recipe:** In your Dataiku Flow, create a **Prepare** recipe using your dataset as input.
2.  **Add a Formula Step:** Click **+ ADD A NEW STEP** and select the **Formula** processor from the library.
3.  **Translate the Logic:** The Dataiku formula language is very similar to Alteryx's and other spreadsheet languages.
    *   **Give the output column a name.**
    *   In the "Expression" box, write your conditional statement.

    #### Example 1: Simple IF Statement
    - **Alteryx Logic:** \`IF [Category] = "A" THEN "Group1" ELSE "Group2" ENDIF\`
    - **Dataiku Formula:** \`if(Category == 'A', 'Group1', 'Group2')\`

    #### Example 2: Nested IF (or CASE) Statement
    - **Alteryx Logic:** \`IF [Value] > 100 THEN "High" ELSEIF [Value] > 50 THEN "Medium" ELSE "Low" ENDIF\`
    - **Dataiku Formula (Nested if):** \`if(Value > 100, 'High', if(Value > 50, 'Medium', 'Low'))\`
    - **Dataiku Formula (Switch/Case):** For multiple conditions, the \`switch\` function can be cleaner.
        > \`switch(true, Value > 100, 'High', Value > 50, 'Medium', 'Low')\`
4.  **Preview and Run:** The preview pane will instantly show the results of your formula. Once the logic is correct, run the recipe.

### 4. Resources and Tools
- **The Formula Processor:** The primary tool for this task.
- **Formula Language Documentation:** In the Formula editor, click the "?" icon to see a full list of all available functions, including logical operators and conditional functions. This is your best friend.

### 5. Next Steps and Progression
- **Combine with other functions:** Nest other functions inside your if-statements for powerful logic. For example: \`if(contains(product_name, 'Premium'), price * 1.1, price)\`.
- **Handle Nulls:** Use the \`isBlank()\` or \`isNULL()\` functions to handle conditional logic for missing values.

### 6. Common Challenges and Solutions
- **Challenge:** "My formula has a syntax error."
- **Solution:** The formula editor will highlight errors. The most common issues are mismatched parentheses, using a single equals sign \`=\` for comparison instead of a double equals \`==\`, or forgetting to quote string literals.
- **Challenge:** "My logic is getting very complex with many nested ifs."
- **Solution:** For very complex conditional logic, consider breaking it down into multiple, simpler Formula steps. This can make it easier to read and debug. Alternatively, for a very large number of conditions, a **Python recipe** using a dictionary as a mapping might be cleaner.
`},{id:217,slug:"handling-alteryx-spatial-or-geocoding-transforms-using-python-or-plugins",question:"How to get started with handling Alteryx spatial or geocoding transforms using Python or plugins?",answer:`
### 1. Introduction/Overview
Migrating Alteryx workflows with spatial tools (for geocoding, distance calculations, or shape manipulation) requires choosing the right equivalent in Dataiku. Dataiku offers both a visual plugin and the ability to use powerful Python libraries to handle these geospatial transformations.

### 2. Prerequisites
- **An Alteryx workflow using spatial tools.**
- **Your data, including latitude/longitude columns or addresses,** loaded into Dataiku.
- **For the plugin:** The "Geospatial" plugin must be installed by a Dataiku admin.
- **For Python:** A code environment with geospatial libraries installed (e.g., \`geopandas\`, \`geopy\`).

### 3. Step-by-Step Instructions

#### Method 1: Using the Visual Geospatial Plugin (Recommended Start)
1.  **Install the Plugin:** An administrator must install the **Geospatial** plugin from the plugin store.
2.  **Use the Geospatial Recipes:** The plugin provides several new visual recipes:
    *   **Geocode:** Takes a dataset with address columns and uses a geocoding service (like OpenStreetMap or Google Maps) to add latitude and longitude columns.
    *   **Create GeoPoint:** Combines separate latitude and longitude columns into a single, native "geopoint" geometry column.
    *   **Calculate distance between points:** Computes the distance between two geopoints.
3.  **Configure and Run:** These visual recipes have a simple UI where you select your input columns and run the transformation.

#### Method 2: Using a Python Recipe (For Advanced Operations)
1.  **When to Use:** Use this for advanced spatial joins (e.g., "points in polygon"), complex shape manipulations, or when you need to use a specific geocoding service not supported by the plugin.
2.  **Set up Environment:** Create a code environment and install libraries like \`geopandas\`, which provides a powerful, Pandas-like interface for geospatial data.
3.  **Write the Python Script:**
    *   Create a **Python recipe**.
    *   Read your data into a GeoDataFrame.
    *   Use the library's functions to perform your spatial operations.
    > \`\`\`python
    > import geopandas
    > from shapely.geometry import Point
    > # Assuming 'df' is your input pandas DataFrame
    > gdf = geopandas.GeoDataFrame(
    >     df, geometry=geopandas.points_from_xy(df.longitude, df.latitude))
    > # Now you can perform geospatial operations on 'gdf'
    > \`\`\`
### 4. Resources and Tools
- **Geospatial Plugin:** Provides easy-to-use visual recipes for common tasks.
- **Python Libraries:**
    - **GeoPandas:** The standard for vector-based geospatial analysis in Python.
    - **Geopy:** Useful for accessing various geocoding services.
- **Geopoint Data Type:** Dataiku has a native data type for geographical points, which can be visualized on maps.

### 5. Next Steps and Progression
- **Map Visualizations:** Use Dataiku's built-in **Map chart** to visualize your datasets that contain geopoints or other geographical data.
- **Spatial Joins:** Perform complex spatial joins in a Python recipe, for example, to count how many of your customers are located inside a specific sales territory polygon.

### 6. Common Challenges and Solutions
- **Challenge:** "Geocoding my addresses is slow and returning errors."
- **Solution:** Geocoding relies on external services. These services often have usage limits (rate limits) and can be slow for large datasets. For bulk geocoding, it's often better to run the process in batches. Also, ensure your address data is as clean as possible before sending it to the service.
- **Challenge:** "Installing geospatial libraries like GeoPandas is difficult."
- **Solution:** Geospatial libraries often have complex OS-level dependencies (like GDAL). This can make installation tricky. It's best to work with your Dataiku administrator to create a standard, pre-built code environment for geospatial analysis that all users can share.
`},{id:218,slug:"generating-pivot-or-cross‑tab-transformations-in-dataiku",question:"How to get started with generating pivot or cross‑tab transformations in Dataiku?",answer:`
### 1. Introduction/Overview
Pivoting data—turning unique row values into distinct columns—is a common data shaping task, especially for creating summary reports. Alteryx's \`Cross Tab\` tool has a direct and powerful equivalent in Dataiku's visual **Pivot recipe**.

### 2. Prerequisites
- **An Alteryx workflow that uses the Cross Tab tool.**
- **The input data in a "long" format,** loaded into a Dataiku dataset. For example, a table with columns like \`Date\`, \`Product_Category\`, and \`Sales\`.

### 3. Step-by-Step Instructions
1.  **Analyze the Alteryx Cross Tab Tool:** Look at its configuration to understand:
    *   What column is being used for the **new column headers**? (e.g., \`Product_Category\`)
    *   What column contains the **values** to fill the new columns? (e.g., \`Sales\`)
    *   What are the **group by** keys that will remain as rows? (e.g., \`Date\`)
2.  **Select the Pivot Recipe:** In your Dataiku Flow, select your "long" input dataset. From the right-hand panel, choose the **Pivot** recipe.
3.  **Configure the Pivot Recipe:** The UI maps directly to the Alteryx concepts:
    *   **Rows to keep:** Drag the column(s) you want to remain as rows here (e.g., \`Date\`).
    *   **Column to pivot:** Drag the column whose values will become the new column headers here (e.g., \`Product_Category\`).
    *   **Value to compute:** Drag the column that contains the values for the new columns here (e.g., \`Sales\`).
    *   **Aggregation:** Select how to aggregate the values. This is usually **Sum** or **Average**.
4.  **Preview and Run:**
    *   The preview pane will show you what the new, "wide" output dataset will look like, with columns for each product category.
    *   Click **Run** to execute the pivot.

### 4. Resources and Tools
- **Pivot Recipe:** The primary visual tool for this transformation.
- **Unpivot Recipe:** Dataiku also has an "Unpivot" recipe, which does the reverse operation: turning a wide table into a long one.

### 5. Next Steps and Progression
- **Handling Missing Values:** After pivoting, some cells might be empty if there was no data for that combination. You can add a **Prepare** recipe after the Pivot to fill these nulls with 0 if needed.
- **Dynamic Pivoting:** If new categories appear in your data (e.g., a new product category), the Pivot recipe will automatically create new columns for them the next time it runs.

### 6. Common Challenges and Solutions
- **Challenge:** "My output has too many columns."
- **Solution:** This happens if the column you chose to pivot has a very large number of unique values. Before pivoting, you may need to add a **Prepare** recipe to clean or group the values in that column to reduce its cardinality.
- **Challenge:** "I need to pivot on multiple value columns."
- **Solution:** The visual Pivot recipe in Dataiku is designed to pivot one value column at a time. If you need to pivot multiple measures simultaneously, the standard pattern is to use a **Python recipe** with the Pandas \`pivot_table\` function, which is more flexible and can handle this case.
`},{id:219,slug:"rebuilding-data-blending-routines-in-flow-zones",question:"How to get started with rebuilding data blending routines in Flow Zones?",answer:`
### 1. Introduction/Overview
"Data blending" is the process of combining data from multiple different sources to create a unified, analysis-ready dataset. In Alteryx, this might be a sprawling part of a workflow. In Dataiku, the best practice is to organize this logic into a dedicated **Flow Zone**, which creates a clean, understandable, and reusable asset.

### 2. Prerequisites
- **An Alteryx workflow that performs data blending.**
- **All the necessary source datasets** loaded into your Dataiku project.

### 3. Step-by-Step Instructions
1.  **Plan the Blending Zone:** Look at the Alteryx workflow and identify the entire section related to data blending. This typically starts with multiple **Input Data** tools and ends in a single, combined dataset.
2.  **Create a Flow Zone:**
    *   In your Dataiku Flow, right-click on the canvas and **Create Flow Zone**.
    *   Give it a clear name, like \`2_Data_Blending\` or \`Staging_Layer\`.
3.  **Move Inputs into the Zone:** Drag all your raw input datasets into this new zone.
4.  **Rebuild the Logic Inside the Zone:**
    *   Inside the Flow Zone, replicate the Alteryx logic using a chain of Dataiku's visual recipes.
    *   This will typically involve a sequence of:
        *   **Prepare** recipes on each input to clean and standardize them.
        *   **Join** recipes to combine the sources based on common keys.
        *   **Stack** recipes to append sources that have similar structures.
        *   A final **Prepare** recipe to clean up the combined dataset (e.g., remove duplicate columns, rename fields).
5.  **Define a Clear Output:** The final dataset in this zone is your "blended" or "unified" dataset. Give it a clear name (e.g., \`customers_unified\`). This dataset is now the trusted source for all downstream analysis and modeling.
6.  **Collapse the Zone:** Once complete, you can collapse the Flow Zone. This hides the complex blending logic, showing only the final, clean output dataset, which dramatically simplifies your main Flow view.

### 4. Resources and Tools
- **Flow Zones:** The key tool for organizing and encapsulating complex logic.
- **Visual Recipes (Join, Stack, Prepare):** The building blocks for the blending logic.
- **The Project Wiki:** Document the purpose of your blending zone and the key business rules it implements.

### 5. Next Steps and Progression
- **Create a Reusable Asset:** This Flow Zone can become a reusable asset. Other projects can now connect directly to your final, blended dataset, ensuring everyone in the organization is using the same, consistent source of truth.
- **Add Data Quality Checks:** Place automated **Metrics and Checks** on the final output dataset of the zone to ensure the quality and integrity of your blended data.

### 6. Common Challenges and Solutions
- **Challenge:** "My blending logic is a 'spaghetti mess' even inside the zone."
- **Solution:** Use the **Arrange** button inside the Flow Zone to automatically clean up the layout. Try to create a clear, left-to-right flow of data. If it's still too complex, consider breaking it down into multiple Flow Zones (e.g., a zone for blending customer data, and another for blending product data).
- **Challenge:** "The performance of the blending zone is slow."
- **Solution:** If your blending involves joining very large datasets, ensure you are using the correct execution engine. If the data is in a database, all the Join and Prepare recipes should be set to **Run on database (SQL)** to leverage push-down computation.
`},{id:220,slug:"automated-deduplication-logic-via-dataiku-rules",question:"How to get started with automated deduplication logic via Dataiku rules?",answer:`
### 1. Introduction/Overview
Removing duplicate records is a critical data cleansing step. Dataiku offers several ways to perform deduplication, from removing exact duplicates to more nuanced methods based on a specific business key. The **Group** recipe is the most powerful and common tool for this task.

### 2. Prerequisites
- **A dataset containing duplicate records.**
- **A clear definition of a duplicate:** Is it a row that is 100% identical to another, or is it a row that has the same *key* (e.g., same \`customer_id\`) as another?

### 3. Step-by-Step Instructions

#### Method 1: Removing Exact Duplicates
1.  **Select the Distinct Recipe:** In your Flow, select the dataset you want to deduplicate.
2.  From the right-hand panel, choose the **Distinct** recipe (it may be listed under "Deduplicate").
3.  **Run the Recipe:** This recipe has no configuration. It will simply remove all rows that are exact duplicates of another row, keeping one copy.

#### Method 2: Deduplicating Based on a Key (Most Common)
1.  **When to Use:** Use this when you want to keep only one record for each unique value of a specific column (e.g., one record per \`customer_id\`).
2.  **Select the Group Recipe:** In your Flow, select your dataset and choose the **Group** recipe.
3.  **Configure the Grouping:**
    *   **Group by:** In the "Key" section, select the column that defines your unique entity (e.g., \`customer_id\`).
    *   **Aggregations:** For all the *other* columns you want to keep, you must tell Dataiku how to aggregate them. To simply keep the value from the *first* record in each group, add each column to the "Aggregations" section and select **First** as the aggregation type.
4.  **Run the Recipe:** The output will be a dataset with exactly one row for each unique \`customer_id\`.

### 4. Resources and Tools
- **Distinct Recipe:** For simple, full-row deduplication.
- **Group Recipe:** The powerful and flexible tool for deduplicating based on a key.

### 5. Next Steps and Progression
- **Advanced Aggregation:** Instead of just taking the "First" record, you can be more intentional. For example, you could group by \`customer_id\` and then take the **Max** of the \`last_seen_date\` to find the most recent record for each customer.
- **Handling Ties:** If you need to break a tie between duplicate keys based on another column, use the **Window** recipe first to create a ranking, then use a **Filter** recipe to keep only the rows where rank = 1.

### 6. Common Challenges and Solutions
- **Challenge:** "The Distinct recipe didn't remove my duplicates."
- **Solution:** This means the rows are not *exactly* identical. There might be a subtle difference, like trailing whitespace in one of the columns or a floating-point precision difference. You almost always want to use the **Group** recipe method for a more robust deduplication.
- **Challenge:** "After using the Group recipe, I'm missing some columns."
- **Solution:** You forgot to add them to the "Aggregations" section. Any column that is not in the "Group by" key must have an aggregation defined for it to be included in the output.
`},{id:221,slug:"setting-up-dataiku-scenarios-to-mimic-legacy-scheduler-runs",question:"How to get started with setting up Dataiku Scenarios to mimic legacy scheduler runs?",answer:`
### 1. Introduction/Overview
Migrating a scheduled job from a legacy system (like Windows Task Scheduler or an Alteryx Server schedule) to Dataiku involves creating a **Scenario**. Scenarios are Dataiku's built-in orchestration tool, allowing you to define a sequence of actions and schedule them to run automatically.

### 2. Prerequisites
- **A migrated Dataiku flow** that you want to schedule.
- **The schedule details from the legacy system** (e.g., "runs every day at 2 AM").

### 3. Step-by-Step Instructions
1.  **Create a New Scenario:**
    *   In your Dataiku project, go to the **Scenarios** page (from the top navigation bar).
    *   Click **+ NEW SCENARIO** and give it a descriptive name (e.g., \`Daily_Sales_Report_Build\`).
2.  **Define the "What" (The Steps):**
    *   Go to the **Steps** tab.
    *   Click **+ ADD STEP** and choose **Build / Train**.
    *   Select the final output dataset(s) of your flow that you want this job to generate. **Best Practice:** You only need to select the *final* item. Dataiku will automatically build all of its upstream dependencies.
3.  **Define the "When" (The Trigger):**
    *   Go to the **Settings** tab.
    *   Click **+ ADD TRIGGER** and select **Time-based**.
    *   Configure the schedule to match the legacy job. You can choose from presets like "Daily" or "Hourly" and set the time, or use a CRON expression for more complex schedules.
4.  **Define the "What If" (The Reporter):**
    *   Go to the **Reporters** tab.
    *   Click **+ ADD REPORTER** and choose **Mail**.
    *   Configure it to send an email to you or your team **On failure**. This ensures you are alerted if the job breaks.
5.  **Activate the Scenario:**
    *   At the top of the scenario page, ensure the main toggle switch is set to **ACTIVE**. The trigger you created must also be toggled on.
    *   The "Next run" time will now be displayed, confirming that it is scheduled.

### 4. Resources and Tools
- **Scenarios Page:** The central hub for all automation and scheduling.
- **Steps Tab:** Defines what the job does.
- **Triggers Tab:** Defines when the job runs.
- **Reporters Tab:** Defines what happens on success or failure.

### 5. Next Steps and Progression
- **Data Quality Gates:** Add a "Run checks" step after your build step to validate the data and fail the job if it doesn't meet quality standards.
- **Chained Scenarios:** You can have one scenario trigger another upon successful completion, allowing you to orchestrate more complex, multi-project workflows.

### 6. Common Challenges and Solutions
- **Challenge:** "My scheduled job didn't run."
- **Solution:** Check the basics first: Is the scenario's master toggle on? Is the trigger's toggle on? If both are on, check the scenario's "Last runs" tab for any error messages. There might have been an issue that prevented the job from starting.
- **Challenge:** "How do I run the job for a specific historical date?"
- **Solution:** Don't change the scenario. Instead, go to the Flow, manually trigger a build of the final dataset, and in the build dialog, specify that you want to build it for a specific partition or with certain project variables. Scenarios are for automated, recurring runs.
`},{id:222,slug:"configuring-retry-logic-and-error-alerts-in-scenarios",question:"How to get started with configuring retry logic and error alerts in Scenarios?",answer:`
### 1. Introduction/Overview
Building robust, production-grade pipelines requires planning for failure. Dataiku Scenarios have built-in features for sending alerts on errors and can be configured with custom retry logic to automatically handle transient issues, like a temporary network hiccup.

### 2. Prerequisites
- **An existing Dataiku Scenario.**
- **Admin configuration:** Your Dataiku administrator must have configured the mail server for email alerts to work.

### 3. Step-by-Step Instructions

#### Part A: Configuring Error Alerts (The Easy Part)
1.  **Navigate to your Scenario** and open the **Reporters** tab.
2.  **Click + ADD REPORTER** and select **Mail** (or Slack, etc.).
3.  **Configure the Reporter:**
    *   **Run condition:** Set this to **On failure**.
    *   **Recipients:** Enter the email address of the person or team who should be notified.
    *   **Message:** Customize the message. **Crucially, include the variable \`\${jobURL}\`**. This provides a direct link to the failed job's log, which is essential for troubleshooting.
4.  **Save** the scenario. Error alerting is now active.

#### Part B: Configuring Retry Logic (The Advanced Part)
Dataiku does not have a simple "retry N times" button. This logic needs to be implemented explicitly using a Python step, which gives you more control.
1.  **Create a "Wrapper" Scenario:** Create a new scenario, let's call it \`Retry_Wrapper_Scenario\`. This scenario will not build anything directly; its only job is to call your main scenario and handle retries.
2.  **Add a Python Step:** Add a single "Execute Python code" step to this wrapper scenario.
3.  **Write the Retry Script:** Use the Dataiku Python API to write a script that tries to run your main scenario in a loop.
    > \`\`\`python
    > import dataiku
    > RETRY_COUNT = 3
    > for i in range(RETRY_COUNT):
    >     try:
    >         # Get a handle on your main scenario and run it
    >         scenario = dataiku.api_client().get_project("MY_PROJECT").get_scenario("my_main_build_scenario")
    >         run = scenario.run_and_wait() # run_and_wait is key
    >         # If the run succeeded, exit the loop
    >         if run.get_info()["result"] == "SUCCESS":
    >             print(f"Run succeeded on attempt {i+1}")
    >             # Use this to pass the success status to the wrapper scenario
    >             dataiku.scenario.set_scenario_outcome(True) 
    >             break 
    >     except Exception as e:
    >         print(f"Attempt {i+1} failed: {e}")
    > # If the loop finishes without success, fail the wrapper scenario
    > else:
    >     dataiku.scenario.set_scenario_outcome(False, "All retry attempts failed.")
    > \`\`\`
4.  **Schedule the Wrapper:** You will now schedule the \`Retry_Wrapper_Scenario\` instead of your main one.

### 4. Resources and Tools
- **Reporters Tab:** For simple, powerful alerting.
- **Python Scenario Step & API:** For implementing advanced control flow like custom retries.

### 5. Next Steps and Progression
- **Exponential Backoff:** Improve your retry script by adding a \`time.sleep()\` inside the loop that waits for a progressively longer time between retries (e.g., 1 min, then 5 mins, then 15 mins).
- **Conditional Retries:** Make the script smarter. It could inspect the error message and only retry on specific, transient errors (like "Connection timed out") but fail immediately on fatal errors (like "Table not found").

### 6. Common Challenges and Solutions
- **Challenge:** "My retry script is complex to write."
- **Solution:** It is an advanced pattern. Start with simple alerting first. Only implement a custom retry wrapper for your most mission-critical scenarios where you expect occasional, transient failures.
- **Challenge:** "I'm getting too many failure alerts."
- **Solution:** This means your pipeline is unstable. The alerts are doing their job by notifying you. The solution is to fix the underlying root cause of the failures, not to turn off the alerts.
`},{id:223,slug:"parameterizing-flow-runs-for-daily-ingestion-cycles",question:"How to get started with parameterizing flow runs for daily ingestion cycles?",answer:`
### 1. Introduction/Overview
Hardcoding values like dates into your filters is not a scalable practice. To handle daily or periodic data loads, you should parameterize your flow using **Project Variables**. This allows a single, generic workflow to be run for any date, which is the key to efficient automation.

### 2. Prerequisites
- **A Dataiku flow designed to process data for a specific period.**
- **An understanding of how your data is partitioned or filtered by date.**

### 3. Step-by-Step Instructions
1.  **Create a Project Variable for the Date:**
    *   In your project, go to **... > Variables**.
    *   Click **Edit** and **+ ADD VARIABLE**.
    *   Create a variable named \`run_date\`. You can give it a default value, like yesterday's date in \`YYYY-MM-DD\` format.
2.  **Use the Variable in Your Flow:**
    *   Go to the recipe that filters your data by date (e.g., a **Prepare** or **SQL** recipe).
    *   Modify the filter expression to use your new variable. The syntax is \`\${variable_name}\`.
    *   **In a Prepare recipe Filter:** \`my_date_column == '\${run_date}'\`
    *   **In a SQL recipe:** \`WHERE order_date = '\${run_date}'\`
3.  **Create an Automation Scenario:**
    *   Go to **Scenarios** and create a new scenario.
4.  **Use the Variable in the Scenario:**
    *   **Method A (Simple):** If you just want to run for the current date, you can use Dataiku's built-in time variables directly in your recipe, like \`\${current_date}\`.
    *   **Method B (More Control):** In your scenario, add a **Python step** at the beginning. This script can calculate the exact date you need (e.g., yesterday, the first day of last month) and set the project variable's value dynamically.
    > \`\`\`python
    > from datetime import datetime, timedelta
    > import dataiku
    > # Calculate yesterday's date
    > yesterday = datetime.now() - timedelta(1)
    > yesterday_str = yesterday.strftime('%Y-%m-%d')
    > # Set the project variable
    > vars = dataiku.api_client().get_project("MY_PROJECT").get_variables()
    > vars['standard']['run_date'] = yesterday_str
    > dataiku.api_client().get_project("MY_PROJECT").set_variables(vars)
    > \`\`\`
5.  **Add the Build Step:** After the Python step, add a step to build the final output of your parameterized flow.
6.  **Schedule the Scenario:** Schedule this scenario to run daily. Each day, it will first update the \`run_date\` variable and then run your flow using that new date.

### 4. Resources and Tools
- **Project Variables:** The core feature for parameterization.
- **Python Scenario Steps:** For dynamically calculating and setting variable values.
- **Dataiku's Built-in Time Variables:** For simple cases, variables like \`\${current_year}\`, \`\${current_month}\` are available.

### 5. Next Steps and Progression
- **Partitioning:** For very large datasets, instead of filtering, you should be using partitioned datasets. You can then use variables to tell the scenario which specific partition to build, e.g., \`project.get_dataset("my_output").build(partitions="\${run_date}")\`.
- **Manual Runs:** To run the flow for a specific historical date, you can manually change the value of the \`run_date\` variable and then trigger the scenario once.

### 6. Common Challenges and Solutions
- **Challenge:** "My variable is not being updated correctly."
- **Solution:** Check the logic in your Python scenario step. Add \`print()\` statements to see what value is being calculated. Ensure you are saving the variables after setting them.
- **Challenge:** "The variable syntax \`\${...}\` is not working in my recipe."
- **Solution:** Make sure you are using it in a compatible recipe and field (most visual recipe inputs and code recipes work). Double-check for typos in the variable name.
`},{id:224,slug:"orchestrating-multi-step-flows-across-project-zones",question:"How to get started with orchestrating multi-step flows across project zones?",answer:`
### 1. Introduction/Overview
Flow Zones are essential for organizing large projects, but they don't change the fundamental way Dataiku's dependency engine works. Orchestrating a pipeline that spans multiple zones is straightforward and involves creating a scenario that builds the final output, regardless of which zone it's in.

### 2. Prerequisites
- **A Dataiku project with a Flow organized into multiple Flow Zones.**
- **A clear "final" dataset or output** that you want the orchestration to produce.

### 3. Step-by-Step Instructions
1.  **Design Your Zoned Flow:** Ensure your Flow is organized logically. For example:
    *   **Zone 1: Ingestion:** Contains all your raw source datasets.
    *   **Zone 2: Preparation:** Contains recipes that clean and join the raw data.
    *   **Zone 3: Outputs:** Contains the final, aggregated datasets for reporting.
    *   The flow of data should be from left to right, from one zone to the next.
2.  **Create a Scenario:** Go to **Scenarios** and create a new scenario (e.g., \`Build_Final_Outputs\`).
3.  **Add a Single Build Step:**
    *   In the **Steps** tab, add a **Build / Train** step.
4.  **Select Only the Final Output:**
    *   In the configuration for the build step, click **+ ADD** and select the dataset(s) from your **final Flow Zone** (e.g., from the "Outputs" zone).
    *   **Do not add datasets from the intermediate zones.**
5.  **Run the Scenario:**
    *   When you run the scenario, Dataiku's dependency engine will automatically perform the following:
        1.  It sees that to build the final dataset in Zone 3, it first needs the prepared dataset from Zone 2.
        2.  It sees that to build the prepared dataset in Zone 2, it first needs the raw data from Zone 1.
        3.  It will automatically build everything in the correct order, across all zones, to produce the final result you requested.

### 4. Resources and Tools
- **Flow Zones:** The tool for visual organization.
- **Scenarios:** The tool for orchestration.
- **Dataiku's Dependency Engine:** The "magic" that handles the cross-zone dependencies automatically.

### 5. Next Steps and Progression
- **Partial Builds:** If you only want to refresh one part of your project, you can create a scenario that only builds the final dataset of a specific zone (e.g., build the final output of the "Preparation" zone).
- **Complex Orchestration:** For very complex projects, you can chain scenarios. For example, a master scenario could first trigger a scenario to refresh Zone 1, and upon its success, trigger another scenario to refresh Zone 2.

### 6. Common Challenges and Solutions
- **Challenge:** "My scenario is rebuilding the entire flow every time, which is slow."
- **Solution:** This is the default behavior if the build mode is set to "Forced rebuild". Change the build mode in your scenario's step to "Build required datasets" or "Since last successful build". This will intelligently check what has changed and only rebuild the necessary parts of the flow, even across zones.
- **Challenge:** "I have a circular dependency between zones."
- **Solution:** Dataiku flows must be Directed Acyclic Graphs (DAGs). You cannot have a situation where a dataset in Zone 2 depends on Zone 3, and a dataset in Zone 3 depends on Zone 2. The flow of data must be one-way. You need to refactor your logic to break this circular dependency.
`},{id:225,slug:"triggering-dataiku-jobs-via-rest-api-from-legacy-systems",question:"How to get started with triggering Dataiku jobs via REST API from legacy systems?",answer:`
### 1. Introduction/Overview
To integrate Dataiku with your broader enterprise architecture, you often need external systems to trigger jobs. The **Dataiku REST API** is the standard, secure way to do this. Any external application, from a legacy scheduler to a modern CI/CD tool, can make a simple HTTP request to start a Dataiku scenario.

### 2. Prerequisites
- **A Dataiku Scenario** that you want to trigger externally.
- **An API Key:** You must generate a personal or global API key in Dataiku with permissions to run scenarios on your target project.
- **A legacy system capable of making an HTTP POST request** (e.g., via a scripting language like PowerShell, Python, or a tool like \`curl\`).

### 3. Step-by-Step Instructions
1.  **Identify the Necessary Information:**
    *   **Your Dataiku URL:** e.g., \`https://dss.mycompany.com\`
    *   **Your Project Key:** e.g., \`SALES_PROJ\`
    *   **Your Scenario ID:** e.g., \`daily_build\`
    *   **Your API Key:** The secret key you generated.
2.  **Construct the API Endpoint URL:** The URL to trigger a scenario has a standard format:
    > \`YOUR_DSS_URL/public/api/projects/YOUR_PROJECT_KEY/scenarios/YOUR_SCENARIO_ID/run\`
3.  **Write the Script in Your Legacy System:**
    *   In your external system, write a script that will make the API call. The most common tool for this is \`curl\`, which is available on most systems.
    *   The request must be an **HTTP POST**.
    *   Authentication is done via HTTP Basic Auth, where the **API key is the username** and the password is left blank.
4.  **Example using \`curl\`:**
    > \`\`\`bash
    > # Replace placeholders with your actual values
    > API_KEY="your_secret_api_key"
    > DSS_URL="https://dss.mycompany.com"
    > PROJECT_KEY="SALES_PROJ"
    > SCENARIO_ID="daily_build"
    > # Make the POST request
    > curl -X POST -u "\${API_KEY}:" "\${DSS_URL}/public/api/projects/\${PROJECT_KEY}/scenarios/\${SCENARIO_ID}/run"
    > \`\`\`
5.  **Schedule the Script:** Use your legacy system's scheduler (like Windows Task Scheduler or cron) to run this script at the desired time.

### 4. Resources and Tools
- **Dataiku REST API Documentation:** It provides details on the "run scenario" endpoint and the expected payload format.
- **\`.json\` file:** For complex payloads, it's easier to write the JSON in a file and pass it to curl (\`-d @payload.json\`).
- **Python \`requests\` library:** The ideal tool for making these API calls from a controlling application.

### 5. Next Steps and Progression
- **Passing Parameters:** You can send a JSON body in your POST request to override project variables for the run, making your triggered jobs dynamic.
- **Checking Job Status:** The initial API call just starts the job. A more robust script would capture the \`jobId\` from the response and then call the job status endpoint in a loop to wait for the job to finish and check if it was successful.

### 6. Common Challenges and Solutions
- **Challenge:** "The request fails with a '401 Unauthorized' error."
- **Solution:** This is an authentication problem. Double-check that your API key is correct and that it has been granted "run scenarios" permission on the target project. Ensure you are using the correct \`-u "key:"\` syntax.
- **Challenge:** "The request fails with a network error like 'Connection timed out'."
- **Solution:** This means your legacy system cannot reach the Dataiku server. There is likely a firewall blocking the connection. You need to work with your network team to create a rule that allows traffic from the source system to the Dataiku server on its port.
`},{id:226,slug:"building-scheduling-daily-incremental-load-jobs-to-mimic-batch-patterns",question:"How to get started with building + scheduling daily incremental load jobs to mimic batch patterns?",answer:`
### 1. Introduction/Overview
For large, transactional datasets, rebuilding the entire table every day is incredibly inefficient and costly. An incremental load pattern, where you only process new or updated records, is the standard best practice. In Dataiku, this is typically achieved using **partitioning**.

### 2. Prerequisites
- **A large source dataset** that grows over time (e.g., a table of daily sales transactions).
- **A date column** in the data that can be used for partitioning (e.g., \`order_date\`).
- **Understanding of Dataiku partitioning.**

### 3. Step-by-Step Instructions
1.  **Partition Your Input Dataset:**
    *   Open your main source dataset in Dataiku.
    *   Go to the **Settings > Partitioning** tab.
    *   Click **Activate partitioning**.
    *   Choose your date column and select a time dimension (e.g., "Day"). Dataiku will now see this dataset as a collection of daily partitions.
2.  **Partition Your Output Datasets:**
    *   All downstream datasets in your flow that depend on this input should also be partitioned in the same way. Dataiku will propagate the partitioning automatically when you build the flow.
3.  **Build Your Transformation Flow:** Create your visual or code recipes to transform the data as needed. The recipes will run on a partition-by-partition basis.
4.  **Create an Incremental Load Scenario:**
    *   Go to **Scenarios** and create a new scenario.
5.  **Configure the Build Step:**
    *   Add a **Build / Train** step and select your final output dataset.
    *   In the build step configuration, for the "Build mode", choose **Build required partitions**.
    *   For the "Partitions to build" parameter, enter **LATEST**. This is the key instruction. It tells Dataiku to only look for the latest available partition in the input and build the corresponding partition in the output.
6.  **Schedule the Scenario:**
    *   In the **Settings > Triggers** tab, add a **Time-based** trigger to run the scenario once per day.
    *   Each day, the scenario will wake up, see the new "LATEST" daily partition from your source system, and run your entire flow for *only that day's data*.

### 4. Resources and Tools
- **Partitioning Settings:** The tab on each dataset where you configure how it's partitioned.
- **Scenario Build Step:** Where you specify the \`LATEST\` partition to build.
- **Job Inspector:** You can view the job logs to confirm that the scenario is only processing a single partition, not the entire dataset.

### 5. Next Steps and Progression
- **Handling Late-Arriving Data:** You can configure the scenario to rebuild multiple partitions at once, e.g., by building for a specific date range instead of just "LATEST".
- **Backfilling:** To initially populate the historical partitions, you can run a one-time job from the Flow view and choose to build all partitions for a specific date range.

### 6. Common Challenges and Solutions
- **Challenge:** "The scenario is still building the whole dataset."
- **Solution:** You have misconfigured the build step. Double-check that the "Partitions to build" parameter is set to \`LATEST\` (or another dynamic partition identifier) and not "ALL". Also ensure your input and output datasets are correctly defined as partitioned.
- **Challenge:** "My source system isn't partitioned; it's just one giant table."
- **Solution:** You can still implement an incremental pattern. Use a **SQL recipe** to read the source table. In the SQL, filter for records where the timestamp is greater than the last successful run time (which you can store in a project variable). This mimics partitioning in cases where the source can't be partitioned.
`},{id:227,slug:"migrating-recursive-workflows-into-scenario-loops",question:"How to get started with migrating recursive workflows into scenario loops?",answer:`
### 1. Introduction/Overview
A recursive workflow is one that calls itself until a certain condition is met. This pattern is rare but can be used for tasks like graph traversal or iterative optimization. In Alteryx, this is handled by complex iterative macros. In Dataiku, this advanced logic is implemented using a **Python scenario step** that programmatically calls its own scenario.

### 2. Prerequisites
- **An Alteryx iterative macro that uses recursion.**
- **A deep understanding of the recursion's logic** and, most importantly, its **termination condition**.
- **Advanced Python skills** and familiarity with the Dataiku API.

### 3. Step-by-Step Instructions
1.  **Isolate the Core Logic:** First, build a standard Dataiku flow that performs the work of a *single* iteration of the recursion.
2.  **Parameterize the Flow:** Identify the variables that change between iterations (e.g., the current node in a graph, the iteration number). Make these **Project Variables**. Your flow should use these variables.
3.  **Create a "Controller" Scenario:**
    *   Create a new scenario that will manage the recursion.
    *   Add a single **Execute Python code** step.
4.  **Write the Recursive Script:** The Python script is the heart of the logic.
    *   **Get Current Parameters:** The script should start by reading the current values of the project variables.
    *   **Check Termination Condition:** The first thing inside the script must be to check the condition that stops the recursion. If the condition is met, the script should exit successfully. This is critical to prevent an infinite loop.
    *   **Run One Iteration:** If the condition is not met, the script should run the core logic flow (e.g., \`project.build("output_of_one_iteration")\`).
    *   **Calculate Next Parameters:** After the run, the script should calculate the parameters for the *next* iteration.
    *   **Trigger the Next Iteration:** The final step is for the script to use the Dataiku API to trigger *itself* (the controller scenario), passing in the new parameters for the next iteration.
    > \`\`\`python
    > # WARNING: Advanced pattern, highly simplified example
    > import dataiku
    > client = dataiku.api_client()
    > project = client.get_project("MY_PROJECT")
    > scenario = project.get_scenario("recursive_scenario")
    > # Get current iteration number from a variable
    > current_iter = int(dataiku.get_custom_variables()["iteration_num"])
    > # 1. Check termination condition
    > if current_iter >= 10:
    >     print("Max iterations reached. Stopping.")
    > else:
    >     # 2. Run core logic
    >     project.build("iteration_output")
    >     # 3. Trigger next iteration with updated parameters
    >     scenario.run(params={'variables': {'iteration_num': str(current_iter + 1)}})
    > \`\`\`
### 4. Resources and Tools
- **Python Scenario Steps:** The only place this logic can be implemented.
- **Dataiku Python API:** Specifically, the functions for running scenarios and passing parameters (\`scenario.run(params={...})\`).
- **Careful Logging:** Add extensive \`print()\` statements to your script so you can trace the recursion from the scenario logs.

### 5. Next Steps and Progression
- **State Management:** Ensure that the state between iterations is managed correctly, either by writing to an intermediate dataset or by passing information through scenario parameters.

### 6. Common Challenges and Solutions
- **Challenge:** "I created an infinite loop and my instance is overloaded."
- **Solution:** This is the primary risk of recursion. Your **termination condition** must be absolutely solid. Before deploying, add a simple "safety valve" counter to your script that forces it to stop after a certain number of iterations, no matter what.
- **Challenge:** "This seems overly complex and dangerous."
- **Solution:** It is. This pattern should be used only when absolutely necessary. Before implementing a recursive scenario, challenge the original requirement. Is there a simpler, non-recursive way to solve the problem? Often, a standard \`for\` loop in a single Python recipe is a better and safer alternative.
`},{id:228,slug:"integrating-dataiku-jobs-into-ci-cd-pipelines-using-git-hooks",question:"How to get started with integrating Dataiku jobs into CI/CD pipelines using Git hooks?",answer:`
### 1. Introduction/Overview
Integrating Dataiku into a CI/CD (Continuous Integration/Continuous Deployment) pipeline automates the testing and deployment of your data projects. The integration point is typically a **webhook** (a more modern and flexible version of a Git hook) that triggers your CI/CD tool whenever changes are pushed to your project's Git repository.

### 2. Prerequisites
- **A Dataiku project connected to a Git provider** (like GitHub, GitLab, Azure DevOps).
- **A CI/CD tool** (like Jenkins, GitLab CI, GitHub Actions).
- **A "test" scenario** created in your Dataiku project. This scenario should run your data quality checks and unit tests.

### 3. Step-by-Step Instructions: The CI Workflow
This process describes how to set up the "Continuous Integration" part of CI/CD.

1.  **Configure a Webhook in Your Git Provider:**
    *   In your Git provider's settings for the project repository, find the "Webhooks" section.
    *   Create a new webhook. The **Payload URL** will be the URL provided by your CI/CD tool to trigger a new pipeline run.
    *   Configure the webhook to trigger on a **push** event (or on a **pull request** event).
2.  **Configure Your CI/CD Pipeline:**
    *   In your CI/CD tool (e.g., in a \`Jenkinsfile\` or \`.github/workflows/main.yml\`), define the steps that should happen when the webhook is received.
3.  **CI Pipeline Step 1: Update Dataiku Project:**
    *   The first step in your CI script should make a **REST API call** to Dataiku.
    *   This call tells the Dataiku project to pull the latest changes from the Git branch that triggered the pipeline.
4.  **CI Pipeline Step 2: Run Tests:**
    *   The next step in your script makes another REST API call to **run your "test" scenario** in Dataiku.
    *   Your script must then poll the job status endpoint until the scenario finishes.
5.  **CI Pipeline Step 3: Check Outcome:**
    *   Once the scenario is finished, your script checks its outcome.
    *   If the test scenario in Dataiku was a "SUCCESS", the CI pipeline proceeds.
    *   If it was a "FAILED", the CI script should fail the entire pipeline, which will block the merge (if using pull requests) and notify the developer of the test failure.

### 4. Resources and Tools
- **Webhooks:** The trigger mechanism in your Git provider.
- **CI/CD Tools:** Jenkins, GitLab CI, GitHub Actions, etc.
- **Dataiku REST API:** The interface used by your CI/CD script to control Dataiku.

### 5. Next Steps and Progression
- **Continuous Deployment (CD):** If the CI step passes, you can add CD steps to your pipeline. This involves making further API calls to:
    1.  Create a project bundle (a deployable artifact).
    2.  Deploy the bundle to a UAT or Production Dataiku instance.
- **Status Checks:** In GitHub, you can configure the CI job as a "status check" on a pull request, which will physically prevent merging until all your Dataiku tests pass.

### 6. Common Challenges and Solutions
- **Challenge:** "How does my CI/CD tool securely connect to the Dataiku API?"
- **Solution:** Do not hardcode API keys in your script. All modern CI/CD tools have a secrets management feature. Store the Dataiku API key as a secret variable in your CI/CD tool, which can then be safely injected into the pipeline at runtime.
- **Challenge:** "The webhook isn't triggering my pipeline."
- **Solution:** Check the webhook delivery logs in your Git provider's UI. It will show if the request was successfully delivered to your CI/CD tool and what the response was. This helps debug network or configuration issues.
`},{id:229,slug:"archiving-intermediate-data-per-ingestion-cycle",question:"How to get started with archiving intermediate data per ingestion cycle?",answer:`
### 1. Introduction/Overview
In many data pipelines, you may need to keep a historical archive of intermediate or final datasets for auditing, debugging, or historical analysis. This process can be easily automated in Dataiku using an **Export recipe** within a scheduled scenario.

### 2. Prerequisites
- **A Dataiku flow** that produces an intermediate or final dataset you want to archive.
- **An archive location:** A designated storage location, typically a folder in cloud storage (S3, GCS, ADLS) or a shared file server.
- **A connection** to this archive location configured in Dataiku.

### 3. Step-by-Step Instructions
1.  **Add an Export Recipe to Your Flow:**
    *   At the point in your flow where the data is ready for archiving, select the dataset you want to save.
    *   From the right-hand panel, choose the **Export** recipe.
2.  **Configure the Export Destination:**
    *   In the Export recipe, click **Add Export**.
    *   Select your pre-configured connection to your archive storage (e.g., an S3 connection).
3.  **Create a Dynamic Archive Path:**
    *   This is the key step. You need to ensure each archive has a unique name so it doesn't overwrite the previous one. Use **Dataiku variables** to create a dynamic path.
    *   In the "Path" field for the export, structure it like a folder hierarchy, using date variables. For example:
        > \`/archive/my_report/year=\${current_year}/month=\${current_month}/day=\${current_day}/report.csv\`
    *   This will automatically create a new, dated folder for each day's archive.
4.  **Automate with a Scenario:**
    *   Create a scenario that runs your main data pipeline.
    *   The **final step** of this scenario should be to **build the Export recipe**.
    *   Schedule this scenario to run daily (or at your desired frequency). Each time it runs, it will build the main dataset and then save a dated copy to your archive location.

### 4. Resources and Tools
- **Export Recipe:** The core tool for writing data to external locations.
- **Dataiku Variables:** Essential for creating dynamic and organized archive paths. Common variables include \`\${current_year}\`, \`\${current_month}\`, \`\${current_day}\`.
- **Scenarios:** The automation engine that runs the archiving process on a schedule.

### 5. Next Steps and Progression
- **Automated Cleanup:** Create a separate scenario with a Python step that runs periodically (e.g., monthly). This script can list the files in your archive location and automatically delete archives older than a certain retention period (e.g., older than 1 year).
- **Data Formats:** For large archives, export the data in a compressed, efficient format like **Parquet** instead of CSV to save storage costs.

### 6. Common Challenges and Solutions
- **Challenge:** "The export to my file server is failing with a permissions error."
- **Solution:** The user account that the Dataiku server runs as on the server needs to have write permissions on the target archive folder on the network drive. This often requires working with your IT infrastructure team.
- **Challenge:** "My archive files are being overwritten."
- **Solution:** Your archive path is not dynamic enough. You must include variables that change with each run, like \`\${current_day}\` or even \`\${timestamp}\`, to guarantee a unique path for each ingestion cycle.
`},{id:230,slug:"implementing-alerting-to-replace-alteryx-server-monitoring",question:"How to get started with implementing alerting to replace Alteryx Server monitoring?",answer:`
### 1. Introduction/Overview
When you migrate scheduled jobs from Alteryx Server, you need to replicate its monitoring and alerting capabilities. In Dataiku, this is achieved through **Reporters** within **Scenarios**. You can easily configure scenarios to send detailed alerts on job failure (or success) to email, Slack, and other services.

### 2. Prerequisites
- **A migrated workflow,** now existing as a Dataiku Scenario that automates a job.
- **Admin configuration:** A Dataiku administrator must have configured the connection to your company's email server or Slack workspace.

### 3. Step-by-Step Instructions
1.  **Open Your Scenario:** Navigate to the **Scenarios** page in your project and select the scenario you want to monitor.
2.  **Go to the Reporters Tab:** This tab is the central hub for all alerting and notifications.
3.  **Add a New Reporter:** Click **+ ADD REPORTER**.
4.  **Choose the Channel:** Select your desired notification channel. **Mail** is the most common.
5.  **Configure the Reporter:**
    *   **Run condition:** This is the most important setting. For monitoring, you will almost always set this to **On failure**.
    *   **Recipients:** Enter the email address or distribution list that should receive the alert (e.g., \`dev-team-alerts@mycompany.com\`).
    *   **Message:** Customize the alert message to be as informative as possible. Use Dataiku's built-in variables. A good failure alert message should always include:
        *   Project: \`\${projectKey}\`
        *   Scenario: \`\${scenarioName}\`
        *   Outcome: \`\${outcome}\`
        *   **Link to Logs:** \`\${jobURL}\` (This is critical!)
6.  **Save and Activate:** Save the scenario. Ensure both the reporter and the main scenario are toggled on. The alerting is now live. The next time this scenario fails, it will automatically send the configured notification.

### 4. Resources and Tools
- **Scenarios > Reporters:** The UI for configuring all alerts.
- **Built-in Variables:** The key to creating dynamic and useful alert messages.

### 5. Next Steps and Progression
- **Tiered Alerting:** Create multiple reporters for the same scenario. For example:
    *   A detailed alert **On failure** sent to the development team.
    *   A simple high-level alert **On completion** (both success and failure) sent to a business stakeholder.
- **Data Quality Alerts:** Combine reporters with data quality checks. Add a "Run checks" step to your scenario. If a quality check fails, the scenario fails, and your "On failure" reporter will fire, effectively creating a data quality alert.
- **Custom Alerts via Python:** For highly customized notification logic, you can add a Python step to your scenario that calls external messaging APIs (like PagerDuty or Microsoft Teams) directly.

### 6. Common Challenges and Solutions
- **Challenge:** "We are not receiving any emails."
- **Solution:** First, confirm that a scenario has actually failed. Second, check with your Dataiku administrator that the mail server integration is configured correctly and that the Dataiku server is not being blocked by any firewalls from sending emails.
- **Challenge:** "We get an alert, but it's not useful for debugging."
- **Solution:** Your alert message is missing key context. The most important piece of information is a direct link to the logs. Ensure your message body includes the \`\${jobURL}\` variable. This allows the person receiving the alert to click the link and immediately start troubleshooting.
`},{id:231,slug:"building-data-quality-checks-using-dataiku-metrics-and-checks",question:"How to get started with building data quality checks using Dataiku Metrics and Checks?",answer:`
### 1. Introduction/Overview
Ensuring data quality is a fundamental part of any robust data pipeline. Dataiku provides a powerful, two-part framework for this: **Metrics**, which compute statistics about your data, and **Checks**, which define the rules that these metrics must obey. This allows you to automatically validate your data at any stage of your flow.

### 2. Prerequisites
- **A dataset in your Flow** that you want to validate.
- **A clear definition of your quality rules** (e.g., "This column should never have nulls," "This value must be between 0 and 100").

### 3. Step-by-Step Instructions

#### Part 1: Defining What to Measure (Metrics)
1.  **Open Your Dataset** and go to the **Status** tab.
2.  Click on **Metrics**. This is where you tell Dataiku what to compute.
3.  Click **+ ADD METRIC**. You'll see a library of available metrics. Some of the most useful are:
    *   **Record count:** Counts the total number of rows.
    *   **Column statistics:** Calculates min, max, mean, std. dev for numerical columns.
    *   **Most frequent values:** Calculates value counts for categorical columns.
4.  Select the metric(s) you need and configure them (e.g., select the column for which to compute stats).
5.  Click **SAVE AND COMPUTE**. Dataiku will now calculate these metrics on your dataset.

#### Part 2: Defining the Rules (Checks)
1.  In the **Status** tab, click on **Checks**. This is where you set your pass/fail conditions.
2.  Click **+ ADD CHECK**. You'll see a list of check types.
3.  Configure a check based on your metrics. For example:
    *   **Check:** "Record count is in a numerical range." -> Set the minimum to 1 to ensure the dataset is never empty.
    *   **Check:** "Column has no invalid values." -> Select your \`customer_id\` column to ensure it has no nulls.
    *   **Check:** "Column's maximum is in a numerical range." -> Select your \`price\` column and set the max to 1000 to catch outliers.
4.  **Set Severity:** For each check, you can set the severity to "Warning" or "Error". An "Error" will fail a scenario job.
5.  Save your checks.

#### Part 3: Automating the Validation
1.  In a **Scenario**, after the step that builds your dataset, add a new step of type **Run checks**.
2.  Select your dataset. When the scenario runs, this step will execute all the checks you defined. If any "Error"-level check fails, the scenario itself will fail.

### 4. Resources and Tools
- **Status Tab (Metrics & Checks):** The UI for defining all your quality rules.
- **Run Checks Scenario Step:** The tool for automating the validation process.

### 5. Next Steps and Progression
- **Data Quality Dashboard:** Create a dashboard that visualizes the history of your key metric values over time. This helps you spot trends and degrading data quality.
- **Custom Python Checks:** For very complex business rules that can't be expressed with the built-in checks, you can write your own check logic in Python.

### 6. Common Challenges and Solutions
- **Challenge:** "I want to check a rule, but there's no metric for it."
- **Solution:** You must first define the metric. The checks can only run on data that has been computed by the metrics. If you need a very custom metric, you can compute it in a Python recipe and save the result to another dataset, then run checks on that result.
- **Challenge:** "My job failed, but I don't know which check was violated."
- **Solution:** The scenario log is your friend. The log for the "Run Checks" step will list exactly which check failed and why (e.g., "Check 'customer_id has no nulls' failed: found 15 empty values").
`},{id:232,slug:"validating-migrated-outputs-against-alteryx-run-results",question:"How to get started with validating migrated outputs against Alteryx run results?",answer:`
### 1. Introduction/Overview
After migrating a workflow from Alteryx to Dataiku, you must prove that the new pipeline produces identical results. This validation process involves running both the old and new pipelines on the same input and then using Dataiku's tools to perform a detailed comparison of the two outputs.

### 2. Prerequisites
- **A migrated Dataiku flow** and the original Alteryx workflow.
- **A static, representative set of input data** that both pipelines can use.

### 3. Step-by-Step Instructions
1.  **Run Both Pipelines:**
    *   Execute the legacy Alteryx workflow using the static input data. Save its final output as a CSV or Excel file.
    *   Execute your new Dataiku flow using the same static input data. This will result in a final output dataset in your Flow.
2.  **Import the Alteryx Output into Dataiku:**
    *   In your Dataiku validation project, click **+ DATASET** and upload the output file generated by Alteryx. Let's call this dataset \`alteryx_output\`.
    *   You now have two datasets in your Flow to compare: \`dataiku_output\` and \`alteryx_output\`.
3.  **Perform a High-Level Comparison (Metrics):**
    *   On both datasets, go to the **Status** tab and compute **Metrics**.
    *   Compare the **Record count**. They must be identical.
    *   For key numerical columns, compare the **Sum**, **Average**, and **Standard Deviation**. These should be extremely close (allowing for minor floating-point differences).
4.  **Perform a Detailed Comparison (Join/Group):**
    *   **Method A (Stack and Group):**
        1. Use a **Stack** recipe to combine \`dataiku_output\` and \`alteryx_output\`. Add a new column to identify the source of each row.
        2. Use a **Group** recipe to group by all the columns. The count for each group should be exactly 2. Any count of 1 indicates a row that exists in one output but not the other.
    *   **Method B (Join):**
        1. Use a **Join** recipe to perform a **Full Outer Join** between the two outputs, joining on their primary key.
        2. In the output, filter for rows where the key from either side is null. This will show you records that are in one file but not the other.
5.  **Document the Validation:** Take screenshots of your validation results and add them to a "Validation" page in your project's **Wiki** as proof that the migration was successful.

### 4. Resources and Tools
- **Metrics and Checks:** For high-level statistical comparisons.
- **Stack, Join, and Group Recipes:** The visual tools for performing a detailed, row-by-row comparison.
- **The Project Wiki:** The place to formally document your validation evidence.

### 5. Next Steps and Progression
- **Automated Validation Scenario:** You can automate this entire comparison process in a dedicated "Validation" scenario. This is especially useful if you are running the old and new pipelines in parallel for an extended period.
- **Floating Point Precision:** Be aware that floating-point calculations can have tiny differences between systems. Your validation logic should tolerate these small differences for numerical fields.

### 6. Common Challenges and Solutions
- **Challenge:** "The row counts match, but the column sums do not."
- **Solution:** This means the data is different. The detailed join/group comparison method is now essential. This will help you isolate the specific rows or columns where the values diverge, which will point you to the bug in your new pipeline's logic.
- **Challenge:** "The outputs have different column orders."
- **Solution:** This is usually not a problem, but if you need the order to be identical, you can use a **Prepare** recipe on both datasets to manually set the column order before you compare them.
`},{id:233,slug:"comparing-row‑counts-and-hash-sums-for-equivalence",question:"How to get started with comparing row‑counts and hash sums for equivalence?",answer:`
### 1. Introduction/Overview
When validating a data migration, you need a robust way to prove that two datasets are identical. While comparing row counts is a good first step, it's not enough. A more rigorous method is to calculate a "hash sum" for each row. A hash function converts a row's data into a unique signature; if the signatures match for all rows, you can be highly confident the datasets are equivalent.

### 2. Prerequisites
- **Two datasets to compare** (e.g., \`alteryx_output\` and \`dataiku_output\`) loaded into Dataiku.
- **The datasets should have the same schema** (column names and order).

### 3. Step-by-Step Instructions
1.  **Check Row Counts:** As a quick first check, compute the **Record count** metric on both datasets. If they don't match, you already know there's a problem and don't need to proceed with hashing.
2.  **Create a Hashing Recipe (Python):** The easiest way to generate hash sums is with a **Python recipe**.
    *   Create a Python recipe that takes one of your datasets as input (e.g., \`alteryx_output\`).
    *   In the script, read the data into a Pandas DataFrame.
    *   Apply a function to each row that concatenates all its values into a single string and then calculates a hash (like MD5) of that string.
    > \`\`\`python
    > import dataiku
    > import pandas as pd
    > import hashlib
    > 
    > df = dataiku.Dataset("alteryx_output").get_dataframe()
    > 
    > # Function to create a hash for a row
    > def hash_row(row):
    >     # Concatenate all column values into a single string
    >     row_string = ''.join(str(x) for x in row.values)
    >     # Return the MD5 hash
    >     return hashlib.md5(row_string.encode()).hexdigest()
    > 
    > # Create a new column with the hash for each row
    > df['row_hash'] = df.apply(hash_row, axis=1)
    > 
    > # Output the original data plus the hash
    > dataiku.Dataset("alteryx_output_hashed").write_with_schema(df)
    > \`\`\`
3.  **Repeat for the Second Dataset:** Duplicate this recipe and run it on your \`dataiku_output\` dataset to create \`dataiku_output_hashed\`.
4.  **Compare the Hashes:**
    *   You now have two datasets that include a \`row_hash\` column.
    *   Use a **Join** recipe to do a **Full Outer Join** between them, joining on \`row_hash\`.
    *   The join output should have zero rows where either side of the join key is null. If it does, this indicates a hash that exists in one output but not the other, meaning you've found a row that is different.

### 4. Resources and Tools
- **Python Recipe:** The environment for the hashing logic.
- **Python's \`hashlib\` library:** The standard library for creating cryptographic hashes.
- **Join Recipe:** The tool for the final comparison of the hash values.

### 5. Next Steps and Progression
- **Create a Reusable "Comparer" Flow:** Build a dedicated Flow Zone or project that takes any two datasets as input and automatically runs this hashing and comparison logic, producing a simple "Match" or "No Match" report.

### 6. Common Challenges and Solutions
- **Challenge:** "The hashes don't match, but the data looks the same."
- **Solution:** This is often caused by subtle, invisible differences.
    *   **Data Types:** Ensure a numerical column is not a string in one dataset and an integer in another (\`'123'\` vs \`123\`).
    *   **Whitespace:** Use a Prepare recipe to trim whitespace from all text columns before hashing.
    *   **Floating-point precision:** Tiny differences in float values (\`1.23456\` vs \`1.23457\`) will produce different hashes. You may need to round numerical columns to a consistent number of decimal places before hashing.
`},{id:234,slug:"embedding-sanity-checks-in-flow-logic-to-auto‑validate-transformations",question:"How to get started with embedding sanity checks in flow logic to auto‑validate transformations?",answer:`
### 1. Introduction/Overview
Beyond checking the quality of your source data, it's a good practice to embed "sanity checks" *after* your transformations. This helps you validate that your own recipe logic is working as expected. For example, after a join, you might want to check that the row count hasn't unexpectedly exploded. This can be automated as part of your main pipeline scenario.

### 2. Prerequisites
- **A Dataiku flow with transformation recipes.**
- **A clear idea of the expected outcome** of your transformations.

### 3. Step-by-Step Instructions
1.  **Identify Key Checkpoints:** Look at your flow and identify critical points where a logic error could have a big impact. A common checkpoint is immediately after a complex **Join** or **Group** recipe.
2.  **Define a Metric for the Checkpoint:**
    *   Open the output dataset of your critical recipe (e.g., \`joined_data\`).
    *   Go to the **Status > Metrics** tab.
    *   Define a metric that captures the state you want to check. For example, compute the **Record count**.
3.  **Define a Check for the Logic:**
    *   Go to the **Checks** tab.
    *   Add a check based on your metric that enforces your business logic.
    *   **Example 1 (Post-Join):** "After joining customers to transactions, the number of rows should not be more than the original number of transaction rows." Create a check: **Record count is in numerical range**, and set the maximum value.
    *   **Example 2 (Post-Filter):** "After filtering for premium customers, the row count should be less than the input row count."
4.  **Integrate into Your Main Scenario:**
    *   Open your main "build" scenario.
    *   Find the step that builds your main pipeline.
    *   **Immediately after that build step,** add a new **Run checks** step.
    *   Configure this step to run the checks on your critical intermediate dataset (e.g., \`joined_data\`).
5.  **Run and Monitor:** Now, when your main scenario runs, it will first build the data and then immediately validate the result. If your join logic was faulty and produced too many rows, the check will fail, the scenario will stop, and you will be alerted *before* the bad data propagates downstream.

### 4. Resources and Tools
- **Metrics and Checks:** The core framework for defining the validation rules.
- **Scenarios:** The automation engine where you embed the checks into your pipeline's execution.

### 5. Next Steps and Progression
- **Custom Python Checks:** For very complex sanity checks (e.g., "The sum of sales for each region must match the sum from the source system"), you can write the logic in a **Python check**.
- **Chained Checks:** You can have multiple "Run checks" steps at different points in your scenario to validate the logic at each major stage of your transformation.

### 6. Common Challenges and Solutions
- **Challenge:** "My sanity check is failing, but the logic seems right."
- **Solution:** The check is telling you that your assumption about the data or logic is wrong. This is a good thing! It has caught a potential issue. For example, if your post-join check fails, it probably means your join key is not as unique as you thought it was, and you need to investigate the source data.
- **Challenge:** "Adding checks everywhere seems like it will slow down my pipeline."
- **Solution:** Computing metrics can add a small amount of overhead. Be strategic. You don't need to check every single dataset. Focus on the most critical, high-risk points in your flow, such as after complex joins or at the end of major transformation stages.
`},{id:235,slug:"implementing-anomaly-detection-to-flag-migration-errors",question:"How to get started with implementing anomaly detection to flag migration errors?",answer:`
### 1. Introduction/Overview
Anomaly detection can be a powerful tool during a migration to catch subtle errors that simple validation might miss. The idea is to compare the statistical profile of the output from your new Dataiku flow with the profile of the original Alteryx output. If the profiles are significantly different, it may indicate a hidden error in your migrated logic.

### 2. Prerequisites
- **Output datasets from both the Alteryx and Dataiku pipelines,** generated from the same input data.
- **The two output datasets loaded into Dataiku.**

### 3. Step-by-Step Instructions
1.  **Launch a Statistical Analysis:**
    *   In Dataiku, open the dataset produced by your new flow (\`dataiku_output\`).
    *   Go to the **Statistics** tab.
    *   Click **+ NEW ANALYSIS**.
2.  **Compare to the Legacy Output:**
    *   In the analysis worksheet, you can add a comparison dataset.
    *   Use the option to **Compare with another dataset** and select your \`alteryx_output\` dataset.
3.  **Visually Inspect Distributions:**
    *   Dataiku will now show you side-by-side comparisons of the distributions for each numerical column and the value counts for categorical columns.
    *   Visually scan these comparisons. Are there any drastic differences?
    *   **Example:** If the \`average_price\` column has a mean of 50 in the Alteryx output but a mean of 75 in the Dataiku output, this is a major anomaly that indicates a likely error in your transformation logic.
4.  **Automate with Metrics and Checks:**
    *   To automate this, compute key statistical **Metrics** (like mean, median, standard deviation) on both datasets.
    *   Create a **Python check** or scenario step that reads the metadata for both datasets, compares the metric values, and fails if the percentage difference is above a certain threshold.

### 4. Resources and Tools
- **Statistics Tab:** The visual tool for comparing the distributions of two datasets.
- **Metrics and Checks:** The framework for automating the comparison.
- **Python Recipes/Checks:** For implementing the custom logic to compare metrics and flag anomalies.

### 5. Next Steps and Progression
- **Drift Analysis:** Dataiku's built-in **Drift Analysis** feature (in the model monitoring section) can also be used for this. You can train a simple "drift" model to see if it can distinguish between the Alteryx and Dataiku outputs. A high-performing drift model indicates that the two datasets are significantly different.
- **Focus on Key Metrics:** You don't need to check every single statistic. Focus on the most important business metrics in the dataset.

### 6. Common Challenges and Solutions
- **Challenge:** "The distributions are slightly different. Is that a problem?"
- **Solution:** This requires judgment. Small differences, especially in floating-point numbers, are expected. You are looking for large, obvious discrepancies that indicate a fundamental difference in the processing logic. Set a reasonable tolerance threshold in your automated checks (e.g., fail if the means differ by more than 5%).
- **Challenge:** "This seems too manual."
- **Solution:** The initial visual inspection is manual, but it's a very powerful and quick way to spot major issues. The goal is to then automate the comparison of the most important statistical metrics using a scenario, turning the manual analysis into a repeatable, automated check.
`},{id:236,slug:"setting-up-automated-validation-scenarios-post‑migration",question:"How to get started with setting up automated validation scenarios post‑migration?",answer:`
### 1. Introduction/Overview
After migrating a workflow, you shouldn't just assume it's working correctly. Setting up a dedicated, automated validation scenario provides ongoing assurance that your new Dataiku pipeline is producing the expected results. This scenario can be run after every main pipeline execution to catch any regressions or errors.

### 2. Prerequisites
- **Your main "build" scenario** that runs your migrated pipeline.
- **A set of validation rules** or a "golden" output dataset to compare against.

### 3. Step-by-Step Instructions
1.  **Create a "Validation" Scenario:**
    *   In your project, go to **Scenarios** and create a new scenario. Name it clearly, like \`Validate_Main_Pipeline\`.
2.  **Define the Validation Steps:** This scenario will contain steps that run *after* your main build is complete.
    *   **Step 1: Data Quality Checks.** Add a **Run checks** step. Configure it to run the predefined Metrics and Checks on your final output dataset. This checks for nulls, valid ranges, etc.
    *   **Step 2: Business Logic Checks.** Add another **Run checks** step, this time on a set of custom checks that validate business rules (e.g., "The total sales must equal the sum of regional sales").
    *   **Step 3 (Optional): Comparison Check.** If you are running the old Alteryx pipeline in parallel for a time, add a step that compares the new Dataiku output with the Alteryx output (e.g., using a Python recipe that checks for equivalence).
3.  **Configure Failure Alerts:**
    *   On this validation scenario, go to the **Reporters** tab.
    *   Add a **Mail** reporter that triggers **On failure**. The email should go to the development team, indicating that a validation rule has been broken.
4.  **Chain the Scenarios:**
    *   Go to your **main build scenario**.
    *   In its **Reporters** tab, add a reporter of type **Run another scenario**.
    *   Configure it to trigger your \`Validate_Main_Pipeline\` scenario **On success**.
5.  **Review the Workflow:** Now your orchestration is:
    *   The main scenario runs on a schedule.
    *   If it succeeds in building the data, it automatically triggers the validation scenario.
    *   If the validation scenario finds any issues, it fails and sends an alert.

### 4. Resources and Tools
- **Scenarios:** The core automation engine.
- **Run Checks Step:** The key step for executing your validation rules.
- **Scenario Reporters:** Used to chain scenarios together and to send failure alerts.

### 5. Next Steps and Progression
- **Decoupling:** Keeping the build logic and the validation logic in separate scenarios is a good practice. It makes the system more modular and easier to manage.
- **Advanced Reporting:** The validation scenario could do more than just fail. It could generate a detailed data quality report (e.g., a dashboard or a dataset) and email that as part of its alert.

### 6. Common Challenges and Solutions
- **Challenge:** "My validation scenario is always failing on small, insignificant floating-point differences."
- **Solution:** Your validation checks need to be more robust. When comparing numerical values from different systems, you should build a tolerance into your checks. Instead of checking for exact equality, check if the values are within a small percentage of each other.
- **Challenge:** "Setting this up seems like a lot of work."
- **Solution:** It is an investment in quality and stability. For a non-critical pipeline, simple data quality checks might be enough. But for a mission-critical workflow, this kind of automated, multi-layered validation is a best practice that prevents bad data from reaching business users.
`},{id:237,slug:"generating-lineage-reports-to-trace-migrated-pipelines",question:"How to get started with generating lineage reports to trace migrated pipelines?",answer:`
### 1. Introduction/Overview
Data lineage provides a visual map of how data flows through your system, from source to destination. This is not something you have to generate; Dataiku creates it automatically. Understanding how to access and interpret this lineage is crucial for debugging, impact analysis, and satisfying audit requirements for your migrated pipelines.

### 2. Prerequisites
- **A migrated Dataiku flow** with at least a few connected recipes and datasets.

### 3. Step-by-Step Instructions

#### Method 1: High-Level Flow Lineage
1.  **View the Flow:** The Dataiku Flow itself is a high-level lineage diagram. It shows the relationships between recipes and datasets.
2.  **Use Upstream/Downstream Highlighting:**
    *   To see how a specific dataset was created, right-click on it in the Flow.
    *   Select **View upstream dependencies**.
    *   Dataiku will highlight the entire chain of recipes and datasets that were used to produce it, all the way back to the raw sources.
3.  **View Downstream Dependencies (Impact Analysis):**
    *   To see what would be affected by a change, right-click on a dataset.
    *   Select **View downstream dependencies**.
    *   Dataiku will highlight every recipe, model, and dashboard that depends on this dataset.
4.  **Export for Reports:** You can take a screenshot of the highlighted Flow to include in a documentation or audit report.

#### Method 2: Detailed Column-Level Lineage
1.  **Open a Dataset:** In your Flow, open your final, migrated output dataset.
2.  **Go to the Lineage Tab:** Click on the **Lineage** tab.
3.  **Select a Column:** On the right side of the screen, you'll see the schema of your dataset. Click on one of them.
4.  **Trace the Column's Origin:** The main panel will now display a detailed graph. This graph shows you exactly which columns from which source datasets were used, and which specific transformations were applied, to create the final column you selected.
5.  **Export for Reports:** You can export this detailed column-level graph as an image to include in your documentation.

### 4. Resources and Tools
- **The Flow:** For high-level lineage and impact analysis.
- **The Lineage Tab:** For detailed, column-level tracing.

### 5. Next Steps and Progression
- **Audits and Compliance:** Column-level lineage is extremely powerful for auditors. When they ask "How was this specific value calculated?", you can show them the exact, unambiguous visual proof.
- **Debugging:** When you find a bug in a final output column, the column-level lineage is the fastest way to trace the problem back to the recipe that created it.
- **Understanding Complex Logic:** Use the lineage graph to deconstruct and understand complex data pipelines built by others.

### 6. Common Challenges and Solutions
- **Challenge:** "My column-level lineage seems incomplete or broken."
- **Solution:** This almost always happens when a code recipe (Python/R/SQL) accesses data without formally declaring it as an input. To ensure full lineage, your code must use the Dataiku APIs to read and write data, and all sources must be added as formal inputs to the recipe.
`},{id:238,slug:"versioning-migrated-datasets-and-recipe-logic",question:"How to get started with versioning migrated datasets and recipe logic?",answer:`
### 1. Introduction/Overview
Versioning is the process of tracking and managing changes to your project over time. In Dataiku, this is handled through a powerful combination of **Git integration** for your recipe logic and **partitioning strategies** for your data.

### 2. Prerequisites
- **A migrated Dataiku project.**
- **For code/logic versioning:** Your project must be connected to a Git repository.
- **For data versioning:** Your data must have a time dimension (like a daily timestamp).

### 3. Step-by-Step Instructions

#### Part A: Versioning Your Recipe Logic (Your Code)
1.  **Connect to Git:** In your project's **Settings > Git**, link your project to a remote Git repository (e.g., on GitHub or Azure DevOps).
2.  **Commit Your Changes:** After you modify a recipe (visual or code), go to the **Git** page in your project.
    *   **Stage** your changes.
    *   Write a clear **commit message** explaining what you changed (e.g., "Updated customer join logic to handle new region").
    *   **Commit** and **Push** your changes.
3.  **View History:** The commit history in your Git provider now serves as a complete version history of your recipe logic. You can see who changed what, when, and why. You can also revert to a previous version if needed.

#### Part B: Versioning Your Datasets (Your Data)
1.  **Use Partitioning:** This is the standard Dataiku method for data versioning.
    *   Open your dataset and go to **Settings > Partitioning**.
    *   Activate partitioning based on a date column (e.g., by "Day").
2.  **How it Works:** Your dataset is no longer a single entity but a collection of independent, time-stamped partitions. When your daily job runs, it creates a new partition for that day, and the old partitions remain unchanged.
3.  **Accessing Old Versions:** You can access the data from a specific historical version by simply querying a specific partition (e.g., "show me the data from the 2023-01-15 partition"). This provides a "time travel" capability for your data.

### 4. Resources and Tools
- **Git Integration:** The core feature for versioning all your project's logic and configuration.
- **Partitioning:** The core feature for versioning the data itself.

### 5. Next Steps and Progression
- **Branching and Merging:** Use a proper Git branching strategy (like feature branches) for collaborative development. All changes should be reviewed via Pull Requests before being merged into the main branch.
- **Dataset Snapshots:** For non-partitioned data, you can take a "snapshot" of a dataset. This creates a separate, point-in-time copy, but this is a manual process and less scalable than partitioning.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't want to version the data, just the code."
- **Solution:** That's exactly what the Git integration does. Git only versions the *definition* of your project (your recipes, settings, etc.). It does not store the multi-gigabyte data files from your datasets.
- **Challenge:** "My data doesn't have a date column for partitioning."
- **Solution:** You can partition on a discrete categorical column (like \`region\`). Alternatively, if you just want to keep a few historical versions, you can create a scenario that, before it runs, uses a Python step to make a "backup" copy of the current output dataset with a timestamp in its name.
`},{id:239,slug:"documenting-transformations-for-auditability",question:"How to get started with documenting transformations for auditability?",answer:`
### 1. Introduction/Overview
For regulated industries or any business that needs to trust its data, being able to prove *how* a number was calculated is essential. Documenting your transformations for auditability means creating a clear, understandable trail that explains the purpose and logic of every step in your pipeline. Dataiku's visual nature and built-in documentation features are designed for this.

### 2. Prerequisites
- **A Dataiku data pipeline.**
- **An understanding of what an auditor or reviewer would need to know.**

### 3. Step-by-Step Instructions: A Multi-Layered Documentation Approach

1.  **The High-Level Narrative (The Project Wiki):**
    *   Use the **Project Wiki** to create a "Business Logic" document.
    *   In plain English, describe the overall goal of the pipeline and the key business rules it implements. For example: "This pipeline calculates customer LTV. It defines an active customer as anyone with a purchase in the last 12 months."
    *   This provides context for a non-technical auditor.
2.  **The Implementation View (The Flow):**
    *   **Use Descriptions on Everything:** This is the most important habit. On every single dataset and recipe in your Flow, use the **Description** field in the Summary tab to explain its purpose.
        *   Dataset: "Cleaned customer records, one row per unique customer."
        *   Recipe: "Filters out test users and handles null values in address fields."
    *   These descriptions are visible on hover in the Flow, making the pipeline self-documenting.
3.  **The Detailed Logic View (Inside the Recipe):**
    *   **Visual Recipes:** For a Prepare recipe, you can add a description to each individual transformation step. If you have a complex Formula step, document what it's calculating in its description.
    *   **Code Recipes:** Use code comments (\`#\` or \`--\`) and docstrings to explain complex parts of your Python or SQL code. Any reviewer should be able to understand the code's intent.
4.  **The Ultimate Proof (The Lineage):**
    *   When an auditor questions a specific output, use the **Column-Level Lineage** graph. This provides an unambiguous, visual, and definitive trace of how a specific field was generated, which is the gold standard for auditability.

### 4. Resources and Tools
- **The Project Wiki:** For high-level, narrative documentation.
- **Description Fields:** For contextual, object-level documentation.
- **Code Comments:** For detailed, line-by-line documentation.
- **Lineage Graph:** For visual, undeniable proof of data provenance.

### 5. Next Steps and Progression
- **Automated Documentation Generation:** For formal reporting, you can use the Dataiku API to write a Python script that extracts all the descriptions and metadata from your project and generates a formatted Word or PDF document.
- **Review and Sign-off:** For critical projects, you can implement a process where a business owner must formally review and "sign off" on the documentation in the Wiki before the project is deployed.

### 6. Common Challenges and Solutions
- **Challenge:** "Writing documentation takes too much time."
- **Solution:** It's an investment that pays for itself many times over during an audit or when a new person joins the team. The best time to write that sentence is right after you build it, while the logic is still fresh in your mind.
- **Challenge:** "The documentation is out of date."
- **Solution:** Make updating the documentation part of the change process. When you modify a recipe's logic, take the extra 10 seconds to update its description to match. This should be checked during code reviews.
`},{id:240,slug:"ensuring-governance-by-attaching-metadata-to-flows",question:"How to get started with ensuring governance by attaching metadata to flows?",answer:`
### 1. Introduction/Overview
Good data governance relies on rich metadata. Attaching metadata to your Dataiku flows and datasets allows you to classify, find, and manage your data assets effectively. This process involves using Dataiku's built-in features like tags and custom properties to create a searchable and governable data catalog.

### 2. Prerequisites
- **A Dataiku project.**
- **A basic understanding of your organization's governance needs** (e.g., what information needs to be tracked for each data asset).

### 3. Step-by-Step Instructions

1.  **Define a Metadata Strategy:**
    *   Decide what information you need to capture for each data asset. This is best done by a central governance team.
    *   Common metadata includes: Data Owner, Data Sensitivity, Data Source, Status, and a brief Description.
2.  **Use Tags for Classification:**
    *   Tags are for simple, keyword-based classification. They are perfect for status, source, or sensitivity.
    *   **How:** Open any dataset or recipe, go to the **Summary** tab, and add tags.
    *   **Best Practice:** Create a standard **Tagging Taxonomy** for your organization (e.g., Sensitivity tags are always \`S1-Public\`, \`S2-Internal\`, \`S3-Confidential\`). Document this in a central Wiki.
3.  **Use Descriptions for Context:**
    *   The **Description** field should be used on every object to explain its business purpose in one or two clear sentences. This is the most important piece of metadata for user understanding.
4.  **Use Custom Metadata for Structured Properties:**
    *   When you need to store structured key-value information, use **Custom Metadata**.
    *   **How:** In the Summary tab, click **+ Add metadata**.
    *   **Example:** You could define a key called \`Data_Owner_Email\` and the value would be the email address of the person responsible for that data. This is more structured than a simple tag.
5.  **Use the Data Catalog to Search:**
    *   Once you have metadata attached, you can use Dataiku's central **Data Catalog** to search and filter for datasets across the entire instance based on their tags, descriptions, or custom properties.

### 4. Resources and Tools
- **Tags:** For simple, keyword classification.
- **Descriptions:** For human-readable context.
- **Custom Metadata:** For structured key-value properties.
- **Data Catalog:** The central, searchable repository powered by your metadata.

### 5. Next Steps and Progression
- **Automated Metadata Tagging:** Create a **Python scenario** that runs periodically. The script can use the Dataiku API to scan datasets and automatically apply tags based on rules (e.g., "if a dataset's name contains 'Salesforce', add the tag \`Source:SFDC\`").
- **Governance Reporting:** Write a script that extracts the metadata from all projects and generates a governance report, showing, for example, all datasets that are missing a "Data Owner" property.

### 6. Common Challenges and Solutions
- **Challenge:** "People are not filling in the metadata."
- **Solution:** Governance requires both tools and process. You must make metadata completion a part of your team's "definition of done" for any new development. It should be checked during peer reviews.
- **Challenge:** "Our tags are inconsistent."
- **Solution:** This is why a centrally documented Tagging Taxonomy is crucial. If the standard is defined and communicated, it's much easier to enforce consistency.
`},{id:241,slug:"converting-alteryx-custom-python-tools-into-dataiku-python-recipes",question:"How to get started with converting Alteryx custom Python tools into Dataiku Python recipes?",answer:`
### 1. Introduction/Overview
When migrating from Alteryx, you will often encounter custom logic written in a **Python Tool**. Converting this to Dataiku is a manual process, but it's straightforward. It involves creating a **Dataiku Python recipe** and replacing the Alteryx-specific API calls with their Dataiku equivalents.

### 2. Prerequisites
- **An Alteryx workflow with a Python Tool.**
- **The Python code from within that tool.**
- **Basic knowledge of the Dataiku Python API.**

### 3. Step-by-Step Instructions
1.  **Analyze the Alteryx Python Script:**
    *   Open the Alteryx Python Tool and examine the code. Identify the key sections:
        *   Code that reads data from an Alteryx input anchor (e.g., \`Alteryx.read("#1")\`).
        *   The core transformation logic (usually using Pandas).
        *   Code that writes data to an Alteryx output anchor (e.g., \`Alteryx.write(my_dataframe, 1)\`).
2.  **Create a Dataiku Python Recipe:**
    *   In your Dataiku Flow, create a new **Python recipe**.
    *   Select the Dataiku dataset(s) that correspond to the Alteryx tool's inputs.
    *   Define an output dataset for the results.
3.  **Copy the Core Logic:**
    *   Copy the core data transformation logic (the part of the script that uses Pandas to manipulate the DataFrame) from the Alteryx tool and paste it into the Dataiku recipe.
4.  **Replace the Input Code:**
    *   Delete the Alteryx-specific input code (e.g., \`Alteryx.read(...)\`).
    *   Replace it with the standard Dataiku API code to read the input dataset into a Pandas DataFrame.
        > \`\`\`python
        > import dataiku
        > my_input_df = dataiku.Dataset("your_input_dataset_name").get_dataframe()
        > \`\`\`
5.  **Replace the Output Code:**
    *   Delete the Alteryx-specific output code (e.g., \`Alteryx.write(...)\`).
    *   Replace it with the standard Dataiku API code to write your final DataFrame to the output dataset.
        > \`\`\`python
        > # Assume 'final_df' is your transformed DataFrame
        > output_dataset = dataiku.Dataset("your_output_dataset_name")
        > output_dataset.write_with_schema(final_df)
        > \`\`\`
6.  **Manage Dependencies:** If the Alteryx script used specific Python libraries, ensure those libraries are added to the **Code Environment** being used by your Dataiku recipe.

### 4. Resources and Tools
- **Dataiku Python Recipe:** Your new environment for the code.
- **Dataiku Python API Documentation:** Essential for finding the correct functions for reading and writing data.

### 5. Next Steps and Progression
- **Refactor and Improve:** The migration is a good opportunity to clean up the original script. Can the code be made more readable or efficient? Can you add comments to explain the logic?
- **Parameterize:** If the original script had hardcoded values, replace them with **Project Variables** in Dataiku for better flexibility.

### 6. Common Challenges and Solutions
- **Challenge:** "The script uses a library that's not installed in Dataiku."
- **Solution:** Go to your project's **Settings > Code Envs**, select the environment your recipe is using, and add the missing package to the list of Python packages.
- **Challenge:** "The script is failing with an error I don't understand."
- **Solution:** Use the job log to find the full Python traceback. Debug the script as you would any other Python code. Add \`print()\` statements to check the state of your DataFrame at different points in the script. You can also use a **Jupyter notebook** in Dataiku to test snippets of your code interactively.
`},{id:242,slug:"rewriting-alteryx-sql-statements-within-dataiku-sql-recipes",question:"How to get started with rewriting Alteryx SQL statements within Dataiku SQL recipes?",answer:`
### 1. Introduction/Overview
If your Alteryx workflow uses tools like "Connect In-DB" or "Dynamic Input" to run SQL queries, migrating this logic to Dataiku is often very simple. You can typically copy the SQL directly into a **Dataiku SQL recipe**, which then pushes the query execution down to the database for high performance.

### 2. Prerequisites
- **An Alteryx workflow that executes a SQL statement.**
- **The SQL query code.**
- **A configured connection to the source database** in your Dataiku instance.

### 3. Step-by-Step Instructions
1.  **Identify the SQL in Alteryx:** Open the Alteryx workflow and find the tool that contains the SQL query. Copy the entire SQL statement.
2.  **Set Up Inputs in Dataiku:**
    *   In your Dataiku Flow, create datasets that point to the tables referenced in your SQL query's \`FROM\` and \`JOIN\` clauses.
3.  **Create a Dataiku SQL Recipe:**
    *   In the Flow, click **+ RECIPE > SQL**.
    *   Select the datasets you just created as inputs to the recipe.
    *   Define an output dataset for the query results.
4.  **Paste and Adapt the SQL:**
    *   Paste the SQL query from Alteryx into the Dataiku SQL recipe editor.
    *   **Make one critical change:** You must replace the original table names in the \`FROM\` and \`JOIN\` clauses with the **names of your Dataiku input datasets**. Dataiku will automatically translate these back to the real table names when it executes the query.
    *   **Alteryx SQL:** \`SELECT * FROM "prod_schema"."customers"\`
    *   **Dataiku SQL:** \`SELECT * FROM customers\` (where "customers" is the name of your input dataset).
5.  **Validate and Run:**
    *   Click the **Validate** button. Dataiku will check the syntax against your database.
    *   Use the **Preview** pane to see a sample of the results.
    *   Click **Run** to execute the full query on the database.

### 4. Resources and Tools
- **SQL Recipe:** The primary tool for running push-down SQL queries.
- **Database Connection:** The link between Dataiku and your database.

### 5. Next Steps and Progression
- **Parameterize with Variables:** If your Alteryx SQL was dynamically generated, replicate this by using **Dataiku Project Variables** in your SQL recipe. For example: \`WHERE region = '\${user_region}'\`.
- **Performance Tuning:** Because the query runs directly on your database, you can use your standard database performance tuning tools (like analyzing the query plan) to optimize it.

### 6. Common Challenges and Solutions
- **Challenge:** "The query fails with 'Table not found'."
- **Solution:** This is the most common mistake. You have forgotten to replace the raw database table names (e.g., \`prod.customers\`) with the names of the Dataiku datasets you added as inputs to the recipe.
- **Challenge:** "The SQL dialect is slightly different."
- **Solution:** Dataiku is aware of the specific SQL dialect of your database (e.g., Snowflake vs. PostgreSQL vs. SQL Server). Usually, standard ANSI SQL will work everywhere. If the Alteryx query used a function specific to one database, and you are migrating to another, you may need to find the equivalent function in the new database's dialect.
`},{id:243,slug:"building-reusable-python-functions-for-repeated-logic",question:"How to get started with building reusable Python functions for repeated logic?",answer:`
### 1. Introduction/Overview
As you build more data pipelines, you'll often find yourself writing the same data cleaning or transformation logic repeatedly. The "Don't Repeat Yourself" (DRY) principle is key to efficient development. In Dataiku, the **Project Library** is the perfect place to store reusable Python functions that can be shared across all recipes and notebooks within a project.

### 2. Prerequisites
- **A piece of Python code** (a function or a class) that you find yourself using in multiple recipes.
- **A Dataiku project.**

### 3. Step-by-Step Instructions
1.  **Identify Reusable Code:**
    *   Look through your Python recipes. Do you have a function for standardizing addresses or parsing a specific file format that you've copied into several different recipes? This is a perfect candidate for a reusable function.
2.  **Create a Library Script:**
    *   In your project's top navigation bar, go to **... > Libraries**.
    *   Click **+ ADD A SCRIPT** and give it a descriptive name (e.g., \`my_cleaning_utils.py\`).
3.  **Define Your Function in the Library:**
    *   The library editor will open. Paste or write your reusable function(s) here. Make sure they are well-documented with docstrings.
    > \`\`\`python
    > # In my_cleaning_utils.py
    > def standardize_phone_number(phone_str):
    >     """Removes formatting and adds country code."""
    >     # ... your logic here ...
    >     return cleaned_number
    > \`\`\`
4.  **Import and Use the Function in a Recipe:**
    *   Now, open any Python recipe or notebook in your project.
    *   You can import the function from your library just like any other Python module. The library name becomes the module name.
    > \`\`\`python
    > # In a Python recipe
    > from my_cleaning_utils import standardize_phone_number
    >
    > # Read your data
    > df = ...
    > # Use the reusable function
    > df['clean_phone'] = df['phone_column'].apply(standardize_phone_number)
    > \`\`\`
5.  **Save and Run:** The recipe will now execute using the centralized, reusable function from your project library.

### 4. Resources and Tools
- **Project Library:** The central repository for shared code within a project.
- **Python's \`import\` statement:** The standard Python way to use code from other modules.

### 5. Next Steps and Progression
- **Create a Shared "Library" Project:** For functions that need to be reused across *multiple projects*, you can create a dedicated "Shared Library" project. In your working project, go to **Settings > Dependencies** and add the shared project as a dependency. You can then import its libraries.
- **Unit Testing:** For critical, shared functions, it's a good practice to write unit tests to ensure they work correctly. You can create a test scenario in your library project that runs these tests.

### 6. Common Challenges and Solutions
- **Challenge:** "\`ModuleNotFoundError\`: I can't import my library."
- **Solution:** Double-check the name of your library file and the name you are using in the \`import\` statement. They must match. Also, ensure you have saved the library file. In some cases, you may need to click the "Reload" button in your recipe to make it aware of new library files.
- **Challenge:** "If I change the function in the library, what happens?"
- **Solution:** All recipes that import that function will automatically use the new version the next time they are run. This is the power of reusable code: you can fix a bug or add a feature in one place, and it's instantly available everywhere. This is also why it's important to test your shared functions carefully!
`},{id:244,slug:"creating-custom-plugins-to-simulate-rare-alteryx-functionality",question:"How to get started with creating custom plugins to simulate rare Alteryx functionality?",answer:`
### 1. Introduction/Overview
When migrating from Alteryx, you might encounter a very specific, rare tool or macro that has no direct equivalent in Dataiku's standard recipes. To create a seamless experience for your users, you can encapsulate this logic into a **custom Dataiku Plugin**. A plugin allows you to create your own reusable, visual recipe with a custom UI.

### 2. Prerequisites
- **A piece of logic** (usually Python code) that you want to wrap in a visual component.
- **Dataiku administrator rights** and filesystem access to a "developer" instance of Dataiku.
- **Familiarity with JSON** for defining the UI.

### 3. Step-by-Step Instructions (High-Level)
Developing a plugin is an advanced topic, but the basic process is as follows:

1.  **Enable Dev Mode:** On your developer Dataiku instance, an admin needs to enable "dev mode". This lets you create new plugin folders in the Dataiku installation directory.
2.  **Create the Plugin Folder:** Create a new folder for your plugin (e.g., \`my-custom-tools\`).
3.  **Define the Recipe's UI (\`recipe.json\`):**
    *   Inside your plugin folder, create a subfolder for your recipe.
    *   In this folder, create a file named \`recipe.json\`.
    *   This JSON file defines the recipe's appearance in the UI: its name, icon, the number of inputs and outputs, and any custom fields you want the user to fill in (e.g., text boxes, dropdowns).
4.  **Write the Backend Code (\`recipe.py\`):**
    *   Create a corresponding \`recipe.py\` file.
    *   This Python script contains the logic of your recipe. It will have access to the inputs, outputs, and any custom parameters the user entered in the UI.
    *   The script reads from the inputs, performs the transformation, and writes to the outputs.
5.  **Test and Iterate:**
    *   As you save your changes, you can go to the Dataiku UI to see your new visual recipe appear in the "+ Recipe" menu.
    *   You can test it and debug the Python code just like a normal recipe.
6.  **Package the Plugin:** Once you are done, you can zip the entire plugin folder. This \`.zip\` file is your distributable plugin, which can then be installed on a production Dataiku instance by an administrator.

### 4. Resources and Tools
- **Dataiku Developer Guide:** The official documentation has a detailed, step-by-step tutorial on creating your first plugin. This is an essential resource.
- **Existing Plugins:** The source code for many official Dataiku plugins is on GitHub. Reading the code of an existing plugin is a great way to learn.

### 5. Next Steps and Progression
- **Add more components:** A plugin is not limited to recipes. You can also add custom dataset connectors, processors for the Prepare recipe, and more.
- **Share with the Community:** If you create a plugin that could be useful to others, consider open-sourcing it and adding it to the official plugin store.

### 6. Common Challenges and Solutions
- **Challenge:** "This seems very complex."
- **Solution:** It is an advanced topic. Start with the "hello world" tutorial in the developer guide. Creating a very simple recipe with one input and one output will teach you the basic file structure and concepts.
- **Challenge:** "My JSON file has a syntax error and the recipe isn't showing up."
- **Solution:** JSON is very strict about syntax (e.g., commas, brackets). Use an online JSON validator to check your \`recipe.json\` file for errors.
`},{id:245,slug:"deploying-parameter-driven-saved-models-to-replace-predictive-macros",question:"How to get started with deploying parameter-driven saved models to replace predictive macros?",answer:`
### 1. Introduction/Overview
In Alteryx, a "predictive macro" might be used to score new data. The Dataiku equivalent is a two-step process: first, deploying a trained model as a **Saved Model** in your Flow, and second, using the **Score recipe** to apply it to new data. This approach is more robust as it versions the model and separates training from scoring.

### 2. Prerequisites
- **A trained predictive model** in Dataiku's Visual Analysis Lab.
- **A new dataset** that you want to score with the model.

### 3. Step-by-Step Instructions
1.  **Train and Deploy the Model:**
    *   In the Visual Analysis Lab, after training and evaluating your models, select the best one.
    *   Click the **Deploy** button.
    *   This creates a new "Saved Model" object in your Flow. This is your versioned, production-ready model artifact.
2.  **Use the Score Recipe:**
    *   In your Flow, select the new, unscored dataset.
    *   From the right-hand panel, choose the **Score** recipe.
    *   For the "Model to use", select the Saved Model you just deployed.
3.  **Configure Scoring Parameters:**
    *   The Score recipe has several options. You can choose whether to output just the final prediction, or also the probabilities for each class.
    *   You can also change the probability threshold used to make the classification decision.
4.  **Run and Analyze:**
    *   Run the Score recipe.
    *   The output dataset will be a copy of your input data, with new columns added for the prediction, probabilities, and explanations. This is the equivalent of the output from your Alteryx predictive macro.

### 4. Resources and Tools
- **Visual Analysis Lab:** Where you train and deploy the model.
- **Saved Model:** The versioned model artifact in the Flow.
- **Score Recipe:** The visual recipe for applying a saved model to new data.

### 5. Next Steps and Progression
- **Parameter-Driven Scoring:** If you need to change a parameter of the scoring process (like the probability threshold) dynamically, you can use **Project Variables**. You can create a variable for the threshold and reference it in the Score recipe's settings. A scenario can then change this variable before running the job.
- **Real-Time Scoring:** To replace a macro that scores single records in real-time, you would deploy your Saved Model to the **API Deployer**, which exposes it as a live REST API endpoint.

### 6. Common Challenges and Solutions
- **Challenge:** "The Score recipe failed with a schema mismatch error."
- **Solution:** This is a common error. It means the columns in the new dataset you are trying to score do not exactly match the columns the model was trained on. You must apply the *exact same* data preparation pipeline (e.g., the same Prepare recipe) to your new data as you did to your original training data before you can score it.
- **Challenge:** "How do I update the model?"
- **Solution:** Go back to the Visual Analysis Lab, retrain your model with new data, and deploy the new version to the *same* Saved Model object. This will create a new version of the model. All downstream Score recipes will automatically start using this new, active version.
`},{id:246,slug:"embedding-third-party-library-logic-for-specialized-data-transforms",question:"How to get started with embedding third-party library logic for specialized data transforms?",answer:`
### 1. Introduction/Overview
Dataiku's power is extensible. When you need a specialized transformation that isn't available in the visual recipes, you can use a **Python recipe** to leverage the vast ecosystem of third-party Python libraries. This allows you to integrate almost any specialized logic directly into your Dataiku flow.

### 2. Prerequisites
- **A specific transformation need** that requires a third-party library (e.g., complex financial calculations with \`numpy_financial\`, or advanced statistical modeling with \`statsmodels\`).
- **A Dataiku project.**
- **Admin rights or collaboration with an admin** to manage code environments.

### 3. Step-by-Step Instructions
1.  **Find the Library:** Identify the Python library you need from a repository like PyPI (the Python Package Index).
2.  **Create a Code Environment:**
    *   In Dataiku, go to **Administration > Code Envs**.
    *   Click **+ NEW PYTHON ENV**.
    *   Give it a name (e.g., \`financial-modeling-env\`).
3.  **Add the Library as a Dependency:**
    *   In the code environment's settings, go to the "Packages to install" section.
    *   Click "Add" and type the name of the library you need (e.g., \`numpy-financial\`). You can specify an exact version for reproducibility.
    *   Click **Save and Update**. Dataiku will install the library into this isolated environment.
4.  **Use the Environment in a Recipe:**
    *   Go to your project. Create a new **Python recipe**.
    *   In the recipe's **Advanced** settings, find the "Code Env" dropdown.
    *   Select the new environment you just created (\`financial-modeling-env\`).
5.  **Write Your Code:**
    *   In the Python recipe editor, you can now \`import\` and use the third-party library as you would in any standard Python script.
    > \`\`\`python
    > import dataiku
    > import numpy_financial as npf
    >
    > df = dataiku.Dataset("loans").get_dataframe()
    > # Use a function from the third-party library
    > df['monthly_payment'] = npf.pmt(rate=df['rate']/12, nper=df['term'], pv=df['principal'])
    > 
    > dataiku.Dataset("loans_with_payments").write_with_schema(df)
    > \`\`\`

### 4. Resources and Tools
- **Code Environments:** The core feature for managing external dependencies in an isolated and reproducible way.
- **Python Recipe:** The place where you write your code to use the library.
- **PyPI (pypi.org):** The public repository where you can find and learn about third-party Python packages.

### 5. Next Steps and Progression
- **Share Environments:** A well-configured code environment can be used by multiple projects, ensuring that all teams are using the same versions of key libraries.
- **Offline Installation:** For air-gapped environments, a Dataiku administrator can download the Python packages manually and place them where Dataiku can install them without needing internet access.

### 6. Common Challenges and Solutions
- **Challenge:** "\`ModuleNotFoundError\`"
- **Solution:** This is the classic error. It means either: a) you forgot to add the library to your code environment, or b) your recipe is not set to use that code environment. Double-check the recipe's "Advanced" settings.
- **Challenge:** "The library installation fails with a dependency conflict."
- **Solution:** This can be tricky. Two libraries might require different versions of a shared underlying dependency. You may need to experiment with the versions in your code environment's package list until you find a combination that works. This is a key reason to create separate environments for different tasks.
`},{id:247,slug:"scripting-api-calls-within-dataiku-to-external-services",question:"How to get started with scripting API calls within Dataiku to external services?",answer:`
### 1. Introduction/Overview
Dataiku can serve as a central hub for data operations, which often includes interacting with external services via their REST APIs. Using a **Python recipe**, you can easily script API calls to fetch data from, or send data to, virtually any modern web service.

### 2. Prerequisites
- **The API documentation for the external service,** including the endpoint URL, required parameters, and authentication method.
- **An API key or other credentials** for the external service.
- **A Dataiku code environment** with the \`requests\` library installed (it's included in most standard environments).

### 3. Step-by-Step Instructions
1.  **Store Your Credentials Securely:**
    *   **Never hardcode API keys in your script.**
    *   Go to your project's **... > Variables**.
    *   Create a new variable (e.g., \`EXTERNAL_API_KEY\`) and set its type to **Password**. This will store it securely.
2.  **Create a Python Recipe:** Create a new **Python recipe**. If you are just fetching data, it might not have an input dataset.
3.  **Write the API Call Script:**
    *   **Import libraries:** \`import dataiku\`, \`import requests\`, \`import json\`.
    *   **Get your API key:** Retrieve the key from your project variables.
    *   **Make the request:** Use the \`requests\` library to make the API call. Set up headers for authentication.
    *   **Handle the response:** Check the response status code to ensure the call was successful. Parse the JSON response.
    *   **Write the output:** Convert the parsed JSON data into a Pandas DataFrame and write it to the recipe's output dataset.
    > \`\`\`python
    > import dataiku
    > import requests
    > import pandas as pd
    >
    > # 1. Get credentials
    > api_key = dataiku.get_custom_variables()["EXTERNAL_API_KEY"]
    > headers = {'Authorization': f'Bearer {api_key}'}
    >
    > # 2. Make request
    > response = requests.get("https://api.external.service/v1/data", headers=headers)
    > response.raise_for_status() # This will raise an error if the call failed
    >
    > # 3. Parse and write output
    > data = response.json()
    > df = pd.DataFrame(data['results'])
    > dataiku.Dataset("external_service_data").write_with_schema(df)
    > \`\`\`
### 4. Resources and Tools
- **Python Recipe:** Your environment for scripting.
- **\`requests\` library:** The standard and best library for making HTTP requests in Python.
- **Project Variables:** The secure way to handle API keys and other secrets.

### 5. Next Steps and Progression
- **Error Handling:** Wrap your API call in a \`try...except\` block to gracefully handle network errors or cases where the API returns an error status code.
- **Pagination:** Many APIs return data in pages. Your script will need to be more complex, looping through the pages by calling the "next page" URL provided in the API response until all data is retrieved.
- **Rate Limiting:** Be respectful of the external API's rate limits. If you are making many calls in a loop, add a \`time.sleep()\` between requests to avoid being blocked.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm getting a 401/403 Authentication Error."
- **Solution:** Double-check your API key and the authentication method required by the external service. Read their API documentation carefully. Are you using the correct header name? Is it "Bearer" or "Basic" authentication?
- **Challenge:** "The server I'm calling doesn't have a valid SSL certificate."
- **Solution:** By default, \`requests\` will raise an error. You can bypass this for testing by adding \`verify=False\` to your request, but this is insecure. The correct long-term solution is for the external service to fix its certificate.
`},{id:248,slug:"looping-datasets-via-python-to-simulate-alteryx-iterative-actions",question:"How to get started with looping datasets via Python to simulate Alteryx iterative actions?",answer:`
### 1. Introduction/Overview
While Dataiku's primary execution model is a directed graph (Flow), you can perform iterative, loop-based operations using a **Python recipe**. This is the common pattern for replacing Alteryx's iterative or batch macros, where you need to perform the same action for a list of different inputs.

### 2. Prerequisites
- **A "control" dataset:** A dataset that contains the list of items you want to loop over (e.g., a list of country codes, filenames, or customer segments).
- **The core transformation logic** that needs to be repeated for each item.
- **Intermediate Python and Pandas skills.**

### 3. Step-by-Step Instructions
1.  **Set up the Recipe Inputs:**
    *   Create a new **Python recipe**.
    *   It should have at least two inputs:
        1.  Your "control" dataset (e.g., \`list_of_countries\`).
        2.  The main data dataset that you want to process (e.g., \`all_sales_data\`).
2.  **Write the Loop in Python:**
    *   Read both input datasets into Pandas DataFrames.
    *   Get the list of items to iterate over from your control DataFrame.
    *   Create an empty list to store the results from each iteration.
    *   Use a \`for\` loop to iterate through your list of items.
    *   **Inside the loop:**
        1.  Filter your main DataFrame for the current item.
        2.  Apply your core transformation logic to this filtered subset.
        3.  Append the resulting DataFrame to your list of results.
    *   **After the loop:** Use \`pd.concat()\` to combine all the results from the list into a single, final DataFrame.
3.  **Write the Output:** Write the final, concatenated DataFrame to the recipe's output dataset.

#### Example Code:
> \`\`\`python
> import dataiku
> import pandas as pd
> 
> # Read inputs
> control_df = dataiku.Dataset("list_of_countries").get_dataframe()
> main_df = dataiku.Dataset("all_sales_data").get_dataframe()
> 
> countries_to_process = control_df['country_code'].tolist()
> all_results = []
> 
> # Loop
> for country in countries_to_process:
>     # Filter for the current item
>     subset_df = main_df[main_df['country'] == country]
>     # Apply core logic
>     # (e.g., calculate a country-specific tax)
>     processed_subset = apply_tax_logic(subset_df) 
>     all_results.append(processed_subset)
> 
> # Concatenate results
> final_df = pd.concat(all_results)
> 
> # Write output
> dataiku.Dataset("sales_with_tax").write_with_schema(final_df)
> \`\`\`

### 4. Resources and Tools
- **Python Recipe:** The environment for all your looping code.
- **Pandas library:** Essential for the filtering and data manipulation inside the loop.

### 5. Next Steps and Progression
- **Performance:** For very large datasets, this row-by-row or group-by-group processing in Python can be slow. A more performant approach would be to use a **Group** recipe or a **Window** recipe if the logic can be expressed visually. If it requires code, a **PySpark recipe** using Pandas UDFs on grouped data would be the scalable solution.
- **Scenario-based Looping:** For cases where the "core logic" is an entire Dataiku Flow, not just a Python function, you should use the **Scenario Looping** pattern instead (see the relevant question for details).

### 6. Common Challenges and Solutions
- **Challenge:** "My loop is very slow."
- **Solution:** You are processing a large dataset in-memory. Profile your code. Can the \`apply_tax_logic\` function be optimized? Is there a vectorized Pandas operation you could use instead of a loop? Ultimately, for large-scale iteration, you need to move to a distributed engine like Spark.
- **Challenge:** "I'm running out of memory."
- **Solution:** The list of result DataFrames (\`all_results\`) is consuming too much memory. You may need to process the data in chunks, writing intermediate results to disk and then combining them at the end.
`},{id:249,slug:"building-audit-logs-within-recipes-for-traceability",question:"How to get started with building audit logs within recipes for traceability?",answer:`
### 1. Introduction/Overview
While Dataiku automatically logs all job executions, sometimes you need to create a custom, business-focused audit log that tracks key metrics about your pipeline run over time. This can be easily achieved by having your recipes write summary statistics to a dedicated "log" dataset.

### 2. Prerequisites
- **A data pipeline** you want to audit.
- **An idea of what you want to log** (e.g., input row count, output row count, sum of a key financial column).

### 3. Step-by-Step Instructions
1.  **Create the Log Dataset:**
    *   In your Flow, create a new, empty dataset called \`pipeline_audit_log\`.
    *   Define its schema manually. It should include columns like: \`run_timestamp\`, \`input_row_count\`, \`output_row_count\`, \`sum_of_sales\`, \`run_status\`.
2.  **Modify Your Core Recipe (e.g., a Python recipe):**
    *   Open the main transformation recipe in your pipeline.
3.  **Gather Audit Metrics:**
    *   At the beginning of your script, get the row count of your input DataFrame.
    *   At the end of your script, after all transformations, get the row count and other key metrics (like the sum of a column) from your final DataFrame.
4.  **Write to the Log Dataset:**
    *   At the very end of your script, get a handle on your \`pipeline_audit_log\` dataset.
    *   Construct a new DataFrame containing a single row with all the audit metrics you just gathered.
    *   **Crucially, write this new row in "Append" mode.** This will add a new entry to your log each time the recipe runs, without overwriting the old entries.
    > \`\`\`python
    > # In your main Python recipe...
    > # ... your transformation logic here ...
    > 
    > # After creating final_df:
    > try:
    >     log_data = {
    >         'run_timestamp': [pd.Timestamp.now()],
    >         'input_row_count': [initial_row_count],
    >         'output_row_count': [final_df.shape[0]],
    >         'sum_of_sales': [final_df['sales_amount'].sum()],
    >         'run_status': ['SUCCESS']
    >     }
    >     log_df = pd.DataFrame(log_data)
    >     
    >     # Write in append mode
    >     log_dataset = dataiku.Dataset("pipeline_audit_log")
    >     log_dataset.write_with_schema(log_df, append=True)
    > except Exception as e:
    >     # Log the failure
    >     # ... similar logic to log a 'FAILURE' status ...
    >     raise e # Re-raise the original error
    > \`\`\`
5.  **Analyze the Audit Log:** Your \`pipeline_audit_log\` dataset now contains a historical record of every run. You can build charts and dashboards on it to monitor trends in your pipeline's inputs and outputs over time.

### 4. Resources and Tools
- **Python Recipe:** The ideal place to gather and write the log data.
- **Append Mode:** The key setting in the \`write_with_schema\` function that allows you to build the log over time.

### 5. Next Steps and Progression
- **Error Logging:** Enhance the \`try...except\` block to also log failures, capturing the error message in a dedicated column in your audit log.
- **Log Analysis Dashboard:** Create a Dataiku dashboard that visualizes your audit log. A line chart of the input and output row counts over time can be a powerful way to spot anomalies.

### 6. Common Challenges and Solutions
- **Challenge:** "My log dataset is being overwritten every time."
- **Solution:** You have forgotten to set the write mode to append. The default mode is to overwrite. You must use \`output_dataset.write_with_schema(df, append=True)\`.
- **Challenge:** "The schema of the log doesn't match."
- **Solution:** When appending, the DataFrame you are writing must have the exact same schema (column names and order) as the target log dataset. Ensure your logging dictionary is constructed correctly.
`},{id:250,slug:"using-global-variables-and-project-parameters-for-dynamic-logic",question:"How to get started with using global variables and project parameters for dynamic logic?",answer:`
### 1. Introduction/Overview
Hardcoding values (like a tax rate, a file path, or a date threshold) directly into your recipes makes your pipelines rigid and hard to maintain. **Project Variables** are Dataiku's solution, acting as global parameters for your project that can be easily changed in one central place and used in any recipe.

### 2. Prerequisites
- **A Dataiku Project.**
- **A value in your logic that you might want to change** without editing the recipe itself.

### 3. Step-by-Step Instructions
1.  **Create a Project Variable:**
    *   In your project's top navigation bar, go to **... > Variables**.
    *   Click **Edit Variables**.
    *   Click **+ ADD VARIABLE**.
    *   Give your variable a unique \`name\` (e.g., \`vat_rate\`) and a \`value\` (e.g., \`0.20\`).
    *   Click **SAVE**.
2.  **Use the Variable in a Visual Recipe:**
    *   Open a recipe, such as a **Prepare** recipe with a **Formula** step.
    *   In any expression field, you can reference your variable using the syntax \`\${variable_name}\`.
    *   **Example:** In a Formula processor, to create a \`price_incl_vat\` column, the expression would be: \`price * (1 + \${vat_rate})\`
3.  **Use the Variable in a Code Recipe (Python):**
    *   In a **Python** or **SQL** recipe, you can also use the same \`\${variable_name}\` syntax. Dataiku will substitute the value before executing the code.
    *   **Python Example:** \`THRESHOLD = \${my_threshold}\`
    *   **SQL Example:** \`WHERE order_date > '\${start_date}'\`
    *   Alternatively, in Python, you can fetch all variables as a dictionary, which is useful when you need to use many variables.
    > \`\`\`python
    > import dataiku
    > variables = dataiku.get_custom_variables()
    > threshold = float(variables.get('my_threshold'))
    > \`\`\`
4.  **Change the Value:** Now, to change the tax rate across your entire project, you don't need to edit any recipes. You simply go back to the **Variables** page and change the value of the \`vat_rate\` variable once.

### 4. Resources and Tools
- **Project Variables Page:** The central management hub for all your parameters.
- **The \`\${...}\` syntax:** The standard way to reference variables.

### 5. Next Steps and Progression
- **Environment-Specific Variables:** This is a primary use case. You can have a project variable for a database name, \`\${db_name}\`. In your "dev" project, its value might be \`dev_db\`, but in the "prod" project, its value would be \`prod_db\`. This allows you to promote your project without changing any recipe code.
- **Password Management:** When creating a variable, set its type to "Password". This will store it securely and hide its value in the UI. This is the correct way to handle API keys and other secrets.
- **Scenario Overrides:** You can override the value of a variable for a specific scenario run, which is powerful for automation.

### 6. Common Challenges and Solutions
- **Challenge:** "My variable isn't being substituted."
- **Solution:** Check for typos in the variable name inside the \`\${...}\`. The name is case-sensitive. Also, ensure you have saved the variables after creating or editing them.
- **Challenge:** "I'm getting a type error in my Python script."
- **Solution:** When using the \`\${...}\` syntax, Dataiku performs a simple text substitution. If your variable \`my_threshold\` has a value of \`50\`, the code becomes \`THRESHOLD = 50\`. If you need it as a string, you would need quotes: \`THRESHOLD = '\${my_threshold}'\`. When using the \`get_custom_variables()\` dictionary method, all values are returned as strings, so you may need to cast them (e.g., \`float(variable_value)\`).
`},{id:251,slug:"connecting-dataiku-to-cloud-data-warehouses-replacing-legacy-outputs",question:"How to get started with connecting Dataiku to cloud data warehouses replacing legacy outputs?",answer:`
### 1. Introduction/Overview
A common goal of modernizing a data stack is to move away from legacy file-based outputs and centralize data in a powerful cloud data warehouse (like Snowflake, BigQuery, or Redshift). Dataiku acts as the perfect ETL/ELT tool for this, allowing you to prepare your data and then load it into your new cloud warehouse.

### 2. Prerequisites
- **A cloud data warehouse account** with connection details (URL, credentials).
- **A user/role** in the warehouse with permissions to create tables and write data.
- **Dataiku administrator rights** to set up the new connection.

### 3. Step-by-Step Instructions
1.  **Configure the Connection in Dataiku (Admin Task):**
    *   Navigate to **Administration > Connections**.
    *   Click **+ NEW CONNECTION** and select your data warehouse type (e.g., **Snowflake**).
    *   Enter the connection details (account URL, username, password, default warehouse/database).
    *   **Test** the connection to ensure credentials and network access are correct, then **Create** it.
2.  **Select Your Final Dataiku Dataset:**
    *   In your Dataiku Flow, navigate to the final, cleaned, and transformed dataset that you want to load into the warehouse.
3.  **Create an Export Recipe:**
    *   With your final dataset selected, choose the **Export** recipe from the right-hand panel.
4.  **Configure the Export Target:**
    *   In the Export recipe, click **Add Export**.
    *   From the "Store into" dropdown, select your newly configured **Snowflake** (or other warehouse) connection.
    *   **Table Name:** Give the new table a name (e.g., \`sales_report_final\`).
    *   **Write Mode:** Choose **Overwrite**. This will drop and recreate the table each time the job runs, ensuring the data is fresh.
5.  **Run the Export:** Execute the Export recipe. Dataiku will now connect to your cloud warehouse, create the new table, and load the data from your Dataiku dataset into it. Your legacy file output has now been replaced.

### 4. Resources and Tools
- **Dataiku Connections:** The central place to manage connections to all external systems.
- **Export Recipe:** The primary tool for loading data *from* Dataiku *into* an external system.

### 5. Next Steps and Progression
- **Automate the Load:** Add this Export recipe as the final step in your main project **Scenario** and schedule it to run daily. This creates an automated pipeline that continuously updates the table in your cloud data warehouse.
- **Push-down Execution:** For maximum efficiency, ensure that the recipes *before* the export step are also running directly on the data warehouse by setting their execution engine to "Run on database".

### 6. Common Challenges and Solutions
- **Challenge:** "The Export recipe fails with a permissions error."
- **Solution:** The database user account that Dataiku is using for the connection does not have the necessary privileges. It needs permissions like \`CREATE TABLE\`, \`INSERT\`, and \`DROP TABLE\` in the target database schema. Work with your database administrator to grant these permissions.
- **Challenge:** "The data load is very slow."
- **Solution:** Cloud data warehouses are optimized for bulk loading. Check the advanced settings of the Export recipe. There are often performance-tuning options specific to the warehouse, such as using an intermediate staging area in cloud storage (e.g., S3) for faster bulk loads.
`},{id:252,slug:"moving-file‑based-etl-feeds-into-managed-dataiku-connections",question:"How to get started with moving file‑based ETL feeds into managed Dataiku connections?",answer:`
### 1. Introduction/Overview
Legacy ETL processes often rely on dropping files (CSVs, etc.) into specific folders on a network drive. Migrating this to Dataiku is an opportunity to create a more robust and governed process by using **Managed Connections**. Instead of hardcoding file paths, you define a formal connection to the file source (like an S3 bucket or SFTP server).

### 2. Prerequisites
- **An existing process that drops flat files** into a folder.
- **Access credentials** for the source filesystem (e.g., SFTP username/password, AWS S3 keys).
- **Dataiku administrator rights** to create the connection.

### 3. Step-by-Step Instructions
1.  **Identify the Source Filesystem:** Determine where the source files are located. Is it a shared network drive (filesystem), an SFTP server, or a cloud storage bucket (S3, GCS, ADLS)?
2.  **Create a Managed Connection (Admin Task):**
    *   Go to **Administration > Connections**.
    *   Click **+ NEW CONNECTION** and select the appropriate type (e.g., **SFTP**, **Amazon S3**).
    *   Enter the connection details (server address, credentials, bucket name, etc.).
    *   **Test** and **Create** the connection.
3.  **Create a New Dataset Using the Connection:**
    *   In your Dataiku project, click **+ DATASET**.
    *   Select the connection type you just created (e.g., **Amazon S3**).
    *   Dataiku will now use the managed connection to allow you to browse the remote filesystem.
4.  **Point to the File or Folder:**
    *   Navigate to the correct folder and select the specific file for your ETL feed.
    *   Alternatively, if new files are dropped into a folder each day, you can point the dataset to the *folder* instead of a single file. Dataiku will then treat all the files in that folder as a single dataset.
5.  **Configure and Create:** Preview the data, confirm the format settings, and click **CREATE**. Your file-based feed is now a properly managed and governed Dataiku dataset.

### 4. Resources and Tools
- **Dataiku Connections:** The feature for creating managed, reusable connections to external systems.
- **Filesystem-based Dataset Connectors:** (SFTP, S3, Filesystem, etc.).

### 5. Next Steps and Progression
- **Partitioning:** If the files are dropped into dated subfolders (e.g., \`/YYYY/MM/DD/\`), you can enable **partitioning** on the dataset. This allows for highly efficient incremental processing.
- **Automation:** Create a **Scenario** with a "Dataset change" trigger. The scenario will automatically run your pipeline whenever a new file is detected in the source folder.

### 6. Common Challenges and Solutions
- **Challenge:** "Dataiku can't connect to our internal network drive."
- **Solution:** The user account that the Dataiku server runs as on the server needs to have read permissions on that specific network folder. You may also need to work with your IT team to ensure the network share is correctly mounted on the Dataiku server machine. This can be complex and often requires infrastructure support.
- **Challenge:** "We receive multiple files with slightly different schemas."
- **Solution:** The "Stack" recipe in Dataiku has advanced options for schema reconciliation. You can create a dataset for each file and then use a Stack recipe to combine them, manually mapping columns that have different names and resolving type mismatches.
`},{id:253,slug:"managing-secrets-to-authenticate-legacy-data-sources",question:"How to get started with managing secrets to authenticate legacy data sources?",answer:`
### 1. Introduction/Overview
Properly managing secrets like passwords, API keys, and tokens is critical for security. Hardcoding them into recipes is a major security risk. Dataiku provides several secure mechanisms for handling these credentials, from built-in storage to integration with dedicated secrets management tools.

### 2. Prerequisites
- **Credentials** for a legacy data source (e.g., a database username and password).
- **Administrator or project admin rights** in Dataiku.

### 3. Step-by-Step Instructions

#### Method 1: Using Connection Settings (Best for DBs)
1.  **When to Use:** When setting up a standard database, SFTP, or cloud storage connection.
2.  **How:**
    *   An administrator goes to **Administration > Connections**.
    *   When creating the new connection, the UI will provide specific fields for the **Username** and **Password**.
    *   Enter the credentials here. Dataiku encrypts and stores them securely. Users of the connection do not need to know the actual password.

#### Method 2: Using Project Variables (Best for APIs)
1.  **When to Use:** For secrets used in code recipes, like an API key.
2.  **How:**
    *   In your project, go to **... > Variables**.
    *   Click **Edit** and **+ ADD VARIABLE**.
    *   Give the variable a name (e.g., \`MY_API_KEY\`).
    *   **Crucially, change its type to "Password"**. This will hide the value in the UI and prevent it from being exposed in logs.
    *   In your Python recipe, retrieve the secret using the Dataiku API:
        > \`api_key = dataiku.get_custom_variables()["MY_API_KEY"]\`

#### Method 3: Using a Secrets Management Vault (Most Secure)
1.  **When to Use:** For enterprise environments with the highest security requirements.
2.  **How (Admin Task):**
    *   An administrator integrates Dataiku with an external secrets vault like **HashiCorp Vault** or **Azure Key Vault**. This is configured in **Administration > Settings**.
3.  **How (User Task):**
    *   When setting up a connection or a project variable, you can now choose to fetch the value from the configured vault instead of entering it directly. Dataiku will retrieve the secret dynamically at runtime.

### 4. Resources and Tools
- **Dataiku Connections UI:** For storing credentials for standard data sources.
- **Project Variables (Password type):** For storing secrets used in code.
- **External Secrets Management Tools:** For the most secure, enterprise-grade secret management.

### 5. Next Steps and Progression
- **Credential Im-personation:** For some database connections, you can configure it so that Dataiku uses the credentials of the *user running the job*, rather than a single shared service account. This is excellent for fine-grained auditing.

### 6. Common Challenges and Solutions
- **Challenge:** "A developer has left the company, and we need to rotate the passwords they used."
- **Solution:** This is why using shared service accounts and storing credentials in the central Dataiku Connections is a best practice. You only need to update the password in one place (the connection settings), and all projects that use that connection are automatically updated. You don't have to hunt through dozens of recipes to find a hardcoded password.
- **Challenge:** "I can see the password when I click 'Edit' on the connection."
- **Solution:** Access to the **Administration > Connections** page should be restricted to a very small number of trusted administrators. Regular users of a connection cannot see the password.
`},{id:254,slug:"pulling-data-from-mdm-platforms-e-g-informatica-talend-into-dataiku",question:"How to get started with pulling data from MDM platforms (e.g. Informatica, Talend) into Dataiku?",answer:`
### 1. Introduction/Overview
Master Data Management (MDM) platforms are the authoritative source for key business entities like "Customer" or "Product". Integrating this master data into Dataiku is crucial for ensuring your analytics are based on a consistent, "golden record". This is typically done by connecting to the MDM platform's underlying database.

### 2. Prerequisites
- **Access to the MDM system.**
- **Connection details for the MDM's database.** MDM tools almost always store their master data in a standard SQL database (like Oracle or SQL Server). You will need the hostname, port, database name, and credentials for a read-only user.
- **Dataiku administrator rights** to configure the new connection.

### 3. Step-by-Step Instructions
1.  **Get Database Connection Details:**
    *   Work with the team that manages your MDM platform. Ask them for read-only credentials to the database that holds the master data tables (often called the "Operational Reference Store" or ORS).
2.  **Configure the Database Connection in Dataiku (Admin Task):**
    *   Go to **Administration > Connections**.
    *   Click **+ NEW CONNECTION** and select the correct database type (e.g., **Oracle**, **SQL Server**).
    *   Enter the connection details and credentials you obtained in step 1.
    *   **Test** the connection to ensure it's working, then **Create** it.
3.  **Import the Master Data as a Dataset:**
    *   In your Dataiku project, click **+ DATASET**.
    *   Select the database connection you just created.
    *   Browse the schemas and tables. Find the key master data tables (e.g., \`C_PARTY\` for customers, \`C_PRODUCT\` for products).
    *   Select the table you need and click **Create**.
4.  **Use the Master Data:** You now have a Dataiku dataset that represents your "golden records". You can join your transactional data (like sales) with this master customer data to enrich it with authoritative information.

### 4. Resources and Tools
- **Database Connectors:** The standard Dataiku connectors for Oracle, SQL Server, etc.
- **The Join Recipe:** The primary tool for enriching your other datasets with the master data.

### 5. Next Steps and Progression
- **Create a "Golden" Project:** Create a dedicated Dataiku project called "Shared Master Data" or similar. Import all your key MDM tables into this project. This project can then be shared with other teams, providing a central, governed place for everyone to access master data.
- **Automation:** Create a scenario that periodically rebuilds your MDM datasets in Dataiku to ensure they are in sync with any updates in the MDM platform.

### 6. Common Challenges and Solutions
- **Challenge:** "The MDM database schema is incredibly complex."
- **Solution:** This is very common. MDM schemas can have hundreds of tables. Work with the MDM team or a business analyst to understand the data model and identify the specific views or tables that contain the clean, consolidated "best version of the truth" records. You rarely want to import the raw, underlying tables.
- **Challenge:** "Connecting to the MDM database is blocked."
- **Solution:** This is a network/firewall issue. You need to work with your network and security teams to allow a connection from the Dataiku server to the MDM database server on the required port.
`},{id:255,slug:"exporting-processed-datasets-to-bi-dashboards-tableau-power-bi",question:"How to get started with exporting processed datasets to BI dashboards (Tableau, Power BI)?",answer:`
### 1. Introduction/Overview
While Dataiku has its own dashboarding capabilities, many organizations have standardized on a dedicated Business Intelligence (BI) tool. The standard and most performant way to integrate Dataiku with these tools is to use Dataiku for data preparation and then export the final, analysis-ready dataset to a database that the BI tool can connect to.

### 2. Prerequisites
- **A final, prepared dataset** in your Dataiku Flow.
- **A BI tool** like Tableau or Power BI.
- **A shared database:** A SQL database that both Dataiku and your BI tool can access (e.g., Snowflake, SQL Server, PostgreSQL).
- **A configured connection** to this database in Dataiku.

### 3. Step-by-Step Instructions
1.  **Prepare Your Data in Dataiku:**
    *   Perform all your complex data cleaning, joining, and aggregation in a Dataiku Flow.
    *   The result should be a single, clean, and relatively small output dataset, perfectly shaped for your BI dashboard.
2.  **Add an Export Recipe:**
    *   In your Flow, select your final dataset.
    *   From the right-hand panel, choose the **Export** recipe.
3.  **Configure the Database Export:**
    *   In the Export recipe, select your shared SQL database connection as the destination.
    *   Give the new table a clear name, like \`POWER_BI_SALES_SUMMARY\`.
    *   Set the write mode to **Overwrite**.
4.  **Run the Export:** Execute the recipe. This creates the summary table in your database.
5.  **Connect Your BI Tool:**
    *   Open Power BI or Tableau.
    *   Create a new data source connection to your shared SQL database.
    *   Import the \`POWER_BI_SALES_SUMMARY\` table.
    *   You can now build your visualizations using this clean, pre-processed data.
6.  **Automate the Refresh:**
    *   In Dataiku, create a **Scenario** that runs your entire flow, ending with the Export recipe. Schedule this scenario to run daily to keep the data in your BI dashboard fresh.

### 4. Resources and Tools
- **The Export Recipe:** The key tool for loading data into the shared database.
- **A SQL Database:** The essential "bridge" between Dataiku and the BI tool.

### 5. Next Steps and Progression
- **Live Connection (Alternative):** Some BI tools offer direct connectors to Dataiku. While this avoids the need for an intermediate database, it can be less performant for large datasets, as the query is processed by the Dataiku backend instead of a dedicated data warehouse.
- **Tableau Hyper Export:** Dataiku has a specific plugin for exporting directly to a Tableau \`.hyper\` file format, which can then be published to Tableau Server.

### 6. Common Challenges and Solutions
- **Challenge:** "My BI dashboard is slow."
- **Solution:** The problem is likely that you are exporting too much raw data and trying to do the transformations in the BI tool. **This is an anti-pattern.** Do all the heavy lifting (joins, aggregations) in Dataiku and export only the small, final summary dataset that is needed for the charts. The BI tool should be for visualization, not complex ETL.
- **Challenge:** "The export to the database fails with a permissions error."
- **Solution:** The database user account that Dataiku is using needs \`CREATE TABLE\` and \`INSERT\` permissions in the target schema. Contact your database administrator to have these permissions granted.
`},{id:256,slug:"syncing-upstream-changes-from-alteryx-outputs-for-dual‑run-validation",question:"How to get started with syncing upstream changes from Alteryx outputs for dual‑run validation?",answer:`
### 1. Introduction/Overview
During a migration, it's a common practice to run the old and new systems in parallel for a period (a "dual-run" period) to ensure the outputs match. This means your Dataiku validation flow needs access to the latest output from the legacy Alteryx workflow. This is achieved by creating a Dataiku dataset that reads directly from the Alteryx output location.

### 2. Prerequisites
- **Your Alteryx workflow** is still running and producing an output file (e.g., a CSV or Excel file) in a consistent location.
- **Dataiku has access to this location** (e.g., a shared network drive or a cloud storage bucket).

### 3. Step-by-Step Instructions
1.  **Set Up a Connection to the Output Location:**
    *   In Dataiku, an administrator must create a **Connection** that points to the filesystem where Alteryx saves its output file. This could be a "Filesystem" connection (for a shared network drive) or an "S3" connection, for example.
2.  **Create a Dataset for the Alteryx Output:**
    *   In your Dataiku validation project, click **+ DATASET**.
    *   Choose the connection you just created.
    *   Browse to the folder and select the specific output file generated by Alteryx.
    *   Give it a clear name, like \`alteryx_daily_output\`, and create the dataset.
3.  **Create Your Validation Flow:**
    *   Now you have two datasets in your Flow: \`dataiku_daily_output\` (from your new migrated pipeline) and \`alteryx_daily_output\` (from the legacy pipeline).
    *   You can now build a validation flow that uses these two datasets as inputs. For example, use a **Join** or **Stack** recipe to compare them row-by-row.
4.  **Automate the Validation:**
    *   Create a **Scenario** that orchestrates the dual-run validation.
    *   **Step 1:** Build your new Dataiku pipeline to generate \`dataiku_daily_output\`.
    *   **Step 2:** Add a step to "Sync" the \`alteryx_daily_output\` dataset. This forces Dataiku to re-read the latest version of the file produced by Alteryx.
    *   **Step 3:** Add a step to run your comparison/validation recipe.
    *   **Step 4:** Add a **Run checks** step or a Python step to check if the outputs matched and fail the scenario if they didn't.

### 4. Resources and Tools
- **Dataiku Connections:** To connect to the Alteryx output location.
- **File-based Datasets:** To represent the Alteryx output in your Flow.
- **Comparison Recipes (Join, Stack, Group):** To perform the actual validation.

### 5. Next Steps and Progression
- **Decommissioning:** After running in parallel for a sufficient period (e.g., a few weeks) with no discrepancies found, you can confidently decommission the legacy Alteryx workflow and remove the validation part of your Dataiku flow.

### 6. Common Challenges and Solutions
- **Challenge:** "Dataiku can't read the file produced by Alteryx."
- **Solution:** This is likely a permissions issue. The user account that the Dataiku server runs as must have read permissions on the folder where Alteryx is saving its output.
- **Challenge:** "The Alteryx job runs at a variable time, so I don't know when the file is ready."
- **Solution:** This can be tricky. One approach is to have the Alteryx workflow, as its very last step, create a small, empty "trigger" file (e.g., \`alteryx_done.txt\`). You can then use a Python scenario step in Dataiku that waits until it sees this trigger file before it starts the validation process.
`},{id:257,slug:"interfacing-dataiku-with-message-queues-for-real‑time-loads",question:"How to get started with interfacing Dataiku with message queues for real‑time loads?",answer:`
### 1. Introduction/Overview
For use cases that require processing data in near real-time, you can interface Dataiku with message queues like Apache Kafka. Dataiku's **Streaming Endpoints** feature allows you to connect to a message queue, apply transformations to the stream of incoming data, and write the results to a destination. This is an advanced feature for event-driven architectures.

### 2. Prerequisites
- **A running message queue** (e.g., a Kafka cluster) with a topic containing streaming data.
- **The "Streaming Endpoints" plugin** installed by a Dataiku administrator.
- **A clear real-time processing goal.**

### 3. Step-by-Step Instructions
1.  **Create a Streaming Endpoint:**
    *   In your project, go to **... > Streaming Endpoints**.
    *   Click **+ NEW STREAMING ENDPOINT**.
    *   Select your source type (e.g., **Kafka**).
2.  **Configure the Source (The "Reader"):**
    *   Provide the connection details for your Kafka cluster (bootstrap servers).
    *   Specify the **Topic** you want to read from.
    *   Define the **Format** of the messages (e.g., JSON).
3.  **Add Transformation Recipes:**
    *   You can now add special **streaming-compatible recipes** to process the data as it arrives.
    *   You can use a **Streaming Prepare** recipe to perform stateless transformations (like filtering rows or applying a formula).
    *   You can use a **Streaming Python** recipe for more complex, custom logic.
4.  **Configure the Sink (The "Writer"):**
    *   After your transformation steps, you must add a "sink" to define where the processed data goes.
    *   You can write the results to another **Kafka topic**, or to a **Dataiku dataset**.
5.  **Start the Stream:**
    *   Once configured, you can **Start** the streaming endpoint. It will now run continuously, consuming messages from the source, transforming them in real-time, and writing them to the sink.

### 4. Resources and Tools
- **Streaming Endpoints Plugin:** The core feature for real-time processing.
- **Kafka Connector:** For reading from and writing to Kafka topics.
- **Streaming Recipes:** A special set of recipes designed to work on continuous data streams.

### 5. Next Steps and Progression
- **Windowing:** For stateful operations (like calculating a moving average over the last 5 minutes of data), you can use a windowing recipe in your streaming pipeline.
- **Monitoring:** The streaming endpoint UI provides dashboards for monitoring the throughput (messages per second) and latency of your stream.

### 6. Common Challenges and Solutions
- **Challenge:** "My stream is not processing any messages."
- **Solution:** Check the logs of the streaming endpoint. Common issues include:
    *   **Connection issues:** Dataiku cannot connect to the Kafka cluster (check firewalls and bootstrap server addresses).
    *   **Deserialization errors:** The format of the messages on the Kafka topic does not match the format you configured in the source (e.g., you specified JSON, but the messages are Avro).
- **Challenge:** "I need to do a join in my real-time flow."
- **Solution:** Joining streams is a complex operation. You can join a stream against a static, small dataset (a "lookup" dataset). Joining two high-velocity streams against each other requires advanced stream processing frameworks and is typically done in a dedicated Python recipe using libraries like Faust or Spark Streaming.
`},{id:258,slug:"replacing-flat‑file-landing-zones-with-structured-dataiku-datasets",question:"How to get started with replacing flat‑file landing zones with structured Dataiku datasets?",answer:`
### 1. Introduction/Overview
A common legacy pattern is to have various processes drop raw data files into a "landing zone" folder on a network drive. While simple, this is ungoverned and lacks schema enforcement. The Dataiku best practice is to replace this with a flow that reads the raw files and immediately creates a structured, typed, and partitioned **Dataiku Dataset**. This becomes your new, governed landing zone.

### 2. Prerequisites
- **An existing process that drops flat files** into a folder.
- **Dataiku access to this folder** via a configured connection.

### 3. Step-by-Step Instructions
1.  **Create a "Raw Data" Dataset:**
    *   In a new Dataiku project or Flow Zone named "Ingestion", create a new dataset that points to the folder where the raw files are dropped.
    *   Configure this dataset to read all files in the folder. Name it something like \`raw_sales_files\`.
2.  **Create a "Staging" Dataset with a Prepare Recipe:**
    *   Select the \`raw_sales_files\` dataset and add a **Prepare** recipe.
    *   The output of this recipe will be your new, structured landing dataset. Name it something clear, like \`staged_sales_transactions\`.
3.  **Enforce Schema and Types:**
    *   In the Prepare recipe, perform the essential parsing and typing needed to create a structured dataset. This includes:
        *   Ensuring column names are correct.
        *   Using processors to parse dates, numbers, and booleans into their correct Dataiku data types.
        *   Removing any malformed or unnecessary rows.
4.  **Run and Build the Staging Dataset:** Execute the recipe. The \`staged_sales_transactions\` dataset is now your new, reliable, structured landing zone.
5.  **Refactor Downstream Processes:** All other Dataiku flows that used to read from the messy file-based landing zone should now be changed to read from this new, clean \`staged_sales_transactions\` dataset instead.

### 4. Resources and Tools
- **File-based Dataset Connectors:** (e.g., Filesystem, S3) to read the raw data.
- **Prepare Recipe:** The key tool for enforcing schema and data types.

### 5. Next Steps and Progression
- **Partitioning:** If the raw files are organized by date, partition your new staging dataset. This makes all downstream processing much more efficient.
- **Data Quality Checks:** Add **Metrics and Checks** to your new staging dataset to ensure the data coming from the source meets your quality standards. If a bad file is dropped, these checks can fail the pipeline and prevent the bad data from propagating.

### 6. Common Challenges and Solutions
- **Challenge:** "The source system sometimes drops files with a different schema."
- **Solution:** The Prepare recipe will fail if the schema changes unexpectedly. This is a good thing! It acts as a safety gate. You will get an alert, and you will need to update the Prepare recipe to handle the new schema before the data can proceed.
- **Challenge:** "This seems like an extra step."
- **Solution:** It is a crucial extra step. Creating a structured, typed, and validated staging dataset is a core principle of good data engineering. It decouples your downstream analytics from the unreliability of raw source files and makes your entire system more robust and easier to maintain.
`},{id:259,slug:"using-jdbc-odbc-to-replace-legacy-extracts-into-dataiku",question:"How to get started with using JDBC/ODBC to replace legacy extracts into Dataiku?",answer:`
### 1. Introduction/Overview
Many legacy systems don't have modern REST APIs, but they almost always have a database that can be queried via standards like JDBC (Java Database Connectivity) or ODBC (Open Database Connectivity). Dataiku can use a generic JDBC connector to pull data from these systems, replacing old extraction scripts or manual data dumps.

### 2. Prerequisites
- **A JDBC driver** for your legacy source system. This is a \`.jar\` file, usually provided by the database vendor.
- **Connection details:** The JDBC connection string, username, and password for the source system.
- **Dataiku administrator rights** to install the driver and configure the connection.

### 3. Step-by-Step Instructions
1.  **Install the JDBC Driver (Admin Task):**
    *   Obtain the JDBC driver \`.jar\` file for your source system.
    *   In Dataiku, go to **Administration > Settings > Misc**.
    *   Under the "JDBC Drivers" section, upload the driver's \`.jar\` file.
    *   You may need to restart Dataiku for the new driver to be recognized.
2.  **Create a Generic DB Connection (Admin Task):**
    *   Go to **Administration > Connections**.
    *   Click **+ NEW CONNECTION** and choose **Other SQL databases**.
3.  **Configure the Connection:**
    *   Select the JDBC driver you just uploaded from the dropdown menu.
    *   Enter the **JDBC URL** (connection string). The format for this is specific to each database and can be found in the driver's documentation.
    *   Enter the **Username** and **Password**.
    *   **Test** the connection to ensure it works.
4.  **Create a Dataset from the Connection:**
    *   In your project, click **+ DATASET > Other SQL databases**.
    *   Select the generic connection you just created.
    *   You can now write a SQL query to extract the data you need from the legacy system. The output of this query will become your new Dataiku dataset.

### 4. Resources and Tools
- **Other SQL databases Connector:** The generic JDBC connector in Dataiku.
- **JDBC Driver:** The specific Java library for your source database.

### 5. Next Steps and Progression
- **SQL Push-down:** Even with a generic JDBC connection, Dataiku can often push down the execution of visual recipes, which is great for performance.
- **Isolate Legacy Connections:** It can be a good practice to create a dedicated Dataiku project whose only job is to connect to the legacy system, run the extraction query, and create a "staging" dataset (e.g., in a more modern data warehouse). Other projects can then use this clean, staged data without needing to connect to the legacy system directly.

### 6. Common Challenges and Solutions
- **Challenge:** "The connection test fails."
- **Solution:** First, ensure the JDBC URL is in the exact format required by the driver. This is a very common source of errors. Second, confirm that there is network connectivity (and no firewall) between the Dataiku server and the legacy database server on the required port.
- **Challenge:** "The driver for my ancient database is not available."
- **Solution:** This can be a major issue. If you cannot find a JDBC driver, you may have to fall back to the legacy system's own data export tools and have it produce a file (e.g., CSV) that Dataiku can then pick up from a shared folder.
`},{id:260,slug:"building-delta‑load-patterns-for-incremental-ingestion-workflows",question:"How to get started with building delta‑load patterns for incremental ingestion workflows?",answer:`
### 1. Introduction/Overview
A delta-load (or incremental load) pattern is a highly efficient way to handle large, growing datasets. Instead of reloading the entire dataset every day, you only ingest the "delta"—the new or changed records since the last run. This saves significant time and computational resources. In Dataiku, this is often done with partitioning or by using variables to track the last loaded value.

### 2. Prerequisites
- **A source dataset that grows over time.**
- **A column in the source data that indicates when a record was added or updated,** such as a timestamp or an auto-incrementing ID.

### 3. Step-by-Step Instructions: The "High-Water Mark" Method

This method is useful when your source is a single, large database table that cannot be partitioned.

1.  **Create a "High-Water Mark" Dataset:**
    *   Create a small dataset in Dataiku called \`last_loaded_value\`. It should have just one column and one row. This will store the maximum value (e.g., the latest timestamp or ID) from your last successful run.
2.  **Create a SQL Recipe for Ingestion:**
    *   Create a **SQL recipe** that takes your source table as input.
    *   The recipe's job is to select only the new records.
3.  **Write the Delta-Load SQL:**
    *   In the SQL recipe, first, you will need to read the last loaded value. This is an advanced step that might require a Python recipe to read the value and pass it as a variable.
    *   A simpler, common pattern is to have the SQL recipe **overwrite** a staging table, and a downstream recipe **append** to the final table.
    > \`\`\`sql
    > -- This query runs in a recipe that overwrites a 'daily_delta' dataset
    > SELECT *
    > FROM source_table
    > WHERE last_update_timestamp > (SELECT MAX(last_update_timestamp) FROM final_historical_table)
    > \`\`\`
4.  **Append to Final Table:**
    *   Create a **Sync** or **Prepare** recipe that takes the \`daily_delta\` as input and appends its data to your main historical table, \`final_historical_table\`.
5.  **Automate in a Scenario:** Create a scenario that runs these two recipes in order.

### 4. Resources and Tools
- **SQL Recipe:** For writing the custom delta-load logic.
- **Partitioning:** The preferred, simpler method if your source data or filesystem layout supports it. (See the question on incremental loads via partitioning).
- **Project Variables / Datasets:** To store the state (the high-water mark) between runs.

### 5. Next Steps and Progression
- **Change Data Capture (CDC):** For even more advanced delta loads, look into CDC mechanisms in your source database, which can provide a dedicated stream of all changes (inserts, updates, deletes).

### 6. Common Challenges and Solutions
- **Challenge:** "What if a historical record is updated, not just new records added?"
- **Solution:** The simple high-water mark method only captures new records. To handle updates, your delta-load logic needs to be more complex. You would need to perform an "upsert" (update or insert) into your final table, which often requires a procedural script in a Python recipe or a \`MERGE\` statement in a SQL recipe.
- **Challenge:** "How do I handle the very first run when there is no high-water mark?"
- **Solution:** Your SQL query needs to handle the case where the subquery for the max value returns NULL. You can use a \`COALESCE\` or \`IFNULL\` function to provide a default value (like the beginning of time) for the very first run.
`},{id:261,slug:"profiling-dataiku-runs-to-tune-recipe-performance-post-migration",question:"How to get started with profiling Dataiku runs to tune recipe performance post-migration?",answer:`
### 1. Introduction/Overview
After migrating a workflow, it's essential to profile its performance in Dataiku to identify and resolve any bottlenecks. Profiling involves analyzing a job run to see how long each step takes, allowing you to focus your optimization efforts on the parts of the pipeline that will have the most impact.

### 2. Prerequisites
- **A migrated Dataiku pipeline** that has been run at least once.
- **Access to the "Jobs" menu** in your Dataiku project.

### 3. Step-by-Step Instructions
1.  **Find the Job Run:**
    *   Navigate to the **Jobs** menu in your project.
    *   Find a recent, successful run of your main pipeline scenario. Click on it to open the job details.
2.  **Use the Job Inspector:**
    *   The job details page provides a visual overview of the run. Look for the **Gantt chart** view.
    *   This chart shows every recipe that was run as a bar on a timeline. The length of the bar corresponds to the duration of that recipe's execution.
3.  **Identify the Bottleneck:**
    *   Scan the Gantt chart. Look for the longest bar. This is your primary bottleneck—the recipe that is taking the most time and is the best candidate for optimization.
4.  **Analyze the Slow Recipe:**
    *   Click on the slow recipe in the job view to see its specific log.
    *   Now, open the recipe itself in the Flow and ask critical performance questions:
        *   **Where is it running?** Check the **Advanced** settings for the **Execution engine**. If it's processing large data "In-Memory", that's almost certainly the problem.
        *   **What is it doing?** Is it a complex Join, a Group By on a very large dataset, or a slow Python script?
5.  **Apply Optimizations:**
    *   **The #1 Fix:** If the engine is "In-Memory", **push down the computation**. Change the engine to **Run on database (SQL)** or **Spark**.
    *   **Other Fixes:** If the recipe is already running on a powerful engine, you may need to optimize the logic itself (e.g., rewrite an inefficient SQL query, optimize a Python script, or ensure database tables are indexed).
6.  **Rerun and Compare:** After applying an optimization, rerun the scenario and look at the new job profile. Compare the new duration of the recipe to the old one to measure your improvement.

### 4. Resources and Tools
- **The Jobs Menu and Job Inspector:** Your primary tools for identifying bottlenecks.
- **The Recipe Execution Engine Setting:** Your primary tool for fixing performance issues.

### 5. Next Steps and Progression
- **Iterative Tuning:** Performance tuning is an iterative process. After fixing the biggest bottleneck, a new, different recipe may become the slowest part of the pipeline. Repeat the profiling and optimization process until the overall pipeline performance meets your requirements.
- **Benchmarking:** Keep a record of your benchmark results to demonstrate the performance improvements you've achieved since the migration.

### 6. Common Challenges and Solutions
- **Challenge:** "The log says the job took 10 minutes, but the Gantt chart shows the recipe only took 2 minutes. Where did the other 8 minutes go?"
- **Solution:** The overhead could be in data movement. For example, if the recipe had to read a very large file from a slow network source before it could even start processing, that time would be part of the job's total duration. This might indicate a need to move your data to a more performant storage system.
`},{id:262,slug:"using-spark-push-down-or-spark-recipes-to-parallelize-heavy-lifts",question:"How to get started with using Spark push-down or Spark recipes to parallelize heavy lifts?",answer:`
### 1. Introduction/Overview
For processing datasets that are too large to fit in the memory of a single server, you must use a distributed computing engine like Apache Spark. Dataiku allows you to leverage Spark in two ways: by "pushing down" the execution of visual recipes to a Spark engine, or by writing custom code in a Spark-native recipe.

### 2. Prerequisites
- **A Dataiku instance integrated with a Spark cluster** (via YARN or Kubernetes).
- **Your large dataset stored on a distributed filesystem** that Spark can access (like HDFS, S3, GCS, or ADLS).
- **A computationally heavy task** (e.g., joining or aggregating a multi-billion row dataset).

### 3. Step-by-Step Instructions

#### Method 1: Spark Push-down (for Visual Recipes)
1.  **When to Use:** This is the easiest and most common method. Use it when your transformation logic can be expressed using Dataiku's standard visual recipes (like Prepare, Join, Group).
2.  **Check Your Engine:** Open the visual recipe you want to accelerate.
3.  **Go to Advanced Settings:** In the recipe's settings, click the **Advanced** tab.
4.  **Change the Execution Engine:** Find the **Execution engine** dropdown menu. Change it from the default ("In-Memory" or "DSS engine") to **Spark**.
5.  **Run:** Click **Run**. Dataiku will now translate your visual recipe steps into an optimized Spark job and submit it to the cluster. The work will be parallelized across all the nodes in the cluster.

#### Method 2: Spark Code Recipes (for Custom Logic)
1.  **When to Use:** Use this when you need to perform a transformation that is not available in the visual recipes or requires complex custom code.
2.  **Create a Spark Recipe:** In your Flow, click **+ RECIPE** and choose one of the Spark code recipes:
    *   **PySpark:** For writing custom logic using Python's Spark API.
    *   **SparkR:** For using R with Spark.
    *   **SparkSQL:** For writing SQL queries that will be executed by the SparkSQL engine.
3.  **Write Your Spark Code:** The recipe will open with a pre-configured Spark session. You can write standard Spark code to read your datasets, perform transformations using the Spark DataFrame API, and write the results.
4.  **Run:** When you run the recipe, Dataiku will execute your script as a Spark application on the cluster.

### 4. Resources and Tools
- **The Execution Engine Dropdown:** The key to enabling Spark push-down for visual recipes.
- **Spark Code Recipes:** For writing custom, distributed computations.
- **The Spark UI:** Essential for monitoring and debugging the Spark jobs that Dataiku submits.

### 5. Next Steps and Progression
- **Performance Tuning:** In the recipe's "Advanced" settings, you can configure specific Spark properties (like the number of executors, driver memory, etc.) to fine-tune the performance of your job for your specific cluster and workload.
- **User-Defined Functions (UDFs):** In a PySpark recipe, you can write a Python function and wrap it as a UDF to apply complex custom logic to your Spark DataFrame at scale.

### 6. Common Challenges and Solutions
- **Challenge:** "The Spark engine option is not available in my recipe."
- **Solution:** This means your input or output dataset is not on a Spark-compatible storage system. For example, if you are reading from an uploaded CSV file (which is on the local Dataiku server's filesystem), Spark cannot access it. Your data must be on HDFS or a cloud storage system.
- **Challenge:** "My Spark job fails with a memory error."
- **Solution:** Your Spark application may need more resources. Go to the Spark UI to debug. Common issues are insufficient driver memory or executor memory. You can try increasing these values in the recipe's advanced Spark configuration settings.
`},{id:263,slug:"partitioning-datasets-to-shrink-compute-footprint",question:"How to get started with partitioning datasets to shrink compute footprint?",answer:`
### 1. Introduction/Overview
Partitioning is one of the most important performance optimization techniques in data engineering. It involves breaking a large dataset into smaller, more manageable chunks (partitions) based on the values of a specific column. When you run a job, you can then tell Dataiku to process only the specific partitions you need, dramatically reducing the amount of data that needs to be read and computed.

### 2. Prerequisites
- **A large, growing dataset.**
- **A column in the dataset that is suitable for partitioning.** This is almost always a **date** or timestamp column.

### 3. Step-by-Step Instructions
1.  **Activate Partitioning on Your Dataset:**
    *   Open your large dataset in the Flow.
    *   Go to the **Settings** tab and then to the **Partitioning** sub-tab.
    *   Click **Activate partitioning**.
2.  **Choose the Partitioning Column:**
    *   Check the box for "Partition by a time dimension".
    *   Select your date or timestamp column.
    *   Choose the time dimension (e.g., **Day**, **Month**, **Hour**). "Day" is the most common.
3.  **Observe the Result:** Your single dataset icon in the Flow now represents a collection of partitions. If you explore the data, you'll see you can now view individual partitions.
4.  **Propagate Partitioning:** When you build a downstream dataset from a partitioned one, Dataiku will automatically make the downstream dataset partitioned in the same way. The entire downstream flow becomes partitioned.
5.  **Use Partitioning to Shrink Compute:**
    *   Now, when you run a job, you don't have to rebuild the entire dataset.
    *   In a **Scenario**, when you add a "Build" step, you can specify which partitions to build. To process only the latest data, set "Partitions to build" to **LATEST**.
    *   When running a manual job, the build dialog will let you choose a specific date range of partitions to build.

### 4. Resources and Tools
- **The Dataset Partitioning Settings:** The UI for defining your partitioning scheme.
- **The Scenario Build Step:** Where you specify which partitions to process (e.g., "LATEST").

### 5. Next Steps and Progression
- **Partitioning on Multiple Columns:** You can partition on more than one column (e.g., by country AND by day). This is useful for very large, multi-dimensional datasets.
- **Backfilling:** To populate all the historical partitions for the first time, you can launch a build from the Flow and choose to build a range of dates (e.g., "all partitions from 2020-01-01 to today").

### 6. Common Challenges and Solutions
- **Challenge:** "Which column should I partition on?"
- **Solution:** You should partition on a column that you will use to filter your data for processing. For 99% of use cases, this is a **date** column. This allows you to implement incremental daily or hourly jobs. Partitioning on a column with very high cardinality (like a user ID) is an anti-pattern and will create too many small files, which is inefficient.
- **Challenge:** "My job is rebuilding all partitions even though I only want the latest."
- **Solution:** You have misconfigured your scenario's build step. Double-check that the "Partitions to build" setting is correctly set to "LATEST" or another dynamic pattern, and not "ALL".
`},{id:264,slug:"implementing-caching-strategies-to-avoid-redundant-recompute",question:"How to get started with implementing caching strategies to avoid redundant recompute?",answer:`
### 1. Introduction/Overview
A key feature of Dataiku's architecture is its built-in, intelligent caching. You don't need to explicitly implement a caching strategy, because **every dataset in a Dataiku Flow is already a cache**. Understanding how this works allows you to build highly efficient pipelines that avoid recomputing unchanged data.

### 2. Prerequisites
- **A Dataiku Flow** with a chain of recipes.

### 3. Step-by-Step Instructions: Understanding Dataiku's Caching

1.  **Datasets as Caches:**
    *   Think of every dataset (the blue squares in your Flow) as a materialized checkpoint.
    *   When you run a recipe, it reads from its input dataset(s), performs its logic, and writes the results to its output dataset, which is physically stored on disk (or in your database).
2.  **How Smart Rebuilding Works:**
    *   Imagine you have a flow: \`A -> (recipe 1) -> B -> (recipe 2) -> C\`.
    *   You run a scenario to build dataset \`C\`. Dataiku builds A, then B, then C.
    *   Now, you make a change *only* to \`recipe 2\`.
    *   You run the scenario to build \`C\` again. Dataiku is smart enough to see that dataset \`B\` and its upstream dependencies (\`recipe 1\`, \`A\`) have not changed. It will not re-run \`recipe 1\`. It will use the cached, on-disk version of dataset \`B\`, and only recompute the part of the flow that was affected by the change (i.e., it will run \`recipe 2\` to regenerate \`C\`).
3.  **Forcing a Rebuild:**
    *   Sometimes you *want* to ignore the cache and rebuild everything from scratch.
    *   When you launch a job (either manually or in a scenario), you can change the **Build mode** to **Forced rebuild**. This tells Dataiku to ignore all cached, intermediate results and re-run every recipe from the very beginning. This is often necessary for daily ingestion jobs.

### 4. Resources and Tools
- **The Dataiku Flow:** The visual representation of your cached data pipeline.
- **Build Modes (Smart, Forced):** Your control over using or ignoring the cache.

### 5. Next Steps and Progression
- **Optimizing the Cache Format:** You can control the physical format of your cache. For large datasets, change the format in the dataset's **Settings** from CSV to a more performant columnar format like **Parquet** instead of CSV. This makes reading from the cache much faster for downstream recipes.
- **Explicit Caching (Sync Recipe):** If you have a very complex set of steps that you want to explicitly "cache" before moving to the next stage, you can use a **Sync** recipe. A Sync recipe just copies data from one dataset to another. It's a good way to create a major, stable checkpoint in your flow, often used when moving data between different storage connections.

### 6. Common Challenges and Solutions
- **Challenge:** "My job is recomputing the whole flow every time."
- **Solution:** Your scenario's build step is likely set to "Forced rebuild". Change it to "Smart" or "Build required datasets" to enable intelligent caching. Another possibility is that an upstream source dataset was updated, which correctly triggered a rebuild of the entire downstream flow.
- **Challenge:** "How do I clear the cache for a specific dataset?"
- **Solution:** Open the dataset, go to its **Actions** menu (in the top right), and click **Clear data**. This will delete the data stored on disk, and the dataset will become "empty". The next time a job needs it, it will have to be recomputed.
`},{id:265,slug:"trimming-intermediate-datasets-to-reduce-storage-and-runtime",question:"How to get started with trimming intermediate datasets to reduce storage and runtime?",answer:`
### 1. Introduction/Overview
As you build a data pipeline, especially one with many joins, your intermediate datasets can accumulate a large number of unnecessary columns. Keeping these extra columns slows down every single downstream step, as more data needs to be read, processed, and written. Actively trimming your datasets to keep only the columns you need is a simple but highly effective performance optimization.

### 2. Prerequisites
- **A Dataiku Flow** with intermediate datasets that have more columns than necessary.

### 3. Step-by-Step Instructions
1.  **Identify an Intermediate Dataset:** Find a dataset in the middle of your flow, for example, the output of a **Join** recipe.
2.  **Analyze Column Usage:** Look at the downstream recipe that uses this dataset as input. Ask yourself: "Which of these columns are actually used by the next step?"
    *   For example, after joining \`customers\` and \`orders\`, you might have two customer ID columns (\`customer_id\` and \`customer_id_1\`). You only need one.
    *   You might also have columns from the customers table (like \`signup_date\`) that are not needed for the final aggregation.
3.  **Add a "Trimming" Prepare Recipe:**
    *   The best practice is to add a new **Prepare** recipe immediately after the recipe that creates the wide dataset (e.g., right after your Join recipe).
4.  **Remove Unused Columns:**
    *   In this new Prepare recipe, the only step you need is to remove columns.
    *   From the column header dropdown of a column you want to remove, select **Delete**.
    *   Alternatively, from the **+ ADD A NEW STEP** menu, choose the **Remove/Keep columns by name** processor and select all the columns you wish to delete.
5.  **Run and Observe:** Run this new trimming recipe. The output dataset will now be narrower, which will reduce its storage size and make all subsequent recipes that read from it run faster.

### 4. Resources and Tools
- **Prepare Recipe:** The primary tool for removing columns.
- **The "Delete Column" action:** The simplest way to trim your dataset.

### 5. Next Steps and Progression
- **Proactive Trimming:** Get into the habit of trimming datasets as part of your standard workflow. After every Join recipe, review the output and consider if you should immediately add a Prepare recipe to clean up the columns.
- **In-Recipe Selection:** Many recipes, like the **Join** recipe, have a "Selected Columns" panel at the bottom. You can deselect unnecessary columns here, which avoids creating them in the first place. This is even better than adding a separate trimming step afterwards.

### 6. Common Challenges and Solutions
- **Challenge:** "I accidentally removed a column that a downstream recipe needed."
- **Solution:** The pipeline will fail with a "Column not found" error. This is easy to fix. Go back to your trimming Prepare recipe, find the "Remove columns" step in the script, and simply delete that step or edit it to keep the required column.
- **Challenge:** "It's tedious to remove many columns one by one."
- **Solution:** Use the **Remove/Keep columns by name** processor. It allows you to select multiple columns at once from a checklist. You can also switch its mode to "Keep", which is useful if you only want to keep a few columns out of many; you can just select the few you want to keep and it will remove all the others.
`},{id:266,slug:"refactoring-complex-join-logic-for-push-down-execution",question:"How to get started with refactoring complex join logic for push-down execution?",answer:`
### 1. Introduction/Overview
When your data is in a powerful SQL database, pushing down computation is key for performance. If you have a complex series of visual **Join** recipes, the most performant solution is often to refactor this logic into a single, comprehensive **SQL recipe**. This allows the database's query optimizer to see the entire join plan at once and create a highly efficient execution strategy.

### 2. Prerequisites
- **A Dataiku Flow with a chain of multiple Join recipes.**
- **All input datasets for the joins must be tables in the same database connection.**
- **Intermediate SQL skills,** including the ability to write multi-table joins.

### 3. Step-by-Step Instructions
1.  **Analyze the Visual Join Chain:** Look at your chain of Join recipes. Understand the inputs to each join, the join types (inner, left), and the join keys.
2.  **Convert the First Join to SQL (Optional Starting Point):**
    *   Open the *first* visual Join recipe in your chain.
    *   In its settings, you may find an option to **Convert to SQL recipe**.
    *   Clicking this will automatically generate the SQL code for that single join and create a new SQL recipe. This can be a great starting point.
3.  **Create a New, Consolidated SQL Recipe:**
    *   Create a new **SQL recipe**. As inputs, add *all* the base datasets that were the original inputs to your entire join chain.
4.  **Write the Consolidated SQL Query:**
    *   In the SQL recipe editor, write a single \`SELECT\` statement that replicates the logic of the entire chain.
    *   This will involve joining multiple tables together. You can use Common Table Expressions (CTEs or \`WITH\` clauses) to keep the logic clean and readable, mimicking the intermediate steps of your original visual flow.
    > \`\`\`sql
    > WITH customers_cleaned AS (
    >     -- Logic from your first Prepare recipe
    >     SELECT * FROM customers WHERE is_active = 1
    > ), orders_cleaned AS (
    >     -- Logic from your second Prepare recipe
    >     SELECT * FROM orders WHERE status = 'COMPLETED'
    > )
    > SELECT *
    > FROM customers_cleaned c
    > LEFT JOIN orders_cleaned o ON c.customer_id = o.customer_id
    > LEFT JOIN products p ON o.product_id = p.product_id; -- Chained join
    > \`\`\`
5.  **Replace the Old Flow:** Once you have validated that your new, single SQL recipe produces the same final output as the old chain of visual recipes, you can safely delete the old recipes and intermediate datasets, simplifying your Flow dramatically.

### 4. Resources and Tools
- **SQL Recipe:** Your primary tool for writing the consolidated query.
- **Convert to SQL Recipe Feature:** A helpful starting point for generating the initial SQL.
- **Database Query Plan Analyzer (\`EXPLAIN\`):** Use your database's native tools to analyze the execution plan of your consolidated query to ensure it's efficient.

### 5. Next Steps and Progression
- **Further Optimization:** With all the logic in one query, a database expert can now easily tune it by adding hints, ensuring correct indexing, and optimizing the query plan.

### 6. Common Challenges and Solutions
- **Challenge:** "The consolidated SQL query is becoming very long and hard to read."
- **Solution:** Use **Common Table Expressions (CTEs)** extensively. Each CTE can represent one logical step or one intermediate dataset from your original visual flow. This makes even a very complex query readable and maintainable.
- **Challenge:** "The performance of my single SQL recipe is worse than the visual chain."
- **Solution:** This is rare, but can happen if the query is written poorly. It likely means the database's query optimizer is choosing a bad execution plan. You may need to rewrite the query, reorder the joins, or work with a DBA to check the table statistics and indexes in the source database.
`},{id:267,slug:"tuning-resource-settings-on-api-nodes-and-job-execution",question:"How to get started with tuning resource settings on API nodes and job execution?",answer:`
### 1. Introduction/Overview
Tuning resource settings is an advanced topic, typically handled by a Dataiku administrator, that involves allocating the right amount of CPU and memory to different parts of the Dataiku platform. Proper tuning is essential for balancing performance, stability, and cost, especially in a production environment.

### 2. Prerequisites
- **Administrator access** to the Dataiku instance and potentially the underlying infrastructure (like Kubernetes).
- **An understanding of your workloads** (e.g., are they CPU-intensive, memory-intensive?).
- **Monitoring tools** to observe resource utilization.

### 3. Step-by-Step Instructions: Key Areas for Tuning

#### 1. Tuning Job Execution (Containerized Environments)
- **What it is:** Controlling the resources allocated to individual recipes that run in containers on Kubernetes.
- **How (Admin Task):**
    1.  Go to **Administration > Containerized Execution**.
    2.  When defining a container configuration, you can specify the **CPU and Memory requests and limits** for the pods that will be created.
    3.  Create different "profiles" (e.g., "small-job", "large-memory-job") with different resource allocations.
- **How (User Task):** In a recipe's "Advanced" settings, the user can then select the appropriate resource profile for their specific job.

#### 2. Tuning API Nodes (Real-time Scoring)
- **What it is:** Allocating resources to the API Deployer nodes that serve your real-time prediction models.
- **How (Admin Task):**
    1.  The API Deployer is a separate service. Its resource allocation is configured when it is deployed.
    2.  If deployed on Kubernetes, you will set the CPU and memory requests/limits in its Deployment or Helm chart configuration.
    3.  You can also tune the number of **replicas** (pods) to handle more concurrent requests.
    4.  You can also tune the number of **threads** within each API node to handle more requests in parallel.

#### 3. Tuning the Main Dataiku Backend
- **What it is:** Adjusting the Java Heap Size for the main Dataiku server process (the "backend").
- **How (Admin Task):**
    1.  This is done by editing the \`install.ini\` configuration file on the Dataiku server.
    2.  You can increase the \`dss.jvm.heap.max\`, which controls the maximum memory the main Java process can use.
    3.  **Caution:** This should be done carefully. Increasing this too much can starve other processes on the server. It's often better to push heavy workloads to other engines (Spark/K8s) rather than scaling up the backend itself.

### 4. Resources and Tools
- **Dataiku Administration Panel:** Where container and other settings are configured.
- **Kubernetes Configuration Files (YAML):** For setting resources on K8s deployments.
- **Cloud Provider Monitoring Tools / Prometheus & Grafana:** Essential for monitoring the actual CPU and memory usage of your pods and VMs to inform your tuning decisions.

### 5. Next Steps and Progression
- **Autoscaling:** For cloud-based deployments, set up autoscaling on your Kubernetes node pools and for your API node deployments. The system will automatically add or remove replicas based on load, which is more efficient than static tuning.

### 6. Common Challenges and Solutions
- **Challenge:** "My containerized job failed with an 'OOMKilled' (Out of Memory) error."
- **Solution:** The memory limit you set in the container configuration was too low for the job. You need to edit the configuration and increase the memory limit.
- **Challenge:** "My API endpoint has high latency under load."
- **Solution:** You may need to scale up your API node. You can either increase the memory/CPU allocated to each replica, or, more commonly, increase the number of replicas to handle more concurrent traffic.
`},{id:268,slug:"archiving-or-deleting-obsolete-datasets-for-performance",question:"How to get started with archiving or deleting obsolete datasets for performance?",answer:`
### 1. Introduction/Overview
Over time, Dataiku projects can accumulate many intermediate or old datasets that are no longer needed. These obsolete datasets consume storage space and can clutter the Flow, making it harder to navigate. A regular cleanup process is a good housekeeping practice for maintaining a performant and manageable instance.

### 2. Prerequisites
- **A Dataiku project that has been in use for some time.**
- **Project administrator rights** to delete datasets.

### 3. Step-by-Step Instructions
1.  **Identify Obsolete Datasets:**
    *   Look for datasets that are "dangling" at the end of a flow—that is, they are not used as an input by any other recipe, model, or dashboard. Dataiku's UI often highlights these.
    *   Look for old test or debug datasets that were created during development and are no longer needed.
    *   Look for old versions of datasets that have been replaced by new pipelines.
2.  **Use the Impact Analysis Feature:**
    *   Before deleting any dataset, **always** perform an impact analysis to be certain it's not in use.
    *   Right-click on the dataset in the Flow and select **View downstream dependencies**.
    *   If there are no downstream dependencies, it is likely safe to delete.
3.  **Choose a Strategy: Archive or Delete?**
    *   **Archive:** If you *might* need the data again someday but don't need it in the active project, you can archive it. Use an **Export recipe** to save the dataset's data to a long-term, cheaper storage location (like a dedicated archive folder in S3). After exporting, you can delete the dataset from the Flow.
    *   **Delete:** If you are certain the dataset is no longer needed, you can delete it permanently.
4.  **Delete the Dataset:**
    *   In the Flow, right-click on the obsolete dataset and select **Delete**.
    *   Dataiku will warn you again if the dataset is used by anything. If it is truly obsolete, you can confirm the deletion. This removes the dataset's definition from the Flow and deletes its underlying data from disk.

### 4. Resources and Tools
- **Downstream Dependency Analysis:** Your most important safety check before deleting anything.
- **The Delete Action:** For permanent removal.
- **Export Recipe:** For archiving data before deletion.

### 5. Next Steps and Progression
- **Automated Cleanup Scenario:** You can create a periodic "cleanup" scenario.
    *   Use a Python step with the Dataiku API to list all datasets in a project.
    *   The script can check when each dataset was last built.
    *   If a dataset hasn't been rebuilt in a long time (e.g., 6 months) and has no downstream dependencies, the script can automatically delete it.

### 6. Common Challenges and Solutions
- **Challenge:** "I accidentally deleted a dataset I needed."
- **Solution:** If you have a recent backup of your Dataiku instance or a project export, you can restore it to recover the deleted dataset definition. If your project is on Git, you can revert the commit that deleted the dataset to get its definition back, but you will still need to rebuild it to regenerate its data. **This is why the impact analysis step is so critical.**
- **Challenge:** "I can't delete a dataset; the option is greyed out."
- **Solution:** This usually means you do not have the necessary permissions. You typically need project administrator rights to delete datasets.
`},{id:269,slug:"benchmarking-dataiku-vs-legacy-alteryx-runtimes",question:"How to get started with benchmarking Dataiku vs legacy Alteryx runtimes?",answer:`
### 1. Introduction/Overview
A key metric for measuring the success of a migration is performance improvement. Benchmarking the runtime of your new Dataiku pipeline against the original Alteryx workflow provides a concrete, quantitative measure of the ROI, which is powerful for communicating the value of the project to leadership.

### 2. Prerequisites
- **A migrated Dataiku pipeline** and the original Alteryx workflow.
- **A representative, static set of input data** for a fair comparison.
- **Access to job execution logs** from both Alteryx Server and Dataiku.

### 3. Step-by-Step Instructions
1.  **Define the Scope:**
    *   Ensure you are comparing apples to apples. The benchmark should cover the entire end-to-end process, from reading the initial sources to writing the final output.
2.  **Establish the Legacy Baseline:**
    *   Run the Alteryx workflow using the static input data.
    *   From the Alteryx Server job logs or the performance profiler, record the total end-to-end execution time. For a more reliable result, run it a few times and take the average.
3.  **Run the Dataiku Pipeline:**
    *   In Dataiku, run the main scenario that executes your new, migrated pipeline using the exact same input data.
    *   Make sure the Dataiku pipeline is running in its optimized state (e.g., with push-down execution enabled).
4.  **Measure the Dataiku Runtime:**
    *   Go to the **Jobs** menu in Dataiku and find the run you just triggered.
    *   Record the total duration of the scenario run. Again, you may want to run it a few times and average the result.
5.  **Calculate and Report the Improvement:**
    *   Compare the two average runtimes.
    *   Calculate the percentage improvement: \`((Old_Time - New_Time) / Old_Time) * 100\`.
    *   Create a simple chart or slide that clearly shows the result, for example: "Migrating the daily sales report to Dataiku reduced the runtime from 2 hours to 5 minutes, a 96% improvement."

### 4. Resources and Tools
- **Alteryx Server Logs / Performance Profiler:** To get the legacy baseline runtime.
- **Dataiku Jobs Menu:** To get the new runtime.
- **A spreadsheet or presentation slide:** To clearly communicate the results.

### 5. Next Steps and Progression
- **Benchmark Cost:** If running in the cloud, you can also benchmark the compute cost. Compare the cost of the Alteryx server resources used for the duration of the job with the cost of the Dataiku and cloud data warehouse resources used.
- **Create a Performance Dashboard:** Create a Dataiku dashboard that tracks the runtimes of your key migrated pipelines over time to ensure they remain performant.

### 6. Common Challenges and Solutions
- **Challenge:** "The new Dataiku pipeline is slower than the Alteryx one."
- **Solution:** This is a major red flag that your migrated pipeline is not correctly optimized. The most likely cause is that you are processing large data "In-Memory" in Dataiku instead of **pushing down the computation** to a database or Spark cluster. Go back and profile the Dataiku job to find and fix the bottleneck. A correctly architected Dataiku pipeline should almost always be significantly faster than a legacy in-memory tool for large data.
- **Challenge:** "The runtimes are very inconsistent."
- **Solution:** This can be caused by "noisy neighbor" problems on shared infrastructure. Both the Alteryx server and the Dataiku instance or database might be busy with other users' jobs. Try to run your benchmark tests during a quiet period (like overnight) to get a more stable comparison.
`},{id:270,slug:"monitor-and-optimize-memory-usage-across-flow-stages",question:"How to get started with monitor and optimize memory usage across flow stages?",answer:`
### 1. Introduction/Overview
Optimizing memory usage is crucial for building stable and scalable data pipelines. High memory consumption can lead to slow performance and "Out of Memory" (OOM) errors. The key principle for optimizing memory in Dataiku is to avoid pulling large datasets into the main server's memory and instead push the computation to more powerful, dedicated engines.

### 2. Prerequisites
- **A Dataiku project,** especially one that processes large datasets.
- **Understanding of where your recipes are being executed.**

### 3. Step-by-Step Instructions

#### Step 1: Identify Memory-Intensive Recipes
1.  **Look for "In-Memory" Execution:** Go through your Flow and open your key transformation recipes (Prepare, Join, Group, Python).
2.  In each recipe, go to the **Advanced** settings and check the **Execution engine**.
3.  **Any recipe that processes large data (e.g., > 1 million rows) and is set to "In-Memory" or "DSS engine" is a potential memory bottleneck.** This means Dataiku is loading the entire dataset into the main server's Java memory (for visual recipes) or a Python process's memory.

#### Step 2: Apply Optimization Strategies
1.  **Push Down to a Database:**
    *   **If your data is in a SQL database,** this is the best solution.
    *   Change the recipe's execution engine to **Run on database (SQL)**.
    *   The memory will now be consumed by your powerful database server, not the Dataiku server.
2.  **Push Down to Spark:**
    *   **If your data is on a distributed filesystem (HDFS, S3),** use this.
    *   Change the execution engine to **Spark**.
    *   The memory usage will be distributed across all the worker nodes in your Spark cluster.
3.  **Optimize Python Recipes:**
    *   If you must use a Python recipe on large data, avoid loading the entire dataset into a single Pandas DataFrame.
    *   Instead, process the data in **chunks**. You can get an iterator from the Dataiku API that reads the dataset row-by-row or in small batches, keeping memory usage low.
    > \`\`\`python
    > # Instead of df = dataset.get_dataframe()
    > # Use an iterator
    > for row in dataset.iter_rows():
    >     # process one row at a time
    > \`\`\`
4.  **Trim Unused Columns:** Before a memory-intensive step, use a Prepare recipe to remove any columns that are not needed. Fewer columns means less data and lower memory usage.

### 4. Resources and Tools
- **Recipe Execution Engine Setting:** Your primary tool for memory optimization.
- **Dataiku's Iterator API:** For processing large datasets in Python recipes with low memory.
- **Server Monitoring Tools (Admin):** An administrator can use tools like \`top\` or a monitoring dashboard (Grafana) to watch the memory usage of the main Dataiku process during a job run.

### 5. Next Steps and Progression
- **Containerize Heavy Jobs:** Use containerized execution to run memory-intensive Python jobs in a dedicated Kubernetes pod with its own memory allocation. This isolates the job and prevents it from impacting the main Dataiku server.

### 6. Common Challenges and Solutions
- **Challenge:** "My job failed with an 'Out of Memory' error."
- **Solution:** This is a clear sign you are trying to process too much data in-memory. You must apply one of the optimization strategies above. Identify the failing recipe and change its engine or refactor its code to be more memory-efficient. Simply increasing the server's RAM is a temporary fix, not a scalable solution.
- **Challenge:** "I can't push down because my data sources are in different systems."
- **Solution:** You need to create a staging layer. First, use Export recipes to load all your necessary data into a single, powerful system (like a Snowflake data warehouse). Then, you can perform all the complex joins and transformations there using push-down execution.
`},{id:271,slug:"training-users-on-dataiku-equivalents-to-alteryx-tools",question:"How to get started with training users on Dataiku equivalents to Alteryx tools?",answer:`
### 1. Introduction/Overview
When transitioning a team from Alteryx to Dataiku, effective training is crucial for adoption. The training should focus on building a "mental map" that connects familiar Alteryx concepts and tools to their new equivalents in the Dataiku platform. This helps users feel comfortable and become productive quickly.

### 2. Prerequisites
- **A group of users with Alteryx experience.**
- **A Dataiku training environment.**
- **A "translation" guide or cheat sheet.**

### 3. Step-by-Step Instructions: A Training Plan

1.  **Create a "Translation Cheat Sheet":**
    *   Before the training, create a simple two-column document that maps the most common Alteryx tools to their Dataiku counterparts.
    *   **Examples:**
        *   Alteryx \`Input Data\` -> Dataiku **Dataset**
        *   Alteryx \`Filter\`, \`Formula\`, \`Select\` -> Processors in a Dataiku **Prepare Recipe**
        *   Alteryx \`Join\` -> Dataiku **Join Recipe**
        *   Alteryx \`Summarize\` -> Dataiku **Group Recipe**
        *   Alteryx Workflow (\`.yxmd\`) -> Dataiku **Flow**
        *   Alteryx Scheduler -> Dataiku **Scenario**
2.  **Hold a Kickoff Session (Focus on Concepts):**
    *   Start by explaining the conceptual mapping. Use the cheat sheet as a guide.
    *   Emphasize that while the names are different, the underlying data transformation concepts they already know are the same.
    *   Show them a completed Dataiku Flow and compare it to an Alteryx workflow to highlight the similarities.
3.  **Conduct a Hands-On Workshop:**
    *   This is the most important part. Guide the users through rebuilding a simple but realistic Alteryx workflow in Dataiku, side-by-side.
    *   Start with connecting to data, then use a Prepare recipe for cleaning, a Join recipe for blending, and a Group recipe for aggregation.
    *   Have them "drive" in their own training environment.
4.  **Provide "Sandbox" Projects:**
    *   Give each user their own sandbox project where they can experiment without fear of breaking anything.
    *   Provide them with a few simple challenges, like "Take this messy CSV and produce a clean, aggregated report."
5.  **Hold "Office Hours":** Schedule regular, informal sessions where users can ask questions and get help as they start to migrate their own workflows.

### 4. Resources and Tools
- **The Translation Cheat Sheet:** A key reference document.
- **A dedicated training project in Dataiku:** With sample data and exercises.
- **Dataiku Academy:** The "Core Designer" path is an excellent resource for them to follow up with after the initial training.

### 5. Next Steps and Progression
- **Showcase a "Migrated" Workflow:** In a follow-up session, present a more complex workflow that has been successfully migrated, highlighting the benefits of the new Dataiku version (e.g., better performance, clearer lineage).
- **Peer Mentoring:** Identify "power users" who pick up Dataiku quickly and encourage them to help mentor their colleagues.

### 6. Common Challenges and Solutions
- **Challenge:** "Users are resistant to change and want to keep using Alteryx."
- **Solution:** Focus on the "what's in it for me". Highlight the benefits of Dataiku that directly address Alteryx pain points, such as better collaboration features, Git integration for version control, and superior performance on large data through push-down execution.
- **Challenge:** "A user is stuck because they can't find the exact Alteryx tool they are used to."
- **Solution:** This is where the trainer's guidance is key. Help them think about the *function* of the tool, not its name. Then, show them how to achieve the same function in Dataiku, which might involve a different processor or a combination of steps.
`},{id:272,slug:"creating-user-guides-for-migrated-workflows",question:"How to get started with creating user guides for migrated workflows?",answer:`
### 1. Introduction/Overview
After a data pipeline is migrated, its business users need to understand how to use its outputs and a support team needs to know how to run and maintain it. Creating a simple user guide is essential for a smooth handover. The built-in **Project Wiki** in Dataiku is the perfect tool for creating and hosting this documentation.

### 2. Prerequisites
- **A fully migrated and validated Dataiku project.**
- **Understanding of the project's purpose and how to run it.**

### 3. Step-by-Step Instructions
1.  **Create a "User Guide" in the Project Wiki:**
    *   In the migrated project, go to the **Wiki**.
    *   Create a new article named "User Guide" or "Run Book".
2.  **Structure the Guide with Key Sections:** Use Markdown headings to create a clear structure. Your guide should include:
    *   **1. Overview:** A brief, non-technical summary of the project's business purpose. What problem does it solve?
    *   **2. Key Outputs:** Describe the final outputs. For a dashboard, explain what each chart shows. For a dataset, describe its key columns. Include links to the relevant dashboards or datasets.
    *   **3. How to Run the Pipeline:** Explain how the pipeline is automated.
        *   Link to the main **Scenario** that runs the job.
        *   Describe its schedule ("This job runs automatically every day at 6 AM").
        *   Explain how to trigger it manually if needed.
    *   **4. How to Handle Failures:**
        *   Describe who receives the failure alerts.
        *   List common, known failure reasons and their resolution steps (e.g., "If it fails with a database connection error, check if the source system is online and then rerun the job.").
    *   **5. Key Contacts:** List the business owner and the technical owner/support team for the project.
3.  **Use Visuals:** Take screenshots of the Flow and the final dashboard and embed them in your Wiki page to make the guide more understandable.
4.  **Review and Publish:** Have a team member who is not familiar with the project read the guide to ensure it's clear and easy to follow. Then, share the link with the business users and the support team.

### 4. Resources and Tools
- **The Project Wiki:** The primary tool for creating and hosting the documentation, keeping it linked directly to the project itself.
- **Screenshots:** Essential for visually explaining the components.

### 5. Next Steps and Progression
- **Create a Template:** Develop a standard user guide template in a "Template" project. This can be copied into new projects to ensure all documentation is consistent.
- **Link from a Central Hub:** If you have a company-wide Confluence or SharePoint site, you can link to the Dataiku project's Wiki user guide from there.

### 6. Common Challenges and Solutions
- **Challenge:** "Nobody reads the documentation."
- **Solution:** Keep it concise and focused on the essential information. A one-page "Run Book" is more likely to be read than a 50-page technical manual. Also, during the handover meeting, walk the support team through the documentation so they know it exists and where to find it.
- **Challenge:** "The documentation becomes outdated as the project changes."
- **Solution:** Make documentation updates a mandatory part of your change management process. Any developer who modifies the pipeline is also responsible for updating the user guide to reflect that change.
`},{id:273,slug:"building-self‑service-applications-via-dataiku-apps",question:"How to get started with building self‑service applications via Dataiku Apps?",answer:`
### 1. Introduction/Overview
Dataiku Apps are a powerful feature for productionizing your data projects for non-technical users. They allow you to create a simple web application with a user-friendly interface that sits on top of your complex Dataiku Flow. This enables business users to "self-serve" by changing parameters, running pipelines, and viewing results without ever needing to see the underlying code or recipes.

### 2. Prerequisites
- **A working, parameterized Dataiku pipeline.** For example, a flow that takes a region or a date range as an input.
- **A clear idea for the application's purpose.** What do you want the user to be able to do?

### 3. Step-by-Step Instructions
1.  **Create a New Webapp:**
    *   In your project's top navigation bar, go to **... > Webapps**.
    *   Click **+ NEW WEBAPP**.
    *   Choose a type. **Standard** is a good place to start for simple interfaces.
2.  **Design the User Interface:**
    *   The Standard webapp editor lets you add "slides".
    *   **Slide 1: User Inputs.** Add widgets to get input from the user. For example, add a **Dropdown menu** to let the user select a country, or a **Date range** picker. Link these input widgets to your **Project Variables**.
    *   **Slide 2: Action.** Add a **Button**. Configure this button to run a specific **Scenario** in your project.
    *   **Slide 3: Results.** After the action slide, add a slide to display the output. You can add a **Chart**, a **Dataset table**, or other widgets that show the results generated by the scenario.
3.  **Write the Backend Logic (The Scenario):**
    *   Your webapp is the "front end." The "back end" is the scenario it triggers.
    *   This scenario must use the project variables set by the user in the webapp to run your parameterized flow. For example, it would build the flow where the filter now uses the country the user selected.
4.  **Test the App:**
    *   Click **View** to interact with your app as a user would.
    *   Enter a value, click the button, and check if the scenario runs and the correct results are displayed.
5.  **Share with Users:** Once complete, you can share a direct link to the webapp with your business users. They will only see the simple UI you've created, not the complex flow behind it.

### 4. Resources and Tools
- **Dataiku Webapps:** The feature for building the application UI.
- **Project Variables:** The mechanism for passing data from the app's UI to the backend flow.
- **Scenarios:** The engine that runs the backend logic when a user clicks a button in the app.

### 5. Next Steps and Progression
- **Code-based Apps:** For more complex, highly interactive UIs, you can build a webapp using Python with frameworks like **Dash** or **Streamlit**, which are fully integrated into Dataiku.
- **Publish to a Dashboard:** You can even add your completed webapp as a tile on a Dataiku Dashboard, making it part of a larger report.

### 6. Common Challenges and Solutions
- **Challenge:** "The app is not updating the results after the scenario runs."
- **Solution:** Your results slide needs to be refreshed. You may need to add a "Refresh" button or have the scenario, as its last step, use the Dataiku API to automatically refresh the webapp's content.
- **Challenge:** "The user experience is confusing."
- **Solution:** Keep it simple! A good self-service app has a very clear, guided purpose. Don't overload the user with too many options. Use text widgets and clear instructions on each slide to guide them through the process.
`},{id:274,slug:"hosting-brown‑bag-sessions-to-share-migration-learnings",question:"How to get started with hosting brown‑bag sessions to share migration learnings?",answer:`
### 1. Introduction/Overview
A "brown-bag" session is an informal training or knowledge-sharing meeting, often held during lunchtime. Hosting these sessions is a highly effective way to share learnings, showcase successes, and build a community of practice around a new platform like Dataiku, especially during a migration project.

### 2. Prerequisites
- **A topic to share:** A recently migrated workflow, a cool new feature you've learned, or a common challenge and its solution.
- **A meeting platform:** (e.g., Zoom, Google Meet, or a physical conference room).

### 3. Step-by-Step Instructions
1.  **Schedule a Recurring, Optional Meeting:**
    *   Schedule a 30-45 minute meeting on a regular basis (e.g., every two weeks).
    *   Make it clear that the meeting is optional and informal.
2.  **Choose a Focused Topic for Each Session:**
    *   Don't try to cover too much. Each session should have one clear topic.
    *   **Good topics for a migration context:**
        *   "Demo: Migrating the 'Daily Sales' Alteryx Flow to Dataiku"
        *   "Best Practices for Documenting Your Migrated Flows"
        *   "A Deep Dive into the Join Recipe vs. the Group Recipe"
        *   "How We Solved a Performance Bottleneck"
3.  **Prepare a Short Demo, Not a Long Presentation:**
    *   People learn by seeing, not by reading slides. Prepare a live demo in Dataiku.
    *   Spend no more than 5-10 minutes preparing. This should be lightweight.
    *   For a workflow demo, show the old Alteryx workflow, then walk through how you rebuilt it in Dataiku, explaining your choices along the way.
4.  **Facilitate Discussion:**
    *   The main goal is interaction. Leave at least half the time for Q&A and discussion.
    *   Encourage attendees to share their own experiences or ask questions about their own projects.
5.  **Record the Session:** Record the meeting and share the recording and any materials (like a link to the Dataiku project you demoed) with the wider team for those who couldn't attend.

### 4. Resources and Tools
- **A meeting platform with screen-sharing.**
- **A live Dataiku instance** for demos.

### 5. Next Steps and Progression
- **Rotate Presenters:** Don't have the same person present every time. Encourage different team members, even junior ones, to share something they've learned. This is a great professional development opportunity.
- **Create a "Center of Excellence" Wiki:** Create a central Wiki page that links to all the past brown-bag recordings and materials, creating a valuable, persistent knowledge base.

### 6. Common Challenges and Solutions
- **Challenge:** "Nobody shows up to the meetings."
- **Solution:** Make sure the topics are relevant and practical. Advertise the topic for each session in advance. Ask people what they want to learn about. Also, getting a senior leader to endorse the sessions can help signal their importance.
- **Challenge:** "I'm not an expert; I don't feel comfortable presenting."
- **Solution:** You don't have to be. A brown-bag is informal. You can present a challenge you are currently facing and use the session as a collaborative brainstorming opportunity. Sharing a problem is often just as valuable as sharing a solution.
`},{id:275,slug:"setting-up-sandbox-projects-for-user-experimentation",question:"How to get started with setting up sandbox projects for user experimentation?",answer:`
### 1. Introduction/Overview
A sandbox is a safe, consequence-free environment where users can learn, explore, and experiment without any risk of affecting production pipelines. Providing new users with a personal sandbox project is one of the most effective ways to accelerate their learning and encourage adoption of Dataiku.

### 2. Prerequisites
- **A Dataiku instance.**
- **Permissions to create new projects.**
- **A set of clean, non-sensitive sample datasets.**

### 3. Step-by-Step Instructions
1.  **Create a "Golden" Datasets Project:**
    *   First, create a central project called something like \`0_SANDBOX_DATASETS\`.
    *   Populate this project with a variety of clean, easy-to-understand, non-sensitive sample datasets (e.g., public data from Kaggle, or anonymized data from your own company).
    *   This project will be the shared source of data for all sandbox projects.
2.  **Create a Personal Sandbox Project for Each User:**
    *   For each new user, create a new, blank project for them.
    *   Use a clear naming convention, like \`SANDBOX_JohnDoe\`.
3.  **Set Up Permissions:**
    *   In the user's personal sandbox project, grant them **Administrator** rights. This gives them full control to do anything they want inside their own project.
    *   In the central \`0_SANDBOX_DATASETS\` project, grant all users **Reader** rights. This allows them to read the sample data but not change it.
4.  **Guide the User:**
    *   Show the user how to go into their sandbox project and import datasets from the shared "Golden" datasets project.
    *   Encourage them to try building their own flows, creating charts, and experimenting with different recipes.
    *   Reassure them that it is impossible for them to break anything outside of their own personal project.

### 4. Resources and Tools
- **Dataiku Projects:** The core container for the sandbox.
- **Project Permissions:** The key to isolating the sandboxes and protecting the source data.
- **Shared Projects:** The mechanism for providing the clean sample datasets.

### 5. Next Steps and Progression
- **Create a "Sandbox Challenge":** Give users a specific, fun challenge to solve in their sandbox, like "Can you build a flow to analyze this movie dataset and find the highest-rated directors?".
- **Project Templates:** For more advanced users, you could provide a "Template" project with a standard Flow Zone structure that they can duplicate into their sandbox to start with a good architectural pattern.

### 6. Common Challenges and Solutions
- **Challenge:** "Users are using their sandbox for real production work."
- **Solution:** This is a governance issue that needs to be addressed. It should be made very clear that sandbox projects are not for production use, are not backed up, and may be deleted. Real work must be done in governed, production-track projects.
- **Challenge:** "The instance is getting cluttered with too many sandbox projects."
- **Solution:** Implement a cleanup policy. For example, a Dataiku administrator can write a script that automatically archives or deletes sandbox projects that have been inactive for a certain period (e.g., 90 days).
`},{id:276,slug:"collecting-feedback-from-business-users-post‑migration",question:"How to get started with collecting feedback from business users post‑migration?",answer:`
### 1. Introduction/Overview
A migration project is not truly "done" when the technical work is complete. It's done when the business users are successfully using and getting value from the new system. Systematically collecting feedback from these users after the cutover is essential for ensuring adoption, identifying hidden issues, and demonstrating the project's success.

### 2. Prerequisites
- **A migrated pipeline** that is now live and being used by business stakeholders.
- **A list of the key users** and consumers of the pipeline's output.

### 3. Step-by-Step Instructions
1.  **Schedule a Post-Migration Check-in:**
    *   About one to two weeks after the new Dataiku pipeline has gone live, schedule a dedicated 30-minute feedback session with the primary business users.
2.  **Ask Open-Ended Questions:** Don't just ask "Is it working?". Guide the conversation with specific, open-ended questions:
    *   "How are you using the new dashboard/report in your daily work?"
    *   "Is the data you're seeing meeting your expectations? Is anything missing or confusing?"
    *   "Has this new pipeline made your process faster or easier? In what ways?"
    *   "What is one thing you like about the new process? What is one thing that could be improved?"
3.  **Listen and Document:**
    *   Your primary role in this meeting is to listen. Take detailed notes.
    *   Document the feedback, both positive and negative. Capture direct quotes if possible.
4.  **Create a "Feedback" Page in the Wiki:**
    *   In the project's **Wiki**, create a page to summarize the feedback you've collected. This creates a formal record.
5.  **Triage and Act on the Feedback:**
    *   Review the feedback with the project team.
    *   **Bugs/Issues:** If the feedback reveals any actual errors, create tickets and prioritize fixing them immediately.
    *   **Enhancement Requests:** If users suggest new features, log these as new user stories in your backlog to be prioritized for future development.
    *   **Positive Feedback:** Don't ignore this! Use positive quotes and testimonials when reporting on the project's success to leadership.

### 4. Resources and Tools
- **A meeting platform.**
- **The Project Wiki:** To document the feedback.
- **A backlog management tool** (like JIRA) to log enhancement requests.

### 5. Next Steps and Progression
- **Ongoing Relationship:** Don't make this a one-time event. Establish a regular check-in cadence (e.g., monthly) with your key business users to maintain an open channel for feedback.
- **Surveys:** For a broader user base, you can use a simple survey tool (like Google Forms) to collect feedback at a larger scale.

### 6. Common Challenges and Solutions
- **Challenge:** "The users are not providing any feedback."
- **Solution:** They may be too busy or unsure of what to say. You need to be proactive. Instead of just asking a general question, show them the dashboard and ask about a specific component. "We added this chart showing regional trends. Is this useful for you? How could we make it better?" Specific questions elicit specific answers.
- **Challenge:** "The users have a lot of new requests that were not in the original scope."
- **Solution:** This is a sign of success! It means they are engaged and are now thinking of new ways to use the data. It's crucial to manage these expectations. Log all new ideas in the backlog, but explain that they will need to be prioritized against other business needs and will be tackled in a future project or sprint.
`},{id:277,slug:"updating-coding-standards-based-on-migrated-practices",question:"How to get started with updating coding standards based on migrated practices?",answer:`
### 1. Introduction/Overview
A large-scale migration is a massive learning opportunity. As you migrate dozens of workflows, your team will develop new, better ways of doing things. It's crucial to capture these "migrated practices" and formalize them by updating your team's official coding and development standards. This ensures that the lessons learned are applied consistently to all future projects.

### 2. Prerequisites
- **Completion of at least one major migration "wave".**
- **A pre-existing set of team development standards** (even if informal).
- **A "Center of Excellence" team or lead** responsible for maintaining standards.

### 3. Step-by-Step Instructions
1.  **Hold a "Lessons Learned" Retrospective:**
    *   After completing a significant migration wave, gather the entire development team.
    *   The goal of the meeting is to discuss: "What new patterns did we invent? What old patterns should we now forbid? What worked well that we should make a standard?"
2.  **Identify New Best Practices:** Look for common patterns that emerged during the migration.
    *   **Example 1:** "We found that using a dedicated 'Staging' Flow Zone to land and type all raw data before preparation was a very effective pattern." -> **New Standard:** "All new projects must have a Staging zone."
    *   **Example 2:** "We wrote a Python function to standardize addresses that was used in five different workflows." -> **New Standard:** "This function should be moved to a central, shared library project and all new flows must use it."
3.  **Update Your Central Standards Document:**
    *   Go to your team's central documentation for standards (e.g., in a company-wide Wiki or Confluence page).
    *   Add new sections or update existing ones to reflect these new best practices. Be specific and provide examples.
4.  **Communicate the Changes:**
    *   It's not enough to just update the document. Announce the updated standards to the entire team.
    *   Explain the "why" behind each new standard, showing how it will lead to better, more maintainable projects.
5.  **Enforce the New Standards:**
    *   Update your "review checklists" for pull requests to include checks for the new standards.
    *   Ensure that senior developers are enforcing the new standards during their code and flow reviews.

### 4. Resources and Tools
- **Retrospective Meetings:** The forum for identifying the new best practices.
- **A Central Wiki or Documentation Hub:** The single source of truth for your team's standards.
- **Peer Reviews / Pull Requests:** The primary mechanism for enforcing the standards.

### 5. Next Steps and Progression
- **Create Reusable Templates:** Turn your new best practices into tangible assets. For example, create a new **Project Template** that already includes your standard Flow Zone structure and a link to the shared code library.
- **Continuous Improvement:** Make this an ongoing process. Your standards should be a living document, updated after every major project or migration wave.

### 6. Common Challenges and Solutions
- **Challenge:** "Developers are not following the new standards."
- **Solution:** First, ensure the standards are well-documented and have been clearly communicated. Second, enforcement during code review is non-negotiable. A pull request that doesn't follow the standards should not be approved. It helps if the standards are automated (e.g., with a code linter) so that it's not based on one person's opinion.
- **Challenge:** "The standards are becoming too bureaucratic and complex."
- **Solution:** Good standards should enable speed and quality, not hinder them. If developers are finding a standard to be cumbersome, listen to their feedback. Hold a discussion to see if the standard can be simplified or if there's a better way to achieve the same goal.
`},{id:278,slug:"building-a-central-wiki-of-migrated-logic-and-flows",question:"How to get started with building a central wiki of migrated logic and flows?",answer:`
### 1. Introduction/Overview
During a large migration, you will create dozens of new projects in Dataiku. Without a central catalog, it can become impossible to find things or understand what has been migrated. Building a central **Wiki** or "Migration Hub" serves as a master index, documenting the purpose of each migrated workflow and linking to its new location in Dataiku.

### 2. Prerequisites
- **An ongoing migration project.**
- **A dedicated place to host the Wiki.** This could be a **dedicated Dataiku project** or a company-wide tool like **Confluence** or **SharePoint**.

### 3. Step-by-Step Instructions
1.  **Choose a Hosting Location:**
    *   Using a dedicated Dataiku project as the Wiki host is a great option because it stays within the same platform. Create a new, empty project called \`MIGRATION_HUB\`.
2.  **Design the Wiki Structure:**
    *   On the main page of your Wiki, create a clear structure. A good structure is to group by business domain.
    *   **Example Structure:**
        *   **Migration Hub Home**
            *   Link to Migration Plan & Timeline
            *   Link to Naming Conventions
        *   **Finance Workflows**
            *   List of all migrated finance workflows.
        *   **Marketing Workflows**
            *   List of all migrated marketing workflows.
3.  **Create an Entry for Each Migrated Workflow:**
    *   As soon as a workflow migration is complete, the developer is responsible for adding an entry to the central Wiki.
    *   Each entry should be a new page or a new row in a table and should contain:
        *   **Legacy Workflow Name:** The original name in Alteryx.
        *   **Business Purpose:** A one-sentence description.
        *   **New Dataiku Project:** A direct link to the Dataiku project where the new flow lives.
        *   **Key Outputs:** Links to the final output datasets or dashboards.
        *   **Business Owner:** The primary stakeholder for this workflow.
4.  **Maintain the Wiki:** Make updating the central Wiki a mandatory step in your "definition of done" for any migration task. The catalog is only useful if it's kept up-to-date.

### 4. Resources and Tools
- **Dataiku Wiki:** A powerful, built-in tool for this.
- **Confluence/SharePoint:** Good alternatives if your company already uses them for documentation.
- **A standardized template** for the Wiki entries to ensure consistency.

### 5. Next Steps and Progression
- **Searchability:** A well-structured Wiki becomes a searchable catalog. A user can go to the Migration Hub and search for "Sales Report" to quickly find the new Dataiku project that produces it.
- **Link from the Original Project:** In the description of your newly migrated Dataiku project, include a link *back* to its entry in the central Migration Hub Wiki. This creates a two-way linkage.

### 6. Common Challenges and Solutions
- **Challenge:** "Nobody is updating the central Wiki."
- **Solution:** This must be a non-negotiable part of the process. A migration task is not "done" until the Wiki is updated. This can be enforced by a project manager or a lead developer during the final review of the task.
- **Challenge:** "The Wiki is becoming a mess."
- **Solution:** Good structure is key. Use a clear hierarchy and a consistent template for your entries. It's the responsibility of the "Center of Excellence" or the migration lead to periodically review and clean up the Wiki to ensure it stays organized.
`},{id:279,slug:"enabling-self‑service-reuse-of-dataiku-recipes",question:"How to get started with enabling self‑service reuse of Dataiku recipes?",answer:`
### 1. Introduction/Overview
Enabling self-service reuse is about empowering your users to build their own analyses using pre-built, trusted components. This prevents them from having to reinvent the wheel and ensures consistency. In Dataiku, this can be achieved by creating shared "golden" datasets and reusable, documented recipe components.

### 2. Prerequisites
- **A Dataiku project** where you have built a useful, reusable piece of logic or a clean dataset.
- **A group of business analysts or data scientists** who could benefit from reusing your work.

### 3. Step-by-Step Instructions: A Framework for Reuse

#### Method 1: Sharing "Golden" Datasets
1.  **When to Use:** This is the most common and important form of reuse.
2.  **How:**
    1.  Create a dedicated project called \`SHARED_DATASETS\` or \`GOLDEN_DATA\`.
    2.  In this project, build and validate the key, authoritative datasets that many other projects will need (e.g., \`customers_master\`, \`product_catalog_clean\`).
    3.  Thoroughly document each dataset, including descriptions for every column.
    4.  Grant all other developer groups **Reader** access to this shared project.
3.  **How Users Reuse:** In their own projects, users can now go to **+ DATASET > Import Dataset from another project**. They can select one of the "golden" datasets and use it as the trusted starting point for their own analysis.

#### Method 2: Sharing Visual Recipe Logic
1.  **When to Use:** When you have a standard set of data cleaning steps that you want others to apply.
2.  **How:**
    1.  Open the **Prepare recipe** that contains your reusable steps.
    2.  Select all the steps in the script panel on the left.
    3.  Click **Copy**.
    4.  You can now paste the JSON definition of these steps into a central **Wiki** page.
    5.  Other users can copy this JSON and paste it into their own Prepare recipes to instantly apply the same logic.

#### Method 3: Sharing Code (Functions)
1.  **When to Use:** When you have a reusable Python or R function.
2.  **How:** Place the function in your project's **Library**. If you need to share it across projects, place it in a central, shared library project and have other projects add it as a dependency. (See question on reusable Python functions).

### 4. Resources and Tools
- **Shared Projects and Permissions:** The core mechanism for sharing datasets.
- **Copy/Paste for Recipe Steps:** An easy way to share visual logic.
- **Project Libraries:** For sharing code.

### 5. Next Steps and Progression
- **Custom Plugins:** For the ultimate self-service experience, a senior developer can wrap a complex piece of logic (like a specific data quality check or a complex transformation) into a **custom visual recipe** by building a plugin. This makes the logic available to all users in the simple, point-and-click recipe menu.

### 6. Common Challenges and Solutions
- **Challenge:** "Users are re-creating the same datasets in their own projects instead of using the shared one."
- **Solution:** This is a communication and training issue. You must actively promote your "golden" datasets. Hold a session to show users where they are and why using them is the best practice. Make it clear that the shared datasets are the single source of truth.
- **Challenge:** "A user wants to change something in a shared dataset."
- **Solution:** They can't, because they only have "Reader" access. This is a good thing! It prevents unauthorized changes. If a change is needed, they should submit a formal request to the team that owns the shared dataset. This ensures changes are managed and governed properly.
`},{id:280,slug:"mentoring-junior-team-members-on-dataiku-migration-patterns",question:"How to get started with mentoring junior team members on Dataiku migration patterns?",answer:`
### 1. Introduction/Overview
Mentoring junior team members during a migration is key to scaling your team's capabilities and ensuring the long-term success of the new platform. A good mentoring approach involves structured guidance, hands-on pairing, and constructive feedback, helping them move from simple tasks to taking ownership of complex workflows.

### 2. Prerequisites
- **A junior team member** new to Dataiku or data migrations.
- **A senior developer or SME** to act as the mentor.
- **A backlog of migration tasks** of varying complexity.

### 3. Step-by-Step Instructions: A Mentoring Roadmap

1.  **Phase 1: Guided Learning (The "I Do, You Watch" phase):**
    *   **Action:** The mentor takes the lead on migrating the first few simple workflows.
    *   **Mentoring:** Schedule a **pair programming** session. The mentor shares their screen and migrates a workflow live, explaining their thought process out loud. "First, I'm looking at the Alteryx join tool. I see it's an inner join, so I'll choose the 'Inner join' type in the Dataiku Join recipe..."
    *   **Goal:** To familiarize the junior developer with the basic patterns and the "translation" process.
2.  **Phase 2: Paired Work (The "We Do" phase):**
    *   **Action:** Assign a slightly more complex workflow to the pair.
    *   **Mentoring:** This time, the junior developer "drives" (shares their screen and builds the flow). The mentor observes, guides, and answers questions. "That's a good start on the Prepare recipe. Have you considered how to handle the null values in that column?"
    *   **Goal:** To build the junior's hands-on skills and confidence.
3.  **Phase 3: Supervised Independence (The "You Do, I Watch" phase):**
    *   **Action:** Assign a workflow for the junior developer to migrate on their own.
    *   **Mentoring:** The mentor is available for questions but does not actively participate. The key mentoring activity here is the **review**. If using Git, this is a formal **Pull Request review**. The mentor provides specific, constructive feedback on the completed work.
    *   **Goal:** To develop problem-solving skills and ownership.
4.  **Phase 4: Full Independence:**
    *   **Action:** The junior developer is now able to take on migration tasks independently, from analysis to implementation and validation.
    *   **Mentoring:** The mentor remains a resource for very complex or novel challenges but is no longer involved in the day-to-day work.

### 4. Resources and Tools
- **Pair Programming / Screen Sharing:** The most effective tool for real-time mentoring.
- **Git and Pull Requests:** The framework for formal, asynchronous code/flow reviews.
- **A Prioritized Backlog:** Allows the mentor to assign tasks of appropriate difficulty for each phase.

### 5. Next Steps and Progression
- **Peer Mentoring:** As a junior developer becomes more proficient, encourage them to help mentor the next new person who joins the team. Teaching others is a great way to solidify knowledge.
- **Specialization:** Help the mentee find an area of the platform they are interested in (e.g., machine learning, automation) and encourage them to become a specialist in that area.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't have time to mentor; I'm too busy with my own migration tasks."
- **Solution:** Mentoring is an investment, not a cost. The time you spend training a junior developer will be paid back many times over when they are able to take work off your plate. You need to formally allocate time for it in your sprint planning.
- **Challenge:** "The junior developer is struggling to grasp a concept."
- **Solution:** Try a different approach. If explaining it isn't working, try finding a different tutorial in the Dataiku Academy or a blog post that explains the concept in a new way. Sometimes a different voice or a different example is all that's needed for it to click.
`},{id:281,slug:"applying-access-control-roles-in-migrated-projects",question:"How to get started with applying access control roles in migrated projects?",answer:`
### 1. Introduction/Overview
Once a workflow is migrated to a new Dataiku project, you must configure its access controls to ensure that only authorized users can view or modify it. This is a critical governance step. Dataiku uses a role-based access control (RBAC) model, where you assign permissions to **groups** of users at the project level.

### 2. Prerequisites
- **A migrated Dataiku project.**
- **A defined set of user groups** in Dataiku (e.g., "Finance_Analysts", "Marketing_Team", "Data_Engineers"). This is typically set up by a Dataiku administrator.
- **Project administrator rights** for the migrated project.

### 3. Step-by-Step Instructions
1.  **Navigate to Project Permissions:**
    *   In your newly migrated project, click the **Settings** icon (the gear) in the top menu.
    *   Go to the **Permissions** tab.
2.  **Add Groups:**
    *   By default, only the project creator (and instance admins) will have access.
    *   Click the **+ ADD GROUP** button.
3.  **Assign Permissions to Each Group:**
    *   Select a group that needs access (e.g., \`Finance_Analysts\`).
    *   Assign the appropriate permission level for that group's role in this specific project. The main permission levels are:
        *   **Reader:** Can view everything in the project (datasets, flows, dashboards) but cannot change, run, or build anything. This is the correct permission for **business consumers** of a report.
        *   **Contributor:** Can read, edit, and run most things in the project. This is for **developers** who will be maintaining or enhancing the project.
        *   **Administrator:** Can do everything a contributor can, plus manage the project's settings, including these permissions. This is for the **project owner** or tech lead.
4.  **Repeat for All Necessary Groups:** Add all the groups that need access and assign their appropriate permission level.
5.  **Save:** Click **Save**. The permissions are applied immediately.

### 4. Resources and Tools
- **Project Settings > Permissions:** The UI for controlling project-level access.
- **Administration > Security > Groups:** Where a Dataiku admin creates and manages the user groups.

### 5. Next Steps and Progression
- **Least Privilege Principle:** Always grant the minimum level of permission required. If a user only needs to view a dashboard, give their group "Reader" access, not "Contributor".
- **Connection-Level Permissions:** Be aware that a user's ability to access data also depends on whether their group has been granted permission to use the underlying **Data Connection** (this is managed by an admin).
- **Audit Permissions:** Periodically review the permissions on your critical projects to ensure they are still correct and that ex-employees or people who have changed roles have been removed.

### 6. Common Challenges and Solutions
- **Challenge:** "I gave a user 'Reader' access, but they still can't see the project."
- **Solution:** They might not be in the group you added. Or, more likely, the project itself might be in a "Project Folder" that they don't have permission to see. Permissions can be inherited from these folders. Check with your instance admin.
- **Challenge:** "How do I give a user access to only one dashboard but not the rest of the project?"
- **Solution:** Dataiku's security model is at the project level. You cannot grant permissions to a single object inside a project. The standard solution to solve this is to create a **separate, dedicated project** just for that dashboard. Use a **Sync** recipe to share the final data into the new dashboard project. Then, you can grant the user "Reader" access to only that new, isolated project.
`},{id:282,slug:"tracking-lineage-from-alteryx-origin-to-dataiku-outputs",question:"How to get started with tracking lineage from Alteryx origin to Dataiku outputs?",answer:`
### 1. Introduction/Overview
Data lineage is the map of your data's journey. During and after a migration, it's crucial to be able to trace a field in a final Dataiku report all the way back to its origin in the legacy Alteryx world. This is achieved by combining Dataiku's automatic, technical lineage with manual documentation that bridges the gap to the original source system.

### 2. Prerequisites
- **A migrated Dataiku flow.**
- **Access to the original Alteryx workflow** for reference.

### 3. Step-by-Step Instructions
1.  **Document the Origin in the Input Dataset:**
    *   This is the most critical step. In your Dataiku flow, find the very first "raw" input datasets.
    *   Open each one, go to the **Summary** tab, and use the **Description** field to clearly document its origin.
    *   **Good Description Example:** "Raw customer data, ingested from the Alteryx workflow 'Finance_Reporting.yxmd', which pulls from the 'dbo.customers' table in the SQL-PROD-01 server."
2.  **Let Dataiku Handle the Rest:**
    *   Once you have documented the origin on your input datasets, Dataiku's automatic lineage takes over.
    *   Every subsequent recipe (Prepare, Join, etc.) and dataset in your flow will have its lineage traced automatically.
3.  **Trace the Full Lineage:**
    *   To see the end-to-end lineage, open your **final output dataset**.
    *   Go to the **Lineage** tab.
    *   Select a column. The lineage graph will visually show you how that column was created, step-by-step, all the way back to the input datasets.
    *   When you click on the input dataset in the lineage graph, its **Description** will appear, completing the trace back to the Alteryx origin you documented in Step 1.

### 4. Resources and Tools
- **The Description Field:** The key tool for manually documenting the link to the legacy world.
- **The Lineage Tab:** Dataiku's powerful, automated feature for tracing technical lineage within the flow.

### 5. Next Steps and Progression
- **Create a "Data Catalog" Wiki:** For a more formal approach, create a page in your project's Wiki that serves as a data catalog. For each source dataset, have an entry that documents its Alteryx origin, the business owner, and its purpose.
- **Auditing:** Use the lineage graph as proof for auditors. You can visually demonstrate the entire journey of a data point, from the legacy source system right through to the final report.

### 6. Common Challenges and Solutions
- **Challenge:** "The lineage graph is broken."
- **Solution:** This can happen if a code recipe reads data from a source that is not declared as a formal input (e.g., reading from a hardcoded file path). To maintain lineage, all data sources must be declared as input datasets to the recipe.
- **Challenge:** "We have so many sources, it's hard to document them all."
- **Solution:** Prioritize. Start by documenting the origins for your most critical, high-risk, or auditable data pipelines. You can then work through the less critical ones over time.
`},{id:283,slug:"enforcing-regulatory-compliance-in-migrated-logic",question:"How to get started with enforcing regulatory compliance in migrated logic?",answer:`
### 1. Introduction/Overview
When migrating data pipelines, especially those handling sensitive information, you must ensure the new system adheres to regulatory compliance standards like GDPR, HIPAA, or CCPA. Dataiku provides the tools for access control, data masking, and auditing necessary to build and prove compliance.

### 2. Prerequisites
- **A clear understanding of the specific compliance rules** you need to enforce.
- **Collaboration with your company's compliance or legal team.**
- **A migrated Dataiku pipeline** that processes sensitive data.

### 3. Step-by-Step Instructions: A Compliance Framework

1.  **Identify and Tag Sensitive Data (PII):**
    *   The first step is to know where your sensitive data is.
    *   Go through your datasets and use **Tags** to label any dataset containing Personally Identifiable Information (PII) with a \`PII\` tag.
    *   You can even use the **Meanings** feature to tag specific columns as containing emails, phone numbers, etc.
2.  **Enforce the Principle of Least Privilege:**
    *   Use **Project Permissions** to strictly control access to projects containing PII.
    *   Only users with a legitimate business need should be in the groups that have "Reader" or "Contributor" access.
3.  **Implement Data Masking or Anonymization:**
    *   If you need to share data with users who should not see the raw PII, create a new, anonymized version of the dataset.
    *   Use a **Prepare recipe** to mask or remove the sensitive columns. For example:
        *   Use a **Find and Replace** processor with a regex to remove a credit card number.
        *   Use a **Formula** processor with a hash function (\`sha256()\`) to create an irreversible, anonymized ID for a user.
4.  **Demonstrate Provenance with Lineage:**
    *   During an audit, use Dataiku's **Lineage** graph to prove how sensitive data is handled.
    *   You can visually show an auditor that the raw PII dataset is only accessed by a specific, secure recipe, and that the output shared with wider audiences is the anonymized version.
5.  **Document Your Compliance Controls:**
    *   In the **Project Wiki**, create a "Compliance" page.
    *   Explicitly document the controls you have put in place to meet each requirement of the regulation. For example: "To adhere to GDPR's Right to be Forgotten, we implement a filter in the \`prepare_customers\` recipe to exclude users who have requested deletion."

### 4. Resources and Tools
- **Tags and Meanings:** For identifying and classifying sensitive data.
- **Project Permissions:** For access control.
- **Prepare Recipe:** For implementing masking and anonymization.
- **Lineage Graph:** For auditing and proving data provenance.
- **Project Wiki:** For formally documenting your compliance strategy.

### 5. Next Steps and Progression
- **Automated Data Retention:** For regulations that require data to be deleted after a certain period, create an automated **Scenario** with a Python step that deletes old data partitions.
- **Formal Sign-offs:** For regulated industries, Dataiku has features for formal project sign-offs to create an auditable approval trail.
- **Automated Governance Checks:** Create a scenario that scans all projects and checks for compliance violations (e.g., "alert if a dataset tagged as \`PII\` does not have restricted permissions").

### 6. Common Challenges and Solutions
- **Challenge:** "How do I know where all the PII is?"
- **Solution:** Dataiku has a "PII Detection" tool that can automatically scan your datasets and suggest columns that may contain PII. This can be a great starting point for your tagging efforts.
- **Challenge:** "Anonymizing the data makes it useless for our analysts."
- **Solution:** This is a common trade-off. You may need to create multiple versions of the data. A fully anonymized version for general use, and a "pseudo-anonymized" version (e.g., with hashed IDs) for analysts who need to join datasets but don't need to see the raw names or emails. Access to the pseudo-anonymized version would still be more restricted.
`},{id:284,slug:"capturing-transformation-metadata-within-dataiku",question:"How to get started with capturing transformation metadata within Dataiku?",answer:`
### 1. Introduction/Overview
Transformation metadata is information *about* your data transformations. Capturing this metadata is essential for governance, maintainability, and collaboration. Dataiku is designed for this, with built-in features to document the "why" and "what" of every step in your pipeline, creating a rich, self-documenting system.

### 2. Prerequisites
- **A Dataiku project with a data pipeline.**
- **A team discipline** to capture metadata as part of the development process.

### 3. Step-by-Step Instructions: The Metadata Layers

1.  **High-Level Metadata (The "Why"):**
    *   **Where:** The **Project Wiki**.
    *   **What to Capture:** Use the Wiki for narrative, long-form documentation. Create a page that describes the overall business purpose of the pipeline, the data sources, and the intended outputs. This explains *why* the flow exists.
2.  **Object-Level Metadata (The "What"):**
    *   **Where:** The **Summary** tab of every dataset, recipe, and model.
    *   **What to Capture:**
        *   **Description:** This is the most important field. Write a clear, one-sentence description for every single object. This description appears on hover in the Flow.
        *   **Tags:** Use tags for classification (e.g., \`status:validated\`, \`source:sfdc\`, \`PII\`).
        *   **Custom Metadata:** Use key-value pairs for structured information (e.g., \`Data Owner: Finance Team\`, \`Data Quality Score\`: \`95%\`).
3.  **Column-Level Metadata (The "Data Dictionary"):**
    *   **Where:** In a dataset's **Settings > Schema** tab.
    *   **What to Capture:** You can add a **description for each individual column**. This is where you explain what a specific field means (e.g., "cust_ltv: Customer Lifetime Value, calculated as..."). This creates a living data dictionary.
4.  **Step-Level Metadata (The "How"):**
    *   **Where:** Inside a **Prepare recipe**.
    *   **What to Capture:** You can add a description to each individual processor step. This is useful for explaining a complex **Formula** or a tricky **Filter** condition.

### 4. Resources and Tools
- **The Project Wiki:** For long-form, narrative documentation.
- **The Summary Tab (Descriptions, Tags, Custom Metadata).**
- **The Dataset Schema editor.**
- **The Prepare recipe step descriptions.**

### 5. Next Steps and Progression
- **Create a Metadata Standard:** Define a company-wide standard for what metadata is required for different types of projects and objects.
- **Automate Metadata Extraction:** Use the Dataiku Python API to write a script that can loop through a project, extract all this metadata, and generate a formal governance report or populate an external data catalog.

### 6. Common Challenges and Solutions
- **Challenge:** "It's too much work to document everything."
- **Solution:** Start small and be consistent. Make it a team rule that, at a minimum, **every recipe must have a clear, one-sentence description**. This single habit provides a huge amount of value for very little effort.
- **Challenge:** "The metadata is out of date."
- **Solution:** This must be part of your change management process. When a developer modifies an object, they are also responsible for updating its description and other metadata. This should be verified during peer review.
`},{id:285,slug:"approving-deployments-from-dev-to-prod-migration-branches",question:"How to get started with approving deployments from dev to prod migration branches?",answer:`
### 1. Introduction/Overview
Moving a project from a development environment to a production environment is a critical control point. A proper deployment process ensures that only tested, reviewed, and approved changes go live. In Dataiku, this is typically handled by creating **Project Bundles** and having a formal approval workflow.

### 2. Prerequisites
- **Separate Dataiku instances** for development and production.
- **A Dataiku project** on the dev instance that is ready for deployment.
- **A defined deployment manager or team** with administrator rights on the production instance.

### 3. Step-by-Step Instructions: A Manual Approval Workflow

1.  **Development and Testing on Dev Instance:**
    *   All development and testing is done on the development instance. If using Git, this work should be done on a feature branch and then merged to a main/release branch after a pull request review.
2.  **Create the Project Bundle:**
    *   When the project is ready for release, a project lead on the dev instance creates a **bundle**.
    *   Go to the project **... > Export**. This will create a \`.zip\` file that contains the entire project definition (flows, recipes, settings, etc.).
3.  **Submit for Approval:**
    *   The developer notifies the deployment manager that a new version of the project is ready for deployment and provides them with the bundle file. This can be done via a ticketing system like JIRA.
4.  **Deployment to Production (Deployment Manager's Task):**
    *   The deployment manager logs into the **production** Dataiku instance.
    *   From the homepage, they click **+ IMPORT PROJECT** and upload the bundle \`.zip\` file.
5.  **Configure for Production:**
    *   During the import process, Dataiku will prompt the deployment manager to remap connections and variables. This is where they will point the project to the production database connections and set the production values for any project variables.
6.  **Run Post-Deployment Checks:** After deploying, the deployment manager should run a "smoke test" scenario in production to ensure the pipeline runs successfully in the new environment.

### 4. Resources and Tools
- **Project Bundles (\`.zip\` files):** The deployable artifacts of a Dataiku project.
- **Separate Dev/Prod Instances:** The core principle of a safe deployment process.
- **A Ticketing System (JIRA, etc.):** For managing the formal approval workflow.

### 5. Next Steps and Progression
- **Automated Deployments (CI/CD):** For more advanced teams, this entire process can be automated. A CI/CD tool like Jenkins can be configured to:
    1.  Be triggered when code is merged to the main branch.
    2.  Automatically create the bundle using the Dataiku API.
    3.  Require a manual approval step in the CI/CD tool.
    4.  Upon approval, automatically deploy the bundle to the production instance.

### 6. Common Challenges and Solutions
- **Challenge:** "A deployment broke the production pipeline."
- **Solution:** This means the testing and approval process was not sufficient. You need to perform a post-mortem to understand what was missed. Was the change tested with production-like data on a staging environment? Was the approval checklist followed?
- **Challenge:** "Deployments are slow and manual."
- **Solution:** This is the primary motivation for moving to an automated CI/CD pipeline. Automating the bundling and deployment process reduces manual effort, minimizes human error, and speeds up your time to delivery.
`},{id:286,slug:"maintaining-audit-trails-across-recipe-and-dataset-changes",question:"How to get started with maintaining audit trails across recipe and dataset changes?",answer:`
### 1. Introduction/Overview
An audit trail is a chronological record of changes that allows you to answer the question, "Who changed what, and when?". This is essential for debugging, accountability, and compliance. Dataiku automatically maintains several layers of audit trails, which you just need to know how to access.

### 2. Prerequisites
- **A Dataiku project** where users have been making changes.
- **Appropriate permissions** to view the logs (project-level or global admin).

### 3. Step-by-Step Instructions: Accessing the Audit Trails

#### Level 1: Project Timeline (For Project-Specific Changes)
1.  **When to use:** When you want to see the history of a single project.
2.  **How to access:**
    *   In your project, go to the **...** menu in the top navigation bar and select **Timeline**.
3.  **What it shows:** A chronological feed of all significant changes made to the project:
    *   "User X created recipe Y."
    *   "User Z modified dataset W."
    *   "User A ran scenario B."
    *   This is your first stop for understanding the recent history of a project.

#### Level 2: Git Commit History (The Gold Standard for Code/Logic)
1.  **When to use:** If your project is connected to Git, this provides the most detailed and robust audit trail for any changes to your project's *definition* (recipes, flow structure, etc.).
2.  **How to access:**
    *   Go to your Git provider's web interface (e.g., GitHub, GitLab).
    *   Navigate to your project's repository and click on "Commits".
3.  **What it shows:**
    *   A list of every single commit, with the author, a timestamp, and a commit message explaining the change.
    *   For each commit, you can see a "diff" showing the exact lines of code or configuration that were added, deleted, or modified.

#### Level 3: Global Audit Log (For Instance-Wide Security Events)
1.  **When to use:** For security and compliance audits of the entire platform.
2.  **How to access (Admin Only):**
    *   Go to **Administration > Logs > Global Audit Log**.
3.  **What it shows:** High-level security events like user logins (success and failure), permissions changes, creation of new connections, etc.

### 4. Resources and Tools
- **Project Timeline:** For day-to-day "who did what" questions.
- **Git History:** For deep, line-by-line auditing of code and logic changes.
- **Global Audit Log:** For platform-wide security auditing.

### 5. Next Steps and Progression
- **Enforce Good Commit Messages:** Train your team to write clear, descriptive commit messages when using Git. A message like "Fixed bug in sales calculation" is much more useful for an audit trail than "stuff".
- **External Log Shipping:** For long-term retention and advanced analysis, an administrator can configure Dataiku to ship its audit logs to an external logging system like Splunk or ELK.

### 6. Common Challenges and Solutions
- **Challenge:** "I need to know who viewed a dashboard."
- **Solution:** This level of read-access auditing is not typically captured in the main audit trails. It may be available in more detailed access logs on the Dataiku server, but this would require investigation by an administrator. The primary control here is preventative: use project permissions to ensure only authorized people can view the dashboard in the first place.
- **Challenge:** "The Project Timeline is too noisy."
- **Solution:** Use the filter controls at the top of the Timeline page. You can filter by user or by the type of object that was changed to find the specific event you are looking for.
`},{id:287,slug:"archiving-legacy-alteryx-projects-with-documented-equivalence",question:"How to get started with archiving legacy Alteryx projects with documented equivalence?",answer:`
### 1. Introduction/Overview
Once a workflow has been successfully migrated to Dataiku and has been running reliably in production, the final step is to formally decommission and archive the original Alteryx project. This must be done carefully, with clear documentation proving that the new system is an approved replacement.

### 2. Prerequisites
- **A migrated Dataiku pipeline** that has been fully validated and is running in production.
- **A successful parallel run period** with no discrepancies.
- **Sign-off from the business owner** to decommission the legacy workflow.

### 3. Step-by-Step Instructions
1.  **Final Validation and Sign-Off:**
    *   Perform one final comparison of the Alteryx and Dataiku outputs to prove they are identical.
    *   Present this evidence to the business owner and get their formal, written sign-off (e.g., via email or in a ticket) to proceed with decommissioning.
2.  **Create an Archive Package:**
    *   Create a folder on a secure, long-term archive server (e.g., a dedicated network drive or cloud storage location).
    *   The folder should be named clearly (e.g., \`Alteryx_Archive_Finance_Reporting_2023-10-27\`).
    *   In this folder, place:
        1.  The Alteryx workflow file (\`.yxmd\`).
        2.  A copy of the source-to-target mapping document.
        3.  A copy of the validation results.
3.  **Create a "Tombstone" Document:**
    *   Inside the archive folder, create a simple text or PDF file named \`README_TOMBSTONE.txt\`.
    *   This document is critical. It should contain:
        *   The name of the legacy Alteryx workflow.
        *   The date it was decommissioned.
        *   The name of the new, replacement Dataiku project.
        *   A direct URL link to the new Dataiku project.
        *   The name of the business owner who approved the decommissioning.
4.  **Disable the Legacy Schedule:**
    *   Go into Alteryx Server or the legacy scheduler.
    *   **Disable** (do not delete yet) the scheduled job for the Alteryx workflow.
5.  **Communicate the Decommissioning:** Send a formal communication to all stakeholders and support teams informing them that the legacy workflow has been replaced and that they should now use the new Dataiku outputs.
6.  **Final Deletion (After a waiting period):** After a safe waiting period (e.g., one or two months) with no issues, you can permanently delete the workflow from the Alteryx server.

### 4. Resources and Tools
- **A long-term archive storage location.**
- **Your validation and mapping documents.**

### 5. Next Steps and Progression
- **Decommissioning Dashboard:** Create a dashboard that tracks the status of your migration project, including a list of all workflows that have been successfully migrated and decommissioned. This helps in tracking the overall progress of retiring your legacy platform.

### 6. Common Challenges and Solutions
- **Challenge:** "A user is still using the output from the old Alteryx workflow."
- **Solution:** This indicates a failure in communication or change management. You need to reach out to the user, understand why they are still using the old output, and guide them to the new, official source in Dataiku. You may need to provide them with additional training.
- **Challenge:** "We are nervous about deleting the old workflow."
- **Solution:** That's a healthy fear. The two-stage disable-then-delete process helps with this. By disabling the schedule first, you prevent it from running but keep the workflow itself available in case you need to reactivate it quickly for an emergency. Only after a successful "cool-down" period should you proceed with the permanent deletion.
`},{id:288,slug:"embedding-privacy-logic-pii-masking-anonymization-in-recipes",question:"How to get started with embedding privacy logic (PII masking, anonymization) in recipes?",answer:`
### 1. Introduction/Overview
Handling sensitive data in a privacy-compliant way (e.g., for GDPR or HIPAA) is a critical data engineering task. In Dataiku, you can embed logic to mask, hash, or anonymize Personally Identifiable Information (PII) directly into your data pipelines using a **Prepare recipe**, creating a safe, shareable version of your data.

### 2. Prerequisites
- **A dataset containing sensitive PII columns** (e.g., email, name, phone number).
- **A clear understanding of your privacy requirements** (e.g., "the last four digits of a social security number must be masked").

### 3. Step-by-Step Instructions
1.  **Identify PII Columns:** In your Flow, open the dataset containing sensitive data. It's a best practice to **tag** the dataset with a \`PII\` tag for easy identification.
2.  **Create a "Anonymization" Prepare Recipe:**
    *   Select your sensitive dataset and create a new **Prepare** recipe.
    *   Name the output dataset clearly, e.g., \`customers_anonymized\`.
3.  **Choose a Masking/Anonymization Technique:** In the Prepare recipe, select the PII column and apply the appropriate processor.

    *   **For Redaction/Masking (hiding part of the data):**
        *   Use the **Find & Replace** processor with a **Regular Expression**.
        *   *Example (Mask all but last 4 digits of an ID):* Find \`.*d{4}\` and replace with \`****-****-****-$1\`.
    *   **For Anonymization (creating a non-reversible ID):**
        *   Use the **Formula** processor with a hashing function. This creates a unique, anonymous ID for each user that can still be used for joining.
        *   *Example:* \`sha256(customer_id_column)\`
    *   **For Removal:**
        *   Simply use the **Delete column** action to remove the PII field entirely.
4.  **Create the Anonymized Dataset:** Run the Prepare recipe. The output dataset (\`customers_anonymized\`) now contains your privacy-protected data.
5.  **Use the Anonymized Data:** All downstream analyses, models, and dashboards intended for a wider audience should now be built using this anonymized dataset, not the original raw data. The original raw PII dataset should have strictly limited permissions.

### 4. Resources and Tools
- **Prepare Recipe:** The primary tool for implementing privacy transformations.
- **Find & Replace Processor:** For regex-based masking.
- **Formula Processor:** For hashing or other custom transformations.
- **Dataiku's Hashing Functions:** \`sha1()\`, \`sha256()\`, \`md5()\`.

### 5. Next Steps and Progression
- **Create a Reusable Anonymization Flow:** If you have a standard anonymization process, you can build it in a dedicated, reusable **Flow Zone** that can be applied to multiple different datasets.
- **PII Detection Plugin:** Dataiku has a plugin that can automatically scan your datasets to help you find columns that likely contain PII, ensuring you don't miss anything.

### 6. Common Challenges and Solutions
- **Challenge:** "I need to join data after anonymization, but the keys don't match anymore."
- **Solution:** You must apply the *exact same* hashing function to the key column in all datasets before you can join them. A good practice is to create a single, shared "golden" customer table that includes the anonymized ID, and then have all other flows join with that table.
- **Challenge:** "Hashing is too slow on my large dataset."
- **Solution:** If the Prepare recipe is slow, you can implement the hashing logic in a **Python** or **SQL** recipe instead, which may offer better performance for certain hashing algorithms or on certain execution engines.
`},{id:289,slug:"version-controlling-project-code-via-git",question:"How to get started with version controlling project code via Git?",answer:`
### 1. Introduction/Overview
Version control is the practice of tracking and managing changes to software code, and it's an essential best practice for any serious data project. Dataiku integrates natively with Git, allowing you to version control your entire project—including all your recipes, schemas, and configurations—in a Git repository like GitHub or GitLab.

### 2. Prerequisites
- **A remote Git repository,** which should be empty.
- **Git must be installed on the Dataiku server** (an admin task).
- **Project administrator rights** to link the project to Git.

### 3. Step-by-Step Instructions
1.  **Convert Your Project to a Git Project:**
    *   In your Dataiku project, go to **Settings** (the gear icon) and then the **Git** tab.
    *   Click the button to **Convert to Git project**.
    *   In the dialog, enter the **URL of your remote Git repository**.
2.  **Make Your First Commit:**
    *   A new **Git** icon now appears in your project's top navigation bar. Click it.
    *   This page shows all the changes you've made to the project. Initially, this will be every object in the project.
    *   Click the checkbox to **Stage all** changes.
    *   Write a commit message in the box at the bottom (e.g., "Initial project commit").
    *   Click the **Commit** button.
3.  **Push to the Remote Repository:**
    *   Your commit now exists locally on the Dataiku server. To share it, you need to push it.
    *   Click the **Push** button to send your commits to the remote repository.
4.  **The Standard Workflow:** From now on, the workflow is:
    *   Make changes to your project (edit a recipe, create a dataset, etc.).
    *   Go to the **Git** page, **stage** your changes, **commit** them with a clear message, and **push** them to the remote.

### 4. Resources and Tools
- **Dataiku's Git Integration Page:** The UI for all your daily Git operations (committing, pushing, pulling, branching).
- **A Git Provider:** GitHub, GitLab, Azure DevOps, Bitbucket, etc.

### 5. Next Steps and Progression
- **Branching:** Don't work directly on the \`main\` branch. Use the Git page to **Create branch**. Make your changes on a feature branch (e.g., \`feature/add-new-sales-report\`).
- **Pull Requests:** When your feature is complete, push your branch and then use your Git provider's interface (e.g., GitHub) to create a **Pull Request**. This allows for code review before merging the changes back into the \`main\` branch.
- **Resolving Conflicts:** If you and a colleague edit the same object, you may have a merge conflict when you try to **Pull** their changes. Dataiku provides a visual diff tool to help you resolve these conflicts.

### 6. Common Challenges and Solutions
- **Challenge:** "Push/Pull failed with an authentication error."
- **Solution:** This means the Dataiku server cannot authenticate with your Git provider. Your Dataiku administrator needs to set up SSH keys or other credentials to allow the server to connect to the Git repository.
- **Challenge:** "What is actually being saved in Git?"
- **Solution:** Dataiku versions the *definition* of your project—the structure of your Flow, the code in your recipes, the settings of your datasets, etc. It does **not** version the actual data within your datasets. This is good, as you don't want to store multi-gigabyte data files in a Git repository.
`},{id:290,slug:"watermarking-datasets-to-flag-migrated-artifacts",question:"How to get started with watermarking datasets to flag migrated artifacts?",answer:`
### 1. Introduction/Overview
During a complex migration, it can be useful to "watermark" your newly created datasets to make it immediately obvious that they came from the new, migrated Dataiku pipeline rather than a legacy system. This is a simple but effective governance practice that can be done easily with a **Prepare recipe**.

### 2. Prerequisites
- **A migrated Dataiku pipeline.**
- **A final output dataset** that you want to watermark.

### 3. Step-by-Step Instructions
1.  **Add a Final Prepare Recipe:**
    *   In your Flow, select your final output dataset.
    *   Add one last **Prepare** recipe to the end of your flow. Name its output clearly, e.g., \`final_report_watermarked\`. This new recipe's sole purpose is to add the watermark.
2.  **Add a New Column:**
    *   In the Prepare recipe, click **+ ADD A NEW STEP** and choose the **Formula** processor.
3.  **Create the Watermark Column:**
    *   **Output column:** Name the new column something clear, like \`data_source\` or \`pipeline_origin\`.
    *   **Expression:** In the expression box, simply enter a static string value.
        > \`"Dataiku Migrated Pipeline"\`
4.  **Run the Recipe:** Execute the recipe. Your final output dataset now has a new column where every single row contains the watermark, making its origin unmistakable.
5.  **Update Downstream Consumers:** Ensure that any downstream consumers (like a BI tool) are now pointed to this new, watermarked dataset.

### 4. Resources and Tools
- **Prepare Recipe:** The tool for adding the new column.
- **Formula Processor:** The specific step for creating the static text value.

### 5. Next Steps and Progression
- **Dynamic Watermarks:** You could make the watermark more dynamic. For example, the formula could be: \`"Migrated_on_" + now()\` to add a timestamp of when the data was generated.
- **Tagging as an Alternative:** A lighter-weight alternative to adding a data column is to simply add a **Tag** to the final dataset (e.g., a tag named \`Origin:Dataiku\`). This doesn't modify the data itself but still provides clear metadata about its origin in the Dataiku UI.

### 6. Common Challenges and Solutions
- **Challenge:** "Adding a new column might break downstream processes."
- **Solution:** This is a valid concern. You must coordinate with the owners of any downstream systems that consume this data. They may need to update their processes to handle the new column. Often, it's better to add the watermark column at the very end of the field list to minimize disruption.
- **Challenge:** "Is this really necessary?"
- **Solution:** For simple, internal projects, it might not be. But in a large enterprise during a confusing migration period where data is coming from both old and new systems, a clear, explicit watermark in the data itself can prevent serious errors caused by people accidentally using data from the wrong source.
`},{id:291,slug:"defining-kpis-to-measure-migration-success",question:"How to get started with defining KPIs to measure migration success?",answer:`
### 1. Introduction/Overview
To justify a migration project and prove its value, you need to define clear Key Performance Indicators (KPIs). These are the specific, measurable metrics you will track to determine if the migration was a success. These KPIs should be defined *before* the project starts and should align with key business drivers like cost, efficiency, and quality.

### 2. Prerequisites
- **An upcoming or ongoing migration project.**
- **Buy-in from leadership and business stakeholders.**

### 3. Step-by-Step Instructions
1.  **Hold a KPI Definition Workshop:**
    *   Gather key stakeholders, including business owners, project managers, and technical leads.
    *   Brainstorm potential success metrics. Group them into categories.
2.  **Define KPIs in Key Categories:**
    *   **Performance & Efficiency:**
        *   **KPI:** "Reduction in Pipeline Runtime."
        *   **How to Measure:** Benchmark the end-to-end runtime of the legacy workflow vs. the new Dataiku pipeline.
    *   **Cost & Resource Savings:**
        *   **KPI:** "Reduction in Infrastructure/License Cost."
        *   **How to Measure:** Calculate the cost of the legacy server and licenses that can be decommissioned, and compare it to the cost of the new infrastructure.
    *   **Data Quality & Reliability:**
        *   **KPI:** "Reduction in Data-Related Support Tickets."
        *   **How to Measure:** Track the number of incidents caused by data errors before and after the migration.
        *   **KPI:** "Scenario Success Rate."
        *   **How to Measure:** In Dataiku, track the success rate of your new production scenarios. Aim for >99%.
    *   **Adoption & Enablement:**
        *   **KPI:** "Number of Workflows Migrated."
        *   **How to Measure:** Keep a running count against your total inventory.
        *   **KPI:** "User Adoption Rate."
        *   **How to Measure:** Track the number of active users on the new Dataiku platform.
3.  **Establish a Baseline:** Before you start the migration, you *must* measure the current state for each KPI. You cannot show improvement if you don't know the starting point.
4.  **Create a Tracking Dashboard:**
    *   Create a Dataiku dashboard (or a simple spreadsheet) to be your "Migration KPI Tracker".
    *   For each KPI, show the baseline, the current value, and the target.
5.  **Report on Progress:** Regularly share this KPI dashboard with leadership and stakeholders to communicate the project's progress and demonstrate the value being delivered.

### 4. Resources and Tools
- **A workshop format** for brainstorming.
- **A dashboarding tool** (like Dataiku Dashboards) to track and report on the KPIs.

### 5. Next Steps and Progression
- **Tie KPIs to Business Outcomes:** Whenever possible, link your technical KPIs to a higher-level business outcome. For example, a "Reduction in Pipeline Runtime" enables "Faster Decision Making" because business users get their reports 4 hours earlier each day.

### 6. Common Challenges and Solutions
- **Challenge:** "We can't get the baseline data for the legacy system."
- **Solution:** This is a common problem with poorly monitored legacy systems. You may need to do some manual work, like timing a few runs of a workflow by hand. Even an estimated baseline is better than no baseline at all.
- **Challenge:** "The KPIs are not improving."
- **Solution:** This is a critical signal that your migration strategy may have issues. Are you not optimizing the workflows as you migrate them? Is user training ineffective? The KPIs are an early warning system. Use them to identify problems with your project and adjust your approach.
`},{id:292,slug:"benchmarking-cost-and-runtime-reduction-post‑migration",question:"How to get started with benchmarking cost and runtime reduction post‑migration?",answer:`
### 1. Introduction/Overview
Benchmarking runtime and cost is one of the most effective ways to prove the value of a migration. It provides concrete, quantitative evidence that the new system is more efficient. This process involves measuring the "before" state in the legacy system and comparing it to the "after" state in Dataiku.

### 2. Prerequisites
- **A migrated workflow** running in Dataiku.
- **The original legacy workflow** (e.g., in Alteryx).
- **Access to runtime logs and cost information** for both systems.

### 3. Step-by-Step Instructions

#### Part A: Benchmarking Runtime
1.  **Establish the Baseline:**
    *   Choose a representative, slow-running legacy workflow.
    *   Run it on a static set of input data.
    *   From the legacy system's logs (e.g., Alteryx Server), find the total end-to-end execution time. Run it a few times and take the average to get a stable baseline.
2.  **Measure the New Runtime:**
    *   Run the newly migrated Dataiku scenario for the same workflow on the exact same input data.
    *   From the Dataiku **Jobs** menu, find the scenario run and record its total duration.
3.  **Calculate and Report:**
    *   Compare the two times. For example: "The legacy Alteryx workflow took 90 minutes. The optimized Dataiku pipeline runs in 5 minutes."
    *   This demonstrates a clear performance improvement.

#### Part B: Benchmarking Cost
1.  **Calculate Legacy Cost:** This can be more complex. You need to estimate the cost of running the legacy job.
    *   This could be a portion of the Alteryx Server's annual license cost, plus the cost of the server hardware it runs on.
    *   For example: (Alteryx Server Annual Cost / Total Annual Job Runs) = Cost Per Job.
2.  **Calculate New Cost:**
    *   If running in the cloud, this is easier to measure.
    *   For example, if the Dataiku job ran on a Snowflake warehouse, you can use Snowflake's query history to find the exact number of credits (which translates to cost) consumed by the query.
3.  **Compare and Report:** Compare the estimated cost per run of the old system vs. a new system. Often, moving to a consumption-based cloud data warehouse results in significant cost savings compared to a fixed-license legacy tool.

### 4. Resources and Tools
- **Job logs** from both systems to get runtimes.
- **Cloud provider cost management dashboards** (for cost benchmarking).
- **A presentation slide** to clearly communicate the "Before vs. After" results to leadership.

### 5. Next Steps and Progression
- **Automate Tracking:** Create a dashboard that tracks the runtime and cost of your key migrated pipelines over time. This helps ensure that they remain efficient.

### 6. Common Challenges and Solutions
- **Challenge:** "It's difficult to calculate the exact cost of a single legacy job."
- **Solution:** You are right. This often requires making reasonable estimates. Work with your finance or IT department to get the total cost of ownership for the legacy platform and then amortize that cost across the number of jobs it runs. The goal is to get a directionally correct comparison.
- **Challenge:** "My new Dataiku pipeline is actually slower or more expensive."
- **Solution:** This is a major red flag and indicates a flaw in your migration. The most likely cause is a poor architecture in Dataiku (e.g., processing large data in-memory instead of pushing it down to the database). This benchmark result is a critical signal that you need to go back and optimize your new pipeline.
`},{id:293,slug:"tracking-number-of-workflows-migrated-per-sprint",question:"How to get started with tracking number of workflows migrated per sprint?",answer:`
### 1. Introduction/Overview
If you are using an Agile or Scrum methodology for your migration project, tracking your team's velocity—the number of workflows migrated per sprint—is a key project management metric. It helps you measure progress, forecast your completion date, and manage stakeholder expectations.

### 2. Prerequisites
- **Your complete inventory of all workflows** to be migrated.
- **Your migration project is organized into sprints** (e.g., 2-week work cycles).
- **A project management tool** (like JIRA or Azure DevOps) or a simple spreadsheet.

### 3. Step-by-Step Instructions
1.  **Create a Migration Backlog:**
    *   Import your entire workflow inventory into your project management tool. Each workflow to be migrated becomes a "User Story" or an item in the backlog.
    *   Include the complexity score for each workflow (e.g., using Story Points).
2.  **Plan Your Sprint:**
    *   At the beginning of each sprint, hold a sprint planning meeting.
    *   The team selects a number of workflow stories from the top of the backlog to work on during that sprint.
3.  **Track Progress During the Sprint:**
    *   Use a sprint board (Kanban board) with columns like "To Do", "In Progress", and "Done".
    *   As the team works on migrating a workflow, the corresponding story moves across the board.
4.  **Define "Done":**
    *   Have a clear "Definition of Done" for a migration story. "Done" should mean not just that the flow is rebuilt, but that it has been fully tested, validated against the original, and documented.
5.  **Measure at the End of the Sprint:**
    *   At the end of the sprint, count the number of workflows (or the total story points) that were moved to the "Done" column. This is your velocity for the sprint.
6.  **Create a Burn-down Chart:**
    *   This is a powerful visualization for tracking progress.
    *   The Y-axis shows the total number of workflows (or story points) remaining in the backlog. The X-axis shows the sprints.
    *   After each sprint, plot the new total remaining. The line should "burn down" towards zero as you complete more migrations. This chart gives you a visual forecast of when the entire project will be complete.

### 4. Resources and Tools
- **A project management tool (JIRA, etc.):** Essential for managing the backlog and sprint board.
- **A Burn-down Chart:** Most agile tools can generate this automatically. It's the best way to visualize progress over time.

### 5. Next Steps and Progression
- **Predictability:** After a few sprints, your team's velocity will become relatively stable. You can use this average velocity to predict how many sprints it will take to complete the remaining backlog.

### 6. Common Challenges and Solutions
- **Challenge:** "Our velocity is very inconsistent from sprint to sprint."
- **Solution:** This is common at the beginning of a project. It can be caused by unexpected technical challenges or changing requirements. Hold a retrospective at the end of each sprint to discuss what went well and what didn't, and use these learnings to make your planning for the next sprint more accurate.
- **Challenge:** "The team is not completing all the stories they committed to in the sprint."
- **Solution:** The team may be too optimistic in their planning. It's better to commit to a smaller number of workflows and complete them all than to overcommit and finish nothing. Use your historical velocity to guide your sprint planning.
`},{id:294,slug:"monitoring-scenario-success-rates-post‑cutover",question:"How to get started with monitoring scenario success rates post‑cutover?",answer:`
### 1. Introduction/Overview
After a workflow is migrated and goes live, you need to monitor its stability and reliability. The **Scenario Success Rate** is a critical KPI for this. It measures the percentage of scheduled runs that complete successfully. A high success rate indicates a healthy, stable pipeline, while a low rate signals underlying problems that need to be addressed.

### 2. Prerequisites
- **Your migrated pipelines are running in production** as scheduled Dataiku Scenarios.
- **Access to the Dataiku project** where the scenarios are located.

### 3. Step-by-Step Instructions
1.  **Navigate to the Scenario Monitoring View:**
    *   There are two main places to monitor this:
        *   **Project Level:** In your project, go to the **Scenarios** page. The "Last runs" tab shows the recent history and outcomes for all scenarios in that project.
        *   **Instance Level (Admin):** A Dataiku administrator can go to **Administration > Monitoring > Jobs** to see a list of all job and scenario runs across the entire instance.
2.  **Filter for Your Key Scenarios:**
    *   In the monitoring view, filter for your key production scenarios. You can filter by project, by scenario name, or by trigger type.
3.  **Calculate the Success Rate:**
    *   Look at the outcomes over a specific time period (e.g., the last 30 days).
    *   Count the number of runs with an outcome of "SUCCESS" and the total number of runs.
    *   **Success Rate = (Number of Successful Runs / Total Number of Runs) * 100**.
4.  **Set a Target:** A reasonable target for a stable production pipeline is a success rate of **> 99%**.
5.  **Investigate Failures:**
    *   If your success rate is below your target, you need to investigate the failures.
    *   Click on each failed run to go to its job log.
    *   Analyze the error message to understand the root cause (e.g., source data issues, bugs in the code) and prioritize fixing these underlying problems.

### 4. Resources and Tools
- **The Scenarios Page ("Last runs" tab):** For project-level monitoring.
- **Administration > Monitoring:** For instance-level monitoring.

### 5. Next Steps and Progression
- **Automated KPI Tracking:** You can create a Dataiku project whose job is to monitor the platform itself. A Python recipe can use the Dataiku API to fetch the run history for all scenarios, calculate the success rates, and write the results to a dataset. You can then build a dashboard to visualize this KPI over time.
- **Alerting on the "Meta" Level:** In addition to alerts for single failures, you could have your monitoring project send a weekly summary report of the overall success rate to the Head of Data.

### 6. Common Challenges and Solutions
- **Challenge:** "A scenario fails and then succeeds on a re-run. How should I count this?"
- **Solution:** The first run was a failure. The fact that a manual re-run worked indicates a transient issue (like a temporary network problem), but it still counts as a failure in your success rate calculation because it required manual intervention. A truly robust pipeline should not have these intermittent failures.
- **Challenge:** "Our success rate is low, but the failures are all due to source system problems."
- **Solution:** While not a fault of your Dataiku logic, these are still pipeline failures. The solution might be to build more resilience into your pipeline. For example, implement a "retry" mechanism in your scenario to automatically handle transient source system issues. You could also create a dashboard that tracks the reason for failure to show that most issues are caused by upstream systems.
`},{id:295,slug:"capturing-feedback-to-refine-migration-templates",question:"How to get started with capturing feedback to refine migration templates?",answer:`
### 1. Introduction/Overview
During a large migration, your team will learn a lot. Capturing this feedback and using it to refine your process and templates is the key to becoming more efficient over time. A "migration template" could be a standard project structure, a checklist, or a set of reusable code snippets.

### 2. Prerequisites
- **An active migration project** with at least one migration wave completed.
- **A "template" or a set of standard practices** that your team is using.
- **A culture that is open to feedback and continuous improvement.**

### 3. Step-by-Step Instructions
1.  **Hold a Retrospective Meeting:**
    *   At the end of a migration wave or a major project, gather the entire team that was involved.
    *   The purpose of this meeting is to reflect on the process. Use a simple "What went well? What didn't go well? What should we change?" format.
2.  **Focus on the Process and Templates:**
    *   Guide the discussion towards your standard templates and processes.
    *   **Ask specific questions:**
        *   "Did our standard project template help? Was anything missing from it?"
        *   "Was our validation checklist useful? Should we add or remove any steps?"
        *   "Did our reusable Python function for address cleaning work as expected?"
3.  **Capture Actionable Feedback:**
    *   Document the feedback. Don't just write down problems; write down specific, actionable suggestions for improvement.
    *   **Bad Feedback:** "The project template is confusing."
    *   **Good, Actionable Feedback:** "We should add a pre-configured 'Validation' Flow Zone to the project template to make the QA process more standard."
4.  **Assign an Owner to Update the Templates:**
    *   For each piece of actionable feedback, assign an owner and a deadline to update the relevant template or documentation.
5.  **Communicate the Changes:**
    *   Once a template has been updated, announce the change to the entire team so that everyone starts using the new, improved version for the next migration wave.

### 4. Resources and Tools
- **Retrospective Meetings:** The primary forum for gathering feedback.
- **A shared document or Wiki page** to capture the feedback and track the action items.
- **Your Project Templates and Standard Operating Procedure documents.**

### 5. Next Steps and Progression
- **Create a "Center of Excellence" (CoE):** For large organizations, a formal CoE can be responsible for owning, maintaining, and communicating all the standard templates and best practices based on feedback from all the different project teams.
- **Measure the Impact:** Try to measure if the template refinements are having an impact. For example, after improving the project template, is the average time to set up a new migrated project decreasing?

### 6. Common Challenges and Solutions
- **Challenge:** "The team is not providing any useful feedback."
- **Solution:** The facilitator of the retrospective needs to ask good, probing questions. It can also be helpful to have a "blameless" culture, where people feel safe to point out problems without fear of being criticized.
- **Challenge:** "We capture the feedback, but the templates never actually get updated."
- **Solution:** This is a failure of accountability. Each improvement suggestion must be treated as a formal task. It needs to be logged (e.g., in JIRA), assigned an owner, and given a deadline. Without this, good ideas will be forgotten.
`},{id:296,slug:"assessing-user-adoption-rates-of-new-dataiku-pipelines",question:"How to get started with assessing user adoption rates of new Dataiku pipelines?",answer:`
### 1. Introduction/Overview
The technical migration is only half the battle. A project is only truly successful if business users adopt and use the new pipelines and dashboards you've built. Assessing user adoption is a crucial post-migration step to measure the project's impact and identify areas where more training or communication is needed.

### 2. Prerequisites
- **A migrated Dataiku pipeline** that has been live for some time.
- **A defined set of end users** for the pipeline's outputs.
- **Dataiku administrator rights** (or collaboration with an admin) to access usage logs.

### 3. Step-by-Step Instructions

#### Method 1: Quantitative Analysis (Usage Logs)
1.  **Access Usage Data:** A Dataiku administrator can access the instance's activity logs. These logs record which users are viewing which dashboards and datasets.
2.  **Analyze the Logs:** These logs can be exported and analyzed in a Dataiku project. You can build a dashboard to answer key adoption questions:
    *   "How many unique users have viewed the new 'Sales Dashboard' this month?"
    *   "Who are the 'power users' of this new dataset?"
    *   "Which departments have the highest and lowest adoption rates?"
3.  **Track Trends:** Monitor these metrics over time. You should see a steady increase in active users after a new pipeline is launched.

#### Method 2: Qualitative Analysis (User Feedback)
1.  **Conduct User Interviews:** Sit down with a few key users of the new pipeline.
2.  **Ask "How" and "Why" Questions:**
    *   "Walk me through how you used the new dashboard to prepare for your weekly meeting."
    *   "Has this new pipeline saved you time? If so, how much?"
    *   "Are you able to answer new questions with this data that you couldn't before?"
3.  **Gather Testimonials:** Collect positive quotes from users about how the new pipeline has helped them. These are incredibly powerful for showcasing the project's success to leadership.

### 4. Resources and Tools
- **Dataiku Usage Logs (Admin):** For quantitative usage data.
- **User interviews and surveys:** For qualitative feedback and context.
- **A Dataiku dashboard:** To visualize your adoption metrics.

### 5. Next Steps and Progression
- **Targeted Training:** If your analysis shows that a specific department has a very low adoption rate, you can organize a targeted training or demo session for that team to help them get started.
- **Identify Decommissioning Opportunities:** If your usage logs show that nobody is using the output of a specific migrated pipeline, it might be a candidate for decommissioning, which saves maintenance effort.

### 6. Common Challenges and Solutions
- **Challenge:** "Adoption is very low."
- **Solution:** This is a critical problem to solve. The root cause is often either a lack of awareness or a lack of training.
    *   **Awareness:** Did you clearly communicate to all potential users that the new pipeline is available and where to find it?
    *   **Training:** Do users know *how* to use the new dashboard or dataset? They may need a hands-on training session.
    *   **Value:** Does the new pipeline actually solve a real business problem for them? If not, you may have a deeper issue with the project's original requirements.
- **Challenge:** "We don't have access to the instance-level usage logs."
- **Solution:** You can still get qualitative feedback. The user interviews are often more insightful than the raw numbers anyway. You can also build simple tracking into your dashboards by including a link to a feedback form.
`},{id:297,slug:"tracking-improvements-in-data-freshness-or-throughput",question:"How to get started with tracking improvements in data freshness or throughput?",answer:`
### 1. Introduction/Overview
Beyond just migrating a workflow, a key goal of modernization is to improve data delivery. Tracking improvements in **data freshness** (how up-to-date the data is) and **throughput** (how much data can be processed) provides powerful metrics to demonstrate the value of your new Dataiku pipelines.

### 2. Prerequisites
- **A migrated Dataiku pipeline** running in production.
- **An understanding of the old legacy process** and its limitations.

### 3. Step-by-Step Instructions

#### Part A: Tracking Data Freshness
1.  **Define the Metric:** Data freshness is the time delay between an event happening in the real world and the data about that event being available in the final report.
2.  **Establish the Baseline:** Document the freshness of the legacy system.
    *   **Example:** "The old Alteryx workflow only ran once per week. Therefore, the data in the weekly sales report was, on average, 3.5 days old."
3.  **Measure the New System:** Document the schedule of your new Dataiku pipeline.
    *   **Example:** "The new Dataiku scenario runs every morning at 6 AM. The data in the new daily dashboard is now never more than 24 hours old."
4.  **Quantify the Improvement:** Report the difference clearly. "We improved data freshness from 3.5 days to less than 1 day, enabling more timely decision-making."

#### Part B: Tracking Throughput
1.  **Define the Metric:** Throughput is the volume of data your pipeline can process in a given amount of time (e.g., rows per second).
2.  **Establish the Baseline:** Analyze the performance of the legacy workflow.
    *   **Example:** "The old Alteryx workflow took 2 hours to process 1 million rows of transaction data (a throughput of ~138 rows/sec)."
3.  **Measure the New System:** Profile the performance of your new, optimized Dataiku pipeline on the same volume of data.
    *   **Example:** "The new, push-down SQL pipeline in Dataiku processed the 1 million rows in 5 minutes (a throughput of ~3,333 rows/sec)."
4.  **Quantify the Improvement:** Report the increase in processing power. "By migrating to a push-down architecture in Dataiku, we increased our data processing throughput by over 20x."

### 4. Resources and Tools
- **Job Logs** from both systems to get runtimes.
- **A simple spreadsheet** to calculate and track the metrics.
- **A presentation slide** to communicate the impressive improvements to stakeholders.

### 5. Next Steps and Progression
- **Create a Monitoring Dashboard:** Build a Dataiku dashboard that tracks these KPIs. You can create a dataset that logs the pipeline runtime and the number of rows processed for each run. A simple formula can then calculate the throughput, which you can plot on a chart over time.

### 6. Common Challenges and Solutions
- **Challenge:** "It's hard to get accurate numbers for the old system."
- **Solution:** This is common. Do your best to find logs or make a reasonable estimate. Even a directionally correct comparison ("it used to take hours, now it takes minutes") is a powerful statement.
- **Challenge:** "Our throughput hasn't improved much."
- **Solution:** This is a sign that your new pipeline is not architected for performance. You are likely not taking advantage of performance features like push-down execution or partitioning. This KPI is a signal that you need to go back and optimize your Dataiku flow.
`},{id:298,slug:"reporting-migration-metrics-to-leadership-and-stakeholders",question:"How to get started with reporting migration metrics to leadership and stakeholders?",answer:`
### 1. Introduction/Overview
Communicating the progress and success of your migration project to leadership is essential for maintaining support and demonstrating value. This involves creating a simple, visual, and business-focused report that summarizes your Key Performance Indicators (KPIs). A Dataiku Dashboard is the perfect tool for this.

### 2. Prerequisites
- **Your defined migration KPIs** (e.g., workflows migrated, runtime reduction, cost savings).
- **A process for collecting the data** for these KPIs.

### 3. Step-by-Step Instructions
1.  **Create a "Migration Tracking" Project:**
    *   Create a new, dedicated Dataiku project for tracking and reporting on the migration itself.
2.  **Build Datasets for Your KPIs:**
    *   In this project, create datasets that hold your KPI data. This might be a manually updated spreadsheet that you upload, or a dataset generated by a Python script that uses APIs to fetch the latest numbers.
    *   **Example Dataset:** A spreadsheet with columns like \`Sprint_Number\`, \`Workflows_Completed\`, \`Total_Workflows_Remaining\`.
3.  **Create a New Dashboard:**
    *   In your tracking project, create a new **Dashboard** called "Alteryx Migration Status".
4.  **Build KPI Tiles and Charts:** Populate the dashboard with visualizations of your KPIs.
    *   **KPI Cards:** Use **Metric** tiles to show the most important, current numbers in a large font (e.g., "Total Workflows Migrated: 42/100").
    *   **Burn-down Chart:** Create a **Line Chart** showing the number of workflows remaining over time (by sprint). This is the best way to visualize progress towards completion.
    *   **Performance Improvement Chart:** Create a **Bar Chart** comparing the "Before" (Alteryx) and "After" (Dataiku) runtimes for a key migrated workflow.
    *   **Cost Savings Chart:** Visualize the estimated cost savings.
5.  **Add Context and Commentary:**
    *   Use **Text** tiles to add a high-level summary, explain what each chart means, and highlight recent successes or upcoming milestones.
6.  **Share the Dashboard:**
    *   Share a link to this dashboard with your project sponsors, steering committee, and other key stakeholders.
    *   Refer to this dashboard during all your regular progress update meetings.

### 4. Resources and Tools
- **Dataiku Dashboards:** The primary tool for building your progress report.
- **Charts and Metric Tiles:** The visual components of your dashboard.
- **Text Tiles:** Crucial for providing the narrative and explaining the data.

### 5. Next Steps and Progression
- **Automated Data Collection:** Instead of manually updating a spreadsheet, write Python recipes that use APIs (e.g., the JIRA API to get the number of completed stories, the Dataiku API to get scenario runtimes) to automatically update your KPI datasets.
- **Email Summaries:** Create a **Scenario** that takes a weekly screenshot of the dashboard and emails it to the executive team.

### 6. Common Challenges and Solutions
- **Challenge:** "Leadership doesn't have time to look at a detailed dashboard."
- **Solution:** Your dashboard is too cluttered. Create a simplified "Executive View" version that shows only the 3-4 most important top-level KPIs. The goal is to be able to understand the project status in 30 seconds.
- **Challenge:** "The metrics don't look good this week. I'm afraid to share the report."
- **Solution:** You must be transparent. Hiding bad news is a mistake. Share the report, but be prepared to explain *why* the metric is down and what your plan is to address it. This builds trust and shows that you are actively managing the project's risks.
`},{id:299,slug:"iterating-migration-plans-based-on-feedback",question:"How to get started with iterating migration plans based on feedback?",answer:`
### 1. Introduction/Overview
A migration plan should not be a static document carved in stone. It must be a living plan that you iterate on and improve based on feedback and real-world experience. Adopting an agile mindset allows you to adapt to new information, changing priorities, and lessons learned, leading to a more successful overall project.

### 2. Prerequisites
- **Your initial migration plan** (e.g., your prioritized list of workflows and timeline).
- **A process for collecting feedback** (e.g., retrospectives, user feedback sessions).
- **Empowerment from leadership** to make changes to the plan.

### 3. Step-by-Step Instructions
1.  **Establish Feedback Loops:** Create formal opportunities to gather feedback. The two most important loops are:
    *   **Team Retrospectives:** After each sprint or migration wave, hold a retrospective with the development team to discuss what could be improved in the *process*.
    *   **Stakeholder Reviews:** After a new pipeline goes live, hold a feedback session with the *business users* to discuss the *product*.
2.  **Listen and Capture:**
    *   In these meetings, actively listen and capture all feedback and suggestions.
    *   Log them in a central place, like a dedicated "Improvements Backlog" in JIRA or a page in your Wiki.
3.  **Analyze and Prioritize Feedback:**
    *   Review the feedback. Look for recurring themes.
    *   **Example 1 (Team Feedback):** "Our initial time estimates were way too optimistic for workflows that use custom Python scripts."
    *   **Example 2 (Stakeholder Feedback):** "The marketing team has a new, urgent need for a customer segmentation flow. This is now more important than the sales report we had planned to migrate next."
4.  **Iterate on the Plan:**
    *   Based on this feedback, make concrete changes to your migration plan.
    *   **In Response to Example 1:** Go back to your backlog and increase the complexity estimate for all remaining workflows that use Python. This will result in a more realistic timeline.
    *   **In Response to Example 2:** Go to your backlog and move the new customer segmentation story to the top of the priority list. Move the sales report story down.
5.  **Communicate the Changes:**
    *   Whenever you make a significant change to the plan (especially to the timeline or priorities), you must communicate this clearly to all stakeholders to manage their expectations.

### 4. Resources and Tools
- **Retrospective meetings and user feedback sessions:** Your primary sources of feedback.
- **Your project management tool (JIRA, etc.):** Where you manage and re-prioritize your backlog.
- **Your migration roadmap/timeline:** The living document that you will update.

### 5. Next Steps and Progression
- **Embrace Agility:** This entire process is the heart of being "agile". You are continuously inspecting your process and your product and adapting your plan based on real-world information.
- **Data-Driven Decisions:** Use your migration KPIs to inform your iterations. If you see that your team's velocity is lower than expected, that's a data point that tells you your plan needs to be adjusted.

### 6. Common Challenges and Solutions
- **Challenge:** "The stakeholders are frustrated that the plan keeps changing."
- **Solution:** This is a communication challenge. You need to frame the changes positively. Explain that the plan is changing *because* you are listening to their feedback and adapting to new information to deliver the most possible value. An agile plan is not a broken promise; it's an intelligent response to reality.
- **Challenge:** "How do we balance iterating the plan with needing a predictable end date?"
- **Solution:** This is the classic agile trade-off between flexibility and predictability. You can use your team's average velocity to create a *forecasted* end date based on the remaining work. You must communicate that this is a forecast, not a guarantee, and it will be updated after each sprint as you get more information.
`},{id:300,slug:"planning-phase‑2-or-next‑wave-migrations-using-lessons-learned",question:"How to get started with planning phase‑2 or next‑wave migrations using lessons learned?",answer:`
### 1. Introduction/Overview
After successfully completing the first phase of a large migration, planning the next phase is an opportunity to build on your success and apply the valuable lessons you've learned. This allows you to create a more accurate, efficient, and predictable plan for the remaining workflows.

### 2. Prerequisites
- **Completion of a major migration phase or wave.**
- **A documented list of "lessons learned"** from a retrospective meeting.
- **Your original, complete inventory of all legacy workflows.**

### 3. Step-by-Step Instructions
1.  **Hold a Formal Phase 1 Retrospective:**
    *   Before you plan anything new, hold a dedicated meeting to review the completed phase.
    *   Discuss what went well, what was challenging, and what you would do differently.
    *   Pay special attention to the accuracy of your original estimates.
2.  **Update Your Planning Model:**
    *   **Refine Your Estimates:** You now have real data on how long it takes your team to migrate workflows of different complexity levels. Update your estimation rubric (e.g., "A 'high complexity' workflow actually takes us 15 days, not the 10 we originally planned").
    *   **Refine Your Templates:** Based on the feedback, update your standard project templates, documentation checklists, and reusable code libraries.
3.  **Re-Prioritize the Remaining Backlog:**
    *   Look at your original inventory of remaining workflows.
    *   Meet with business stakeholders to confirm that the priorities are still the same. Have business needs changed since the project started?
    *   Re-apply your prioritization matrix (Value vs. Complexity) to the remaining workflows, using your newly refined complexity estimates.
4.  **Plan the Next Wave(s):**
    *   Using your updated backlog and more accurate estimates, plan out the next one or two migration waves or sprints.
    *   Because your velocity estimates are now based on real data, your new timeline will be much more reliable.
5.  **Create a "Phase 2 Kickoff" Deck:**
    *   Create a short presentation for stakeholders and leadership.
    *   It should summarize the successes of Phase 1 (using your KPIs!), outline the plan and timeline for Phase 2, and explain how you have incorporated the lessons learned to make the next phase even more efficient.

### 4. Resources and Tools
- **Retrospective Meetings:** The source of your "lessons learned".
- **Your Migration Backlog / Inventory:** The list of remaining work.
- **Your refined estimation models and templates.**

### 5. Next Steps and Progression
- **Continuous Improvement:** Treat each phase as a learning opportunity. The goal is for each migration wave to be smoother and more predictable than the last.
- **Celebrate Success:** Don't forget to celebrate the completion of Phase 1 with the team and the business stakeholders. Recognizing the achievement is crucial for maintaining morale during a long project.

### 6. Common Challenges and Solutions
- **Challenge:** "The remaining workflows are all the most complex ones."
- **Solution:** This is a common pattern. You've picked the low-hanging fruit, and now the hard work begins. Your planning for Phase 2 must reflect this. Your velocity (in terms of workflows per sprint) will likely be lower, but the value delivered by migrating these core, complex systems will be even higher. Make sure your timeline and stakeholder expectations reflect this increased level of effort.
- **Challenge:** "We have a lot of new requests that weren't in the original inventory."
- **Solution:** The migration project has been a success, and now other teams want to get involved. This is a good problem to have. These new requests should be added to the backlog and prioritized along with the remaining legacy workflows. You may need to have a conversation with leadership about extending the project timeline or adding more resources to handle the increased scope.
`}]},96487:()=>{},96760:(e,t,a)=>{"use strict";a.d(t,{ab:()=>o});let o=[{id:501,slug:"deploying-ml-models-as-rest-apis",question:"How to get started with deploying machine learning models as REST APIs in Dataiku using the Dataiku Model API.",answer:`
### 1. Introduction/Overview
Deploying a machine learning model as a REST API endpoint is the final step in productionizing it for real-time use. This allows other applications to send data to your model and get an instant prediction back. Dataiku's API Deployer is the dedicated component for creating, managing, and scaling these production-ready API services.

### 2. Prerequisites
- **A "Saved Model" in your Flow:** You must have already trained a model and deployed it from the Lab to your Flow.
- **Access to an API Deployer node:** The API Deployer is a separate piece of the Dataiku infrastructure that must be set up by an administrator.
- **Permissions:** You need permissions to create new API services.

### 3. Step-by-Step Instructions
1.  **Create the API Service:** From your project's Flow, select your Saved Model. In the right-hand panel, choose **API Designer**. This will open the API Designer interface.
2.  **Define an Endpoint:** Create a new endpoint within your service. A common endpoint type is a **Prediction** endpoint that is directly tied to your Saved Model.
3.  **Test the Endpoint:** The API Designer provides a user interface for testing your endpoint. You can enter sample feature values in a JSON format and see the prediction your model returns.
4.  **Deploy the Service:** Once you are satisfied, click **Deploy**. This will package your model and its dependencies and deploy it as a live, running service on the API Deployer node.
5.  **Use the Live API:** In the API Deployer UI, you can find the endpoint's URL and code snippets in various languages (\`curl\`, Python) that show your application developers how to call the live API.

### 4. Resources and Tools
- **API Designer:** The UI within a project for creating the definition of an API service.
- **API Deployer:** The production-grade service that runs and manages your live model endpoints.
- **Saved Model:** The versioned model artifact that gets deployed.

### 5. Next Steps and Progression
- **Versioning:** You can deploy multiple versions of your model to the same endpoint and use traffic splitting to manage the transition.
- **Monitoring:** The API Deployer provides built-in monitoring dashboards to track the latency, error rate, and traffic of your API endpoints.
- **Scaling:** If your API is deployed on Kubernetes, you can easily scale the number of model replicas up or down to handle changes in traffic.

### 6. Common Challenges and Solutions
- **Challenge:** "My deployed API is returning an error."
- **Solution:** Check the logs of the API service in the API Deployer UI. The most common error is a schema mismatch, where the JSON being sent by the client application does not match the feature schema the model expects.
- **Challenge:** "The API is too slow."
- **Solution:** This could be due to a complex model or insufficient resources. You may need to deploy the API service on a more powerful machine or increase the number of replicas if running on Kubernetes.
`},{id:502,slug:"automating-model-retraining-tuning-deployment",question:"How to get started with automating model retraining, tuning, and deployment in Dataiku DSS.",answer:`
### 1. Introduction/Overview
Models are not static; they need to be periodically retrained on new data to maintain their accuracy. Automating this MLOps pipeline is a key practice for maintaining production models efficiently. In Dataiku, this is achieved by creating a **Scenario** that orchestrates the data preparation, retraining, and deployment steps.

### 2. Prerequisites
- **A full model pipeline already built:** You should have a complete Flow that prepares the training data and a "Saved Model" that has been trained on it.
- **A strategy for deployment:** How do you decide if a newly retrained model is "better" and should be deployed? (e.g., based on its accuracy score).

### 3. Step-by-Step Instructions
1.  **Create a Retraining Scenario:** In your project, go to **Scenarios** and create a new scenario (e.g., \`Weekly_Churn_Model_Retrain\`).
2.  **Add a "Build Training Data" Step:** The first step should be a **Build / Train** step that rebuilds the final, clean training dataset. This ensures you always train on the latest data.
3.  **Add a "Retrain Model" Step:** Add a second **Build / Train** step. This time, select your **Saved Model** object from the Flow. This tells the scenario to retrain the model using the dataset from the previous step. In the step's settings, you can also enable hyperparameter search to find the best parameters for the new data.
4.  **Add an "Evaluate Model" Step:** Add a step to run an **Evaluate** recipe, which compares the performance of your newly trained model version against a hold-out test set.
5.  **Add an Automated "Deploy" Step:** This is the advanced part. Add a **Python code** step. This script will use the Dataiku API to:
    *   Get the performance of the new model version (from the Evaluate recipe's output).
    *   Get the performance of the currently active production version.
    *   If the new version is better, the script activates it, making it the new production model.
6.  **Schedule the Scenario:** Add a **Time-based trigger** to run this entire scenario on a schedule (e.g., every Sunday night).

### 4. Resources and Tools
- **Scenarios:** The core automation engine in Dataiku.
- **Build / Train Step:** The key step for retraining a Saved Model.
- **Evaluate Recipe:** For generating performance metrics.
- **Python API:** For programmatically comparing model versions and deploying the winner.

### 5. Next Steps and Progression
- **Alerting:** Add a **Reporter** to the scenario to send an email or Slack message summarizing the retraining run, including which model was deployed and its new performance score.
- **Champion/Challenger Deployment:** Instead of activating the new model directly, the script could deploy it as a "challenger" to be A/B tested against the current champion.

### 6. Common Challenges and Solutions
- **Challenge:** The retrained model is actually worse than the old one.
- **Solution:** Your deployment script must have a safety check. It should *never* deploy a new model if its performance is worse than the currently active one. This prevents automated performance degradation.
- **Challenge:** The retraining job fails.
- **Solution:** Your scenario should have a failure reporter to alert you immediately. The cause is often an issue with the new training data (e.g., a data quality problem), which needs to be investigated.
`},{id:503,slug:"performing-champion-challenger-workflows",question:"How to get started with performing champion/challenger and version-comparison workflows in Dataiku for model updates.",answer:`
### 1. Introduction/Overview
A Champion/Challenger test (or A/B test) is a method for comparing the performance of a new model (the "challenger") against the current production model (the "champion") in a live environment. Dataiku's API Deployer has built-in features to manage this process, allowing you to safely validate a new model before rolling it out to all users.

### 2. Prerequisites
- **Two deployed model versions:** You need to have at least two versions of your model deployed to the same "Saved Model" object in your Flow.
- **An API Service:** Your model must be deployed as an endpoint in an API service via the API Deployer.

### 3. Step-by-Step Instructions
1.  **Deploy Both Versions:** In the API Deployer, when you deploy your API service, ensure that both your champion and challenger model versions are included in the deployment.
2.  **Configure the Endpoint:** Navigate to your API service in the API Deployer and open the endpoint that uses your model.
3.  **Enable Champion/Challenger Mode:** In the endpoint's settings, you can define a champion/challenger split.
    *   Set your current production model version as the **Champion**.
    *   Set your new model version as the **Challenger**.
4.  **Set the Traffic Split:** Configure how to route live prediction requests. A common strategy is to send the majority of traffic to the champion and a small percentage to the challenger. For example:
    *   **Champion:** 90% of traffic.
    *   **Challenger:** 10% of traffic.
5.  **Analyze the Results:**
    *   The API Deployer will automatically log the prediction requests and responses for each model version separately.
    *   You must have a process to join these prediction logs with the actual outcomes (ground truth).
    *   By analyzing this data, you can compare the real-world accuracy, business KPIs, and any other relevant metrics for the champion and challenger.
6.  **Promote the Winner:** After a sufficient evaluation period, if the challenger proves to be superior, you can go back to the API Deployer, deactivate the old champion, and make the challenger the new champion with 100% of the traffic.

### 4. Resources and Tools
- **API Deployer:** The core service for managing live model deployments and A/B tests.
- **Prediction Logs:** The data source for analyzing the results of the test.

### 5. Next Steps and Progression
- **Gradual Rollout:** Start with a small amount of traffic to the challenger (e.g., 1%) and gradually increase it as you gain confidence in its performance and stability.
- **Automated Analysis:** Create a Dataiku project that automatically ingests the prediction logs, joins them with ground truth data, and produces a dashboard comparing the champion and challenger performance in near real-time.

### 6. Common Challenges and Solutions
- **Challenge:** How long should I run the test for?
- **Solution:** This depends on your traffic volume and the business cycle. You need to run the test long enough to get a statistically significant number of results for both model versions.
- **Challenge:** The challenger model is causing errors.
- **Solution:** This is exactly why you do a champion/challenger test! Because it only receives a small percentage of traffic, the impact of the errors is contained. You can quickly disable the challenger in the API Deployer to stop sending it traffic while you debug the issue.
`},{id:504,slug:"monitoring-deployed-model-performance-data-drift",question:"How to get started with monitoring deployed model performance and data drift using Dataiku’s unified monitoring features.",answer:`
### 1. Introduction/Overview
A model's job isn't done when it's deployed. Its performance can degrade over time as the real world changes. This is known as "model drift." Monitoring your models for both performance degradation and data drift is a critical MLOps practice, and Dataiku provides a dedicated, unified interface for this.

### 2. Prerequisites
- **A "Saved Model"** deployed in your Dataiku Flow.
- **Ongoing new data:** A stream of new data for the model to make predictions on.
- **Ongoing ground truth:** A way to get the actual outcomes for those predictions.

### 3. Step-by-Step Instructions

#### Part 1: Monitoring Data Drift
1.  **Open Your Saved Model:** In the Flow, double-click on the green "Saved Model" object.
2.  **Go to Model Views > Drift Analysis:** On the left panel, select "Drift Analysis".
3.  **Compute Drift:** The tool needs a new dataset to compare against the original training data. Select a recent dataset and click **Compute**.
4.  **Analyze the Results:** Dataiku will show you a "drift score" for the whole dataset and for each individual feature. This score measures how much the statistical distribution of the new data has changed compared to the training data. High drift on key features is a major warning sign.

#### Part 2: Monitoring Model Performance
1.  **Create an Evaluation Dataset:** You need a dataset that contains both your model's predictions and the actual, true outcomes. This is often created by joining the output of a Score recipe with a later dataset that contains the ground truth.
2.  **Use the Evaluate Recipe:** In your Flow, select this evaluation dataset and use the **Evaluate** recipe.
3.  **Configure and Run:** In the recipe, tell it which column has the predictions and which has the actuals. Running this recipe generates a new dataset containing all the standard performance metrics (Accuracy, Precision, Recall, AUC, etc.).
4.  **Visualize Performance Over Time:** Open your Saved Model again and go to **Model Views > Performance**. You can point this view to the output of your Evaluate recipe. It will then create charts showing how your model's performance metrics have trended over time.

### 4. Resources and Tools
- **Saved Model Views:** The central UI for monitoring.
- **Evaluate Recipe:** The tool for calculating performance metrics on new data.
- **Scenarios:** Used to automate the entire monitoring process on a schedule.

### 5. Next Steps and Progression
- **Automated Checks and Alerting:** Create a **Scenario** that runs these monitoring tasks (drift analysis, evaluation) periodically. Add a **Run checks** step that fails if the drift score is too high or if the model's accuracy drops below a certain threshold. Configure a reporter on the scenario to send you an alert when this happens.
- **Model Retraining:** If monitoring detects significant drift or performance degradation, it's a signal that you need to trigger your model retraining pipeline.

### 6. Common Challenges and Solutions
- **Challenge:** I don't get the ground truth for my predictions until much later.
- **Solution:** This is a common reality. Your performance monitoring pipeline will have a delay. You need to design your flow to handle this, for example, by storing predictions and then joining them with the actuals when they become available. Data drift, however, can be monitored immediately as it doesn't require the ground truth.
`},{id:505,slug:"integrating-prometheus-grafana-for-metrics",question:"How to get started with integrating Prometheus/Grafana to visualize Dataiku flow and model metrics.",answer:`
### 1. Introduction/Overview
For enterprise-grade monitoring, you may want to centralize metrics from all your applications, including Dataiku, into a dedicated monitoring stack like Prometheus (for data collection) and Grafana (for visualization). Dataiku can expose its internal metrics via a standard Java protocol (JMX), which can then be scraped by Prometheus.

### 2. Prerequisites
- **A running Prometheus and Grafana stack.**
- **A Dataiku instance.**
- **Administrator access** to the Dataiku server to enable the JMX port.
- **A "JMX Exporter" agent.**

### 3. Step-by-Step Instructions
1.  **Enable JMX Port in Dataiku (Admin Task):**
    *   The Dataiku administrator needs to edit the \`install.ini\` configuration file.
    *   They must enable the JMX port by setting \`dss.jmx.enabled = true\` and specifying a port number.
    *   This requires a restart of the Dataiku backend.
2.  **Deploy the JMX Exporter:**
    *   Prometheus does not scrape JMX directly. You need a small helper application called the **JMX Exporter** (a Java agent).
    *   This agent attaches to the Dataiku Java process, reads the JMX metrics, and exposes them in a simple HTTP format that Prometheus can understand.
    *   You need to configure the JMX Exporter with a YAML file to tell it which specific Dataiku metrics to expose.
3.  **Configure Prometheus to Scrape Dataiku:**
    *   In your Prometheus configuration file (\`prometheus.yml\`), add a new "scrape config".
    *   This tells Prometheus the address and port of the JMX Exporter agent so it can periodically pull the metrics.
4.  **Build a Grafana Dashboard:**
    *   In Grafana, add Prometheus as a data source.
    *   Create a new dashboard.
    *   You can now build panels that query the Dataiku metrics stored in Prometheus. You could visualize things like:
        *   Number of running jobs.
        *   Scenario success/failure rates.
        *   Custom metrics you've computed in your flows.

### 4. Resources and Tools
- **Prometheus:** The time-series database for storing metrics.
- **Grafana:** The dashboarding tool for visualizing metrics.
- **JMX Exporter:** The bridge between Dataiku's JMX metrics and Prometheus.

### 5. Next Steps and Progression
- **Custom Metrics:** You are not limited to the default metrics. In a Python recipe, you can use a library to push custom business metrics directly to a Prometheus Pushgateway, allowing you to monitor anything in your flow.
- **Alerting:** Configure alerts in Grafana or Prometheus's Alertmanager to be notified if a key Dataiku metric crosses a critical threshold (e.g., "number of failed jobs in the last hour > 5").

### 6. Common Challenges and Solutions
- **Challenge:** This setup seems very complex.
- **Solution:** It is an advanced, enterprise-grade monitoring setup. For many use cases, Dataiku's built-in monitoring dashboards are sufficient. You should only implement this if you have a specific need to correlate Dataiku metrics with other systems in a central monitoring platform.
- **Challenge:** I can't find the metric I want.
- **Solution:** You may need to configure the JMX Exporter's YAML file to explicitly "whitelist" the metric you are interested in. You can use a tool like JConsole to browse all the available JMX metrics exposed by the Dataiku instance to find the one you need.
`},{id:506,slug:"configuring-alerting-for-model-accuracy-failures",question:"How to get started with configuring alerting for model accuracy or pipeline failures in Dataiku scenarios.",answer:`
### 1. Introduction/Overview
A core MLOps practice is to be notified immediately when a production pipeline or model fails. In Dataiku, this is handled by combining **Checks**, which define your failure conditions, with **Reporters**, which send the notifications. This allows you to create automated alerts for everything from data quality issues to model performance degradation.

### 2. Prerequisites
- **A Dataiku Scenario** that automates your pipeline.
- **A clear definition of failure** (e.g., "model AUC drops below 0.75," "input dataset has > 10% nulls").
- **Mail server or other messaging service configured** by your Dataiku administrator.

### 3. Step-by-Step Instructions
1.  **Define Your Failure Condition (The Check):**
    *   First, you must define the rule that constitutes a failure.
    *   Go to the dataset or model where the metric lives and navigate to the **Status > Checks** tab.
    *   **Example 1 (Pipeline Failure):** Check a dataset for a valid number of rows.
    *   **Example 2 (Model Accuracy Failure):** On your "Saved Model", create a check on a metric like "AUC" and set a minimum value (e.g., \`AUC > 0.75\`).
    *   Set the severity of the check to **Error**.
2.  **Add the Check to Your Scenario:**
    *   Open your main automation scenario.
    *   Add a **Run checks** step after the step that generates the data or model to be checked.
    *   Configure this step to run the checks you just defined. If any "Error"-level check is violated, this step will fail, causing the entire scenario to fail.
3.  **Configure the Alert (The Reporter):**
    *   In the same scenario, go to the **Reporters** tab.
    *   Click **+ ADD REPORTER** and select **Mail** or **Slack**.
    *   **Set the Run Condition to "On failure"**.
    *   Configure the recipients and the message. It is crucial to include variables like \`\${scenarioName}\` and \`\${jobURL}\` in the message so the recipient knows what failed and has a direct link to the logs.
4.  **Activate and Schedule:** Save the scenario and ensure it's scheduled to run. Now, if your data quality or model accuracy check fails, the scenario will fail, and an alert will be sent automatically.

### 4. Resources and Tools
- **Metrics and Checks:** The framework for defining your pass/fail rules.
- **Run Checks Scenario Step:** The automation component that executes your rules.
- **Reporters:** The component that sends the notifications.

### 5. Next Steps and Progression
- **Custom Python Alerts:** For more complex alerting logic or to send notifications to a system not natively supported by reporters, you can use a Python scenario step to call any external API (e.g., PagerDuty, Microsoft Teams).
- **Tiered Alerting:** Configure different reporters for different outcomes. For example, a high-urgency alert on failure to the on-call engineer, and a simple "FYI" summary on success to the business stakeholder.

### 6. Common Challenges and Solutions
- **Challenge:** I'm getting alerts, but I don't know what they mean.
- **Solution:** Your alert message is not informative enough. Edit the reporter to include more context. The job URL (\`\${jobURL}\`) is the most important piece of information, as it allows the recipient to go directly to the log and see which specific check failed and why.
- **Challenge:** I'm getting too many "false alarm" alerts.
- **Solution:** Your check's threshold may be too sensitive. For example, if you are checking for data drift, a small amount of drift is normal. You need to tune the threshold of your check to only fire when a truly significant change has occurred.
`},{id:507,slug:"containerizing-flows-and-models-with-docker-kubernetes",question:"How to get started with containerizing Dataiku flows and models with Docker or Kubernetes for portable deployment.",answer:`
### 1. Introduction/Overview
Containerization is the process of packaging an application and its dependencies into a single, portable unit called a container image (e.g., a Docker image). Dataiku leverages this technology to provide scalable, isolated, and reproducible environments for running data pipelines and models.

### 2. Prerequisites
- **Understanding of container concepts:** Familiarity with Docker and, ideally, Kubernetes.
- **A Kubernetes cluster** or a Docker host that Dataiku can connect to.
- **Administrator rights** in Dataiku to configure containerized execution.

### 3. Step-by-Step Instructions
1.  **Configure Containerized Execution (Admin Task):**
    *   An administrator must first connect Dataiku to the container infrastructure.
    *   In **Administration > Containerized Execution**, they will create a new configuration, selecting either **Docker** or **Kubernetes**.
    *   They will need to provide connection details and credentials for the Docker/Kubernetes API.
2.  **Define a Base Image:**
    *   The administrator must also define a base **Docker image**. This image should contain a base operating system and the necessary versions of Python or R. Dataiku provides official base images that you can use.
    *   You can also build your own custom images with specific libraries pre-installed.
3.  **Run a Recipe in a Container:**
    *   As a user, open a code recipe (e.g., Python).
    *   Go to the **Advanced** settings tab.
    *   In the **Container** dropdown, select the container configuration your admin set up.
4.  **How It Works:**
    *   When you click **Run**, Dataiku takes your recipe's code and dependencies.
    *   It sends instructions to Kubernetes or Docker to start a new container using the defined base image.
    *   It runs your recipe code inside this new, isolated container.
    *   Logs are streamed back to the Dataiku UI.
    *   When the job is done, the container is destroyed.

### 4. Resources and Tools
- **Docker & Kubernetes:** The underlying container technologies.
- **Containerized Execution Settings (Admin):** The UI in Dataiku for setting up the integration.
- **Recipe Advanced Settings:** The UI for choosing a container configuration for a specific job.

### 5. Next Steps and Progression
- **Resource Management:** In the container configuration, the administrator can set CPU and memory limits for the containers, preventing a single runaway job from consuming all cluster resources.
- **Deploying API Models:** You can deploy real-time scoring APIs as dedicated, scalable services on Kubernetes, which is the standard for production ML serving.
- **Deploying Dataiku Itself:** For a fully container-native setup, the entire Dataiku platform can be deployed on Kubernetes using an official Helm chart.

### 6. Common Challenges and Solutions
- **Challenge:** My containerized job fails immediately.
- **Solution:** This often means the Docker image is missing a required dependency. For example, your Python recipe uses a library that was not installed in the base image. You need to create a custom image that includes all necessary packages.
- **Challenge:** My job is stuck in a "pending" state.
- **Solution:** This is a Kubernetes issue. It means the cluster does not have enough free resources (CPU or memory) to schedule the container for your job. You may need to add more nodes to your cluster or adjust the job's resource requests.
`},{id:508,slug:"scaling-scoring-apis-on-kubernetes",question:"How to get started with scaling Dataiku scoring APIs by deploying them on Kubernetes clusters.",answer:`
### 1. Introduction/Overview
When a machine learning model is deployed as a real-time API, it may need to handle thousands or millions of prediction requests per day. A single server can't handle this load. Deploying the API service on a Kubernetes cluster is the standard way to achieve high availability and elastic scalability.

### 2. Prerequisites
- **A Kubernetes cluster** (e.g., AWS EKS, Azure AKS, Google GKE).
- **A Dataiku API Deployer** instance that is configured to use this Kubernetes cluster.
- **A "Saved Model"** that has been packaged into an API service in Dataiku.

### 3. Step-by-Step Instructions
1.  **Configure the API Deployer for K8s (Admin Task):**
    *   An administrator must first install and configure the API Deployer to use your Kubernetes cluster as its backend. This means that when you deploy a service, the API Deployer will create Kubernetes objects (like Deployments and Services) instead of just running a process on a VM.
2.  **Deploy your API Service:**
    *   From your Dataiku project, create your API service using the **API Designer** and deploy it to your Kubernetes-enabled API Deployer.
3.  **How It Works:**
    *   The API Deployer will create a Kubernetes **Deployment** for your model. This Deployment manages a set of identical pods.
    *   Each **pod** is a running container that holds your model and can serve prediction requests.
    *   The API Deployer also creates a Kubernetes **Service** (of type LoadBalancer or NodePort), which provides a single, stable IP address to route traffic to the pods.
4.  **Scaling the Service:**
    *   To handle more traffic, you simply scale the number of pods.
    *   **Manual Scaling:** In the API Deployer UI or using \`kubectl\`, you can manually change the number of **replicas** in the Deployment from 1 to 10, for example. Kubernetes will automatically create 9 new pods.
    *   **Automatic Scaling (HPA):** The best practice is to configure a **Horizontal Pod Autoscaler (HPA)** for your deployment. You can set a rule like "if the average CPU usage across all pods goes above 70%, automatically add more pods." Kubernetes will then handle scaling up and down for you.

### 4. Resources and Tools
- **Kubernetes:** The container orchestration platform.
- **Dataiku API Deployer:** The service that manages the deployment to Kubernetes.
- **Kubernetes HPA (Horizontal Pod Autoscaler):** The tool for automatic scaling.
- **Monitoring Tools (Prometheus/Grafana):** To monitor the CPU/memory usage and determine the correct autoscaling thresholds.

### 5. Next Steps and Progression
- **Rolling Updates:** When you deploy a new version of your model, Kubernetes performs a "rolling update," gradually replacing the old pods with new ones, ensuring there is no downtime.
- **High Availability:** By running multiple replicas of your model across different nodes in your Kubernetes cluster, you get high availability for free. If one node or pod fails, traffic is automatically routed to the healthy ones.

### 6. Common Challenges and Solutions
- **Challenge:** How do I choose the right number of replicas?
- **Solution:** This depends on your traffic and the resources each pod consumes. The best approach is to use a Horizontal Pod Autoscaler (HPA) and let Kubernetes figure it out dynamically based on load.
- **Challenge:** The API is still slow even after adding more replicas.
- **Solution:** This may mean your model itself is the bottleneck, or each pod needs more resources. You may need to profile your model's prediction code to optimize it or increase the CPU/memory allocated to each pod in the Deployment configuration.
`},{id:509,slug:"using-flow-mode-batch-scoring-vs-real-time-api",question:"How to get started with using Dataiku Flow mode for batch scoring versus real-time API deployment.",answer:`
### 1. Introduction/Overview
Once you have a trained model, there are two primary ways to use it to make predictions: **batch scoring** and **real-time scoring**. The choice depends entirely on your business use case. Dataiku supports both modes seamlessly.

### 2. Prerequisites
- **A deployed "Saved Model"** in your Dataiku Flow.
- **A clear understanding of your business need:** Do you need to score a large list of customers all at once, or does an application need to get a prediction for a single customer instantly?

### 3. Step-by-Step Instructions

#### Method 1: Batch Scoring (for large datasets, offline)
- **When to Use:**
    *   When you have a large dataset of records to score (e.g., "score all 1 million of our customers to find who is at risk of churn").
    *   When the predictions are not needed instantly.
- **How to Implement:**
    1.  In your Dataiku Flow, select your large, unscored dataset.
    2.  From the right-hand panel, choose the **Score** recipe.
    3.  Select your Saved Model.
    4.  Run the recipe. Dataiku will efficiently apply the model to all rows of the input dataset and produce a new output dataset containing the predictions.
    5.  You can then schedule this batch scoring recipe to run periodically (e.g., daily) using a **Scenario**.

#### Method 2: Real-time Scoring (for on-demand, single predictions)
- **When to Use:**
    *   When an application needs an immediate prediction for a single record (e.g., "a customer is checking out on our website; what product should we recommend to them *right now*?").
    *   When low latency is critical.
- **How to Implement:**
    1.  Take your Saved Model and deploy it to the **API Deployer**.
    2.  This creates a live **REST API endpoint**.
    3.  Your client application (e.g., your e-commerce website) can then make an HTTP request to this endpoint, sending the data for a single customer in the request body.
    4.  The API will instantly return the model's prediction in the response.

### 4. Resources and Tools
- **Score Recipe:** The primary tool for batch scoring.
- **API Deployer:** The service for deploying and managing real-time API endpoints.

### 5. Next Steps and Progression
- **Hybrid Approaches:** Some use cases might use both. For example, you might run a batch job every night to pre-calculate predictions for all your users, but also have a real-time API to score new users who sign up during the day.

### 6. Common Challenges and Solutions
- **Challenge:** "I tried to use the API to score a million records one by one, and it was very slow."
- **Solution:** You are using the wrong tool for the job. Real-time APIs are designed for single, low-latency requests. For scoring a large volume of records, you should always use the **Score recipe** in batch mode, as it is orders of magnitude more efficient.
- **Challenge:** "My batch scoring job is taking too long."
- **Solution:** Ensure your Score recipe is running on an appropriate engine. For large datasets, it should be pushed down to a **Spark cluster** or your **database** for better performance.
`},{id:510,slug:"packaging-versioning-python-r-environments",question:"How to get started with packaging and version-controlling Dataiku Python/R environments for reproducible model builds.",answer:`
### 1. Introduction/Overview
A model is more than just its code; it's also the environment it was trained in. For a model build to be truly reproducible, you must be able to perfectly recreate the environment, including the specific versions of all Python or R packages. Dataiku's Code Environments feature allows you to define, package, and version control these environments.

### 2. Prerequisites
- **A Dataiku project** that uses a code environment.
- **Your project is connected to a Git repository.**

### 3. Step-by-Step Instructions
1.  **Define Your Code Environment:**
    *   In **Administration > Code Envs**, create a new code environment for your project.
    *   In the "Packages to install" list, add all the Python or R libraries your project needs.
    *   **Crucially, pin the exact versions.** Instead of just \`pandas\`, specify \`pandas==1.3.5\`. This is the key to reproducibility.
2.  **Export the Environment Definition:**
    *   On the code environment's page, there is an **Export** button.
    *   Clicking this will download a \`json\` file. This file contains the entire definition of your environment, including the full list of packages and their specified versions.
3.  **Add the Definition to Version Control:**
    *   Add this exported \`.json\` file to your Dataiku project's folder structure (e.g., in a folder named \`code-envs\`).
    *   Commit this file to your project's **Git repository**.
4.  **How to Use the Versioned Definition:**
    *   A new developer joining the project can pull the Git repository.
    *   They can then go to **Administration > Code Envs**, click **+ Import Environment**, and upload the \`.json\` file from the repository.
    *   This will create a new code environment on their instance that is an exact replica of the one used for the original model build.

### 4. Resources and Tools
- **Code Environments:** The core Dataiku feature for managing dependencies.
- **The Export/Import feature** on the Code Environment page.
- **Git:** The version control system for storing the environment's definition file.

### 5. Next Steps and Progression
- **Infrastructure as Code (IaC):** In a fully automated setup, your CI/CD pipeline or an IaC tool like Terraform could use the Dataiku API to automatically create or update code environments based on the \`.json\` file stored in Git.
- **Containerization:** The next level of packaging is to use the environment definition to build a **Docker image**. This packages not only the Python/R libraries but also the underlying operating system dependencies, creating a fully self-contained, portable environment.

### 6. Common Challenges and Solutions
- **Challenge:** Building the environment from the definition file fails.
- **Solution:** This can happen if a very old package version is no longer available from public repositories or if there are deep-seated OS-level dependency conflicts. This is a rare but difficult problem that highlights the value of using containers (Docker) to lock down the entire environment, including the OS.
- **Challenge:** How do I know which versions of all the dependencies to pin?
- **Solution:** When you are developing, you can start with unpinned versions. Once your project is working, you can run a command like \`pip freeze > requirements.txt\` in your local environment. This will generate a complete list of all packages and their exact current versions, which you can then copy into the Dataiku code environment package list.
`},{id:511,slug:"managing-code-environments-for-reproducibility",question:"How to get started with managing Dataiku code environments (Python/R) to pin package versions and ensure reproducible pipelines.",answer:`
### 1. Introduction/Overview
Reproducibility is a cornerstone of good science and MLOps. If you can't reproduce a model build, you can't trust it. The most common cause of irreproducibility is changes in the underlying code environment. Managing your code environments by **pinning package versions** is the essential first step to ensuring your pipelines are reliable and reproducible.

### 2. Prerequisites
- A Dataiku project that uses Python or R code.
- Administrator rights to manage code environments.

### 3. Step-by-Step Instructions
1.  **Create a Dedicated Environment for Your Project:**
    *   Avoid using the default, global code environment. Every significant project should have its own dedicated environment.
    *   Go to **Administration > Code Envs** and create a new environment. Give it a name that matches your project (e.g., \`churn-model-env\`).
2.  **Add Your Packages:**
    *   In the environment's settings, add the Python or R packages your code needs (e.g., \`pandas\`, \`scikit-learn\`, \`xgboost\`).
3.  **Pin the Versions (The Critical Step):**
    *   Do not just add the package name. Add the exact version number using the \`==\` syntax.
    *   **Bad:** \`pandas\`
    *   **Good:** \`pandas==1.3.5\`
    *   **Why?** If you just specify \`pandas\`, Dataiku will install the latest version. Six months from now, the latest version will be different, and it might contain breaking changes that cause your old code to fail. Pinning the version ensures you will always get the exact same environment.
4.  **Use the Environment in Your Project:**
    *   Go to your project's **Settings > Code Env** and select your new, dedicated environment as the default for the project.
5.  **Version Control the Environment:**
    *   Export the environment's JSON definition and commit it to your project's Git repository. This creates a permanent, version-controlled record of your reproducible environment.

### 4. Resources and Tools
- **Code Environments:** The core Dataiku feature for managing dependencies.
- **Pip's version specifiers:** The \`==\` syntax for pinning versions.
- **Git:** For version controlling your environment definition.

### 5. Next Steps and Progression
- **Isolate Environments:** Use different environments for different projects. Project A might need an old version of a library, while Project B needs the latest version. Separate environments prevent these from conflicting.
- **Update with Intention:** When you do need to update a package to a newer version, do it deliberately. Create a new version of your environment (or update the existing one), test it thoroughly to make sure it doesn't break your code, and then commit the new environment definition.

### 6. Common Challenges and Solutions
- **Challenge:** "How do I find the correct version numbers to pin?"
- **Solution:** When you are first developing the project, you can start with unpinned versions. Once your code works, you can inspect the environment to see which specific versions were installed. A common practice is to use a command like \`pip freeze\` in a terminal with the same packages, which will output a complete list of packages and their exact versions that you can then copy into the Dataiku environment settings.
- **Challenge:** "Pinning everything is tedious."
- **Solution:** It is a small, one-time effort that saves you from huge, unpredictable headaches in the future. The cost of debugging a mysterious failure caused by an unexpected package update is far higher than the cost of pinning versions upfront.
`},{id:512,slug:"containerizing-workflows-for-sharing-environments",question:"How to get started with containerizing the entire Dataiku workflow (code, data, dependencies) to capture and share environments.",answer:`
### 1. Introduction/Overview
Containerizing a workflow means packaging the code, its dependencies, and sometimes even the data into a single, portable unit called a **Docker container**. This is the gold standard for reproducibility and sharing, as it guarantees that the workflow will run in the exact same environment every time, regardless of the host machine.

### 2. Prerequisites
- **Docker installed** on your local machine and/or on the server.
- **A Dataiku project** you want to containerize.
- **Familiarity with Docker concepts** (Dockerfile, images, containers).

### 3. Step-by-Step Instructions: A Conceptual Workflow
Dataiku leverages containers in several ways. The most common pattern for sharing a reproducible environment is to containerize the *execution environment* for a recipe.

1.  **Create a Dockerfile:**
    *   A Dockerfile is a text file that contains the instructions for building a Docker image.
    *   You would start from a base image (e.g., an official Python image or a Dataiku-provided base image).
    *   Then, add commands to install any OS-level dependencies (e.g., \`apt-get install ...\`) and Python/R packages (e.g., \`pip install -r requirements.txt\`).
2.  **Build the Docker Image:**
    *   Run the command \`docker build -t my-custom-env:1.0 .\` to build the image from your Dockerfile.
3.  **Push the Image to a Registry:**
    *   Push your newly built image to a Docker registry (like Docker Hub, AWS ECR, or GCP GCR) so that Dataiku and other users can access it.
4.  **Configure Dataiku to Use the Image:**
    *   In Dataiku, an administrator goes to **Administration > Containerized Execution**.
    *   They create a new container configuration that points to your custom image in the registry.
5.  **How it's Used for Sharing:**
    *   Now, when you share your Dataiku project with a colleague on a different Dataiku instance, they also need access to your custom Docker image.
    *   When they run your recipe, they can select the same container configuration. The recipe will then run inside a container created from your image, guaranteeing the exact same environment and dependencies you used.

### 4. Resources and Tools
- **Dockerfile:** The recipe for creating your environment image.
- **Docker Registry (Docker Hub, ECR, etc.):** The storage location for your shared images.
- **Dataiku's Containerized Execution feature.**

### 5. Next Steps and Progression
- **Full Project Containerization:** For a completely self-contained deployment, you can run the entire Dataiku platform itself on Docker or Kubernetes. This is a more advanced setup but provides ultimate portability.
- **Infrastructure as Code:** You can use tools like Terraform to automate the building and pushing of your Docker images as part of a CI/CD pipeline.

### 6. Common Challenges and Solutions
- **Challenge:** "My Docker image is huge."
- **Solution:** Use multi-stage builds and be mindful of the layers in your Dockerfile to keep the image size down. Use a minimal base image where possible.
- **Challenge:** "How do I handle secrets like API keys in a container?"
- **Solution:** **Never** bake secrets into a Docker image. Use your container orchestrator's (e.g., Kubernetes Secrets) or your cloud provider's secrets management tools to securely inject secrets into the container at runtime as environment variables.
`},{id:513,slug:"exporting-importing-projects-between-instances",question:"How to get started with exporting and importing Dataiku projects between DSS instances to replicate environments.",answer:`
### 1. Introduction/Overview
Moving a project between different Dataiku instances (e.g., from a development server to a production server) is a core MLOps workflow. This is done by exporting the project as a single, self-contained **bundle** file, which can then be imported into the target instance.

### 2. Prerequisites
- **Two Dataiku instances** (e.g., a "source" and a "target" instance).
- **A completed project** on the source instance.
- **Permissions:** You need export permissions on the source project and import permissions on the target instance.

### 3. Step-by-Step Instructions
1.  **Export the Project from the Source Instance:**
    *   Navigate to the homepage of the project you want to move.
    *   Click the **...** menu in the top right corner.
    *   Select **Export**.
2.  **Configure the Export:**
    *   A dialog will appear. You can choose what to include in the bundle.
    *   **Crucially, decide whether to include the data.** For most deployments, you should **deselect** "Export all input datasets" and "Export all managed datasets". You want to deploy the project *logic*, not the data itself. The new instance will connect to its own production data sources.
    *   Click **EXPORT**. This will download a \`.zip\` file to your local machine. This is your project bundle.
3.  **Import the Project into the Target Instance:**
    *   Log into the target Dataiku instance.
    *   From the homepage, click **+ IMPORT PROJECT**.
    *   Upload the \`.zip\` bundle file you just downloaded.
4.  **Remap Connections:**
    *   During the import process, Dataiku will prompt you to remap connections.
    *   For each data connection used in the original project, you must map it to the corresponding connection on the new instance (e.g., map the "dev_database" connection to the "prod_database" connection).
5.  **Complete the Import:** Once the remapping is done, complete the import. A complete, identical copy of your project's Flow, recipes, and settings now exists on the new instance.

### 4. Resources and Tools
- **The Project Export/Import Feature:** The primary tool for this workflow.

### 5. Next Steps and Progression
- **Automation:** This entire process can be automated via the Dataiku REST API. A CI/CD pipeline can automatically create the bundle, download it, and upload it to the production instance as part of an automated deployment process.
- **Project Variables:** Use project variables for anything that might change between environments (like file paths or database names). The import wizard will also prompt you to set the values for these variables for the new environment.

### 6. Common Challenges and Solutions
- **Challenge:** After importing, a job fails with a "Connection not found" error.
- **Solution:** You did not remap the connections correctly during the import process. You can go to the project's **Settings > Dependencies** to see and remap the connections after the import.
- **Challenge:** The imported project doesn't have any data.
- **Solution:** This is the expected behavior if you chose not to export the data. You need to run a scenario in the new project to build the datasets using the new production data sources.
`},{id:514,slug:"defining-standardized-project-templates",question:"How to get started with defining standardized Dataiku project templates for consistent pipeline setup across teams.",answer:`
### 1. Introduction/Overview
As more teams in your organization adopt Dataiku, ensuring consistency becomes vital. A **Project Template** is a pre-built, empty project that contains your team's standard structure and best practices. New projects can be started by duplicating this template, which accelerates development and ensures all projects have a similar, easy-to-understand layout.

### 2. Prerequisites
- **A clear idea of your team's best practices** for project organization.
- **Permissions to create projects.**

### 3. Step-by-Step Instructions
1.  **Create a New "Template" Project:**
    *   From the Dataiku homepage, create a new, blank project.
    *   Give it a clear name that identifies it as a template, for example: \`TEMPLATE - Standard Analytics Project\`.
2.  **Build the Standard Structure:** In this empty project, build out the reusable components that all your projects should have.
    *   **Flow Zones:** Create your standard set of Flow Zones (e.g., \`1_Ingestion\`, \`2_Data_Prep\`, \`3_Modeling\`, \`4_Outputs\`). This enforces a consistent pipeline architecture.
    *   **Wiki:** Create a standard Wiki structure with placeholder pages for key documentation like a "Project Brief", "Data Dictionary", and "Meeting Notes". You can even include a checklist for new developers.
    *   **Tags:** Add your team's standard set of tags to the project. They will then be available for autocomplete in the new projects.
    *   **Code Libraries:** If you have common helper functions, you can include them in the project's library.
3.  **Document the Template:** In the template project's Wiki, clearly explain that this is a template and how to use it.
4.  **Using the Template:**
    *   When a developer needs to start a new project, they navigate to the homepage of the template project.
    *   They click the **...** menu and select **Duplicate project**.
    *   They give their new project a name and start building within the pre-defined structure.

### 4. Resources and Tools
- **The "Duplicate project" feature:** The key to using the template.
- **Flow Zones and Wikis:** The primary components of a good template.

### 5. Next Steps and Progression
- **Multiple Templates:** You might create different templates for different types of projects. For example, a template for a simple BI reporting project might be different from a template for a complex MLOps project.
- **Shared Template Project:** Make the template project read-only for most users, with only a "Center of Excellence" team able to modify it. This ensures the standard is maintained.
- **Automated Project Creation:** For advanced use cases, you can use the Dataiku API to write a script that automates the duplication and setup of a new project from the template.

### 6. Common Challenges and Solutions
- **Challenge:** "Developers are still creating blank projects instead of using the template."
- **Solution:** This is a training and communication issue. You must clearly communicate that using the template is the standard, required process for starting new projects. Enforce this during project kickoff reviews.
- **Challenge:** "The template is becoming too restrictive or complex."
- **Solution:** A template should provide a helpful starting structure, not a rigid straitjacket. Gather feedback from your developers. Are there parts of the template that are not useful? Be willing to evolve and simplify the template based on team feedback.
`},{id:515,slug:"using-time-travel-snapshots-for-reproducibility",question:"How to get started with using Dataiku’s managed “time travel” snapshots or project backups to reproduce past runs.",answer:`
### 1. Introduction/Overview
Reproducing a past result requires being able to restore both the code and the data to their exact state at a specific point in time. Dataiku addresses this through two main mechanisms: **Git integration** for code snapshots and **Partitioning** for data snapshots. While Dataiku doesn't have a single "time travel" button, these features provide the necessary components for reproducibility.

### 2. Prerequisites
- **Git integration:** Your project must be linked to a Git repository to version your code.
- **Partitioned datasets:** Your time-based data should be partitioned.

### 3. Step-by-Step Instructions

#### Part 1: Reproducing the Code
1.  **Find the Commit:** Use your Git provider's UI (e.g., GitHub) to find the commit hash that corresponds to the version of the code you want to reproduce.
2.  **Create a New Branch:** In Dataiku's Git page for the project, create a new branch from that specific commit.
3.  **Restore the Code:** This new branch now contains an exact snapshot of all your project's recipes and configurations as they were at the time of that commit.

#### Part 2: Reproducing the Data
1.  **Use Partitioning:** This is the standard Dataiku pattern for data versioning. If your output dataset is partitioned by day, each partition is effectively a "snapshot" of the data for that day.
2.  **Rebuild a Specific Partition:** To reproduce a result from January 15th, you can now run a job that explicitly builds only the \`2023-01-15\` partition. This job will use the restored code (from your Git branch) and the correct historical input data (assuming your source data is also partitioned or snapshotted).

#### Part 3: Manual Snapshots (for non-partitioned data)
1.  **When to use:** When you need a point-in-time copy of a non-partitioned dataset.
2.  **How:** Use a **Sync** recipe to create a copy of the dataset. Give the output dataset a name that includes the date or version (e.g., \`customers_snapshot_2023_10_27\`). This is a manual process but can be useful for creating specific checkpoints.

### 4. Resources and Tools
- **Git Integration:** The essential tool for versioning and restoring your code.
- **Partitioning:** The primary mechanism for versioning and time-traveling your data.
- **Sync Recipe:** For creating manual, point-in-time snapshots of datasets.
- **Project Backups (Admin):** An administrator can take a full backup of a Dataiku instance, which can be restored for disaster recovery, but this is a much heavier process than using Git and partitioning for standard reproducibility.

### 5. Next Steps and Progression
- **Automated Snapshots:** You could create a scenario with a Python step that runs periodically and uses the Sync recipe pattern to automatically create dated snapshots of a critical dataset.

### 6. Common Challenges and Solutions
- **Challenge:** "My source data is not partitioned or versioned."
- **Solution:** This is a major challenge for reproducibility. If your raw source data (e.g., a database table) is constantly being updated in place, it can be very difficult to reproduce a past result. You may need to work with your data engineering team to implement a snapshotting or change-data-capture (CDC) process on the source system.
`},{id:516,slug:"sharing-versioning-notebooks-sql-recipes",question:"How to get started with sharing and versioning Dataiku notebooks and SQL recipes across projects for reproducibility.",answer:`
### 1. Introduction/Overview
To ensure consistency and avoid duplicated work, teams often need to share and version reusable pieces of code, such as common SQL queries or standard analytical notebooks. Dataiku supports this through a combination of Git integration and a shared library project structure.

### 2. Prerequisites
- **A need for reusable code:** You have a notebook or a SQL recipe that multiple projects could benefit from.
- **All relevant projects connected to Git.**

### 3. Step-by-Step Instructions

#### Part 1: Versioning Notebooks and Recipes
1.  **Connect to Git:** This is the foundational step. Ensure the project containing your notebook or recipe is connected to a Git repository.
2.  **How it Works:** Notebooks (\`.ipynb\` files) and SQL recipes (\`.sql\` files) are stored as files within the project's directory structure. When you **commit** your project, you are creating a versioned snapshot of that notebook or recipe file.
3.  **View History:** You can use your Git provider's UI (e.g., GitHub) to see the full commit history, compare different versions of a notebook, and see exactly what code was changed, by whom, and when.

#### Part 2: Sharing Reusable Notebooks and Recipes
1.  **Create a "Shared Library" Project:**
    *   Create a new, central Dataiku project named something like \`SHARED_ANALYTICS_LIBRARY\`.
    *   This project's purpose is to store reusable assets.
2.  **Populate the Library Project:**
    *   Create your standard, reusable SQL recipes and exploratory notebooks within this library project.
    *   Document them thoroughly in the project's Wiki, explaining what each one does and how to use it.
3.  **How Other Projects Use Them (Two Methods):**
    *   **Method A (Copy/Paste):** A developer can simply open the shared library project, copy the SQL code from a recipe or the cells from a notebook, and paste them into their own project. This is simple but creates a disconnected copy.
    *   **Method B (Project Dependency - Better):** In a developer's own project, they can go to **Settings > Dependencies** and add the \`SHARED_ANALYTICS_LIBRARY\` project as a dependency. This allows them to see the shared project's flows and potentially use its datasets as inputs, creating a clearer linkage.

### 4. Resources and Tools
- **Git Integration:** For versioning.
- **A Shared "Library" Project:** The central place for storing reusable assets.
- **Copy/Paste:** A simple mechanism for sharing code snippets.

### 5. Next Steps and Progression
- **Reusable Python Functions:** For Python code, the best practice is to put reusable functions in the library project's **Python Libraries** folder. Other projects can then add a dependency and \`import\` these functions directly.
- **Project Templates:** Create a project template that already includes a dependency on the shared library project, making it easy for new projects to access the reusable assets.

### 6. Common Challenges and Solutions
- **Challenge:** A user copied a SQL recipe but it failed in their project.
- **Solution:** The SQL recipe likely referred to a dataset by its Dataiku name. When copied, the user needs to update the \`FROM\` and \`JOIN\` clauses to refer to the dataset names in *their* project.
- **Challenge:** "The diff for a notebook file in GitHub is an unreadable JSON."
- **Solution:** This is true, as \`.ipynb\` files store a lot of metadata. Use Dataiku's own diff viewer on the Git page, which is optimized for comparing notebooks. There are also browser extensions available that can render notebook diffs more cleanly on GitHub.
`},{id:517,slug:"integrating-projects-with-git-for-tracking-changes",question:"How to get started with integrating Dataiku projects with Git to track code changes and environment changes.",answer:`
### 1. Introduction/Overview
Integrating your Dataiku project with Git is the cornerstone of MLOps and DataOps. It provides a robust system for version control, collaboration, and auditing. This integration allows you to track every change to your project's logic, from visual recipes to code environments.

### 2. Prerequisites
- **A Dataiku project.**
- **An empty remote Git repository** on a provider like GitHub, GitLab, or Azure DevOps.
- **Git installed on the Dataiku server** and properly configured by an administrator.

### 3. Step-by-Step Instructions
1.  **Link the Project to the Repository:**
    *   In your Dataiku project, go to **Settings > Git**.
    *   Click **Convert to Git project**.
    *   Provide the remote repository URL and click **Link**.
2.  **Commit Your Initial Project:**
    *   Navigate to the **Git** page from the project's top menu.
    *   You will see all your project objects listed as "uncommitted changes".
    *   Stage all the changes, write an initial commit message (e.g., "Initial commit"), and click **Commit**.
    *   Click **Push** to send the project to your remote repository.
3.  **Track Changes to Code and Recipes:**
    *   Now, any change you make—editing a Python script, adding a step to a Prepare recipe, modifying a chart—will appear as a change on the Git page.
    *   You can commit these changes with a clear message to create a version history.
4.  **Track Changes to the Environment:**
    *   Go to **Administration > Code Envs** and find the environment your project uses.
    *   Click **Export** to download the environment's definition as a \`.json\` file.
    *   Add this \`.json\` file to your project's directory (e.g., create a folder called \`code-envs\` and upload it there).
    *   Go to the Git page and **commit** this file. Now, your code environment's definition is version-controlled alongside your project logic.

### 4. Resources and Tools
- **Dataiku's Git Integration:** The UI for managing commits, branches, etc.
- **Git Provider (e.g., GitHub):** The web interface for viewing history and managing pull requests.
- **Code Environment Export/Import:** The feature for packaging your environment definition.

### 5. Next Steps and Progression
- **Branching Strategy:** Implement a feature branching workflow. All new work should be done on a separate branch and then merged via a pull request.
- **CI/CD Integration:** Use Git commits and pull requests as triggers to run automated testing and deployment pipelines.
- **Visual Diffs:** When reviewing changes on the Dataiku Git page, it provides a "visual diff" that clearly shows changes to visual components, not just the raw JSON.

### 6. Common Challenges and Solutions
- **Challenge:** "What is actually being stored in Git?"
- **Solution:** Git stores the *definition* of your project: all recipe code and configurations, flow structure, notebook contents, and the environment \`.json\` file you committed. It does **not** store the actual data from your datasets.
- **Challenge:** "My push to the remote repository failed."
- **Solution:** This is usually an authentication issue. Your Dataiku administrator needs to ensure that the Dataiku server has the correct SSH keys or credentials to authenticate with your Git provider.
`},{id:518,slug:"automating-documentation-for-reproducibility-compliance",question:"How to get started with automating documentation of Dataiku flows and environments to enforce reproducibility and compliance.",answer:`
### 1. Introduction/Overview
Manual documentation often becomes outdated. Automating the generation of documentation ensures that it is always in sync with the actual state of your project. This is particularly important for reproducibility and compliance, providing a reliable audit trail. In Dataiku, this can be achieved by using the Python API to extract metadata from your project.

### 2. Prerequisites
- **A well-documented project:** Your automation can only extract information that is there. You must first have a discipline of adding descriptions to your datasets and recipes.
- **A Python recipe or scenario step.**
- **Knowledge of the Dataiku Python API.**

### 3. Step-by-Step Instructions
1.  **Create a "Documentation Generation" Recipe:**
    *   In your project, create a new **Python recipe**. This recipe's job will be to generate the documentation.
    *   Create a **Managed Folder** as the output for this recipe, which is where the final document will be saved.
2.  **Write the Documentation Script:**
    *   In the Python recipe, use the Dataiku API to get a handle on your project.
    *   **Iterate through objects:** Write a loop that goes through all the datasets and recipes in the project (\`project.list_datasets()\`, \`project.list_recipes()\`).
    *   **Extract Metadata:** Inside the loop, for each object, get its definition (\`.get_definition()\`). From the definition, you can extract its name, type, description, tags, schema (for datasets), and a recipe's code or steps.
    *   **Format the Output:** Format this extracted metadata into a human-readable format like Markdown or HTML.
    *   **Save the Document:** Write the final formatted string to a file in your output managed folder.
3.  **Generate Environment Documentation:**
    *   Your script can also use the API to get the list of packages in the project's code environment (\`project.get_code_env().get_definition()\`) and include this in the documentation.
4.  **Automate with a Scenario:**
    *   Create a **Scenario** that runs your documentation generation recipe.
    *   You can schedule this to run periodically (e.g., weekly) or have it triggered after every major change to the project.
    *   You can add a reporter to the scenario to email the generated document to stakeholders.

### 4. Resources and Tools
- **Dataiku Python API:** The key to programmatically accessing project metadata.
- **Python Recipe and Managed Folder:** The components for creating and storing the documentation.
- **Markdown or HTML:** Good formats for the final output document.

### 5. Next Steps and Progression
- **Static Site Generator:** Your script could generate a set of Markdown files that can be used as the source for a static site generator like Jekyll or Hugo, creating a full, professional-looking documentation website for your project.
- **Compliance Reporting:** Tailor the script to generate a report specifically formatted for a compliance audit, automatically pulling all the necessary lineage and governance information.

### 6. Common Challenges and Solutions
- **Challenge:** The generated documentation is not useful because the descriptions are empty.
- **Solution:** This highlights a process problem. The automation script is a "garbage in, garbage out" system. Your team must have the discipline to write good descriptions on their Dataiku objects for the generated documentation to be valuable.
- **Challenge:** The script is complex to write.
- **Solution:** Start simple. Your first version could just list all the datasets in the project and their descriptions. You can add more detail (like schemas, recipe logic, etc.) over time.
`},{id:519,slug:"creating-dev-test-prod-instances",question:"How to get started with creating separate dev/test/prod Dataiku instances and migrating projects through them.",answer:`
### 1. Introduction/Overview
A multi-environment setup (Dev, Test/QA, Prod) is the standard for enterprise-grade software development and MLOps. It ensures that changes can be developed and tested in isolation before being promoted to the live production environment, minimizing risk. This separation is achieved by having separate, independent Dataiku instances for each environment.

### 2. Prerequisites
- **Sufficient infrastructure:** You need the server resources (VMs or Kubernetes capacity) to run multiple Dataiku instances.
- **A Dataiku license** that supports multiple environments.
- **Administrator-level skills** for installing and configuring the instances.

### 3. Step-by-Step Instructions: The Promotion Workflow
1.  **Set Up the Instances:** An administrator installs and configures three separate Dataiku DSS instances.
    *   **Dev (Development):** Where developers have broad permissions to build, experiment, and test new projects.
    *   **Test/QA (Quality Assurance):** A locked-down, production-like environment. Developers do not have access. A dedicated QA team tests the deployed projects here.
    *   **Prod (Production):** The live environment that serves business users. It is highly restricted and only deployment managers can make changes.
2.  **Development on Dev:** All new work happens on the Dev instance. Developers build and test their projects here.
3.  **Deployment from Dev to Test:**
    *   When a project is ready for QA, a developer **exports** it from the Dev instance as a **project bundle** (\`.zip\` file).
    *   A deployment manager **imports** this bundle into the Test instance, remapping the data connections to point to test data sources.
4.  **Validation on Test:** The QA team or automated tests are run against the project on the Test instance to validate its correctness and performance.
5.  **Deployment from Test to Prod:**
    *   If testing is successful, the same project bundle is then taken and imported into the **Prod** instance by the deployment manager.
    *   The connections are remapped to the live production data sources.
    *   The production scenarios are then activated.

### 4. Resources and Tools
- **Separate Servers/VMs/Namespaces:** To physically isolate the environments.
- **Project Bundles:** The mechanism for moving a project between instances.
- **CI/CD Tools (Jenkins, etc.):** To automate the bundling and deployment process.

### 5. Next Steps and Progression
- **Automate Deployments:** Use a CI/CD tool to automate the process of creating the bundle and deploying it to the next environment, with manual approval gates in the pipeline.
- **Infrastructure as Code:** Use tools like Terraform to define the infrastructure for each environment in code, ensuring they are consistent and can be recreated easily.

### 6. Common Challenges and Solutions
- **Challenge:** Keeping the environments in sync is difficult.
- **Solution:** This is why automated deployment pipelines are so important. Manual deployments are prone to human error. An automated process ensures that exactly the same bundle is deployed to Test and then to Prod.
- **Challenge:** This seems like a lot of overhead.
- **Solution:** It is, and it's essential for any system that the business relies on. For a small team doing non-critical analysis, a single instance might be sufficient. But for enterprise MLOps, a multi-environment strategy is a non-negotiable best practice for ensuring stability and quality.
`},{id:520,slug:"connecting-projects-to-git-for-mlops-workflows",question:"How to get started with connecting Dataiku projects to a Git repository and managing branches for MLOps workflows.",answer:`
### 1. Introduction/Overview
For a robust MLOps workflow, version control is not optional. Connecting your Dataiku project to a Git repository is the foundation for tracking changes, enabling team collaboration, and automating your CI/CD pipelines. A disciplined branching strategy is key to managing this effectively.

### 2. Prerequisites
- A Dataiku project.
- An empty remote Git repository (e.g., on GitHub, GitLab).
- Git configured on your Dataiku instance by an administrator.

### 3. Step-by-Step Instructions
1.  **Connect Project to Git:** In your project's **Settings > Git**, link the project to your remote Git repository URL.
2.  **Adopt a Branching Strategy:** Your team must agree on a branching model. A common and effective one is **GitFlow**:
    *   **\`main\` branch:** This branch represents the production-ready, stable version of your project. It should be protected so no one can push to it directly.
    *   **\`develop\` branch:** This is the main integration branch. All completed features are merged into \`develop\`.
    *   **Feature branches:** All new work must be done on a new feature branch, created from \`develop\`.
        *   Example name: \`feature/add-customer-ltv-model\`.
3.  **The MLOps Workflow:**
    1.  A data scientist wants to develop a new model. They pull the latest \`develop\` branch, then create a new feature branch: \`feature/new-churn-model\`.
    2.  They work on this branch in Dataiku, building the necessary recipes and training the model. They commit their changes regularly to this branch.
    3.  When the model is ready for review, they push the branch and open a **Pull Request (PR)** to merge their feature branch into \`develop\`.
    4.  Another team member (e.g., a senior data scientist or an MLOps engineer) reviews the PR, checking the code, the model's performance, and the documentation.
    5.  Once approved, the PR is merged. This triggers a CI/CD pipeline that can automatically deploy the project to a testing environment.

### 4. Resources and Tools
- **Dataiku's Git Integration:** The UI for creating branches, committing, and pushing.
- **Git Provider (GitHub, etc.):** The platform for managing pull requests and branch protection rules.
- **A documented branching strategy** in your team's Wiki.

### 5. Next Steps and Progression
- **Release Branches:** When you are ready to deploy to production, you can create a \`release\` branch from \`develop\`. After final testing, this release branch is merged into \`main\` and tagged with a version number.
- **Hotfixes:** If a critical bug is found in production, a \`hotfix\` branch is created from \`main\`, the bug is fixed, and it is then merged back into both \`main\` and \`develop\`.

### 6. Common Challenges and Solutions
- **Challenge:** "What should I put in a commit?"
- **Solution:** A commit should be a small, logical unit of work. Don't wait until you've built the entire project to commit. Commit after you complete each significant step (e.g., "Commit 1: Add raw datasets", "Commit 2: Create customer cleaning recipe", "Commit 3: Train initial model").
- **Challenge:** "We have merge conflicts all the time."
- **Solution:** This often means multiple people are working on the same recipe on different branches. Improve communication and task breakdown to avoid this. Also, encourage developers to pull the latest changes from \`develop\` into their feature branch frequently to integrate changes early and in smaller chunks.
`},{id:521,slug:"using-native-git-integration-for-team-collaboration",question:"How to get started with using Dataiku’s native Git integration to enable team collaboration on data pipelines.",answer:`
### 1. Introduction/Overview
Dataiku's native Git integration is the key to enabling effective team collaboration. It allows multiple developers to work on the same project simultaneously without overwriting each other's work, and it provides a full audit trail of every change. The core concept is that each developer works in their own isolated **branch**.

### 2. Prerequisites
- A Dataiku project connected to a remote Git repository (e.g., on GitHub).
- All team members have been added as contributors to the Dataiku project.

### 3. Step-by-Step Instructions: A Collaborative Workflow
1.  **Create a Feature Branch:**
    *   Before starting any new work, a developer must create their own branch.
    *   Go to the **Git** page in the project.
    *   Click **Switch branch > + Create branch**.
    *   Give it a clear name (e.g., \`jane/add-new-dashboard\`).
2.  **Work in Isolation:**
    *   The developer now works on this branch. Any changes they make—creating recipes, editing dashboards, etc.—are saved only on this branch and are invisible to their teammates.
    *   They should **commit** their changes to their branch frequently with clear messages.
3.  **Share and Get Feedback:**
    *   When the developer wants to share their work or get feedback, they **push** their branch to the remote repository.
    *   They can then ask a colleague to **pull** that specific branch to their own local Dataiku instance to review the work in progress.
4.  **Merge Changes with a Pull Request:**
    *   When the feature is complete and ready to be integrated into the main project, the developer goes to the Git provider's website (e.g., GitHub).
    *   They open a **Pull Request (PR)** to merge their feature branch into the main branch (e.g., \`develop\` or \`main\`).
5.  **Review and Merge:**
    *   Another team member reviews the PR. They can see all the changes to the project's definition.
    *   If they approve, the branch is merged.
6.  **Update Other Branches:** All other developers can now **pull** the latest changes from the main branch into their own branches to get the new feature.

### 4. Resources and Tools
- **Dataiku's Git Page:** The central UI for branching, committing, pushing, and pulling.
- **Pull Requests (on GitHub, etc.):** The formal process for code review and merging.
- **The "Changes" tab:** Shows a "diff" of what has been modified on your current branch.

### 5. Next Steps and Progression
- **Resolve Conflicts:** If two developers modify the same recipe, a merge conflict will occur when they try to merge their branches. Dataiku provides a visual "diff and merge" tool to help resolve these conflicts by choosing which version of the changes to keep.

### 6. Common Challenges and Solutions
- **Challenge:** "I made changes on the main branch by mistake and now it's a mess."
- **Solution:** This is why you should use **branch protection rules** in your Git provider to prevent anyone from pushing directly to the main branch. All changes should be forced to go through a pull request.
- **Challenge:** "My colleague and I both edited the same visual recipe, and the merge is difficult."
- **Solution:** This is the hardest part of visual tool collaboration. The best solution is to improve communication and task breakdown to avoid having two people edit the same object at the same time. If it's unavoidable, keep your changes small and focused.
`},{id:522,slug:"setting-up-jenkins-pipelines-with-dataiku-api",question:"How to get started with setting up Jenkins pipelines that use the Dataiku Python API to run and deploy projects.",answer:`
### 1. Introduction/Overview
Jenkins is a popular open-source automation server used for building CI/CD pipelines. You can integrate Dataiku into a Jenkins pipeline by having Jenkins call the **Dataiku REST API** to trigger actions like running tests or deploying projects. This automates your MLOps workflow.

### 2. Prerequisites
- A running Jenkins server.
- A Dataiku project connected to Git.
- A Dataiku API key with permissions to run scenarios and create bundles.

### 3. Step-by-Step Instructions
1.  **Install Necessary Jenkins Plugins:** Ensure you have plugins for Git and for handling credentials.
2.  **Store the Dataiku API Key in Jenkins:**
    *   In Jenkins, go to **Manage Jenkins > Credentials**.
    *   Add a new "Secret text" credential.
    *   Store your Dataiku API key here and give it a memorable ID (e.g., \`DATAİKU_API_KEY\`).
3.  **Create a Jenkins Pipeline Job:**
    *   Create a new "Pipeline" job in Jenkins.
    *   Configure it to be triggered by changes to your project's Git repository (e.g., using a webhook).
4.  **Write the \`Jenkinsfile\`:**
    *   A \`Jenkinsfile\` is a script (written in Groovy) that defines the stages of your pipeline. It lives in your Git repository.
    *   The script will use shell steps (\`sh\`) to execute \`curl\` commands that call the Dataiku REST API.
    > \`\`\`groovy
    > pipeline {
    >     agent any
    >     environment {
    >         // Load the API key securely
    >        
    
    >     }
    >     stages {
    >         stage('Run Tests') {
    >             steps {
    >                 echo 'Triggering Dataiku test scenario...'
    >                 sh "curl : -X POST https://dss.mycompany.com/public/api/projects/MYPROJ/scenarios/run_tests/run"
    >                 // Note: A real pipeline would need to poll for job completion here
    >             }
    >         }
    >         stage('Deploy to Prod') {
    >             steps {
    >                 // Add steps to create and deploy a project bundle via the API
    >             }
    >         }
    >     }
    > }
    > \`\`\`
### 4. Resources and Tools
- **Jenkins:** The CI/CD automation server.
- **Dataiku REST API:** The interface for Jenkins to control Dataiku.
- **\`curl\`:** A simple command-line tool for making the API calls from within the Jenkins script.

### 5. Next Steps and Progression
- **Polling for Job Status:** The example above just triggers the job. A robust pipeline needs to be more sophisticated. It should capture the \`jobId\` from the API response and then use another API endpoint in a loop to poll for the job's status, only proceeding to the next stage when the job completes successfully.
- **Parameterized Builds:** Pass parameters from Jenkins into your Dataiku scenario runs to make them more dynamic.

### 6. Common Challenges and Solutions
- **Challenge:** "My Jenkins job can't connect to Dataiku."
- **Solution:** This is a network issue. Ensure the Jenkins agent machine can reach the Dataiku server's URL and port. Check for firewalls.
- **Challenge:** "How do I manage the API key securely?"
- **Solution:** Always use the Jenkins Credentials store. Never hardcode the API key in your \`Jenkinsfile\`. The \`credentials()\` helper in the pipeline script is the correct way to access it securely.
`},{id:523,slug:"building-github-actions-gitlab-ci-workflows",question:"How to get started with building GitHub Actions or GitLab CI workflows to automate Dataiku project deployment.",answer:`
### 1. Introduction/Overview
Modern Git providers like GitHub and GitLab have powerful, built-in CI/CD capabilities. You can create a workflow defined in a YAML file directly in your repository. This workflow can automate the testing and deployment of your Dataiku project by calling the Dataiku REST API.

### 2. Prerequisites
- A Dataiku project stored in a GitHub or GitLab repository.
- A Dataiku API key.

### 3. Step-by-Step Instructions (Using GitHub Actions)
1.  **Store the API Key as a Secret:**
    *   In your GitHub repository, go to **Settings > Secrets and variables > Actions**.
    *   Create a new **repository secret** named \`DATAİKU_API_KEY\` and paste your API key as the value.
2.  **Create a Workflow File:**
    *   In your repository, create a directory path \`.github/workflows/\`.
    *   Inside this folder, create a new YAML file, for example, \`ci.yml\`.
3.  **Define the Workflow in YAML:**
    *   The YAML file defines the trigger and the jobs.
    *   The workflow will be triggered on a push to the main branch.
    *   The job will have steps that use \`curl\` to call the Dataiku REST API, using the secret you stored.
    > \`\`\`yaml
    > name: Dataiku CI/CD
    >
    > on:
    >   push:
    >     branches: [ main ]
    >
    > jobs:
    >   test-and-deploy:
    >     runs-on: ubuntu-latest
    >     steps:
    >     - name: Run Tests in Dataiku
    >       env:
    >         API_KEY: \${{ secrets.DATAİKU_API_KEY }}
    >       run: |
    >         curl -u $API_KEY: -X POST https://dss.mycompany.com/public/api/projects/MYPROJ/scenarios/run_tests/run
    >         # A real workflow would need to poll for job completion here
    > \`\`\`
4.  **Commit the Workflow File:** Commit and push the \`ci.yml\` file. GitHub Actions will now automatically detect and run this workflow on the next push to the main branch.

### 4. Resources and Tools
- **GitHub Actions / GitLab CI:** The integrated CI/CD platform.
- **YAML:** The language for defining the workflow pipeline.
- **Repository Secrets:** The secure way to store credentials.
- **Dataiku REST API:** The interface your workflow script will call.

### 5. Next Steps and Progression
- **Pull Request Trigger:** Change the \`on:\` trigger to \`pull_request\` to have your tests run automatically every time a PR is opened. You can configure this as a required status check, preventing merging until the tests pass.
- **Multi-Step Deployment:** Add more steps to your job to create a project bundle and deploy it to a production Dataiku instance.
- **Use Community Actions:** There are pre-built actions in the GitHub Marketplace that can simplify calling Dataiku APIs.

### 6. Common Challenges and Solutions
- **Challenge:** My workflow run fails with a 401 Unauthorized error.
- **Solution:** Check that your secret is named correctly and that the API key has the right permissions in Dataiku. The syntax for passing secrets to shell scripts in YAML can be tricky, so double-check the documentation for your CI/CD provider.
- **Challenge:** The workflow runner can't connect to my Dataiku instance.
- **Solution:** If your Dataiku instance is behind a corporate firewall, the public, cloud-hosted runners used by GitHub/GitLab may not be able to reach it. You may need to set up **self-hosted runners** on a machine within your own network.
`},{id:524,slug:"enabling-continuous-delivery-with-cicd-tools",question:"How to get started with enabling continuous delivery of Dataiku flows using Bamboo, Jenkins, or Bitbucket pipelines.",answer:`
### 1. Introduction/Overview
Continuous Delivery (CD) is the practice of automating the release of new code or project versions to production. Integrating Dataiku with a CI/CD tool (like Jenkins, Bamboo, or Bitbucket Pipelines) enables you to create a reliable, repeatable, and automated process for deploying your data pipelines.

### 2. Prerequisites
- **A CI/CD tool** configured and running.
- **A multi-environment Dataiku setup** (e.g., Dev, Test, Prod).
- **Your Dataiku project connected to Git.**
- **A Dataiku API key** for your CI/CD tool.

### 3. Step-by-Step Instructions: The CD Pipeline
A Continuous Delivery pipeline is typically an extension of a Continuous Integration (CI) pipeline.

1.  **Trigger:** The pipeline is triggered by a merge to your main or release branch in Git.
2.  **CI Phase (Automated Testing):**
    *   The first stages of the pipeline are for CI.
    *   The script calls the Dataiku API to run a "test" scenario on a Dev or Test instance.
    *   The pipeline only proceeds if all tests pass.
3.  **Build Artifact:**
    *   If tests pass, the next stage calls the Dataiku API to create a **project bundle** (\`.zip\` file).
    *   This bundle is your release artifact. The script downloads it and archives it in the CI/CD tool.
4.  **Manual Approval Gate:**
    *   A key part of CD (as opposed to Continuous *Deployment*) is a manual approval gate.
    *   The pipeline pauses and waits for a human (e.g., a release manager or a product owner) to click an "Approve for Production" button.
5.  **Deploy to Production:**
    *   Upon approval, the final stage of the pipeline runs.
    *   It takes the archived project bundle and calls the Dataiku API on the **production instance** to import the bundle, updating the production project to the new version.
    *   It can then run a final "smoke test" scenario in production.

### 4. Resources and Tools
- **Your CI/CD Tool (Jenkins, Bamboo, etc.):** The engine for the pipeline.
- **Dataiku REST API:** The interface for controlling all the Dataiku actions.
- **Project Bundles:** The versioned, deployable artifacts.

### 5. Next Steps and Progression
- **Automated Rollbacks:** A more advanced pipeline could include a rollback stage. If the post-deployment smoke test fails, the script could automatically re-deploy the *previous* successful version of the project bundle.
- **Infrastructure as Code:** The deployment pipeline could also trigger Terraform or similar scripts to provision or update the necessary cloud infrastructure for the project.

### 6. Common Challenges and Solutions
- **Challenge:** Deployments are risky.
- **Solution:** A well-designed CD pipeline *reduces* risk. By automating the process, you eliminate the human error associated with manual deployments. The automated testing and manual approval gates ensure that only validated changes go live.
- **Challenge:** "How does the pipeline handle the different database connections for dev and prod?"
- **Solution:** When you import a project bundle via the API, you can provide parameters to remap the connections. The deployment script would specify that the project should use the "prod_db" connection instead of the "dev_db" connection.
`},{id:525,slug:"integrating-infrastructure-as-code-for-provisioning",question:"How to get started with integrating infrastructure-as-code (e.g. Terraform, Azure DevOps) to provision Dataiku infrastructure.",answer:`
### 1. Introduction/Overview
Infrastructure as Code (IaC) is the practice of managing and provisioning your IT infrastructure using machine-readable definition files, rather than manual configuration. Using an IaC tool like Terraform to deploy your Dataiku infrastructure ensures your environments are consistent, repeatable, and version-controlled.

### 2. Prerequisites
- **An IaC tool** like Terraform or an equivalent (AWS CloudFormation, Azure ARM Templates).
- **A cloud provider account** (AWS, Azure, GCP).
- **A Dataiku license file.**

### 3. Step-by-Step Instructions: A Terraform Example
1.  **Install Terraform:** Install the Terraform CLI on your machine.
2.  **Configure Your Provider:** Create a \`.tf\` file to configure the cloud provider you are using (e.g., AWS). This is where you will provide your cloud credentials.
3.  **Define Your Infrastructure Resources:** In your \`.tf\` files, define all the resources needed to run Dataiku.
    *   **A Virtual Machine:** Define an \`aws_instance\` (for EC2) or equivalent, specifying the instance type, OS image (AMI), and networking details (VPC, subnet, security group).
    *   **A Database:** Define an \`aws_db_instance\` (for RDS) to host the backend PostgreSQL database for Dataiku.
    *   **Storage:** Define an S3 bucket for Dataiku's data directory.
4.  **Install Dataiku with a Provisioner:**
    *   Use a Terraform **provisioner** (like \`remote-exec\`) to run a script on the VM after it's created.
    *   This script will download the Dataiku software, run the installer, and place your license file.
5.  **Plan and Apply:**
    *   Run \`terraform plan\`. Terraform will show you all the resources it is going to create.
    *   Run \`terraform apply\`. Terraform will now connect to your cloud provider and create all the defined resources and configure them. After a few minutes, you will have a fully running Dataiku instance.

### 4. Resources and Tools
- **Terraform:** The leading IaC tool.
- **Cloud Provider SDKs:** Used by Terraform to interact with the cloud APIs.
- **Shell Scripting:** For automating the software installation on the provisioned VM.

### 5. Next Steps and Progression
- **Kubernetes Provisioning:** Use Terraform to provision a managed Kubernetes cluster (like EKS) and then use the Helm provider for Terraform to deploy Dataiku onto that cluster.
- **CI/CD Integration:** Integrate your Terraform scripts into a CI/CD pipeline. When you commit a change to your \`.tf\` files, the pipeline can automatically run \`terraform apply\` to update your infrastructure.
- **State Management:** Use a remote backend for your Terraform state file (like an S3 bucket) to securely share the state of your infrastructure with your team.

### 6. Common Challenges and Solutions
- **Challenge:** Writing Terraform code is complex.
- **Solution:** Start with the official Terraform documentation for your cloud provider, which includes many examples. Begin by just trying to provision a single VM, then gradually add more resources like the database and storage.
- **Challenge:** A change to a resource requires it to be destroyed and recreated.
- **Solution:** This is how Terraform works sometimes. The \`terraform plan\` command will warn you when this is going to happen. This emphasizes the need for a good backup and restore strategy for your Dataiku instance data.
`},{id:526,slug:"adding-scenario-runs-validation-tests-to-ci-pipeline",question:"How to get started with adding Dataiku scenario runs or validation tests into your CI pipeline as build steps.",answer:`
### 1. Introduction/Overview
A Continuous Integration (CI) pipeline should do more than just check for syntax errors; it should validate that your changes haven't broken the logic of your data pipeline. A standard practice is to add a step to your CI pipeline that triggers a "test" scenario in Dataiku and fails the build if the tests don't pass.

### 2. Prerequisites
- **A CI pipeline** set up for your Dataiku project (e.g., in Jenkins or GitHub Actions).
- **A dedicated "test" scenario** created in your Dataiku project. This scenario should contain "Run checks" steps that validate your data quality rules.

### 3. Step-by-Step Instructions
1.  **Create a Test Scenario in Dataiku:**
    *   Go to your Dataiku project and create a new scenario named \`run_all_tests\`.
    *   This scenario should contain a series of **Run checks** steps that execute the predefined data quality checks on your critical datasets.
    *   It can also contain steps to run unit tests for your Python recipes if you have them.
2.  **Modify Your CI/CD Script:**
    *   Open your CI pipeline's configuration file (e.g., \`Jenkinsfile\` or \`ci.yml\`).
3.  **Add a "Run Tests" Stage:**
    *   Add a new stage to your pipeline, right after the step that checks out the code.
4.  **Trigger the Test Scenario:**
    *   In this stage, add a script step that makes a **REST API call** to trigger your \`run_all_tests\` scenario in Dataiku.
5.  **Poll for Results (Critical Step):**
    *   The API call to run a scenario is asynchronous; it just starts the job. Your CI script must then **poll** for the result.
    *   This involves:
        1.  Capturing the \`jobId\` from the response of the initial "run" API call.
        2.  Entering a loop that calls the "get job status" API endpoint every few seconds.
        3.  Exiting the loop when the job's status is no longer "RUNNING".
6.  **Fail the Build:**
    *   After the loop, check the final outcome of the job.
    *   If the outcome is not "SUCCESS", your CI script must exit with a non-zero status code. This will cause the CI pipeline to fail, blocking the merge and notifying the developer of the test failure.

### 4. Resources and Tools
- **Your CI/CD Tool (Jenkins, GitHub Actions, etc.).**
- **Dataiku's REST API:** Specifically the endpoints for running a scenario and getting a job's status.
- **\`curl\` and \`jq\`:** Command-line tools that are useful in your script for making the API calls and parsing the JSON responses.

### 5. Next Steps and Progression
- **Required Status Checks:** In GitHub, you can configure your "Run Tests" CI job as a required status check for pull requests. This will physically prevent a PR from being merged until all your Dataiku validation tests have passed.

### 6. Common Challenges and Solutions
- **Challenge:** The polling script is complex to write.
- **Solution:** It is. This is the most complex part of the integration. You need to handle timeouts and different failure modes. Look for example scripts online or in your CI/CD tool's documentation for "polling a REST API".
- **Challenge:** The tests take a long time and slow down the CI pipeline.
- **Solution:** Your Dataiku test scenario should be designed to run quickly. It should run on a small, representative sample of the data, not the full production dataset. The goal is to quickly catch logic errors, not to do a full production run.
`},{id:527,slug:"creating-automated-tests-for-ci",question:"How to get started with creating automated tests (unit or integration) for Dataiku projects as part of CI.",answer:`
### 1. Introduction/Overview
Automated testing is a pillar of CI/CD. For a Dataiku project, this involves creating a suite of tests that can be run automatically to validate the correctness of your data pipelines. These tests fall into two main categories: data quality checks and unit tests for code.

### 2. Prerequisites
- **A Dataiku project** integrated with a CI/CD pipeline.
- **A clear understanding of what constitutes "correct"** for your data and logic.

### 3. Step-by-Step Instructions

#### Part 1: Creating Data Quality Tests
1.  **Define Checks on Your Datasets:** This is the easiest way to create tests.
    *   In Dataiku, open a critical dataset and go to the **Status > Checks** tab.
    *   Define rules about the data. For example:
        *   The \`customer_id\` column should never be empty.
        *   The \`order_total\` should always be a positive number.
        *   The row count should be within a certain range.
2.  **Create a "Test" Scenario:**
    *   Create a scenario named \`run_data_quality_tests\`.
    *   In this scenario, add a **Run checks** step for each dataset you want to validate.
3.  **Trigger from CI:** Your CI pipeline will trigger this scenario. If any check fails, the scenario fails, which in turn fails your CI build.

#### Part 2: Creating Unit Tests for Python Code
1.  **Write Testable Code:** Write your complex logic as pure functions in your project's **Library**. These functions should take data as input and return a result, without relying on global state.
2.  **Use a Testing Framework:** Use a standard Python testing framework like \`pytest\`.
3.  **Write Test Functions:** In your library, create a separate file (e.g., \`test_my_utils.py\`). In this file, write test functions that call your logic with sample input and assert that the output is what you expect.
    > \`\`\`python
    > from my_utils import my_sum_function
    >
    > def test_sum_function():
    >     assert my_sum_function(2, 3) == 5
    > \`\`\`
4.  **Create a "Test" Recipe:** In your Flow, create a Python recipe that uses the \`pytest\` library to discover and run all the tests in your library. If any test fails, the recipe should fail.
5.  **Add to Test Scenario:** Add this test-running recipe to your main \`run_data_quality_tests\` scenario.

### 4. Resources and Tools
- **Dataiku Metrics and Checks:** For declarative, data-focused testing.
- **Python Recipes and Libraries:** For code-based unit testing.
- **\`pytest\`:** The standard Python testing framework.

### 5. Next Steps and Progression
- **Test Coverage:** Use tools to measure your test coverage and ensure that your most critical code is well-tested.
- **Integration Tests:** Your test scenario can also be considered an integration test, as it runs parts of your flow together to ensure they work.

### 6. Common Challenges and Solutions
- **Challenge:** "Writing tests is time-consuming."
- **Solution:** It is an upfront investment that pays for itself many times over by catching bugs early, when they are cheapest to fix. Start by adding tests for your most critical logic, and gradually increase coverage over time.
- **Challenge:** "I don't know what to test."
- **Solution:** Think about edge cases. What happens if your function receives a null value? A negative number? An empty string? Your tests should cover these cases. For data checks, think about what would constitute "bad" data for your use case.
`},{id:528,slug:"migrating-projects-between-environments-with-scripts-apis",question:"How to get started with migrating Dataiku projects between development and production environments using automated scripts or APIs.",answer:`
### 1. Introduction/Overview
Automating the migration of projects between environments (e.g., from Dev to Prod) is a core MLOps practice that ensures deployments are fast, reliable, and repeatable. This is achieved by writing a script that uses the **Dataiku REST API** to perform the export and import operations.

### 2. Prerequisites
- **Separate Dev and Prod Dataiku instances.**
- **A scripting environment** (e.g., a Jenkins server, a local machine with Python).
- **API keys** for both the dev and prod instances with administrative permissions.

### 3. Step-by-Step Instructions: A Deployment Script
The following outlines the logic for a Python deployment script.

1.  **Set Up the Script:**
    *   Import necessary libraries (\`requests\`, \`json\`).
    *   Define variables for your instance URLs, project keys, and API keys. Store API keys securely, not in the script.
2.  **Step 1: Create the Bundle:**
    *   Make a POST request to the **bundle export endpoint** on your **Dev** instance API.
    *   This API call tells Dataiku to create a \`.zip\` bundle of your project.
3.  **Step 2: Download the Bundle:**
    *   Make a GET request to the **bundle download endpoint**, passing the bundle ID from the previous step.
    *   Stream the response to a local file on your script's machine (e.g., \`my_project.zip\`).
4.  **Step 3: Upload the Bundle:**
    *   Make a POST request to the **bundle import endpoint** on your **Prod** instance API.
    *   This request needs to be a multipart/form-data request, with the \`my_project.zip\` file attached.
5.  **Step 4: Remap Connections and Deploy:**
    *   The import API call can include a JSON payload that specifies how to remap connections and variables for the production environment.
    *   Dataiku will then import the project, apply the remappings, and the new version of your project will be live in production.
6.  **Step 5: Cleanup:** Delete the local bundle file.

### 4. Resources and Tools
- **Dataiku REST API Documentation:** Essential for finding the exact endpoints and required parameters for bundle operations.
- **Python \`requests\` library:** A powerful tool for making the HTTP calls in your script.
- **A CI/CD tool (Jenkins, etc.):** The ideal place to run your deployment script.

### 5. Next Steps and Progression
- **Error Handling:** Your script must have robust error handling. It should check the status code of every API response and fail the deployment if any step does not succeed.
- **Pre- and Post-Deployment Steps:** A complete pipeline would include steps to run tests before creating the bundle and to run a smoke test scenario after the deployment to production.

### 6. Common Challenges and Solutions
- **Challenge:** The API calls are complex, especially the file upload.
- **Solution:** This is true. The file upload requires a correctly formatted multipart request. Look for examples in the documentation for your specific language or library (e.g., the Python \`requests\` library has clear documentation on how to post files).
- **Challenge:** "The import failed with a dependency error."
- **Solution:** This can happen if your project depends on a plugin or a code environment that exists on your Dev instance but has not been installed on the Prod instance. Your deployment process must also include a step to ensure the environments are synchronized.
`},{id:529,slug:"triggering-scenarios-from-ci-cd-on-new-code-data",question:"How to get started with triggering Dataiku scenarios from a CI/CD pipeline whenever new code or data is available.",answer:`
### 1. Introduction/Overview
A key principle of CI/CD is automation. You want your pipelines to run automatically in response to events, rather than manual triggers. This can be achieved by setting up triggers for two main types of events: new code (via Git webhooks) and new data (via API calls).

### 2. Prerequisites
- A CI/CD pipeline (e.g., in GitHub Actions, Jenkins).
- A Dataiku project connected to Git.
- A process that delivers new data files to a known location.

### 3. Step-by-Step Instructions

#### Triggering on New Code
1.  **Set Up a Webhook:** In your Git provider (GitHub, GitLab, etc.), go to your project repository's settings and find the "Webhooks" section.
2.  **Configure the Webhook:**
    *   Create a new webhook.
    *   The **Payload URL** should be the trigger URL for your CI/CD pipeline.
    *   Configure the webhook to fire on a **push** or **pull_request** event.
3.  **The CI/CD Workflow:**
    *   When a developer pushes a change, the webhook fires, which triggers your CI/CD pipeline.
    *   The pipeline script can then call the Dataiku API to run a test scenario on the new code.

#### Triggering on New Data
1.  **Set Up an External Triggering Process:**
    *   The system that generates the new data needs to be able to send a signal when it's done.
2.  **Method A (Direct API Call):**
    *   The simplest method. After the external process finishes writing the new data file, it should, as its final step, make a **REST API call** to Dataiku to trigger the scenario that processes that file.
3.  **Method B (Cloud Event Triggering):**
    *   A more robust, cloud-native pattern.
    *   Configure a cloud event service (like AWS Lambda with an S3 trigger, or an Azure Function with an Event Grid trigger).
    *   When a new file is dropped into your cloud storage bucket, the event fires, which invokes your Lambda function.
    *   The code in the Lambda function then makes the REST API call to trigger the Dataiku scenario.

### 4. Resources and Tools
- **Git Webhooks:** The standard mechanism for triggering CI on code changes.
- **Dataiku REST API:** The interface for triggering scenarios from external systems.
- **Cloud Functions (Lambda, etc.):** The glue for creating event-driven data triggers in the cloud.

### 5. Next Steps and Progression
- **Dataiku's Native Triggers:** For simpler cases, Dataiku has a built-in "Dataset changed" trigger in scenarios. This works well if the new data is being produced by another Dataiku job, but can be less reliable for data dropped by external systems. The API-based approach is generally more robust for external triggers.

### 6. Common Challenges and Solutions
- **Challenge:** The external system can't make an API call.
- **Solution:** If the system is very old, you might need an intermediate step. The system could create a "trigger file" (an empty file like \`_SUCCESS\`) after it's done. You could then have a simple script that runs on a schedule (e.g., every 5 minutes), checks for the existence of this trigger file, and if it exists, makes the API call to Dataiku and then deletes the trigger file.
`},{id:530,slug:"configuring-dataiku-with-aws-sagemaker-lambda-s3",question:"How to get started with configuring Dataiku to use AWS SageMaker, Lambda, and S3 for model training and inference.",answer:`
### 1. Introduction/Overview
Dataiku is an open platform that integrates with the broader cloud ecosystem. You can use Dataiku as a control plane to orchestrate jobs on powerful AWS services like SageMaker for model training, Lambda for serverless functions, and S3 for scalable storage. This integration is typically done using the AWS SDK (Boto3) from within a Python recipe.

### 2. Prerequisites
- An AWS account with SageMaker, Lambda, and S3 services enabled.
- An AWS IAM user or role with permissions to access these services.
- A Dataiku code environment with the \`boto3\` library installed.

### 3. Step-by-Step Instructions: Integration Patterns

#### 1. Using S3 for Data Storage
- **How:** This is the most common integration. In **Administration > Connections**, create a new **Amazon S3** connection. Provide your AWS credentials (or better, use an IAM role if Dataiku is on EC2).
- **Use Case:** You can now create datasets that read from and write to S3 buckets directly from the Dataiku UI. This should be your default storage for large datasets.

#### 2. Using AWS SageMaker for Model Training
- **How:**
    1.  In a Dataiku **Python recipe**, use \`boto3\` to interact with SageMaker.
    2.  The script would first take data from a Dataiku dataset and upload it to S3 in the format SageMaker expects.
    3.  Then, it would use \`boto3\` to create and run a SageMaker training job, pointing it to the data in S3 and specifying the training algorithm.
    4.  The script can then monitor the training job and retrieve the model artifacts when it's done.
- **Use Case:** To leverage a specific algorithm or a distributed training capability available in SageMaker that is not native to Dataiku.

#### 3. Using AWS Lambda for Inference or Tasks
- **How:**
    1.  In a Python recipe, use \`boto3\` to invoke a Lambda function.
    2.  You can pass a payload of data to the Lambda function.
    3.  The Lambda function can perform its logic (e.g., call a proprietary model for a prediction) and return a result to your Dataiku script.
- **Use Case:** To integrate a serverless, event-driven component or a pre-existing business function into your Dataiku flow.

### 4. Resources and Tools
- **Python Recipe:** The environment for your integration code.
- **Boto3:** The official AWS SDK for Python.
- **AWS IAM:** The service for managing the permissions that allow Dataiku to call other AWS services.

### 5. Next Steps and Progression
- **Dataiku Plugins:** For a tighter integration, you could develop a custom Dataiku plugin that provides a visual recipe for calling your SageMaker or Lambda functions, hiding the Boto3 code from the end user.

### 6. Common Challenges and Solutions
- **Challenge:** "My recipe fails with an 'Access Denied' error when calling an AWS service."
- **Solution:** This is an IAM permissions issue. The IAM user or role that Dataiku is using does not have the necessary permissions in its policy. For example, to run a SageMaker job, it needs permissions like \`sagemaker:CreateTrainingJob\`.
- **Challenge:** "How do I manage my AWS credentials in Dataiku?"
- **Solution:** The best and most secure practice is to run your Dataiku instance on an EC2 virtual machine and assign an **IAM Role** to that instance. In your Dataiku S3 connection, you can then specify to use the IAM role, which avoids the need to store static access keys.
`},{id:531,slug:"integrating-with-azure-ml-services-storage",question:"How to get started with integrating Dataiku DSS with Azure ML services and Azure Storage for end-to-end pipelines.",answer:`
### 1. Introduction/Overview
Dataiku seamlessly integrates with the Microsoft Azure cloud ecosystem. You can use Azure Blob Storage for scalable data storage and call out to Azure Machine Learning (AML) services for specialized training or deployment. This integration is typically done from a Python recipe using the Azure SDK.

### 2. Prerequisites
- An Azure subscription with Azure Blob Storage and Azure Machine Learning resources.
- An Azure Service Principal (an application identity) with permissions to access these resources.
- A Dataiku code environment with the Azure SDKs installed (\`azure-storage-blob\`, \`azure-ai-ml\`).

### 3. Step-by-Step Instructions: Integration Patterns

#### 1. Using Azure Blob Storage for Data
- **How:** In **Administration > Connections**, create a new **Azure Blob Storage** connection. Provide your storage account name and credentials (e.g., the Service Principal ID and secret).
- **Use Case:** You can now create datasets that read from and write to containers in your Azure Blob Storage account. This is the recommended storage location for large datasets when running on Azure.

#### 2. Using Azure ML for Model Training
- **How:**
    1.  In a Dataiku **Python recipe**, use the Azure ML SDK.
    2.  Your script can take data from a Dataiku dataset, upload it to Azure Blob Storage, and then submit a "command job" to your Azure ML workspace.
    3.  This job can run a training script using compute resources managed by Azure ML.
- **Use Case:** To leverage specific features of the Azure ML platform, like its managed compute clusters or its experiment tracking capabilities, as part of a larger Dataiku workflow.

#### 3. Using Azure ML for Real-time Inference
- **How:**
    1.  You can train a model in Dataiku.
    2.  Use a Python recipe with the Azure ML SDK to deploy this model to an **Azure ML Managed Endpoint**.
    3.  Alternatively, from a Dataiku recipe, you can make an API call to an *existing* model endpoint that is already deployed on Azure ML.
- **Use Case:** To host your final, production model on Azure's scalable, managed inference infrastructure.

### 4. Resources and Tools
- **Python Recipe:** The environment for your Azure integration code.
- **Azure SDK for Python:** The official Microsoft libraries for interacting with Azure services.
- **Azure Service Principal:** The secure way to grant Dataiku programmatic access to your Azure resources.

### 5. Next Steps and Progression
- **Dataiku Plugins:** You can create a custom Dataiku plugin to provide a visual recipe for your users that calls an Azure ML endpoint, hiding the complexity of the SDK code.

### 6. Common Challenges and Solutions
- **Challenge:** "My script fails with an authentication error."
- **Solution:** This is a permissions issue with your Service Principal. In the Azure portal, go to the resource you are trying to access (e.g., the Storage Account or the ML Workspace) and check its Access Control (IAM) settings. Ensure your Service Principal has been assigned the appropriate role (e.g., "Storage Blob Data Contributor" or "AzureML Data Scientist").
- **Challenge:** "How do I securely store my Service Principal secret?"
- **Solution:** The best practice is to store the secret in **Azure Key Vault**. Then, you can configure Dataiku to connect to the Key Vault to retrieve the secret at runtime. Alternatively, you can store it as a "Password" type project variable in Dataiku.
`},{id:532,slug:"connecting-to-gcp-bigquery-vertex-ai",question:"How to get started with connecting Dataiku to Google Cloud BigQuery and Vertex AI for training and prediction.",answer:`
### 1. Introduction/Overview
Dataiku has strong native integration with the Google Cloud Platform (GCP). You can use BigQuery as a high-performance data source with full push-down computation, and you can integrate with Vertex AI to leverage Google's powerful, managed machine learning services as part of your Dataiku pipelines.

### 2. Prerequisites
- A GCP project with BigQuery and Vertex AI APIs enabled.
- A GCP Service Account with the necessary permissions (e.g., BigQuery User, Vertex AI User).
- A JSON key file for the service account.
- Dataiku administrator rights to configure connections.

### 3. Step-by-Step Instructions: Integration Patterns

#### 1. Using BigQuery for Data and Compute
- **How:**
    1.  In Dataiku, go to **Administration > Connections** and create a new **Google BigQuery** connection.
    2.  Authenticate by pasting the contents of your service account's JSON key file.
    3.  You can now create datasets in your Flow that point directly to BigQuery tables.
- **Use Case (Push-down):** When you use a visual recipe (like Prepare or Join) on BigQuery datasets, set the execution engine to **Run on database (SQL)**. Dataiku will generate BigQuery-optimized SQL and run it directly on the BigQuery engine, which is extremely performant for large datasets.

#### 2. Using Vertex AI for Model Training & Prediction
- **How:**
    1.  In a Dataiku **Python recipe**, use the Google Cloud AI Platform SDK.
    2.  To train a model, your script can prepare data, upload it to Google Cloud Storage (GCS), and then use the SDK to submit a training job to Vertex AI.
    3.  To get predictions, your script can call a deployed model endpoint on Vertex AI.
- **Use Case:** To leverage Google's AutoML capabilities or to run large-scale, distributed training jobs on infrastructure managed by Vertex AI.

### 4. Resources and Tools
- **BigQuery Connector:** Dataiku's native connector for reading, writing, and pushing down computation to BigQuery.
- **Python Recipe:** The environment for writing your Vertex AI integration code.
- **Google Cloud SDK for Python:** The library for interacting with Vertex AI and other GCP services.
- **GCP Service Account:** The secure, recommended way for Dataiku to authenticate with Google Cloud.

### 5. Next Steps and Progression
- **Explore other GCP Services:** From a Python recipe, you can use the Google Cloud SDK to connect to many other services, like Google Cloud Storage, Pub/Sub, or the various NLP and Vision APIs.
- **Dataiku Plugins:** For a tighter integration, you could build a Dataiku plugin that provides a visual recipe for calling a specific Vertex AI endpoint.

### 6. Common Challenges and Solutions
- **Challenge:** "My connection to BigQuery is failing."
- **Solution:** This is almost always a permissions issue. In the GCP IAM console, ensure your Service Account has been granted the necessary roles for your project (e.g., \`BigQuery User\`, \`BigQuery Data Viewer\`). Also, ensure the BigQuery API is enabled in your GCP project.
- **Challenge:** "My BigQuery queries from Dataiku are expensive."
- **Solution:** BigQuery's pricing is based on bytes scanned. To control costs, make sure your queries are as efficient as possible. Use partitioned tables and filter on the partition column in a \`WHERE\` clause whenever possible. You can use the BigQuery UI to estimate the cost of a query before running it.
`},{id:533,slug:"deploying-workloads-on-emr-hdinsight-dataproc",question:"How to get started with deploying Dataiku workloads on AWS EMR, Azure HDInsight, or GCP Dataproc for scalable Spark processing.",answer:`
### 1. Introduction/Overview
Managed Spark services like AWS EMR, Azure HDInsight, and Google Cloud Dataproc provide scalable, on-demand clusters for big data processing. Dataiku can be configured to submit Spark jobs directly to these services, allowing you to run your visual recipes and code recipes on massive datasets without managing the cluster yourself.

### 2. Prerequisites
- **A managed Spark cluster:** You must have an EMR, HDInsight, or Dataproc cluster already created in your cloud account.
- **Network connectivity:** The Dataiku instance must be able to communicate with the cluster's master node. This often requires running Dataiku in the same VPC or setting up network peering.
- **Administrator rights** in Dataiku to configure the Spark connection.

### 3. Step-by-Step Instructions
1.  **Configure the Connection in Dataiku (Admin Task):**
    *   Go to **Administration > Settings**.
    *   Navigate to the **Spark** section.
    *   Here you can configure Dataiku to connect to your specific managed service. The configuration details vary by provider, but you will typically need to provide the master node's address and specify the framework type (e.g., YARN).
    *   Dataiku's documentation provides specific guides for connecting to EMR, HDInsight, and Dataproc.
2.  **Ensure Data is on Compatible Storage:**
    *   Your input and output datasets must be on a storage system that the Spark cluster can access. This is typically the cloud provider's object storage (S3 for EMR, ADLS for HDInsight, GCS for Dataproc).
3.  **Use the Spark Engine:**
    *   Once configured, you can now run jobs on the cluster.
    *   **For a visual recipe:** Go to its **Advanced** settings and change the **Execution engine** to **Spark**.
    *   **For a code recipe:** Create a **PySpark** or **SparkR** recipe.
4.  **How it Works:** When you run the recipe, Dataiku will submit the job to your managed cluster's resource manager (YARN). The cluster will then execute the Spark application, and the logs will be streamed back to the Dataiku UI.

### 4. Resources and Tools
- **Dataiku's official documentation:** Has detailed, provider-specific guides for these integrations.
- **Cloud provider consoles:** For creating and managing the EMR/HDInsight/Dataproc clusters.
- **The Spark UI:** Accessible through the cluster's management console, this is essential for monitoring and debugging your jobs.

### 5. Next Steps and Progression
- **Autoscaling Clusters:** Configure your managed cluster to autoscale. It can automatically add worker nodes when a large job is submitted and remove them when idle, which is highly cost-effective.
- **Dynamic Clusters:** For advanced use cases, Dataiku can be configured to programmatically create a new cluster just for a specific job run and then destroy it when the job is complete.

### 6. Common Challenges and Solutions
- **Challenge:** "Dataiku can't submit the job to the cluster."
- **Solution:** This is almost always a network connectivity or permissions issue. Ensure the Dataiku server can reach the cluster's master node and that any firewalls are configured to allow the necessary traffic. The user Dataiku is running as may also need specific permissions to submit jobs to the YARN queue.
- **Challenge:** "My job is failing on the cluster."
- **Solution:** Use the Spark UI for your managed cluster to debug. Find the failed application and look at the logs for the executors. This will give you the detailed error traceback.
`},{id:534,slug:"managing-credentials-via-aws-iam-roles-azure-service-principals",question:"How to get started with managing Dataiku credentials via AWS IAM roles or Azure service principals for secure access.",answer:`
### 1. Introduction/Overview
Storing static credentials like access keys or secrets is a security risk. The modern, best-practice approach in the cloud is to use **identity-based authentication**. This involves giving the Dataiku server itself an identity (an AWS IAM Role or an Azure Service Principal) and granting that identity permissions to access other cloud resources. This method is more secure because it avoids the need to store and manage long-lived passwords or keys.

### 2. Prerequisites
- **Dataiku running on a cloud VM:** Your Dataiku instance should be running on an AWS EC2 instance or an Azure VM.
- **Administrator access** to your AWS or Azure account to manage identities.

### 3. Step-by-Step Instructions

#### For AWS (using IAM Roles)
1.  **Create an IAM Role:** In the AWS IAM console, create a new role. Attach policies to this role that grant it the necessary permissions (e.g., permission to read from a specific S3 bucket).
2.  **Attach the Role to the EC2 Instance:** When you launch the EC2 instance for Dataiku (or by modifying an existing one), attach the IAM role you just created.
3.  **Configure the Dataiku Connection:**
    *   In Dataiku, go to **Administration > Connections** and create a new connection (e.g., to S3).
    *   In the authentication settings, instead of entering an access key, select **Use IAM role**.
    *   Leave the credential fields blank. Dataiku will automatically use the credentials of the role attached to the EC2 instance it's running on.

#### For Azure (using Service Principals and Managed Identities)
1.  **Enable Managed Identity:** In the Azure portal, go to the settings for the VM where Dataiku is running. Enable the "System-assigned managed identity". This gives the VM its own identity in Azure Active Directory.
2.  **Grant Permissions:** Go to the resource you want to access (e.g., a Storage Account). In its **Access Control (IAM)** settings, add a new role assignment. Assign the necessary role (e.g., "Storage Blob Data Contributor") to the managed identity of your Dataiku VM.
3.  **Configure the Dataiku Connection:** In the Dataiku connection settings (e.g., for Azure Blob Storage), choose to authenticate using the **Managed Identity**.

### 4. Resources and Tools
- **AWS IAM Console / Azure Active Directory Portal:** The cloud provider tools for creating and managing identities and permissions.
- **Dataiku Connections Page:** Where you configure Dataiku to use the role-based authentication.

### 5. Next Steps and Progression
- **Principle of Least Privilege:** When defining the policies for your IAM role or Service Principal, grant only the minimum permissions necessary. For example, grant read-only access to a specific S3 bucket, not read/write access to all buckets.
- **Cross-Account Access (AWS):** You can use IAM roles to grant a Dataiku instance in one AWS account secure access to resources in a different AWS account.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm getting an 'Access Denied' error even when using an IAM role."
- **Solution:** The policy attached to the IAM role is not correct. Use the AWS or Azure policy simulator tools to debug the policy and ensure it grants the specific action (e.g., \`s3:GetObject\`) on the specific resource (e.g., \`arn:aws:s3:::my-bucket/*\`).
- **Challenge:** "This option isn't available for all connection types."
- **Solution:** This is true. Role-based authentication is typically supported for the cloud provider's own native services (e.g., S3, Redshift, Blob Storage). For other connections, like a generic database, you may still need to use traditional username/password credentials.
`},{id:535,slug:"linking-to-cloud-data-warehouses-snowflake-redshift-bigquery",question:"How to get started with linking Dataiku to cloud data warehouses (Snowflake, Redshift, BigQuery) for large-scale data access.",answer:`
### 1. Introduction/Overview
Cloud data warehouses are the centerpiece of the modern data stack. Dataiku provides powerful, native connectors for all major warehouses like Snowflake, Redshift, and BigQuery. Connecting Dataiku allows you to seamlessly read, write, and—most importantly—push down transformations to leverage the full power of your data warehouse.

### 2. Prerequisites
- **An account with a cloud data warehouse provider.**
- **Connection details:** You will need the server URL, username, password, and default warehouse/database names.
- **Dataiku Administrator rights** to create the shared connection.
- **Network access:** The Dataiku server must be able to reach the data warehouse's public endpoint.

### 3. Step-by-Step Instructions
1.  **Navigate to Connections:** As an administrator, go to **Administration > Connections** in Dataiku.
2.  **Create a New Connection:** Click **+ NEW CONNECTION**.
3.  **Select Your Warehouse Type:** From the list of database types, select your specific provider (e.g., **Snowflake**, **Amazon Redshift**, **Google BigQuery**).
4.  **Enter Connection Details:**
    *   Fill in the form with the credentials and connection details for your warehouse. Each warehouse has slightly different parameters (e.g., for Snowflake you need your account name, for BigQuery you need a project ID).
    *   It is highly recommended to use a dedicated service account user for Dataiku, with a strong password.
5.  **Test the Connection:**
    *   Click the **Test** button at the bottom of the form.
    *   A "Test successful" message confirms that your details are correct and that Dataiku can communicate with your data warehouse.
6.  **Create and Share:** Click **CREATE**. The connection is now available for users on the instance (if you grant them permission in the connection's settings).
7.  **Using the Connection:**
    *   In a project, a user can now click **+ DATASET** and select your warehouse (e.g., **Snowflake**). They will be able to browse the tables they have access to and import them as Dataiku datasets.

### 4. Resources and Tools
- **Dataiku Connections Page:** The central place for managing all external data connections.
- **Cloud Data Warehouse UI:** Where you find your connection details and manage user permissions.

### 5. Next Steps and Progression
- **Push-down Execution:** This is the key benefit. When working with visual recipes on Snowflake datasets, check the recipe's "Advanced" settings and set the execution engine to "Run on database (SQL)". Dataiku will generate Snowflake-optimized SQL instead of pulling data out.
- **Time Travel:** Use a SQL recipe to leverage Snowflake's \`AT\` or \`BEFORE\` clauses to query historical versions of your data.
- **Writing to Snowflake:** Use an **Export** recipe to create new tables in Snowflake from your Dataiku flow.

### 6. Common Challenges and Solutions
- **Challenge:** The connection test fails with a timeout or network error.
- **Solution:** This is a networking issue. The Dataiku server cannot reach the data warehouse. If your warehouse has a firewall or network policy, you must whitelist the IP address of your Dataiku server to allow inbound connections.
- **Challenge:** "The connection succeeds, but I can't see any tables."
- **Solution:** This is a permissions issue within your data warehouse. The user account that Dataiku is using does not have \`USAGE\` permission on the database or schema, or \`SELECT\` permission on the tables. You need to grant these privileges in the data warehouse itself.
`},{id:536,slug:"optimizing-spark-compute-on-cloud-clusters-autoscaling",question:"How to get started with optimizing Dataiku Spark compute on cloud clusters and autoscaling for cost-efficiency.",answer:`
### 1. Introduction/Overview
Running Spark jobs on cloud-based managed clusters (like EMR, Dataproc, or HDInsight) offers incredible power, but also the risk of high costs if not managed correctly. Optimizing your Spark jobs involves right-sizing your clusters and using autoscaling to ensure you only pay for the compute you actually use.

### 2. Prerequisites
- **A Dataiku instance integrated with a managed Spark service** on AWS, Azure, or GCP.
- **Large-scale Spark jobs** that you need to run.
- **Administrator access** to your cloud provider account to configure the clusters.

### 3. Step-by-Step Instructions

#### 1. Right-Sizing Your Cluster
- **Don't overprovision.** It's tempting to choose the largest possible instance types for your worker nodes, but this is often wasteful.
- **Analyze your workload.** Use the Spark UI to monitor your jobs. Look at the memory and CPU usage of your executors. Are they consistently underutilized? If so, you can choose smaller instance types for your worker nodes to save money.
- **Use different clusters for different workloads.** You could have a smaller, persistent cluster for general development and a larger, more powerful cluster that is only spun up for a specific, heavy production job.

#### 2. Implementing Autoscaling
- **What it is:** Autoscaling automatically adds or removes worker nodes from your cluster in response to load.
- **How to configure:** All major managed Spark services support autoscaling. In your cluster's configuration (e.g., in the EMR or Dataproc console), you can enable autoscaling and set rules:
    *   **Minimum size:** The number of nodes to always keep running.
    *   **Maximum size:** The maximum number of nodes the cluster can scale up to.
    *   **Scaling metric:** The metric used to trigger scaling events. This is often based on YARN memory or vCore availability.
- **Benefit:** When you submit a large job, the cluster will automatically add nodes to handle the load. When the job is finished and the cluster is idle, it will automatically scale back down to the minimum size, significantly reducing costs.

#### 3. Using Spot/Preemptible Instances
- **What they are:** Cloud providers sell their unused capacity at a steep discount as "spot" (AWS) or "preemptible" (GCP) instances. These instances can be terminated with little notice.
- **How to use:** You can configure your Spark cluster's worker nodes to use these cheaper instances.
- **Best for:** Fault-tolerant workloads that are not time-critical. If a few spot instances are terminated, Spark is often able to recompute the lost work on other nodes. This is a great way to reduce the cost of large, non-urgent jobs.

### 4. Resources and Tools
- **Cloud Provider Consoles (EMR, Dataproc, etc.):** Where you configure cluster size and autoscaling rules.
- **The Spark UI:** Essential for monitoring your job's resource utilization to inform your sizing decisions.
- **Cloud Cost Management Tools:** To track the cost of your clusters and measure the savings from your optimizations.

### 5. Next Steps and Progression
- **Dynamic Cluster Allocation:** For ultimate cost-efficiency, Dataiku can be configured to programmatically create a new cluster for a specific job run and then terminate the cluster as soon as the job is complete. This ensures you pay for compute only for the exact duration of your job.

### 6. Common Challenges and Solutions
- **Challenge:** "My autoscaling cluster is not scaling up when I submit a big job."
- **Solution:** Your autoscaling rules may be misconfigured. Check the scaling metric. The job might not be consuming enough YARN memory to trigger the scale-up event. You may need to adjust the thresholds in your scaling policy.
- **Challenge:** "My job on spot instances failed because too many nodes were terminated."
- **Solution:** This is the inherent risk of spot instances. They are not suitable for all workloads. Use them for jobs that can tolerate interruption. You can also configure your cluster to use a mix of on-demand and spot instances to balance cost and reliability.
`},{id:537,slug:"setting-up-hybrid-architecture-on-prem-and-cloud",question:"How to get started with setting up Dataiku in a hybrid architecture (on-prem DSS accessing cloud resources).",answer:`
### 1. Introduction/Overview
A hybrid architecture, where your main Dataiku instance is installed on-premise but connects to data and compute resources in the cloud, is a common pattern for organizations that are beginning their cloud journey. This setup allows you to leverage the scalability of the cloud while keeping the core platform within your own data center.

### 2. Prerequisites
- **An on-premise Dataiku instance.**
- **A cloud provider account** (AWS, Azure, or GCP).
- **Secure network connectivity** between your on-premise data center and your cloud VPC.

### 3. Step-by-Step Instructions: The Key Components

#### 1. Establish Secure Network Connectivity
- **This is the foundational step.** Your on-premise Dataiku server needs a secure, private, and fast connection to your cloud environment. The standard options are:
    - **AWS Direct Connect / Azure ExpressRoute / Google Cloud Interconnect:** A dedicated, private physical connection. This offers the highest performance and security but is also the most expensive.
    - **Site-to-Site VPN:** An encrypted tunnel over the public internet. This is a more common and cost-effective solution.
- **Work with your network team.** Setting up this connectivity is a specialized task that requires collaboration with your network engineering and security teams.

#### 2. Configure Firewall Rules
- You will need to configure firewalls on both your on-premise network and in your cloud provider's VPC to allow traffic between the Dataiku server and the specific cloud services you want to use. You should only open the necessary ports.

#### 3. Set Up Data Connections in Dataiku
- Once network connectivity is established, an administrator can go to **Administration > Connections** in your on-premise Dataiku instance.
- They can now create connections to your cloud data services (e.g., **Amazon S3**, **Snowflake**, **Azure Blob Storage**), using the private IP addresses or internal DNS names of those services.

#### 4. Use the Hybrid Resources
- **For Storage:** You can now create datasets that read data directly from cloud storage buckets like S3.
- **For Compute:** You can configure visual recipes to push down execution to a cloud data warehouse like Snowflake or Redshift. The query will be sent from your on-premise Dataiku instance, over the private connection, to the cloud database for execution.

### 4. Resources and Tools
- **VPN / Direct Connect:** The core networking technologies.
- **Cloud Provider Networking Tools (VPCs, Security Groups, etc.).**
- **Dataiku Connections:** The feature for connecting to your cloud data sources.

### 5. Next Steps and Progression
- **Hybrid Compute:** While less common, you could even configure your on-premise Dataiku to submit Spark jobs to a managed cloud cluster like AWS EMR. This requires careful network configuration to ensure all the necessary Spark communication ports are open over the VPN.
- **Data Movement:** Be mindful of data transfer costs. Moving large amounts of data from the cloud back to your on-premise instance can be expensive. A good hybrid architecture tries to keep the data and the computation in the same place (preferably the cloud) as much as possible.

### 6. Common Challenges and Solutions
- **Challenge:** "Dataiku can't connect to my S3 bucket."
- **Solution:** This is almost always a networking or firewall issue. Use standard network troubleshooting tools (\`ping\`, \`traceroute\`) from your on-premise Dataiku server to verify that it can reach the cloud service's endpoint. Work with your network team to diagnose firewall blocks.
- **Challenge:** "My jobs are slow."
- **Solution:** This could be due to latency on your network connection. A VPN over the internet will always have higher latency than a dedicated physical connection. Also, if you are pulling large amounts of data from the cloud to be processed in-memory on your on-premise Dataiku server, this will be very slow. You should refactor your flow to push the computation to the cloud.
`},{id:538,slug:"integrating-cloud-event-triggers",question:"How to get started with integrating cloud event triggers (S3 file arrival, Azure Event Grid) to kick off Dataiku scenarios.",answer:`
### 1. Introduction/Overview
For a truly event-driven architecture, you want your data pipelines to run automatically as soon as new data becomes available. By integrating Dataiku with cloud event services, you can create a system where, for example, a new file arriving in an S3 bucket instantly triggers the Dataiku scenario designed to process it.

### 2. Prerequisites
- **A cloud storage location** where new data files are dropped (e.g., AWS S3, Azure Blob Storage).
- **A serverless function service** in your cloud provider (e.g., AWS Lambda, Azure Functions).
- **A Dataiku scenario** that you want to trigger, and an **API key** with permission to run it.

### 3. Step-by-Step Instructions: AWS S3 + Lambda Example

1.  **Create the Target Dataiku Scenario:**
    *   In Dataiku, create the scenario that processes the data. For example, it could have a step to build a dataset that points to the S3 bucket.
2.  **Create the Lambda Function:**
    *   In the AWS Lambda console, create a new Python function.
    *   This function's job is simple: make a REST API call to Dataiku to trigger your scenario. Use the \`requests\` library for this.
    *   Store your Dataiku URL and API key securely using AWS Secrets Manager or as environment variables.
    > \`\`\`python
    > import requests
    > import os
    >
    > def lambda_handler(event, context):
    >     api_key = os.environ['DATAİKU_API_KEY']
    >     # Make the POST request to trigger the scenario
    >     requests.post(
    >         'https://dss.mycompany.com/public/api/projects/MYPROJ/scenarios/process_new_file/run',
    >         auth=(api_key, '')
    >     )
    > \`\`\`
3.  **Set Up the S3 Trigger:**
    *   In the Lambda function's configuration, click **+ Add trigger**.
    *   Select **S3** as the source.
    *   Choose your S3 bucket.
    *   Set the **Event type** to **All object create events**.
4.  **How It Works:**
    *   An external process drops a new file into your S3 bucket.
    *   The S3 event notification is sent automatically.
    *   This triggers your Lambda function to execute.
    *   The Lambda function makes a REST API call to Dataiku.
    *   Dataiku receives the call and starts your \`process_new_file\` scenario.

### 4. Resources and Tools
- **Cloud Event Services:** S3 Events, Azure Event Grid, Google Cloud Pub/Sub.
- **Serverless Functions:** AWS Lambda, Azure Functions, Google Cloud Functions.
- **Dataiku REST API:** The interface that allows your function to trigger the scenario.

### 5. Next Steps and Progression
- **Passing the Filename:** You can make your trigger smarter. The event payload sent to the Lambda function contains information about the new file, including its name. You can pass this filename as a parameter in your API call to Dataiku. The Dataiku scenario can then use this parameter to process only that specific new file, rather than re-scanning the whole bucket.
- **Error Handling:** Add error handling and logging to your Lambda function so you are notified if the API call to Dataiku fails.

### 6. Common Challenges and Solutions
- **Challenge:** "The Lambda function can't connect to my Dataiku instance."
- **Solution:** This is a network issue. If your Dataiku instance is in a private VPC, your Lambda function must also be configured to run within that same VPC so it can reach the Dataiku server's private IP address.
- **Challenge:** "The trigger is firing, but my Dataiku job fails because the file isn't there."
- **Solution:** There can sometimes be a very small delay between the event firing and the file being fully consistent and readable. You can add a small \`time.sleep()\` in your Lambda function before calling the Dataiku API to mitigate this race condition.
`},{id:539,slug:"using-kubernetes-cloud-deployment-templates",question:"How to get started with using Dataiku’s Kubernetes or cloud deployment templates to run DSS on Azure Kubernetes Service, EKS, or GKE.",answer:`
### 1. Introduction/Overview
For a scalable, resilient, and production-grade deployment, running Dataiku on a managed Kubernetes cluster is the recommended approach. To simplify this complex process, Dataiku provides official deployment templates, most notably a **Helm chart**, which automates the provisioning of all the necessary Kubernetes resources.

### 2. Prerequisites
- **A managed Kubernetes cluster:** You need an EKS (AWS), AKS (Azure), or GKE (GCP) cluster running.
- **\`kubectl\` and \`helm\` installed:** You need these command-line tools configured to connect to your cluster.
- **A Dataiku license file.**

### 3. Step-by-Step Instructions
1.  **Add the Dataiku Helm Repository:**
    *   Helm uses repositories to find charts. You first need to add the official Dataiku repository to your Helm client.
    > \`\`\`bash
    > helm repo add dataiku https://charts.dataiku.com
    > helm repo update
    > \`\`\`
2.  **Create a \`values.yaml\` File:**
    *   The Helm chart is configured using a YAML file, conventionally named \`values.yaml\`.
    *   You need to create this file and specify the configuration for your deployment. At a minimum, you must accept the EULA and provide your license key.
    *   You can also configure storage types, resource limits, and many other parameters here.
3.  **Install the Chart:**
    *   Run the \`helm install\` command, giving your release a name and pointing to your values file.
    > \`\`\`bash
    > helm install my-dataiku-instance dataiku/dataiku -f values.yaml
    > \`\`\`
4.  **What Helm Does:** Helm will now connect to your Kubernetes cluster and create all the necessary objects defined in the chart:
    *   **Deployments:** For the Dataiku backend, frontend, and other components.
    *   **Services:** To expose the Dataiku UI and other services.
    *   **PersistentVolumeClaims:** To request persistent storage for your data.
5.  **Access Dataiku:** After a few minutes, the pods will be running. You can find the external IP address of the frontend service (\`kubectl get services\`) and access the Dataiku UI in your browser.

### 4. Resources and Tools
- **Helm:** The package manager for Kubernetes.
- **The Official Dataiku Helm Chart:** The supported, pre-packaged deployment template.
- **\`kubectl\`:** The command-line tool for interacting with your cluster.
- **Your cloud provider's Kubernetes documentation.**

### 5. Next Steps and Progression
- **Customize the Configuration:** Explore the default \`values.yaml\` file for the Helm chart to see all the available configuration options. You can customize storage classes, node selectors, ingress controllers, and much more.
- **Infrastructure as Code:** Use a tool like Terraform with its Helm provider to automate the entire process, from provisioning the K8s cluster itself to deploying the Dataiku Helm chart onto it.
- **Upgrading:** To upgrade Dataiku, you can simply update the version in your Helm configuration and run \`helm upgrade\`. Helm will handle the rolling update process.

### 6. Common Challenges and Solutions
- **Challenge:** "The pods are stuck in a 'Pending' state."
- **Solution:** This means your Kubernetes cluster does not have enough resources (CPU or memory) to schedule the pods. You may need to add more nodes to your cluster.
- **Challenge:** "The pod is in a 'CrashLoopBackOff' state."
- **Solution:** The container is starting and then immediately failing. Use \`kubectl logs <pod-name>\` to view the startup logs from the container. This will usually reveal the root cause, such as an inability to connect to its backend database or an invalid license key.
`},{id:540,slug:"creating-scheduling-scenarios-for-end-to-end-workflows",question:"How to get started with creating and scheduling Dataiku Scenarios to automate end-to-end ML workflows.",answer:`
### 1. Introduction/Overview
A Scenario is Dataiku's built-in tool for automation and orchestration. It allows you to define a sequence of actions—like rebuilding a dataset, retraining a model, and sending a report—and then schedule it to run automatically. Mastering scenarios is the key to moving your ML project from development to production.

### 2. Prerequisites
- **A complete ML workflow in your Flow:** You should have a full pipeline, from data preparation to a final "Saved Model" object.
- **A clear automation goal:** Know what you want the scenario to do (e.g., "Retrain and evaluate the churn model every week").

### 3. Step-by-Step Instructions
1.  **Navigate to the Scenarios Page:** In your project's top navigation bar, click on **Scenarios**.
2.  **Create a New Scenario:** Click **+ NEW SCENARIO** and give it a descriptive name (e.g., \`Weekly_Model_Retrain_And_Evaluate\`).
3.  **Define the Steps (The "What"):**
    *   Go to the **Steps** tab. This is where you define the sequence of actions.
    *   **Step 1: Rebuild Data.** Add a **Build / Train** step. Select the final training dataset as the item to build.
    *   **Step 2: Retrain Model.** Add another **Build / Train** step. This time, select your **Saved Model** object. This tells the scenario to retrain the model on the data built in the previous step.
    *   **Step 3: Evaluate Model.** Add a step to run an **Evaluate recipe** to calculate the new model's performance.
    *   **Step 4: Send Report.** Add a **Reporter** (in the Reporters tab) to email the results of the evaluation.
4.  **Define the Trigger (The "When"):**
    *   Go to the **Settings** tab.
    *   Click **+ ADD TRIGGER** and select **Time-based**.
    *   Configure the schedule (e.g., "Weekly" on "Sunday" at "01:00").
    *   Enable the trigger using the toggle switch.
5.  **Activate and Save:** Ensure the main toggle at the top of the scenario is set to **Active**, and then **Save** your changes. The scenario is now live and will run on schedule.

### 4. Resources and Tools
- **The Scenarios Page:** Your central hub for all project automation.
- **The Step Library:** The list of available actions your scenario can perform.
- **Triggers and Reporters:** The tools for scheduling and alerting.

### 5. Next Steps and Progression
- **Automated Deployment:** Add a Python step that uses the API to compare the new model's performance to the old one and automatically deploy it if it's better.
- **Data Quality Gates:** Add a "Run checks" step to validate your training data before the model retraining step. If the data quality is poor, the scenario will fail, preventing you from training a bad model.

### 6. Common Challenges and Solutions
- **Challenge:** "My scenario failed."
- **Solution:** Go to the "Last runs" tab and click on the failed run. This will take you to the job log, which provides a detailed error message and shows exactly which step failed and why.
- **Challenge:** "My scenario has too many steps and is confusing."
- **Solution:** Consider breaking your workflow into multiple, simpler scenarios. For example, have one scenario for data preparation and another for model training. You can then have the first scenario trigger the second one upon successful completion.
`},{id:541,slug:"using-event-based-triggers-for-orchestration",question:"How to get started with using event-based triggers in Dataiku Scenarios (e.g., file drop or schedule) for orchestration.",answer:`
### 1. Introduction/Overview
While time-based triggers are common, event-based triggers allow you to create more reactive and efficient pipelines. Instead of running on a fixed schedule, the pipeline runs as soon as a specific event occurs, like new data becoming available. Dataiku Scenarios have built-in triggers for this.

### 2. Prerequisites
- **A Dataiku Scenario** that you want to trigger.
- **An event source:** A dataset in your flow that gets updated by an upstream process.

### 3. Step-by-Step Instructions

#### Triggering on a Dataset Change
1.  **Navigate to Scenario Settings:** Open your scenario and go to the **Settings** tab.
2.  **Add a Trigger:** Click **+ ADD TRIGGER**.
3.  **Select "Dataset change":** Choose this as the trigger type.
4.  **Configure the Trigger:**
    *   **Dataset:** Select the dataset from your project that should act as the trigger.
    *   **Trigger when:** You can choose to trigger when the dataset's data changes, its schema changes, or both. For most ingestion workflows, you will choose "Data changes".
5.  **Enable and Save:** Enable the trigger using the toggle switch and save the scenario.
6.  **How it Works:** Dataiku will now monitor the status of the trigger dataset. As soon as it is modified (e.g., rebuilt by an upstream scenario), this trigger will fire, and your scenario will launch.

#### Triggering on Scenario Completion
1.  **Navigate to the *Upstream* Scenario:** Open the scenario that *produces* the data (e.g., your daily ingestion scenario).
2.  **Go to the Reporters Tab:** A trigger for one scenario is a "Reporter" for another.
3.  **Add a "Run scenario" Reporter:**
    *   Click **+ ADD REPORTER**.
    *   For the channel, select **Run another scenario**.
    *   **Run condition:** Set this to **On success**.
    *   **Scenario to run:** Select your downstream scenario (e.g., your model training scenario).
4.  **How it Works:** Now, when your ingestion scenario finishes successfully, it will automatically trigger your model training scenario, creating a perfectly chained, event-driven workflow.

### 4. Resources and Tools
- **Triggers Panel:** In a scenario's settings, this is where you configure "Dataset change" triggers.
- **Reporters Panel:** In a scenario's settings, this is where you configure a scenario to trigger another one.

### 5. Next Steps and Progression
- **API Triggers:** For events happening outside Dataiku (like a file being dropped in S3), you can use an external process (like an AWS Lambda function) to make a REST API call to trigger your Dataiku scenario. This provides ultimate flexibility.

### 6. Common Challenges and Solutions
- **Challenge:** My "Dataset change" trigger isn't firing.
- **Solution:** Check that the upstream dataset is actually being rebuilt and modified. Dataiku triggers on a change to its internal state, not just a change in the underlying data source. The dataset must be rebuilt by a Dataiku job.
- **Challenge:** I'm creating an infinite loop by having Scenario A trigger Scenario B, and B trigger A.
- **Solution:** Dataiku will usually detect and prevent you from creating obvious circular dependencies. You must design your orchestration logic carefully to be a Directed Acyclic Graph (DAG).
`},{id:542,slug:"calling-external-scripts-apis-from-scenarios",question:"How to get started with calling external scripts or APIs from Dataiku Scenarios to extend automation.",answer:`
### 1. Introduction/Overview
Dataiku pipelines often need to interact with the outside world. A Scenario can be used to orchestrate these interactions, such as calling an external API to fetch data, pushing a notification to a custom messaging service, or running a local shell script. This is done using a **Python scenario step**.

### 2. Prerequisites
- **A Dataiku Scenario.**
- **Details of the external service:** The URL for the API or the command for the script.
- **A Python code environment** with any necessary libraries (e.g., \`requests\`).

### 3. Step-by-Step Instructions
1.  **Add a Python Step to Your Scenario:**
    *   In your scenario's **Steps** tab, click **+ ADD STEP**.
    *   Choose **Execute Python code**.
2.  **Write the Interaction Code:**
    *   The editor will open. You can now write Python code to perform the external action.

    #### Example 1: Calling an External REST API
    > \`\`\`python
    > import requests
    > import json
    > # This could be to trigger another system after a Dataiku job
    > payload = {'status': 'complete', 'dataset': '\${datasetName}'} # You can use scenario variables
    > try:
    >     response = requests.post("https://api.external-system.com/notify", json=payload)
    >     response.raise_for_status() # Fails the step if the API call returns an error
    >     print("Successfully notified external system.")
    > except Exception as e:
    >     print(f"Failed to call external API: {e}")
    >     raise e # Make sure to re-raise the exception to fail the scenario
    > \`\`\`

    #### Example 2: Running a Local Shell Script
    > \`\`\`python
    > import subprocess
    > # This runs a script located on the Dataiku server machine
    > result = subprocess.run(["/path/to/my/script.sh"], capture_output=True, text=True)
    > if result.returncode != 0:
    >     print(f"Script failed with error: {result.stderr}")
    >     raise Exception("External script failed.")
    > print(f"Script output: {result.stdout}")
    > \`\`\`
3.  **Place the Step in Your Scenario:** Place this Python step at the appropriate point in your scenario's sequence (e.g., after a build step is complete).

### 4. Resources and Tools
- **Python Scenario Step:** The environment for your custom integration code.
- **Python \`requests\` library:** The standard for making HTTP calls.
- **Python \`subprocess\` module:** For running local command-line scripts.
- **Project Variables:** For securely storing API keys or other parameters needed by your script.

### 5. Next Steps and Progression
- **Error Handling:** Robust error handling is crucial. Always check the response codes from API calls and the return codes from shell scripts to ensure the step fails correctly when the external system has a problem.
- **Passing Data:** You can use the Dataiku API within the script to first read data from a dataset and then pass that data in the payload of your API call.

### 6. Common Challenges and Solutions
- **Challenge:** The script fails with a "Connection timed out" error.
- **Solution:** This is a network issue. The Dataiku server cannot reach the external API endpoint. You need to work with your network team to ensure any firewalls are configured to allow outbound traffic from the Dataiku server to the destination IP and port.
- **Challenge:** Where do I store the API key for the external service?
- **Solution:** **Never hardcode it in the script.** Store it as a "Password" type **Project Variable**. Your script can then retrieve it securely using \`dataiku.get_custom_variables()\`.
`},{id:543,slug:"integrating-airflow-to-orchestrate-dataiku-tasks",question:"How to get started with integrating Apache Airflow to orchestrate pipelines that include Dataiku tasks.",answer:`
### 1. Introduction/Overview
For complex, enterprise-wide workflows that span multiple systems, a dedicated orchestrator like Apache Airflow is often used. You can integrate Dataiku into an Airflow DAG (Directed Acyclic Graph), allowing Airflow to trigger Dataiku jobs and manage dependencies between Dataiku and other systems.

### 2. Prerequisites
- **A running Airflow instance.**
- **The Dataiku "provider" for Airflow installed** on your Airflow instance.
- **A configured "Connection" in Airflow** to your Dataiku instance, containing your Dataiku API key.

### 3. Step-by-Step Instructions
1.  **Install the Airflow Provider:** On your Airflow environment, install the official Dataiku provider.
    > \`pip install apache-airflow-providers-dataiku\`
2.  **Configure the Airflow Connection:**
    *   In the Airflow UI, go to **Admin > Connections**.
    *   Create a new connection of type **Dataiku**.
    *   Give it a connection ID (e.g., \`dataiku_default\`).
    *   Provide the host URL for your Dataiku instance and your API key.
3.  **Write Your Airflow DAG:**
    *   In a Python file for your DAG, import the Dataiku operators.
    *   You can now define tasks in your DAG that perform actions in Dataiku.
    *   **Example: Triggering a scenario**
        > \`\`\`python
        > from airflow.models.dag import DAG
        > from airflow.providers.dataiku.operators.scenario import DataikuRunScenarioOperator
        >
        > with DAG(dag_id='dataiku_orchestration_dag', ...) as dag:
        >     run_dataiku_job = DataikuRunScenarioOperator(
        >         task_id='run_my_dataiku_scenario',
        >         project_key='MYPROJECT',
        >         scenario_id='my_scenario',
        >         dataiku_conn_id='dataiku_default' # The connection you configured
        >     )
        > \`\`\`
4.  **Define Dependencies:** In your DAG, you can now define dependencies between your Dataiku task and tasks from other systems. For example, you can have a task that loads data into a database, and only upon its success will the \`DataikuRunScenarioOperator\` be triggered.

### 4. Resources and Tools
- **The official Airflow Provider for Dataiku:** Contains the necessary operators (\`DataikuRunScenarioOperator\`, \`DataikuBuildJobOperator\`, etc.).
- **Airflow Connections:** The secure way to manage your API key for Dataiku.
- **Airflow DAGs:** The Python scripts that define your end-to-end workflow.

### 5. Next Steps and Progression
- **Passing Parameters:** The Dataiku operators in Airflow allow you to pass parameters to your scenario run, making the orchestration dynamic.
- **Two-Way Integration:** You can also have a Dataiku scenario use a Python step to trigger an Airflow DAG via the Airflow REST API, allowing for bidirectional control.

### 6. Common Challenges and Solutions
- **Challenge:** The Airflow task fails with a connection error.
- **Solution:** Check the Airflow connection settings. Ensure the host URL is correct and that the Airflow worker nodes can reach the Dataiku instance over the network. Check firewalls.
- **Challenge:** Which tool should I use for orchestration, Scenarios or Airflow?
- **Solution:** For orchestration *within* a Dataiku project, use Scenarios. They are simpler and tightly integrated. Use a powerful external orchestrator like Airflow when you need to manage complex dependencies *between* Dataiku and other external systems (like data warehouses, CRMs, etc.) as part of a larger enterprise workflow.
`},{id:544,slug:"using-rest-api-to-trigger-runs-from-external-orchestrator",question:"How to get started with using the Dataiku REST API to trigger project runs from an external orchestrator.",answer:`
### 1. Introduction/Overview
The Dataiku REST API is the universal key to integrating Dataiku with any external system, including custom or third-party orchestration tools. By making a simple, authenticated HTTP request, any external tool can trigger a job run in Dataiku, enabling powerful automation workflows.

### 2. Prerequisites
- **An external orchestrator or script** that can make HTTP POST requests.
- **A Dataiku Scenario** to be triggered.
- **Your Dataiku instance URL, Project Key, and Scenario ID.**
- **A Dataiku API key** with permissions to run the scenario.

### 3. Step-by-Step Instructions
1.  **Generate a Dataiku API Key:**
    *   In your Dataiku profile, go to **Settings > API keys**.
    *   Generate a new key and grant it permission to run scenarios on your target project.
2.  **Construct the Endpoint URL:**
    *   The REST API endpoint for running a scenario is:
    > \`https://YOUR_DSS_URL/public/api/projects/YOUR_PROJECT_KEY/scenarios/YOUR_SCENARIO_ID/run\`
3.  **Make the API Call from Your Orchestrator:**
    *   From your external tool's scripting environment, make an **HTTP POST** request to the endpoint URL.
    *   You must provide your API key for authentication. The standard method is HTTP Basic Auth, where the **API key is the username** and the password is left blank.
4.  **Example with \`curl\` (a common command-line tool):**
    > \`\`\`bash
    > #!/bin/bash
    > API_KEY="your_secret_api_key_here"
    > DSS_URL="https://dss.mycompany.com"
    > PROJECT_KEY="SALES_REPORTING"
    > SCENARIO_ID="build_daily"
    >
    > # Trigger the job
    > curl -X POST -u "\${API_KEY}:" "\${DSS_URL}/public/api/projects/\${PROJECT_KEY}/scenarios/\${SCENARIO_ID}/run"
    > \`\`\`
5.  **Handle the Response:** The API call will return a JSON response. A robust orchestrator script should check the HTTP status code to confirm the request was accepted and parse the JSON to get the \`jobId\` of the newly started run.

### 4. Resources and Tools
- **The Dataiku REST API Documentation:** Available from the Help menu in your instance, it's the definitive reference for all endpoints.
- **\`curl\` and Postman:** Essential tools for testing and debugging your REST API calls.
- **HTTP Client library:** Whatever language your orchestrator uses, it will have a library for making HTTP requests (e.g., \`requests\` in Python, \`Invoke-RestMethod\` in PowerShell).

### 5. Next Steps and Progression
- **Asynchronous Monitoring:** The "run" API call just starts the job. A complete orchestration script needs to then take the returned \`jobId\` and use the "get job status" API endpoint in a polling loop to wait for the job to complete and check if its outcome was a success or failure.
- **Passing Parameters:** You can include a JSON body in your POST request to pass parameters to the scenario, allowing the external orchestrator to dynamically control the job run.

### 6. Common Challenges and Solutions
- **Challenge:** "401 Unauthorized" error.
- **Solution:** Your API key is invalid or doesn't have the necessary permissions. Regenerate the key and double-check its permissions in the Dataiku UI. Also, ensure your authentication header is formatted correctly.
- **Challenge:** "Connection Refused" or timeout.
- **Solution:** This is a network issue. The machine running your orchestrator cannot reach the Dataiku server. Check firewalls and network routing between the two systems.
`},{id:545,slug:"chaining-projects-by-invoking-scenarios",question:"How to get started with chaining Dataiku projects by invoking one project’s scenario from another’s.",answer:`
### 1. Introduction/Overview
As your Dataiku usage matures, you will often need to create dependencies between different projects. For example, a central "Data Ingestion" project might prepare data that is then used by several downstream "Analysis" projects. You can orchestrate this by having a scenario in one project trigger a scenario in another.

### 2. Prerequisites
- **Two or more Dataiku projects** (e.g., Project A and Project B).
- **A scenario in Project A** that should trigger a scenario in Project B.
- **An API key** with permissions to run scenarios on both projects.

### 3. Step-by-Step Instructions
1.  **Create a Global API Key:**
    *   For this pattern, it's often easiest to have an administrator create a global API key for a dedicated "service account" user.
    *   This service account should be granted "run scenarios" permissions on both Project A and Project B.
2.  **Create the Triggering Scenario Step:**
    *   Open your "upstream" scenario in **Project A**.
    *   Add a new step of type **Execute Python code**.
3.  **Write the Python Script:**
    *   The script will use the Dataiku Python API client to connect to the instance and trigger the downstream scenario.
    *   You must explicitly create an API client handle using the API key.
    > \`\`\`python
    > import dataiku
    >
    > # --- Configuration ---
    > # It is best practice to store these in project variables
    > API_KEY = "your_service_account_api_key"
    > DOWNSTREAM_PROJECT_KEY = "PROJECT_B"
    > DOWNSTREAM_SCENARIO_ID = "scenario_to_be_triggered"
    >
    > # --- Logic ---
    > # Get a client handle using the API key
    > client = dataiku.api_client(api_key=API_KEY)
    >
    > # Get a handle on the downstream scenario
    > scenario = client.get_project(DOWNSTREAM_PROJECT_KEY).get_scenario(DOWNSTREAM_SCENARIO_ID)
    >
    > # Run the scenario
    > print(f"Triggering scenario '{DOWNSTREAM_SCENARIO_ID}' in project '{DOWNSTREAM_PROJECT_KEY}'...")
    > job = scenario.run_and_wait() # Use run_and_wait to wait for completion
    >
    > if job.get_info().get("result") != "SUCCESS":
    >     raise Exception("Downstream scenario failed!")
    >
    > print("Downstream scenario completed successfully.")
    > \`\`\`
4.  **Use Reporters as an Alternative:**
    *   For a simpler dependency, you can use a **Reporter**.
    *   In the upstream scenario in Project A, go to the **Reporters** tab.
    *   Add a reporter of type **Run another scenario**.
    *   Configure it to run the downstream scenario in Project B upon the successful completion of the Project A scenario.

### 4. Resources and Tools
- **Python Scenario Step:** For maximum flexibility and control.
- **Dataiku Python API:** The \`api_client()\` function is key.
- **"Run another scenario" Reporter:** For simpler, direct chaining.

### 5. Next Steps and Progression
- **Passing Information:** You can use the Python script to read a value from a dataset in Project A and then pass that value as a parameter to the scenario run in Project B, allowing for information to be passed between the chained projects.

### 6. Common Challenges and Solutions
- **Challenge:** "Permission Denied" when running the scenario.
- **Solution:** The API key you are using does not have permissions on the downstream project (Project B). You must grant the API key's owner the necessary rights on all projects involved in the chain.
- **Challenge:** Which method should I use, Python step or Reporter?
- **Solution:** Use the "Run another scenario" reporter for simple, direct dependencies. Use a Python step when you need more complex logic, such as passing dynamic parameters, running the downstream job conditionally, or implementing custom error handling.
`},{id:546,slug:"handling-errors-retries-in-scenarios",question:"How to get started with handling errors and retries in Dataiku Scenarios for robust pipeline automation.",answer:`
### 1. Introduction/Overview
In a production environment, pipelines can fail for many reasons (e.g., temporary network issues, source system downtime). A robust pipeline should handle these errors gracefully. This involves two key practices: **alerting** so you know a failure occurred, and **retrying** to automatically recover from transient failures.

### 2. Prerequisites
- An existing Dataiku Scenario.

### 3. Step-by-Step Instructions

#### Part 1: Error Alerting (Essential)
1.  **Configure a Reporter:**
    *   In your scenario, go to the **Reporters** tab.
    *   Click **+ ADD REPORTER** and choose **Mail** or **Slack**.
2.  **Set the Condition:** Set the **Run condition** to **On failure**.
3.  **Write an Actionable Message:** Configure the message to include the project key, scenario name, and, most importantly, the \`\${jobURL}\` variable. This provides a direct link to the logs for easy troubleshooting.
4.  **How it Works:** Now, if any step in your scenario fails, the scenario will be marked as "FAILED", and this reporter will automatically send the alert.

#### Part 2: Automatic Retries (Advanced)
Dataiku does not have a simple "retry N times" checkbox. You implement this with a "wrapper" scenario.
1.  **Create a Wrapper Scenario:** Create a new scenario (e.g., \`Retry_My_Job\`). Its only purpose is to call your main scenario.
2.  **Add a Python Step:** Add a single **Execute Python code** step to this wrapper.
3.  **Write the Retry Script:** Use a \`for\` loop and a \`try...except\` block to call your main scenario multiple times.
    > \`\`\`python
    > import dataiku
    > import time
    >
    > MAX_RETRIES = 3
    > RETRY_DELAY_SECONDS = 60 # Wait 1 minute
    >
    > main_scenario = dataiku.api_client().get_project("MY_PROJ").get_scenario("my_main_scenario")
    >
    > for i in range(MAX_RETRIES):
    >     try:
    >         print(f"Attempt {i+1}/{MAX_RETRIES}...")
    >         job = main_scenario.run_and_wait()
    >         if job.get_info()["result"] == "SUCCESS":
    >             print("Main scenario succeeded.")
    >             return # Exit successfully
    >     except Exception as e:
    >         print(f"Caught exception: {e}")
    >
    >     print(f"Waiting {RETRY_DELAY_SECONDS}s before next attempt.")
    >     time.sleep(RETRY_DELAY_SECONDS)
    >
    > # If the loop finishes, all retries have failed
    > raise Exception(f"Scenario failed after {MAX_RETRIES} attempts.")
    > \`\`\`
4.  **Schedule the Wrapper:** You schedule the wrapper scenario, not the original one. It will handle the retries, and if all retries fail, the wrapper itself will fail, triggering its own failure alerts.

### 4. Resources and Tools
- **Reporters:** The primary tool for alerting.
- **Python Scenario Step:** The environment for implementing custom retry logic.
- **Python's \`time\` module:** Useful for adding a delay between retries.

### 5. Next Steps and Progression
- **Exponential Backoff:** Improve your retry script by adding a \`time.sleep()\` inside the loop that waits for a progressively longer time between retries (e.g., 1 min, then 5 mins, then 15 mins).
- **Conditional Retries:** Make your retry script smarter. It could inspect the error message from the failed job and only retry for specific, known transient errors (like a connection timeout), but fail immediately for fatal errors.

### 6. Common Challenges and Solutions
- **Challenge:** "This retry logic is complicated."
- **Solution:** It is an advanced pattern. Only use it for your most critical production scenarios that are expected to encounter occasional, recoverable failures. For many pipelines, simple failure alerting is sufficient.
- **Challenge:** "My pipeline is failing every night, and the retries aren't helping."
- **Solution:** The retry mechanism is for transient issues, not persistent bugs. If the job fails consistently, it means there is an underlying bug in your data or your code that you must fix. The alerts are correctly notifying you of this problem.
`},{id:547,slug:"building-complex-multi-step-pipelines-with-flow-scenario-logic",question:"How to get started with building complex multi-step pipelines in Dataiku by combining Flow and Scenario logic.",answer:`
### 1. Introduction/Overview
In Dataiku, there is a fundamental separation of concerns for building complex pipelines. The **Flow** is used to define the "what"—the sequence of data transformations and dependencies. **Scenarios** are used to define the "how" and "when"—the automation, scheduling, error handling, and operational logic. Combining these two effectively is the key to creating robust, maintainable pipelines.

### 2. Prerequisites
- **A clear understanding of the difference between the Flow and Scenarios.**
- **A complex business problem to solve.**

### 3. Step-by-Step Instructions: A Best-Practice Approach
1.  **Design the Data Flow First:**
    *   Focus on the data transformations. Use the visual Flow to chain together your recipes (Prepare, Join, Group, Python, etc.) to produce the desired final dataset or model.
    *   Organize your Flow logically using **Flow Zones** (e.g., "Ingestion", "Preparation", "Modeling").
    *   At this stage, you should run your recipes manually to ensure the logic is correct.
2.  **Create an Orchestration Scenario:**
    *   Once your Flow is logically correct, go to the **Scenarios** page.
    *   Create a new scenario to automate the execution of the Flow.
3.  **Add a Single "Build" Step:**
    *   In your scenario, you do not need to recreate the logic of the Flow. You just need to tell it what to build.
    *   Add a single **Build / Train** step.
    *   In this step, select only the **final output(s)** of your Flow.
    *   Dataiku's dependency engine will automatically trace the lineage backward through the entire Flow and execute all the necessary recipes in the correct order.
4.  **Add Operational Logic to the Scenario:**
    *   Now, add the operational logic around the build step.
    *   **Before the build:** You could have a Python step to check a precondition.
    *   **After the build:** Add a **Run checks** step for data quality validation.
    *   **In the Reporters tab:** Configure alerts **On failure** and notifications **On success**.
    *   **In the Settings tab:** Add a **Trigger** to schedule the entire pipeline.

### 4. Resources and Tools
- **The Flow:** The visual canvas for defining data dependencies and transformations.
- **Scenarios:** The orchestration layer for automation, control flow, and monitoring.

### 5. Next Steps and Progression
- **Decouple into Multiple Scenarios:** For very large and complex systems, you can break the orchestration into multiple scenarios. For example:
    *   Scenario 1: Handles data ingestion. Upon its success, it triggers...
    *   Scenario 2: Handles the main data transformation. Upon its success, it triggers...
    *   Scenario 3: Handles model retraining and deployment.
- This creates a modular, microservices-like architecture for your orchestration.

### 6. Common Challenges and Solutions
- **Challenge:** "I put all my logic in a single, giant Python scenario step."
- **Solution:** This is a common anti-pattern. While possible, it's hard to maintain and debug, and you lose all the benefits of Dataiku's visual Flow and automatic lineage. The code for data transformation should live in **recipes** in the Flow. The scenario should only contain the orchestration and operational code.
- **Challenge:** "My scenario has dozens of 'Build' steps."
- **Solution:** This is unnecessary and makes the scenario hard to manage. You only need one "Build" step that targets the final output. Dataiku's dependency engine is smart enough to figure out the rest.
`},{id:548,slug:"integrating-streaming-sources-kafka",question:"How to get started with integrating streaming sources (e.g., Kafka) to continuously update Dataiku flows.",answer:`
### 1. Introduction/Overview
For use cases that require processing data in near real-time, Dataiku can integrate with streaming platforms like Apache Kafka. This is an advanced feature that allows you to build pipelines that process messages continuously as they arrive, rather than in discrete batches.

### 2. Prerequisites
- **A running Kafka cluster** and a topic with flowing messages.
- **The "Streaming Endpoints" plugin** installed on your Dataiku instance.
- **A clear real-time processing goal** (e.g., "filter a stream of log messages and write all errors to a separate topic").

### 3. Step-by-Step Instructions
1.  **Create a Streaming Endpoint:** In your project, go to the **...** menu and select **Streaming Endpoints**. Click **+ NEW STREAMING ENDPOINT**.
2.  **Configure the Source:**
    *   Choose **Kafka** as the source type.
    *   Provide the address of your Kafka **Bootstrap servers**.
    *   Specify the **Topic** you want to consume messages from.
    *   Define the **Format** of the messages (e.g., JSON).
3.  **Add a Streaming Recipe:**
    *   You can't use standard recipes on a stream. You must use streaming-compatible recipes.
    *   Click **+ RECIPE** and choose a streaming recipe, like **Streaming Prepare**.
    *   In this recipe, you can perform stateless transformations on each message as it passes through (e.g., filter messages based on a field's value, or use a formula to create a new field).
4.  **Configure the Sink:**
    *   After your transformation, you must define a "sink" where the processed data will be sent.
    *   You can add another Kafka connection as a sink to write the results to a different topic.
    *   You can also sink the data to a **Dataiku dataset**. Dataiku will collect the messages in micro-batches and append them to the dataset.
5.  **Start and Monitor the Stream:**
    *   Once the source, transformations, and sink are configured, click **Start**.
    *   The Streaming Endpoint will now run continuously. The monitoring tab provides real-time charts of the message throughput and processing latency.

### 4. Resources and Tools
- **Streaming Endpoints Plugin:** The core feature for this functionality.
- **Kafka Connector:** For configuring the source and sink.
- **Streaming Recipes:** The special set of recipes that can operate on a continuous stream.

### 5. Next Steps and Progression
- **Stateful Streaming:** For more complex operations like calculating a moving average over a time window, you would need to use a **Streaming Python** recipe with a stream processing library like Faust.
- **Real-time Scoring:** You could build a stream that reads customer events, calls a deployed Dataiku model via its API to get a prediction, and then writes the event enriched with the prediction to an output stream.

### 6. Common Challenges and Solutions
- **Challenge:** The stream is not starting and is showing a connection error.
- **Solution:** This is a network issue. Ensure the Dataiku server can reach the Kafka bootstrap servers on the correct port. Check firewalls.
- **Challenge:** Messages are not being processed correctly.
- **Solution:** Check the stream's logs. The most common issue is a **deserialization error**. This means the format of the messages on the Kafka topic does not match what you configured in the source settings (e.g., you configured JSON, but the messages are actually Avro-encoded).
`},{id:549,slug:"creating-automated-data-quality-checks-for-orchestration",question:"How to get started with creating automated data quality checks in Dataiku as part of orchestration.",answer:`
### 1. Introduction/Overview
Automated data quality checks are a critical part of a robust orchestration pipeline. They act as a safety gate, ensuring that bad data does not propagate downstream. In Dataiku, this is achieved by defining **Metrics and Checks** on your datasets and then executing them as part of a **Scenario**.

### 2. Prerequisites
- A Dataiku project with a data pipeline.
- A clear definition of your data quality rules (e.g., "column X must not have nulls," "column Y must be between 0 and 100").

### 3. Step-by-Step Instructions
1.  **Define the Quality Rules on a Dataset:**
    *   Navigate to a key dataset in your Flow (e.g., a cleaned dataset before it's used for modeling or reporting).
    *   Go to the **Status** tab.
    *   First, in the **Metrics** section, define what to measure (e.g., compute the "Number of empty values" for a specific column, or the "Min/Max" for another).
    *   Second, in the **Checks** section, define the rule based on the metric (e.g., "Number of empty values is equal to 0", "Min is >= 0").
    *   Set the severity of any critical check to **Error**.
2.  **Create an Orchestration Scenario:**
    *   Go to the **Scenarios** page and open your main automation scenario.
3.  **Add the Validation Step:**
    *   In your scenario's sequence of steps, find the point *after* your key dataset has been built.
    *   Add a new step by clicking **+ ADD STEP** and choosing **Run checks**.
4.  **Configure the Step:**
    *   In the step's configuration, select the dataset on which you defined the checks.
5.  **How it Works:**
    *   When the scenario runs, it will first build the dataset.
    *   It will then execute the "Run checks" step. This step will re-compute the metrics and run your checks.
    *   If any check with a severity of "Error" fails, this step will fail, which will cause the entire scenario to fail.
6.  **Add Alerting:** Configure a **Reporter** on your scenario to send an email or Slack alert **On failure**. This will notify you immediately that a data quality rule has been violated.

### 4. Resources and Tools
- **Status Tab (Metrics & Checks):** The UI for defining your data quality rules.
- **Run Checks Scenario Step:** The tool that executes your rules as part of an automated pipeline.
- **Reporters:** The tool for sending alerts when a quality check fails.

### 5. Next Steps and Progression
- **Custom Python Checks:** For complex business rules that can't be expressed with the built-in checks, you can write a custom check using Python and the Dataiku API.
- **Data Quality Dashboard:** Create a dashboard that visualizes the history of your metrics (e.g., a line chart of the row count over time). This helps you spot trends and anomalies.

### 6. Common Challenges and Solutions
- **Challenge:** "My data quality check failed. How do I know why?"
- **Solution:** Go to the log of the failed scenario run. The log for the "Run Checks" step will give a very clear message, such as: "Check 'price_is_positive' on dataset 'sales_prepared' failed: Found 15 rows where 'price' was < 0."
- **Challenge:** "I want to be warned of an issue, but I don't want it to stop my entire pipeline."
- **Solution:** In the "Checks" settings, you can set the **Severity** of a check to **Warning** instead of "Error". A warning will be logged, but it will not cause the "Run Checks" step to fail.
`},{id:550,slug:"setting-up-user-roles-permissions",question:"How to get started with setting up Dataiku user roles and permissions for project access control.",answer:`
### 1. Introduction/Overview
Setting up user permissions is a fundamental governance task. Dataiku uses a Role-Based Access Control (RBAC) model, which is both powerful and easy to understand. Access is managed by adding **Groups** of users to a **Project** and assigning them a specific permission level (their "role").

### 2. Prerequisites
- **Dataiku Administrator rights:** To create and manage user groups.
- **Project Administrator rights:** To manage permissions on a specific project.
- **A defined set of user roles** for your organization (e.g., Analyst, Data Scientist, Business User).

### 3. Step-by-Step Instructions

#### Part 1: Create Groups (Admin Task)
1.  **Navigate to Administration:** Go to **Administration > Security > Groups**.
2.  **Create Groups for Roles:** Create new groups that correspond to the roles in your organization. For example, create a group named \`data-analysts\`, another named \`data-scientists\`, and another named \`report-consumers\`.
3.  **Add Users to Groups:** Add the individual Dataiku users to their appropriate groups.

#### Part 2: Grant Permissions to a Project
1.  **Navigate to Project Settings:** In the project you want to control access to, go to **Settings > Permissions**.
2.  **Add a Group:** Click **+ ADD GROUP**.
3.  **Select a Group and Assign a Role:**
    *   Select a group (e.g., \`report-consumers\`).
    *   Assign a permission level. The three key levels are:
        *   **Reader:** Can view everything in the project (datasets, flows, dashboards) but cannot edit or run anything. **This is the correct role for business users who only need to view reports.**
        *   **Contributor:** Can do most development work, like creating and editing recipes. **This is for your core developers and analysts.**
        *   **Administrator:** Has full control over the project, including its settings and permissions. **This is for the project owner.**
4.  **Save:** Save your changes. The permissions are applied instantly.

### 4. Resources and Tools
- **Administration > Security:** The central place for managing users and groups.
- **Project Settings > Permissions:** The UI for controlling access to a specific project.

### 5. Next Steps and Progression
- **Least Privilege:** Always grant the minimum level of permission necessary. Don't make everyone an administrator.
- **Connection-Level Security:** Remember that project permissions are only one layer. An administrator also sets permissions on the underlying data connections, which provides an additional layer of security.
- **Project Folders:** Organize projects into folders on the Dataiku homepage and set permissions on the folders themselves to control visibility for different departments.

### 6. Common Challenges and Solutions
- **Challenge:** "How do I give a user access to just one dashboard in my project?"
- **Solution:** You can't. Dataiku's security model is at the project level. The standard and best practice to solve this is to **create a new, separate project** for that specific dashboard. Use a **Sync recipe** to share the final data to this new "dashboard project". Then, you can grant the user "Reader" access to *only* that new, isolated project.
- **Challenge:** "A user is a Contributor but can't read from a specific database."
- **Solution:** Their group needs to be granted "Read" access on that specific **Data Connection** in the global **Administration > Connections** settings. Project permissions and connection permissions work together.
`},{id:551,slug:"using-policy-governance-features-lineage-tags",question:"How to get started with using Dataiku’s policy and governance features (data lineage, tags) to enforce compliance.",answer:`
### 1. Introduction/Overview
Enforcing compliance and governance policies requires tools for data classification, access control, and auditability. Dataiku provides a suite of features designed for this, with **Tags** for classification and **Lineage** for auditability being two of the most important.

### 2. Prerequisites
- **A defined governance policy:** You need to know what you need to enforce (e.g., "PII data must be identified and its access restricted").
- **A Dataiku project** containing data that falls under the policy.

### 3. Step-by-Step Instructions

#### Part 1: Data Classification with Tags
1.  **Define a Tagging Taxonomy:** As a first step, your organization should define a standard set of tags for governance. For example:
    *   **Sensitivity:** \`PII\`, \`Confidential\`, \`Public\`
    *   **Data Quality:** \`Raw\`, \`Staged\`, \`Validated\`
    *   **Ownership:** \`Owner:Finance\`, \`Owner:Marketing\`
2.  **Apply Tags to Datasets:**
    *   Go through your projects and apply these standard tags to your datasets.
    *   To do this, open a dataset, go to the **Summary** tab, and add the appropriate tags.
3.  **Use Tags for Governance:**
    *   You can now search for all datasets with a specific tag (e.g., find all PII data) in the Data Catalog.
    *   You can build automated scenarios that check for policy violations (e.g., "alert if a dataset tagged 'PII' does not have restricted project permissions").

#### Part 2: Auditability with Data Lineage
1.  **Understand Automatic Lineage:** Dataiku automatically tracks the lineage of all your data. You don't need to do anything to enable it.
2.  **Use Lineage to Prove Compliance:**
    *   When an auditor asks, "How was this number calculated?" or "How was this sensitive data handled?", the lineage graph is your answer.
    *   Open the final dataset and go to the **Lineage** tab.
    *   Click on a specific column. The graph will visually show every single source column and transformation step that was used to create that final value.
    *   This provides a clear, undeniable audit trail of data provenance.
3.  **Document the Controls:** Combine the technical features with clear documentation. In your project's **Wiki**, explain how your use of tags, permissions, and recipes enforces the specific compliance policy.

### 4. Resources and Tools
- **Tags:** The primary feature for data classification.
- **Data Lineage Graph:** The primary feature for auditability and provenance.
- **Project Permissions:** The primary feature for access control.
- **Project Wiki:** The place to document your compliance strategy.

### 5. Next Steps and Progression
- **PII Detection Plugin:** Use this plugin to automatically scan datasets and suggest columns that may contain PII, helping to accelerate your tagging efforts.
- **Automated Governance Scenarios:** Write a Python scenario that uses the Dataiku API to periodically scan all projects and generate a report on their compliance with your tagging and documentation standards.

### 6. Common Challenges and Solutions
- **Challenge:** "Tagging all our datasets is too much work."
- **Solution:** Start with your most critical and sensitive data first. Prioritize tagging datasets that contain PII or are used for financial reporting. You can also write scripts to automate some of the tagging based on dataset names or schemas.
- **Challenge:** The lineage for my Python recipe is broken.
- **Solution:** This happens if the Python code reads or writes data without using the Dataiku APIs. To maintain lineage, your code must use \`dataiku.Dataset(...)\` to get handles on its inputs and outputs.
`},{id:552,slug:"configuring-ldap-sso-integration",question:"How to get started with configuring Dataiku’s LDAP/SSO integration for enterprise user management.",answer:`
### 1. Introduction/Overview
For enterprise environments, managing users manually in Dataiku is not scalable. Integrating Dataiku with your corporate directory (like Active Directory via LDAP) and a Single Sign-On (SSO) provider allows for centralized user management and a seamless, secure login experience for your users. This is an administrator-level task.

### 2. Prerequisites
- **Dataiku Administrator rights.**
- **Details of your corporate directory:** The LDAP server address, port, and credentials for a service account that can read user information.
- **Details of your SSO provider:** If using SSO, you'll need information from your identity provider (like Okta, Azure AD, or PingFederate), such as metadata URLs or client secrets.

### 3. Step-by-Step Instructions

#### Part 1: LDAP Integration (for user and group synchronization)
1.  **Navigate to Security Settings:** As an admin, go to **Administration > Security > LDAP**.
2.  **Enable LDAP:** Click to enable LDAP integration.
3.  **Configure the Connection:** Fill in the connection details for your LDAP server (hostname, port, user DN, password).
4.  **Configure User and Group Mapping:**
    *   You need to tell Dataiku how to find users and groups in your directory. This involves specifying the Base DNs for users and groups and mapping attributes (e.g., mapping the LDAP \`sAMAccountName\` attribute to the Dataiku login).
5.  **Test and Synchronize:** Test the connection and then run a synchronization. Dataiku will now pull in users and their group memberships from your corporate directory.

#### Part 2: SSO Integration (for seamless login)
1.  **Choose an SSO Protocol:** Dataiku supports standard protocols like SAML 2.0 and OpenID Connect.
2.  **Configure the SSO Provider:** In your identity provider's admin console (e.g., Okta), you will need to create a new "Application" for Dataiku. This process will give you the necessary information (like a metadata URL) to configure Dataiku.
3.  **Configure Dataiku:**
    *   In Dataiku, go to **Administration > Security > Single Sign-On**.
    *   Enable SSO and select your protocol (e.g., SAML).
    *   Fill in the information provided by your identity provider.
4.  **How it Works:** Now, when a user goes to the Dataiku URL, they will be redirected to your company's standard login page. After they authenticate there, they will be automatically logged into Dataiku without needing a separate password.

### 4. Resources and Tools
- **Administration > Security:** The central place for all authentication configuration.
- **Your Corporate Directory (Active Directory, etc.).**
- **Your SSO Provider's Admin Console (Okta, Azure AD, etc.).**
- **Dataiku's official documentation:** Provides detailed guides for integrating with specific providers.

### 5. Next Steps and Progression
- **Just-in-Time Provisioning:** With SSO enabled, you can configure just-in-time provisioning. When a new user logs into Dataiku for the first time via SSO, their Dataiku account will be created automatically.

### 6. Common Challenges and Solutions
- **Challenge:** "The LDAP connection test fails."
- **Solution:** This is often a network issue (firewalls blocking the LDAP port) or a problem with the Base DN and user/group filters. The LDAP query syntax can be tricky and needs to be exact. Work with your Active Directory administrator to get the correct filter syntax.
- **Challenge:** "SSO login is failing with a 'SAML assertion invalid' error."
- **Solution:** This means there is a mismatch in the configuration between Dataiku and your identity provider. The "identifiers" or "audience URIs" must match exactly on both sides. Also, check that the clocks on both the Dataiku server and the identity provider server are synchronized.
`},{id:553,slug:"enabling-audit-logging-for-lineage-changes",question:"How to get started with enabling audit logging in Dataiku to track data lineage and project changes.",answer:`
### 1. Introduction/Overview
Audit logging is a critical governance feature that provides a chronological record of activities, which is essential for security, compliance, and troubleshooting. Audit logging is enabled by default in Dataiku, automatically capturing key events. The main task is knowing where to find and how to use these logs.

### 2. Prerequisites
- **A Dataiku instance** with user activity.
- **Appropriate permissions:** Access to audit logs is restricted. You need project-level access for project timelines and global administrator rights for the instance-wide audit log.

### 3. Step-by-Step Instructions: Finding the Audit Logs

#### Level 1: Project-Specific Audit Trail (The Timeline)
- **What it tracks:** Changes made to a specific project.
- **How to access:**
    1.  Open the project you want to audit.
    2.  Go to the **...** menu in the top navigation bar and select **Timeline**.
- **What it shows:** A user-friendly feed of all significant modifications within that project, such as "User A created recipe X," "User B modified dashboard Y," etc. This is perfect for understanding the recent history of a project.

#### Level 2: Instance-Wide Security Audit Trail (Global Audit Log)
- **What it tracks:** High-level security and administrative events for the entire Dataiku instance.
- **How to access (Admin only):**
    1.  Go to **Administration > Logs > Global Audit Log**.
- **What it shows:** This log captures events like user logins, failed login attempts, API key creation, changes to global settings, and permissions modifications.

#### Level 3: Code and Lineage Change Audit Trail (Git)
- **What it tracks:** The most detailed, line-by-line history of all changes to your project's *definition* (recipes, flow structure, etc.).
- **How to access:**
    1.  This requires your project to be integrated with **Git**.
    2.  You can view the commit history either in Dataiku's **Git** page or on your Git provider's web UI (e.g., GitHub).
- **What it shows:** A complete, immutable record of every change, who made it, when, and (via the commit message) why.

### 4. Resources and Tools
- **Project Timeline:** For day-to-day project auditing.
- **Global Audit Log:** For instance-wide security audits.
- **Git Integration:** For the most detailed, developer-focused audit trail.

### 5. Next Steps and Progression
- **Log Exporting (Admin):** An administrator can configure Dataiku to forward its logs to an external Security Information and Event Management (SIEM) system like Splunk for centralized analysis and long-term retention.
- **Alerting on Audit Events:** In an external SIEM, you can set up alerts for specific audit events, such as multiple failed login attempts for a single user.

### 6. Common Challenges and Solutions
- **Challenge:** "I need to know who viewed a specific dashboard."
- **Solution:** This very granular read-access logging is not part of the standard high-level audit trails. Your primary control for this is preventative: use project permissions to ensure only authorized people can access the project containing the dashboard in the first place.
- **Challenge:** "The audit log is too noisy."
- **Solution:** Use the filtering capabilities. Both the Project Timeline and the Global Audit Log have search and filter bars that allow you to narrow down the events by user, object type, or time period.
`},{id:554,slug:"storing-credentials-securely-secret-management",question:"How to get started with storing credentials securely in Dataiku (secret management) and encrypting sensitive data.",answer:`
### 1. Introduction/Overview
Properly managing secrets like passwords, API keys, and database credentials is the most fundamental aspect of data security. **Never hardcode secrets in your code.** Dataiku provides a multi-layered approach to secret management, from built-in secure storage to integration with enterprise-grade secret vaults.

### 2. Prerequisites
- **Credentials** that need to be stored (e.g., a database password, an API key).
- **Appropriate permissions** in Dataiku to manage connections or project variables.

### 3. Step-by-Step Instructions: Choosing the Right Method

#### Method 1: For Data Connections (The Standard Practice)
- **When to Use:** For storing credentials for databases, cloud storage, and other standard data sources.
- **How It Works (Admin Task):**
    1.  An administrator goes to **Administration > Connections**.
    2.  When they create a new connection (e.g., to Snowflake), the UI provides dedicated, secure fields for the **Username** and **Password**.
    3.  Dataiku encrypts and stores these credentials. Users who are granted permission to *use* the connection never see the password itself.
- **This is the primary and most common method.**

#### Method 2: For Code Recipes (Project Variables)
- **When to Use:** For secrets that are used in your custom code, like an API key for an external service.
- **How It Works:**
    1.  In your project, go to **... > Variables**.
    2.  Create a new variable (e.g., \`MY_API_KEY\`).
    3.  **Crucially, set the variable's type to "Password"**. This encrypts the value, hides it in the UI, and prevents it from being exposed in logs.
    4.  In your Python recipe, you can then retrieve this secret using \`dataiku.get_custom_variables()\`.

#### Method 3: For Enterprise Security (External Vaults)
- **When to Use:** For organizations with the highest security requirements and a dedicated secrets management platform.
- **How It Works (Admin Task):**
    1.  An administrator integrates Dataiku with a tool like **HashiCorp Vault**, **Azure Key Vault**, or **AWS Secrets Manager**. This is configured in **Administration > Settings**.
    2.  Now, when setting up a connection or a project variable, you will have a new option to fetch the value directly from the configured external vault instead of entering it into Dataiku.

### 4. Resources and Tools
- **Dataiku Connections:** The built-in secure store for data source credentials.
- **Project Variables (Password type):** The secure store for code-level secrets.
- **External Secret Management Tools:** The enterprise-grade solution.

### 5. Next Steps and Progression
- **IAM Roles:** For cloud deployments, the most secure method is to avoid static secrets altogether and use temporary credentials provided by **IAM Roles** (for AWS) or **Managed Identities** (for Azure). Dataiku's connection settings support this authentication method.

### 6. Common Challenges and Solutions
- **Challenge:** "I see a password in plain text in a Python recipe in a shared project."
- **Solution:** This is a major security violation. The code must be changed immediately. The hardcoded password should be removed from the code and stored properly as a "Password" type project variable.
- **Challenge:** "I don't have admin rights to create a connection."
- **Solution:** This is correct by design. Centralizing connection management with administrators is a key security control. You should submit a request to your Dataiku admin team to have the new connection created.
`},{id:555,slug:"tagging-datasets-for-sensitive-information-pii-phi",question:"How to get started with tagging datasets in Dataiku to manage sensitive information (PII/PHI) for compliance.",answer:`
### 1. Introduction/Overview
For compliance with regulations like GDPR and HIPAA, the first step is always to know where your sensitive data is. Tagging is Dataiku's primary mechanism for classifying your data assets. By applying tags like \`PII\` (Personally Identifiable Information) or \`PHI\` (Protected Health Information), you can easily find, manage, and secure your sensitive datasets.

### 2. Prerequisites
- **An understanding of what constitutes PII/PHI** in your organization's context.
- **A Dataiku project containing sensitive data.**

### 3. Step-by-Step Instructions
1.  **Define a Sensitivity Tagging Standard:**
    *   Your organization should have a simple, clear taxonomy for sensitivity. For example:
        *   \`SENSITIVITY:PUBLIC\`
        *   \`SENSITIVITY:INTERNAL\`
        *   \`SENSITIVITY:CONFIDENTIAL\`
        *   \`SENSITIVITY:PII\`
        *   \`SENSITIVITY:PHI\`
    *   Document this standard in a central Wiki so everyone uses the same tags.
2.  **Identify Sensitive Datasets:**
    *   Go through your project's Flow and identify any dataset that contains sensitive information.
3.  **Apply the Tags:**
    *   For each sensitive dataset, open it and go to the **Summary** tab.
    *   In the "Tags" section, type your standard tag (e.g., \`SENSITIVITY:PII\`) and press Enter. You can add multiple tags.
4.  **Use Tags to Enforce Governance:**
    *   **Find Data:** Use the central **Data Catalog** to search for all datasets across the entire instance that have the \`PII\` tag. This gives you a complete inventory of your sensitive data.
    *   **Secure Data:** Use this inventory to review permissions. Any project containing a PII-tagged dataset should have its **Project Permissions** restricted to only authorized users.
    *   **Automate Checks:** Create a **Scenario** with a Python step that uses the Dataiku API to find all PII-tagged datasets and checks if they are in projects with appropriate security settings, sending an alert if a violation is found.

### 4. Resources and Tools
- **Tags:** The lightweight, flexible classification feature.
- **Data Catalog:** The central, searchable inventory powered by your tags.
- **Project Permissions:** The tool for restricting access to projects containing tagged data.
- **Python API:** For automating governance checks based on tags.

### 5. Next Steps and Progression
- **PII Detection Plugin:** Dataiku has a plugin that can automatically scan datasets and suggest columns that may contain PII (e.g., by recognizing email or phone number patterns). This can help you find sensitive data you might have missed.
- **Column-level Tagging (Meanings):** For more granularity, you can use the **Meanings** feature on a dataset to tag individual *columns* with their business meaning, including PII types.

### 6. Common Challenges and Solutions
- **Challenge:** "It's a lot of work to manually tag all our old datasets."
- **Solution:** It can be. Prioritize your efforts. Start by tagging the data in your most critical and sensitive new projects. You can then work backward. You can also use the Python API to write a script that does bulk tagging based on dataset names or schemas.
- **Challenge:** "What if a user forgets to tag a dataset containing PII?"
- **Solution:** This is a process and training issue. Make data classification a mandatory step in your project development checklist. Additionally, running the automated PII detection plugin periodically can help you find untagged sensitive data.
`},{id:556,slug:"using-api-to-automate-governance-checks",question:"How to get started with using Dataiku’s API to automate governance checks and policy enforcement.",answer:`
### 1. Introduction/Overview
As Dataiku usage scales, manually checking every project for compliance with your governance policies becomes impossible. By using the **Dataiku Python API** in an automated **Scenario**, you can create a "governance bot" that programmatically scans your entire instance and reports on policy violations.

### 2. Prerequisites
- **A clear, defined governance policy:** You need to know the rules you want to check (e.g., "All projects must have a description," "All datasets containing PII must be in a restricted project").
- **A dedicated "Governance" project** in Dataiku to house your checking logic.
- **An API key** for a service account with administrator-level permissions (required to read all projects' metadata).

### 3. Step-by-Step Instructions
1.  **Create a "Governance" Scenario:** In your dedicated governance project, create a new scenario (e.g., \`Run_Weekly_Governance_Checks\`).
2.  **Add a Python Step:** Add a single **Execute Python code** step. This is where your checking logic will live.
3.  **Write the Governance Script:** In the Python step, write a script that uses the API to check your policies.
    *   **Get a client handle:** \`client = dataiku.api_client(api_key=...)\`
    *   **Get all projects:** \`projects = client.list_projects()\`
    *   **Loop and check:** Loop through each project and perform your checks.
    > \`\`\`python
    > # Example: Check for projects missing descriptions
    > projects_without_desc = []
    > for proj in projects:
    >     project_handle = client.get_project(proj['projectKey'])
    >     metadata = project_handle.get_metadata()
    >     if not metadata.get('description'):
    >         projects_without_desc.append(proj['projectKey'])
    > # The output of your check is a list of non-compliant projects
    > \`\`\`
4.  **Create a Report:**
    *   Take the list of non-compliant projects from your script and write it to a **Dataiku dataset**. This dataset is your governance report.
5.  **Add Alerting:**
    *   In your scenario, add a **Reporter** that triggers **On completion**.
    *   Configure it to email the governance report dataset to the platform administrators or governance team.
6.  **Schedule the Scenario:** Schedule the scenario to run periodically (e.g., weekly).

### 4. Resources and Tools
- **Dataiku Python API:** The key to programmatically accessing the metadata of all projects and datasets.
- **Python Scenario Step:** The environment for your governance script.
- **Reporters:** The tool for distributing the final governance report.

### 5. Next Steps and Progression
- **More Advanced Checks:** You can write much more complex checks.
    *   Find all datasets tagged \`PII\` and check if their project permissions are sufficiently restricted.
    *   Find all projects that haven't been modified in over a year and flag them for archiving.
    *   Find all code environments that are using outdated, insecure library versions.
- **Automated Remediation:** For some checks, you could even have the script take automated action, such as automatically applying a "missing_description" tag to a non-compliant project.

### 6. Common Challenges and Solutions
- **Challenge:** The API script is complex to write.
- **Solution:** Start simple. Your first governance check could be a very simple one, like finding projects with no description. As you get more comfortable with the API, you can add more sophisticated checks. Refer to the Dataiku API documentation for examples.
- **Challenge:** "Permission Denied" error when running the script.
- **Solution:** The API key used to run the script needs broad permissions to be able to read the metadata from all projects. It's best practice to use a dedicated service account with administrator privileges for this task.
`},{id:557,slug:"implementing-role-based-access-control-rbac",question:"How to get started with implementing role-based access control (RBAC) in Dataiku for different teams.",answer:`
### 1. Introduction/Overview
Role-Based Access Control (RBAC) is a security paradigm that manages access based on a user's role within an organization. In Dataiku, this is the primary security model. It involves defining **Groups** that represent roles (e.g., "Data Scientists") and then assigning permissions to these groups for specific **Projects**. This is far more scalable and manageable than assigning permissions to individual users.

### 2. Prerequisites
- **Clearly defined roles and teams** in your organization (e.g., Marketing, Finance, Data Engineering).
- **A Dataiku administrator** to create and manage the groups.
- **Project owners** to manage permissions on their own projects.

### 3. Step-by-Step Instructions: The RBAC Workflow

1.  **Define Roles and Map to Groups (Admin Task):**
    *   Work with business leaders to define the key user roles.
    *   An administrator then goes to **Administration > Security > Groups** and creates a group for each role (e.g., \`marketing_analysts\`, \`finance_consumers\`, \`data_engineers\`).
    *   The admin then adds the relevant users to these groups.
2.  **Grant Project Access Based on Role (Project Owner Task):**
    *   The owner of a specific project (e.g., the "Sales Reporting" project) goes to their project's **Settings > Permissions**.
    *   They add the groups that need access.
    *   They assign the appropriate permission level (the "role" for that project).
        *   The \`finance_consumers\` group might get **Reader** access.
        *   The \`data_analysts\` group that maintains the project would get **Contributor** access.
3.  **How It Works:**
    *   A user's effective permissions are the sum of the permissions of all the groups they belong to.
    *   When a user tries to access a project, Dataiku checks if any of their groups have been granted access to that project. If not, they can't even see it.

### 4. Resources and Tools
- **Groups:** The core component for representing roles.
- **Project Permissions:** The UI for implementing the access control rules.

### 5. Next Steps and Progression
- **Least Privilege:** A core principle of RBAC. Always grant the minimum level of permission needed for a role to do its job.
- **Connection-Level RBAC:** An administrator can also apply RBAC to the underlying data connections, ensuring that, for example, only the \`data_engineers\` group can write to the production data warehouse.
- **Review and Audit:** Periodically review group memberships and project permissions to ensure they are up-to-date with any organizational changes.

### 6. Common Challenges and Solutions
- **Challenge:** "A user needs access to Project A and Project B, but only read access to A and write access to B."
- **Solution:** This is exactly what RBAC is designed for. You would add their group to Project A with "Reader" permissions and to Project B with "Contributor" permissions. Permissions are specific to each project.
- **Challenge:** "Managing hundreds of individual user permissions is a nightmare."
- **Solution:** You should almost never be assigning permissions to individual users. This is an anti-pattern. You should *only* assign permissions to groups. User management then becomes as simple as adding or removing users from the appropriate groups in one central place.
`},{id:558,slug:"maintaining-documentation-catalogs-for-audits",question:"How to get started with maintaining documentation and catalogs for regulatory audits.",answer:`
### 1. Introduction/Overview
For regulatory audits (like those for GDPR, SOX, or HIPAA), you must be able to produce clear, reliable documentation that proves your data pipelines are compliant and your results are trustworthy. In Dataiku, this involves maintaining a combination of narrative documentation, object-level metadata, and leveraging the platform's automatic lineage generation.

### 2. Prerequisites
- **A Dataiku project** subject to audit.
- **An understanding of the specific audit requirements.**

### 3. Step-by-Step Instructions: Building an Audit-Ready Project

1.  **Create the Master Document (Project Wiki):**
    *   Use the **Project Wiki** as your central audit document.
    *   Create a page titled "Compliance Documentation".
    *   On this page, describe in plain English how the project's design and logic adhere to the relevant regulations. For each clause of the regulation, explain the specific control you've implemented.
2.  **Maintain a Data Dictionary:**
    *   For your key datasets, especially those containing sensitive or critical data, you must document the meaning of each column.
    *   Do this in the dataset's **Settings > Schema** tab by adding a **description for each column**. This is your data dictionary.
3.  **Annotate Everything (Metadata):**
    *   Use **Tags** to classify all your datasets (e.g., \`PII\`, \`Source:SAP\`, \`Status:Validated\`).
    *   Use the **Description** field on every single recipe and dataset to explain its purpose. This creates a self-documenting flow that is easy for an auditor to understand.
4.  **Leverage Automatic Lineage:**
    *   When an auditor asks, "Show me exactly how this number on the final report was calculated," your answer is the **Lineage graph**.
    *   Open the final dataset, go to the **Lineage** tab, and select the column in question.
    *   You can export a screenshot of this graph as definitive, unambiguous proof of the data's entire transformation journey.
5.  **Use Git for Change Management:**
    *   Connect your project to Git. This creates an immutable, time-stamped audit trail of every single change made to your pipeline's logic, including who made the change and why (via the commit message).

### 4. Resources and Tools
- **Project Wiki:** For narrative documentation.
- **Column Descriptions:** For the data dictionary.
- **Tags & Descriptions:** For object-level metadata.
- **Lineage Graph:** For visual proof of data provenance.
- **Git Integration:** For an immutable change log.

### 5. Next Steps and Progression
- **Automated Documentation:** Write a Python scenario that uses the API to extract all this metadata and automatically generate a formatted PDF report suitable for handing to an auditor.
- **Formal Sign-offs:** For the most critical projects, use Dataiku's sign-off features to create a formal, auditable record of who reviewed and approved the project for production.

### 6. Common Challenges and Solutions
- **Challenge:** "We have an audit next week, and we have no documentation."
- **Solution:** This is a difficult situation. You will have to work backward. The lineage graph will still exist automatically, which is a huge help. You will then need to go through the flow object by object and add the necessary descriptions and tags. This highlights the importance of making documentation part of your day-to-day process, not an afterthought.
`},{id:559,slug:"collaborating-with-security-compliance-teams",question:"How to get started with collaborating with security and compliance teams to align Dataiku configurations with company policies.",answer:`
### 1. Introduction/Overview
Effective collaboration between data teams and security/compliance teams is essential for building a governed and trusted analytics platform. As a Dataiku SME or platform owner, your role is to proactively engage with these teams, understand their requirements, and demonstrate how Dataiku's features can be used to enforce company policies.

### 2. Prerequisites
- **A Dataiku instance.**
- **A designated contact** in your corporate security and/or compliance departments.

### 3. Step-by-Step Instructions: A Collaboration Framework

1.  **Schedule a "Dataiku for Security" Kickoff Session:**
    *   Be proactive. Don't wait for them to come to you with a problem.
    *   Schedule a meeting to introduce them to Dataiku.
2.  **Speak Their Language:** Don't talk about recipes and datasets. Talk about risks and controls. Frame the discussion around their concerns.
3.  **Demonstrate Key Governance Features:** In the kickoff session, give them a live demo of the specific Dataiku features that address their needs:
    *   **For Access Control:** Show them how **Project Permissions** and **Groups** are used to implement Role-Based Access Control.
    *   **For Data Security:** Show them how **Connections** are configured to use secure credentials (like IAM roles or Key Vaults) and how sensitive data can be masked with **Prepare** recipes.
    *   **For Auditing:** Show them the **Global Audit Log** and the **Project Timeline**. Explain how **Git integration** provides an immutable change log.
    *   **For Data Classification:** Show them how **Tags** are used to identify PII and other sensitive data.
4.  **Establish a Shared Governance Model:**
    *   Work with them to create a standard **Tagging Taxonomy** for data sensitivity.
    *   Collaborate on defining the standard user **Groups** and their permissions.
    *   Document these agreed-upon standards in a central, shared Wiki.
5.  **Provide Read-Only Access:**
    *   Give the security and compliance teams read-only administrator access to the Dataiku instance. This allows them to independently view logs and audit project permissions without being able to make changes.

### 4. Resources and Tools
- **Live Demos:** The most effective way to communicate technical capabilities.
- **A Shared Wiki:** The place to document your joint governance standards.
- **Read-only Admin Access:** Empowers the security team and builds trust.

### 5. Next Steps and Progression
- **Regular Check-ins:** Schedule a quarterly meeting to review the platform's usage, discuss any new security features in the latest Dataiku release, and address any new compliance requirements.
- **Automated Reporting:** Build a governance dashboard in Dataiku that reports on compliance with your shared standards (e.g., "Number of projects missing an owner tag"). Automate the delivery of this report to the compliance team.

### 6. Common Challenges and Solutions
- **Challenge:** "The security team wants to lock everything down, which makes it hard for users to get their work done."
- **Solution:** This is a classic negotiation. Your job is to show them how you can meet the *intent* of their security control in a way that is less restrictive. For example, instead of denying all access to a database, you can use Dataiku to provide access to a sanitized, anonymized version of the data.
- **Challenge:** "They don't understand the platform and are asking for features that don't make sense."
- **Solution:** This points to a need for more education. Offer to run a more detailed training session for their team. The more they understand how Dataiku works, the more productive your collaboration will be.
`},{id:560,slug:"enabling-collaborative-development-with-project-flow-sharing",question:"How to get started with enabling collaborative development in Dataiku using project and Flow sharing.",answer:`
### 1. Introduction/Overview
Dataiku is designed from the ground up for team collaboration. It provides a shared, visual workspace where multiple developers, analysts, and business users can work together on the same project. The key to enabling this is understanding how to use shared projects and the visual Flow as a common language.

### 2. Prerequisites
- **A team of two or more users.**
- **A shared Dataiku project.**

### 3. Step-by-Step Instructions: Collaboration Patterns

1.  **Use a Shared Project:**
    *   The foundation of collaboration is a single project where all team members are added as **Contributors**. This gives them permission to create and edit objects.
2.  **The Flow as a Shared Language:**
    *   The **Visual Flow** is the central canvas where all work is visible. A data analyst can create a visual recipe, and a data scientist can immediately see it, use its output, and build a model on it. The Flow makes the work of others discoverable and transparent.
3.  **Use Asynchronous Communication Tools:**
    *   **Descriptions:** The most important habit. Every team member should write a clear description on every dataset and recipe they create. This explains the "why" behind their work to their colleagues.
    *   **Discussions:** Use the "Discussions" feature on any object to ask a question or leave a comment for a specific teammate. For example, you can @-mention a colleague on a dataset and ask, "Is this data ready for me to use?"
4.  **Manage Concurrent Work with Git:**
    *   For technical teams, **Git integration** is essential to prevent conflicts.
    *   Each developer should work on their own **branch**. This isolates their changes.
    *   When work is complete, they use a **Pull Request** to merge their changes back into the main branch, allowing for a code review by a peer.

### 4. Resources and Tools
- **Shared Projects with Contributor permissions.**
- **The Visual Flow.**
- **Discussions and Descriptions.**
- **Git Integration for parallel development.**

### 5. Next Steps and Progression
- **Paired Sessions:** Encourage team members to have short, paired sessions where they share their screen and build a part of the flow together. This is a very effective way to share knowledge.
- **Project Wiki:** Use the Wiki to document the project's goals, key decisions, and meeting notes, creating a single source of truth for the entire team.
- **Dataiku Apps:** A data scientist can build a model, and then a business analyst can build a user-friendly Dataiku App on top of it, creating a collaborative handoff to business users.

### 6. Common Challenges and Solutions
- **Challenge:** "Two developers edited the same recipe at the same time and created a conflict."
- **Solution:** This is exactly what Git branching is designed to prevent. If you are not using Git, this requires good communication and a clear division of tasks. With Git, a merge conflict will be created, and the developers will need to work together to resolve it.
- **Challenge:** "I don't know what my teammate's recipe does."
- **Solution:** This is a documentation problem. Your team needs to enforce the discipline of writing a clear description on every object. If you find an undocumented recipe, ask the owner to add a description.
`},{id:561,slug:"setting-up-code-review-processes-for-python-r-scripts",question:"How to get started with setting up code review processes for Dataiku Python/R scripts (e.g., via pull requests).",answer:`
### 1. Introduction/Overview
Code review is a critical practice where a team member reviews another's code to check for correctness, style, and adherence to best practices. It's one of the most effective ways to improve code quality and share knowledge. For Dataiku code recipes, the standard way to facilitate this is by using **Git integration** and **Pull Requests**.

### 2. Prerequisites
- **A Dataiku project connected to a Git repository** (e.g., on GitHub, GitLab).
- **A team of two or more developers.**
- **A defined branching strategy** (e.g., feature branching).

### 3. Step-by-Step Instructions: The Code Review Workflow
1.  **Create a Feature Branch:**
    *   The developer who is writing the code must start by creating a new Git branch for their work (e.g., \`feature/new-python-parser\`). They make all their changes on this branch.
2.  **Complete the Code and Commit:**
    *   The developer writes their Python or R recipe.
    *   They commit their changes to their feature branch with clear commit messages.
3.  **Open a Pull Request (PR):**
    *   When the code is ready for review, the developer pushes their branch to the remote repository.
    *   They then go to the Git provider's web UI (e.g., GitHub) and open a new **Pull Request**.
    *   The PR's purpose is to merge their feature branch into the main development branch.
    *   In the PR description, they should explain what the code does and how to test it. They then assign a teammate as a **reviewer**.
4.  **Perform the Review:**
    *   The reviewer receives a notification.
    *   In the PR interface, they can see a "diff" of the exact code changes.
    *   They review the code for:
        *   **Correctness:** Does it do what it's supposed to do?
        *   **Style:** Does it follow the team's coding style guide?
        *   **Maintainability:** Is the code clear, readable, and well-commented?
        *   **Error Handling:** Does it handle potential errors gracefully?
    *   The reviewer can leave comments on specific lines of code to ask questions or suggest improvements.
5.  **Iterate and Approve:**
    *   The original developer makes any necessary changes based on the feedback and pushes them to the branch.
    *   Once the reviewer is satisfied, they **approve** the Pull Request.
6.  **Merge:** After approval, the feature branch is merged into the main branch, and the new code becomes part of the project.

### 4. Resources and Tools
- **Git Integration in Dataiku:** For managing branches and commits.
- **Pull Request Feature (on GitHub, GitLab, etc.):** The core tool for the review process.
- **A written code review checklist** in your team's Wiki can help ensure reviews are consistent.

### 5. Next Steps and Progression
- **Required Status Checks:** You can configure your repository to require that automated tests (run by a CI pipeline) must pass before a PR can be merged.
- **Branch Protection Rules:** Protect your main branch so that no one can merge a PR without at least one approval.

### 6. Common Challenges and Solutions
- **Challenge:** "Code reviews are slowing us down."
- **Solution:** Keep Pull Requests small and focused on a single logical change. A PR that changes hundreds of lines is very difficult and slow to review. A small, focused PR can be reviewed and merged quickly.
- **Challenge:** "The feedback in my review feels personal or harsh."
- **Solution:** Teams need to foster a culture of constructive, blameless code reviews. Feedback should always be about the code, not the person. A good practice is to phrase suggestions as questions: "Have you considered using a dictionary here? It might be more readable."
`},{id:562,slug:"creating-project-templates-shared-libraries",question:"How to get started with creating Dataiku project templates and shared libraries for reuse across teams.",answer:`
### 1. Introduction/Overview
To scale analytics and ensure consistency, it's crucial to make common tasks and components reusable. Dataiku supports this through two main features: **Project Templates**, for reusing project structures, and **Shared Libraries**, for reusing code.

### 2. Prerequisites
- **An identified need for reuse:** You have noticed that teams are repeatedly building the same kinds of flows or writing the same kinds of functions.
- **Permissions to create projects.**

### 3. Step-by-Step Instructions

#### Part 1: Creating a Project Template
1.  **Identify a Standard Project Structure:** Look at your existing projects. What is a common, successful structure? This usually involves a standard set of **Flow Zones** (e.g., Ingestion, Preparation, Output).
2.  **Build the Template Project:**
    *   Create a new, blank project named \`TEMPLATE_Standard_Analytics\`.
    *   In this project, create the empty Flow Zones.
    *   Create a standard **Wiki** structure with placeholder pages.
    *   Do **not** add any project-specific data or recipes.
3.  **Using the Template:** When a team needs to start a new project, they find the template project, click the "..." menu, and **Duplicate** it. They now have a new project with your best-practice structure already in place.

#### Part 2: Creating a Shared Code Library
1.  **Identify Reusable Code:** Find a Python or R function that is used in multiple different projects (e.g., a function to parse a specific internal file format).
2.  **Create a "Library" Project:**
    *   Create a new project named \`SHARED_CODE_LIBRARY\`.
    *   In this project's **... > Libraries** section, create a new script file (e.g., \`company_utils.py\`).
    *   Place your reusable function inside this script.
3.  **Using the Shared Library:**
    *   Now, in any *other* project, go to **Settings > Dependencies**.
    *   Add a dependency on the \`SHARED_CODE_LIBRARY\` project.
    *   Once the dependency is added, any Python recipe in your current project can now import and use the function:
    > \`\`\`python
    > from company_utils import my_reusable_function
    > \`\`\`
### 4. Resources and Tools
- **The "Duplicate project" feature.**
- **The Project Library feature.**
- **Project Dependencies settings.**

### 5. Next Steps and Progression
- **Versioning the Library:** The shared library project should be connected to Git. This allows you to version your shared code. Be careful, as changes to the shared library can affect all projects that depend on it.
- **Creating a Plugin:** For the ultimate reusability, a developer can package the shared logic into a custom Dataiku plugin, which can provide a visual recipe for the code.

### 6. Common Challenges and Solutions
- **Challenge:** A change to the shared library broke another project.
- **Solution:** This is the primary risk of shared code. It requires careful testing and communication. Before changing a function in the shared library, you must understand which projects depend on it and test to ensure your change is backward-compatible.
- **Challenge:** "How do I find out what reusable components are available?"
- **Solution:** The shared library project must be well-documented. Its Wiki should serve as a catalog, clearly explaining what functions are available and how to use them.
`},{id:563,slug:"using-flow-view-bookmarks-to-communicate-pipeline-design",question:"How to get started with using the Dataiku Flow view and bookmarks to communicate pipeline design to stakeholders.",answer:`
### 1. Introduction/Overview
The visual Flow is one of Dataiku's most powerful communication tools. It provides an intuitive, high-level map of your data pipeline that is understandable by both technical and non-technical audiences. Using features like Flow Zones and Bookmarks effectively can turn your Flow into a powerful presentation tool.

### 2. Prerequisites
- **A Dataiku project with a Flow.**
- **A need to present or explain your pipeline** to colleagues or stakeholders.

### 3. Step-by-Step Instructions

1.  **Organize Your Flow:**
    *   A clean Flow is a prerequisite for clear communication.
    *   Group your recipes and datasets into logical **Flow Zones** (e.g., "Data Ingestion," "Data Preparation," "Modeling," "Final Outputs").
    *   Give your datasets and recipes clear, descriptive names.
2.  **Add Documentation to the Flow:**
    *   Use the **Description** field on your key datasets and recipes. This text appears when you hover over the object, providing instant context.
    *   Drag **Text Boxes** onto the Flow canvas to act as large section headers or comments.
3.  **Create Bookmarks for a Guided Tour:**
    *   Bookmarks allow you to save specific views of your Flow (a certain zoom level and position) that you can jump back to.
    *   Zoom in on the first important part of your flow (e.g., the Ingestion zone).
    *   In the bottom-right menu of the Flow, click the **Bookmarks** icon and **Add new bookmark**. Name it "1. Data Sources".
    *   Pan to the next logical section, and create another bookmark named "2. Cleaning and Joining".
    *   Continue this for all the key stages of your pipeline.
4.  **Presenting with Bookmarks:**
    *   During your presentation, instead of manually panning and zooming (which can be disorienting for the audience), simply click through your saved bookmarks.
    *   This creates a smooth, guided tour of your pipeline, allowing you to explain each stage clearly.

### 4. Resources and Tools
- **Flow Zones:** For high-level organization.
- **Descriptions and Text Boxes:** For adding context.
- **Bookmarks:** The tool for creating your guided presentation.

### 5. Next Steps and Progression
- **Collapse Zones:** For a very high-level executive overview, collapse all your Flow Zones. This will show a simple, high-level diagram of how your major stages connect, which can be understood in seconds.
- **Share a Link:** You can share a direct URL to your Dataiku project with stakeholders (if they have read access), and they can explore the Flow themselves.

### 6. Common Challenges and Solutions
- **Challenge:** "My Flow is too messy to show anyone."
- **Solution:** Take 15 minutes to clean it up. Use the "Arrange" button to automatically tidy the layout within each Flow Zone. A clean Flow signals a well-organized and professional project.
- **Challenge:** "The stakeholders are getting lost in the technical details."
- **Solution:** You are probably showing them too much detail. Use the zone collapsing and bookmarks to control the narrative. Focus their attention on the parts of the flow that are relevant to the business logic, and gloss over the purely technical steps.
`},{id:564,slug:"integrating-project-management-with-jira-trello",question:"How to get started with integrating Dataiku project management with tools like JIRA or Trello for tracking tasks.",answer:`
### 1. Introduction/Overview
While Dataiku has a built-in "TODO" list, most enterprise teams use a dedicated project management tool like JIRA or Trello. You can integrate Dataiku with these tools to link your development work directly to your team's tasks and user stories. This integration is typically done via the tools' REST APIs.

### 2. Prerequisites
- **A project management tool** (JIRA, Trello, etc.) with API access.
- **An API key or token** for that tool.
- **A Dataiku project.**

### 3. Step-by-Step Instructions: One-Way Integration (Dataiku -> JIRA)

This common pattern involves updating a JIRA ticket from a Dataiku scenario.

1.  **Store Credentials:** In your Dataiku project, store your JIRA username and API token securely as **Project Variables** of the "Password" type.
2.  **Create a Scenario:** Create a Dataiku scenario that performs an action you want to track (e.g., a model retraining scenario).
3.  **Add a Python Step:** At the end of the scenario, add a **Execute Python code** step.
4.  **Write the API Call Script:**
    *   The script will use the \`requests\` library to make a call to the JIRA REST API.
    *   It can add a comment to a specific JIRA ticket to indicate that the Dataiku job has run.
    > \`\`\`python
    > import dataiku
    > import requests
    >
    > # Get JIRA details from variables
    > variables = dataiku.get_custom_variables()
    > JIRA_URL = "https://mycompany.atlassian.net"
    > JIRA_TICKET = "PROJ-123"
    > JIRA_USER = variables.get("JIRA_USER")
    > JIRA_TOKEN = variables.get("JIRA_TOKEN")
    >
    > # The comment to add
    > comment_text = f"Weekly model retraining scenario completed successfully. Job URL: {dataiku.get_custom_variables().get('jobURL')}"
    >
    > # Make the API call to add a comment
    > requests.post(
    >     f"{JIRA_URL}/rest/api/2/issue/{JIRA_TICKET}/comment",
    >     auth=(JIRA_USER, JIRA_TOKEN),
    >     json={"body": comment_text}
    > )
    > \`\`\`

### 4. Resources and Tools
- **REST APIs** for your project management tool (JIRA, Trello, etc.).
- **Python Scenario Step:** The environment for your integration script.
- **\`requests\` library:** For making the API calls.

### 5. Next Steps and Progression
- **Two-Way Integration:** A more advanced integration could involve a Dataiku webapp that allows a user to select a JIRA story, and then have Dataiku use the API to pull requirements from that story to configure a pipeline.
- **CI/CD Integration:** In your Git commit messages in Dataiku, include the JIRA ticket number (e.g., "PROJ-123 Add new feature"). Many CI/CD tools can be configured to automatically link the commit to the JIRA ticket, providing full traceability.

### 6. Common Challenges and Solutions
- **Challenge:** "The API call to JIRA is failing."
- **Solution:** Check your credentials and the JIRA API endpoint URL. The JIRA REST API can be complex, so refer to their official documentation carefully. Use a tool like Postman to test your API calls before scripting them.
- **Challenge:** "This seems like a lot of custom work."
- **Solution:** It is. Native, out-of-the-box integration for these tools is not a core Dataiku feature. This custom API-based integration should only be built if there is a strong business need for it. For many teams, simply referencing JIRA ticket numbers in Git commit messages and project descriptions is a sufficient level of integration.
`},{id:565,slug:"structuring-projects-for-handover-between-scientists-engineers",question:"How to get started with structuring Dataiku projects to hand off between data scientists and MLOps engineers.",answer:`
### 1. Introduction/Overview
A smooth handover between the data scientist who builds a model and the MLOps engineer who productionizes it is crucial for an efficient MLOps lifecycle. Structuring your Dataiku project with clear separation of concerns using **Flow Zones** is the key to making this handover seamless.

### 2. Prerequisites
- A project involving both data science (modeling) and MLOps (deployment, automation).
- A team with distinct Data Scientist and MLOps Engineer roles.

### 3. Step-by-Step Instructions: A Zoned Workflow
1.  **Define Clear Roles and Responsibilities:**
    *   **Data Scientist:** Responsible for data exploration, feature engineering, and model training. Their "product" is a high-performing, validated, and saved model.
    *   **MLOps Engineer:** Responsible for everything that happens *after* the model is created: deploying it as an API, setting up automated retraining, and monitoring its performance in production.
2.  **Structure the Project with Flow Zones:** Create a Flow that reflects this separation of responsibility.
    *   **Zone 1-3 (Data Scientist's Workspace):**
        *   \`1_Ingestion\`: Raw data.
        *   \`2_Data_Preparation\`: Cleaning and joining.
        *   \`3_Feature_Engineering_and_Modeling\`: The data scientist works here to create features and use the **Visual ML Lab** to train models. The final output of this zone is a **Saved Model** object.
    *   **Zone 4 (The Handover Point):**
        *   The **Saved Model** itself acts as the formal handover point.
    *   **Zone 5-6 (MLOps Engineer's Workspace):**
        *   \`5_Deployment\`: The MLOps engineer takes the Saved Model as input and builds the deployment pipeline. This might include a **Score** recipe for batch scoring or deploying to the **API Deployer** for real-time.
        *   \`6_Monitoring_and_Automation\`: The MLOps engineer creates the **Scenarios** for automated retraining, performance monitoring, and alerting.
3.  **The Handover Process:**
    *   When the data scientist has produced a final, validated Saved Model, they notify the MLOps engineer.
    *   The MLOps engineer can then start their work in the downstream zones, confident that the upstream modeling part is complete.

### 4. Resources and Tools
- **Flow Zones:** The essential tool for creating a visual separation of concerns.
- **The Saved Model object:** Acts as the formal, versioned artifact that is handed over from the data scientist to the MLOps engineer.

### 5. Next Steps and Progression
- **Git and Pull Requests:** The handover can be formalized with Git. The data scientist could complete their work on a feature branch. The MLOps engineer would then review the branch via a Pull Request before it's merged, ensuring the model is ready for production.
- **Shared Code Libraries:** Both roles can contribute to and use a shared library of Python functions for common tasks.

### 6. Common Challenges and Solutions
- **Challenge:** The model works in the lab, but fails when the MLOps engineer tries to deploy it.
- **Solution:** This often points to an environment issue. The data scientist might have used a specific library on their local machine that wasn't declared in the project's code environment. The MLOps engineer must ensure the production environment (e.g., the API node) is an exact replica of the training environment.
- **Challenge:** The data scientist and MLOps engineer are "throwing work over the wall" at each other.
- **Solution:** While the zones create a separation of concerns, the two roles must still collaborate. The MLOps engineer should be involved early in the project to advise on production constraints. The data scientist should remain involved to help interpret the model's performance once it's in production.
`},{id:566,slug:"using-notebooks-for-collaborative-exploration",question:"How to get started with using Dataiku notebooks for collaborative data exploration and sharing results.",answer:`
### 1. Introduction/Overview
Dataiku's integrated Jupyter notebooks are a powerful tool for exploratory data analysis (EDA), prototyping, and collaboration. They allow multiple team members to share code, visualizations, and narrative text in a single, interactive document.

### 2. Prerequisites
- A Dataiku project.
- A dataset to explore.
- Basic knowledge of Python and libraries like Pandas and Matplotlib.

### 3. Step-by-Step Instructions
1.  **Create a New Notebook:**
    *   In your project, navigate to the **Notebooks** tab.
    *   Click **+ NEW NOTEBOOK** and choose a language (e.g., Python).
2.  **Load Data:** In the first code cell, use the Dataiku API to load a sample of your dataset into a Pandas DataFrame.
    > \`import dataiku\`
    > \`df = dataiku.Dataset("my_data").get_dataframe()\`
3.  **Explore and Visualize:**
    *   Use subsequent cells to explore the data using Pandas (\`df.describe()\`, \`df.head()\`).
    *   Use libraries like Matplotlib or Seaborn to create visualizations. The plots will be displayed inline directly in the notebook.
4.  **Document with Markdown:**
    *   Switch a cell's type from "Code" to "Markdown".
    *   Use these cells to write your commentary, explain your thought process, and summarize your findings. This turns your notebook from a simple script into a narrative report.
5.  **Share with Colleagues:**
    *   Other contributors to the project can open the same notebook. They will see your code, your markdown text, and the saved outputs of your code cells.
    *   They can add their own code or comments, making it a collaborative document.
6.  **Publish Insights:**
    *   From a notebook cell, you can "publish" a chart or a dataset to the Flow or a Dashboard. This allows you to easily share a key finding from your exploration with a wider audience.

### 4. Resources and Tools
- **Jupyter Notebooks:** The interactive environment.
- **Markdown Cells:** For adding narrative and documentation.
- **The "Publish" Feature:** For sharing insights from the notebook.

### 5. Next Steps and Progression
- **Versioning:** If your project is connected to Git, your notebooks (\`.ipynb\` files) will be version-controlled, so you can track changes over time.
- **"Productionizing" a Notebook:** Once you have finalized a piece of analysis in a notebook, the best practice is to refactor the clean code into a reusable **Python recipe** to make it a formal part of your pipeline.

### 6. Common Challenges and Solutions
- **Challenge:** My notebook is a mess of out-of-order cells.
- **Solution:** It's a good practice to periodically use the **Kernel > Restart & Run All** command. This runs all your cells from top to bottom, ensuring your logic is reproducible and sequential.
- **Challenge:** A colleague can't see the output of my code.
- **Solution:** You need to make sure you **save the notebook** after running the cells. The outputs are saved as part of the notebook's JSON structure.
`},{id:567,slug:"managing-concurrent-changes-to-avoid-conflicts",question:"How to get started with managing concurrent changes in Dataiku (e.g., branches or shared projects) to avoid conflicts.",answer:`
### 1. Introduction/Overview
When multiple developers work on the same project, they can inadvertently overwrite each other's changes. This is known as a "conflict." Managing concurrent changes is a fundamental challenge in team-based development. The industry-standard solution, and the best practice in Dataiku, is to use **Git and a feature branching workflow**.

### 2. Prerequisites
- **A team of two or more developers.**
- **A Dataiku project connected to a remote Git repository.**

### 3. Step-by-Step Instructions: The Git Branching Workflow
1.  **Isolate Your Work in a Branch:**
    *   **Never work directly on the \`main\` branch.**
    *   Before starting a new task, each developer must go to the **Git** page in Dataiku and **create a new branch** for their work (e.g., \`dave/fix-join-logic\`).
    *   This branch is a personal, isolated copy of the project.
2.  **Commit Changes to Your Branch:**
    *   As the developer makes changes, they commit them frequently to their own branch. These commits are not visible to their teammates yet.
3.  **Stay Up-to-Date:**
    *   Periodically, the developer should **pull** the latest changes from the shared \`main\` branch into their feature branch. This integrates their colleagues' completed work and helps to resolve any potential conflicts early and in small chunks.
4.  **Merge with a Pull Request:**
    *   When the developer's feature is complete, they push their branch and open a **Pull Request (PR)** on the Git provider's website (e.g., GitHub).
    *   The PR is a request to merge their isolated changes back into the main project. It's also a formal code review process.
5.  **Handle Merge Conflicts:**
    *   If the PR has a **merge conflict** (meaning another developer changed the *exact same lines* in the same recipe on the main branch), it cannot be merged automatically.
    *   Dataiku provides a **visual diff and merge tool**. The developer must use this tool to look at the two conflicting versions and manually choose which changes to keep.
    *   Once the conflict is resolved, the PR can be merged.

### 4. Resources and Tools
- **Git Integration:** The core technology for this workflow.
- **Branches:** The mechanism for isolating work.
- **Pull Requests:** The process for controlled merging and code review.
- **Dataiku's Visual Diff and Merge Tool:** For resolving conflicts.

### 5. Next Steps and Progression
- **Good Communication:** While Git provides the technical tools, good old-fashioned communication is still essential. If you know you and a colleague need to work on a similar part of the flow, talk to each other to coordinate your work and minimize the chance of conflicts.

### 6. Common Challenges and Solutions
- **Challenge:** "I'm not using Git. How do I avoid conflicts?"
- **Solution:** Without Git, you must rely entirely on communication and process. You would need to "lock" parts of the flow, where only one person is allowed to work on a specific recipe at a time. This is slow and inefficient. Using Git is strongly recommended for any team of more than one person.
- **Challenge:** "Resolving a merge conflict on a visual recipe is hard."
- **Solution:** It can be. This is why it's a best practice to break down work into small, independent tasks. The smaller the change, the less likely it is to conflict with someone else's work.
`},{id:568,slug:"documenting-projects-wiki-comments",question:"How to get started with documenting Dataiku projects (wiki pages, comments) to align data scientists and engineers.",answer:`
### 1. Introduction/Overview
Clear documentation is the bridge that connects the work of data scientists, who focus on modeling and analysis, and data engineers, who focus on building robust pipelines. Dataiku provides a multi-layered documentation system to ensure that all aspects of a project, from the high-level business goal to the low-level code logic, are clearly explained.

### 2. Prerequisites
- A Dataiku project.
- A team commitment to documenting their work.

### 3. Step-by-Step Instructions: A Documentation Strategy

1.  **For the Business Context: Use the Project Wiki.**
    *   The **Wiki** is the place for long-form, narrative documentation.
    *   Create a "Project Brief" page that explains the business problem, the goals of the project, and the key stakeholders. This ensures everyone understands *why* the project exists.
2.  **For the Pipeline Architecture: Use the Flow Itself.**
    *   **Descriptions:** This is the most important habit. Every single object (dataset, recipe) in the Flow has a **Description** field. Write a clear, one-sentence summary of its purpose. This makes the Flow self-documenting.
    *   **Flow Zones:** Organize the Flow into named zones like "Ingestion", "Data Prep", and "Modeling". This visually communicates the high-level architecture.
3.  **For Column-Level Details: Use the Schema View.**
    *   In a dataset's **Settings > Schema**, you can add a **description to each column**. This creates a comprehensive **Data Dictionary** that explains what each feature represents.
4.  **For Code Logic: Use Comments and Docstrings.**
    *   In any **Python** or **SQL recipe**, use inline comments and function docstrings to explain *how* the code works and the intent behind any complex logic.

### 4. Resources and Tools
- **Project Wiki:** For the high-level narrative.
- **Description Fields:** For object-level context.
- **Column Descriptions:** For the data dictionary.
- **Code Comments:** For detailed logic explanation.

### 5. Next Steps and Progression
- **Create a Documentation Template:** Create a template project that includes a pre-structured Wiki with all the standard sections a good project document should have.
- **Enforce during Reviews:** Make documentation a part of your review process. A pull request shouldn't be approved if the new recipes or datasets are missing descriptions.

### 6. Common Challenges and Solutions
- **Challenge:** "Data scientists and engineers speak different languages."
- **Solution:** Dataiku's visual Flow acts as the common language. A data scientist can look at a data engineer's visual Join recipe and understand its logic without needing to read complex code. Likewise, a data engineer can see the inputs and outputs of a data scientist's "Saved Model" without needing to understand the complex math inside it.
- **Challenge:** "No one wants to write documentation."
- **Solution:** Lead by example and make it a non-negotiable part of your team's "Definition of Done". The time spent writing a good description is a fraction of the time that will be wasted later when someone (possibly your future self) has to reverse-engineer an undocumented pipeline.
`},{id:569,slug:"training-onboarding-team-members-on-best-practices",question:"How to get started with training and onboarding team members on Dataiku best practices and workflows.",answer:`
### 1. Introduction/Overview
A structured onboarding plan is essential for getting new team members up to speed on Dataiku and ensuring they follow your team's best practices from day one. A good plan combines self-paced learning with practical, hands-on exercises and mentorship.

### 2. Prerequisites
- **A new team member.**
- **A designated mentor** or manager.
- **A Dataiku instance** with a sandbox environment.

### 3. Step-by-Step Instructions: A 4-Week Onboarding Plan

**Week 1: The Basics**
- **Goal:** Understand the core concepts of Dataiku.
- **Tasks:**
    1.  **Dataiku Academy:** Assign the **Core Designer** learning path. This is non-negotiable.
    2.  **Sandbox Project:** Create a personal sandbox project for the new hire.
    3.  **First Flow:** Give them a simple task: "Upload this CSV and use a Prepare recipe to remove null values."
    4.  **Daily Check-ins:** The mentor should have a short, daily check-in to answer questions.

**Week 2: Team Standards and First "Real" Task**
- **Goal:** Learn the team's specific ways of working.
- **Tasks:**
    1.  **Review Team Wiki:** Have them read the team's documentation on naming conventions, Git branching strategy, and documentation standards.
    2.  **Paired Task:** Assign a simple, real task. The mentor and new hire should work on it *together* via pair programming. The mentor drives first, then the new hire.
    3.  **Introduce Git:** Show them how to create a branch, commit changes, and open a pull request.

**Week 3: Independent Work and First Review**
- **Goal:** Apply learning to a task independently.
- **Tasks:**
    1.  **Assign a Solo Task:** Give them a small, well-defined user story to complete on their own.
    2.  **First Pull Request:** They will submit their first PR. The mentor performs a thorough but constructive review, providing feedback on code, documentation, and adherence to standards.

**Week 4: Integration and Presentation**
- **Goal:** Solidify knowledge and integrate with the team.
- **Tasks:**
    1.  **Demo:** Have the new hire give a short demo of the feature they built to the rest of the team.
    2.  **Start Contributing:** They should now be ready to start picking up regular tasks from the sprint backlog.

### 4. Resources and Tools
- **Dataiku Academy:** The foundation of the training.
- **A Sandbox Project:** For safe experimentation.
- **A Team Wiki:** To document all your standards and best practices.
- **A Mentor:** The most important resource is a dedicated person to guide them.

### 5. Next Steps and Progression
- **Continued Learning:** Encourage them to pursue advanced Dataiku Academy certifications.
- **Mentoring Others:** Once they become proficient, have them help mentor the next new team member.

### 6. Common Challenges and Solutions
- **Challenge:** The new hire is overwhelmed by the amount of information.
- **Solution:** The structured, week-by-week plan helps to manage this. Reassure them that they are not expected to know everything at once and that the goal is steady progress.
- **Challenge:** The new hire is making mistakes.
- **Solution:** This is expected and is part of learning. The key is to catch these mistakes early during reviews and use them as teaching moments, explaining the best practice and the "why" behind it.
`},{id:570,slug:"exposing-workflows-as-restful-apis",question:"How to get started with exposing Dataiku workflows as RESTful APIs for applications to consume.",answer:`
### 1. Introduction/Overview
Exposing your work as a RESTful API is how you make your models and data logic available to other applications in real-time. Dataiku's **API Deployer** is the dedicated, production-grade service for creating, deploying, and managing these APIs.

### 2. Prerequisites
- **An artifact to deploy:** This is typically a **Saved Model** from your Flow, or a custom **Python function**.
- **Access to an API Deployer instance,** which must be set up by a Dataiku administrator.
- **A clear understanding of the API's purpose:** What data will it take as input, and what will it return?

### 3. Step-by-Step Instructions
1.  **Create the API Service in the Designer:**
    *   From your project Flow, select the asset you want to expose (e.g., a Saved Model).
    *   In the right-hand panel, click **API Designer**.
    *   Click **+ CREATE YOUR FIRST API SERVICE**.
    *   Give your service a name (e.g., \`churn-prediction-service\`).
2.  **Define the Endpoint:**
    *   Inside the service, create a new endpoint.
    *   Choose the endpoint type. For a model, this would be a **Prediction** endpoint.
    *   Point the endpoint to your Saved Model.
    *   You can test the endpoint in the designer's "Test" tab by providing a sample JSON input.
3.  **Deploy the Service:**
    *   Once you are happy with your service definition, click the **Deploy** button.
    *   The API Deployer will package your model and its dependencies and sends it to the API Deployer.
4.  **Use the Live API:**
    *   Navigate to the API Deployer UI. You will see your service deployed and running.
    *   The UI provides the live **endpoint URL** and **code snippets** showing how other applications can call it.
    *   You can now give this information to your application developers to integrate.

### 4. Resources and Tools
- **API Designer:** The UI within a project for defining the API's structure.
- **API Deployer:** The production service that runs and manages the live APIs.
- **Saved Model or Python Function:** The underlying logic that the API exposes.

### 5. Next Steps and Progression
- **Versioning:** You can deploy multiple versions of your model to the same endpoint and use traffic splitting to safely roll out updates.
- **Scaling:** If the API is deployed on Kubernetes, you can easily scale the number of model server replicas to handle high request volumes.
- **Monitoring:** Use the API Deployer's built-in dashboards to monitor the latency, traffic, and error rates of your production APIs.

### 6. Common Challenges and Solutions
- **Challenge:** "My deployment is failing."
- **Solution:** Check the deployment logs in the API Deployer. A common issue is that the code environment used by the model is not present on the API node. The API node needs to have all the same package dependencies as the design node where the model was trained.
- **Challenge:** "The live API is returning an error."
- **Solution:** Check the logs of the API service in the API Deployer UI. This will show the full traceback of any errors that occurred during prediction. A common issue is that the JSON sent by the client application does not match the schema the model is expecting.
`},{id:571,slug:"deploying-api-nodes-in-docker-kubernetes",question:"How to get started with deploying Dataiku API nodes in a Docker or Kubernetes environment for production.",answer:`
### 1. Introduction/Overview
For a scalable and resilient production environment, you should deploy your Dataiku API nodes as containers, orchestrated by Kubernetes. This allows you to easily manage, scale, and update your real-time model scoring services without downtime.

### 2. Prerequisites
- **A Kubernetes cluster** (e.g., EKS, AKS, GKE).
- **A Docker registry** (e.g., Docker Hub, ECR) to store your images.
- **Familiarity with Kubernetes and Helm concepts.**
- **A Dataiku API Deployer license.**

### 3. Step-by-Step Instructions
1.  **What is an API Node?** The API node is a specific component of Dataiku that is optimized for low-latency model serving. It's separate from the main "design" node where you build flows.
2.  **Use the Official Helm Chart:** The recommended way to deploy the API Deployer and its API nodes on Kubernetes is by using the official **Dataiku Helm chart**.
3.  **Configure the Deployment:**
    *   You will create a \`values.yaml\` file to configure the Helm chart.
    *   In this file, you will specify that you want to deploy the \`api-deployer\` and one or more \`api-node\` components.
    *   You will configure the number of **replicas** (pods) for your API nodes, their **resource requests/limits** (CPU/memory), and how they should be exposed (e.g., via a Kubernetes Service of type LoadBalancer).
4.  **Deploy with Helm:**
    *   Run the command \`helm install ...\` with your configuration.
    *   Helm will create the necessary Kubernetes Deployments and Services for your API nodes.
5.  **Deploy a Model:**
    *   In the Dataiku design node, when you deploy an API service, you can now select this Kubernetes-based infrastructure as the deployment target.
    *   The API Deployer will then run your model on the API node pods in your Kubernetes cluster.

### 4. Resources and Tools
- **Dataiku's official Helm chart:** The supported method for K8s deployment.
- **Kubernetes:** The container orchestration platform.
- **The API Deployer:** The Dataiku service that manages the deployments.

### 5. Next Steps and Progression
- **Autoscaling:** Configure a **Horizontal Pod Autoscaler (HPA)** on your API node Deployment. This will automatically increase or decrease the number of pods based on CPU or memory load, allowing your service to handle traffic spikes elastically.
- **Monitoring:** Use tools like Prometheus and Grafana to scrape metrics from your API node pods to monitor performance and resource consumption.
- **Rolling Updates:** When you deploy a new version of a model, Kubernetes will perform a zero-downtime rolling update, gradually replacing the old pods with new ones.

### 6. Common Challenges and Solutions
- **Challenge:** "My model pod is crashing (CrashLoopBackOff)."
- **Solution:** Use \`kubectl logs <pod-name>\` to view the logs from the failing container. This is often caused by an environment issue, for example, the Docker image used for the API node is missing a Python library that your model requires.
- **Challenge:** "How do I know how many replicas I need?"
- **Solution:** Start with a few (e.g., 2-3 for high availability). Then, load test your API endpoint and monitor the CPU usage of the pods. Use this data to configure a Horizontal Pod Autoscaler (HPA) to manage the replica count automatically based on load.
`},{id:572,slug:"securing-apis-with-oauth-api-tokens",question:"How to get started with securing Dataiku APIs using OAuth or API tokens.",answer:`
### 1. Introduction/Overview
When you expose a model as a public API, you must secure it to ensure only authorized applications can access it. Dataiku's API Deployer provides several built-in authentication methods, from simple static API keys to more advanced integration with OAuth 2.0 providers.

### 2. Prerequisites
- **An API service deployed** on the Dataiku API Deployer.
- **Administrator access** to the API Deployer to configure security settings.

### 3. Step-by-Step Instructions

#### Method 1: Static API Keys (Simple and Common)
1.  **Generate a Key:** In the main Dataiku instance, an administrator can generate a static API key and grant it permissions to call a specific API service.
2.  **Configure the API Service:** In the API Deployer, open your deployed service. In its settings, you can add an "Authentication" method and choose **API Key**.
3.  **How Clients Authenticate:** The client application must include the API key in an HTTP header with every request. The standard header is \`Authorization: Bearer YOUR_API_KEY\`. The API Deployer will reject any request without a valid key.

#### Method 2: OAuth 2.0 Integration (Advanced Enterprise Security)
1.  **When to Use:** When you need to integrate with your company's central identity provider (like Okta or Azure Active Directory) and want users or applications to authenticate using standard OAuth 2.0 flows.
2.  **Configure the Identity Provider:** In your OAuth provider's admin console, you will need to register your Dataiku API service as a new application. This will provide you with a \`client_id\` and \`client_secret\`.
3.  **Configure Dataiku:**
    *   In the API Deployer, add a new "Authentication" method and choose **OAuth2**.
    *   Provide the details from your identity provider (e.g., the authorization URL, token URL, client ID, and secret).
4.  **How Clients Authenticate:**
    *   The client application first goes through an OAuth 2.0 flow (e.g., client credentials flow) with your identity provider to get a temporary JWT access token.
    *   The client then calls the Dataiku API, presenting this JWT in the \`Authorization: Bearer YOUR_JWT_TOKEN\` header.
    *   The API Deployer will validate the JWT with the identity provider before allowing the request to proceed.

### 4. Resources and Tools
- **API Deployer Security Settings:** The UI for configuring authentication methods.
- **Your Identity Provider (Okta, Azure AD, etc.):** Where you configure the OAuth application.

### 5. Next Steps and Progression
- **API Gateway:** For even more advanced security patterns like rate limiting, you can place a dedicated API Gateway (like AWS API Gateway) in front of your Dataiku API nodes. The gateway can handle authentication before forwarding the request to Dataiku.

### 6. Common Challenges and Solutions
- **Challenge:** "My client gets a 401 Unauthorized error."
- **Solution:** **For API Keys:** The key is likely invalid, expired, or doesn't have permissions for the service it's trying to call. **For OAuth:** The JWT token is likely expired or invalid. The client needs to refresh the token. There could also be a configuration mismatch between Dataiku and the identity provider (e.g., wrong audience URI).
- **Challenge:** "Where should the client application store the API key?"
- **Solution:** The client application must treat the API key as a secret. It should be stored securely in a secret manager or an environment variable, not hardcoded in its source code.
`},{id:573,slug:"monitoring-api-endpoints-with-prometheus-elk-stack",question:"How to get started with monitoring Dataiku API endpoints using tools like Prometheus or ELK Stack.",answer:`
### 1. Introduction/Overview
For production-grade observability, you'll want to integrate your Dataiku API endpoint monitoring with a centralized stack like Prometheus (for metrics) and the ELK Stack (Elasticsearch, Logstash, Kibana) for logging. This allows you to correlate API performance with other system events and create sophisticated dashboards and alerts.

### 2. Prerequisites
- **A deployed Dataiku API endpoint.**
- **A running Prometheus/Grafana stack and/or ELK stack.**
- **Administrator access** to the Dataiku API node server(s).

### 3. Step-by-Step Instructions

#### Part 1: Monitoring Metrics with Prometheus/Grafana
1.  **Expose Metrics from the API Node:** The Dataiku API node exposes performance metrics (like request count, latency, error codes) via a standard Java protocol called JMX.
2.  **Deploy the JMX Exporter:** You need a **JMX Exporter** agent running alongside your API node. This agent scrapes the JMX metrics and exposes them on a simple HTTP endpoint that Prometheus can read.
3.  **Configure Prometheus to Scrape:** In your Prometheus configuration, add a new "scrape job" that points to the HTTP endpoint of the JMX Exporter for each of your API nodes. Prometheus will now periodically collect and store the metrics.
4.  **Build a Grafana Dashboard:** In Grafana, use Prometheus as a data source. You can now build dashboards with panels that query and visualize your API endpoint's latency, throughput (requests per second), and error rates over time.

#### Part 2: Centralizing Logs with the ELK Stack
1.  **Deploy a Log Shipper:** On each Dataiku API node server, you need to install a log shipping agent, such as **Filebeat**.
2.  **Configure Filebeat:** Configure Filebeat to monitor the log files generated by the Dataiku API node (e.g., \`api-node.log\`).
3.  **Ship Logs to Logstash:** Configure Filebeat to send the log events to your **Logstash** instance. Logstash can be used to parse and enrich the log data (e.g., parsing the JSON log format).
4.  **Index in Elasticsearch:** Logstash then sends the processed logs to **Elasticsearch** for indexing.
5.  **Analyze in Kibana:** You can now use **Kibana** to search, filter, and create dashboards on your centralized API logs. This is extremely powerful for troubleshooting errors.

### 4. Resources and Tools
- **JMX Exporter:** The bridge between Dataiku's Java metrics and Prometheus.
- **Filebeat:** The agent for shipping logs.
- **The ELK Stack / Prometheus & Grafana:** Your central observability platforms.

### 5. Next Steps and Progression
- **Alerting:** Set up alerts in Grafana or Prometheus's Alertmanager. For example, you can create an alert that fires if the API endpoint's p99 latency goes above 500ms, or if the rate of 5xx errors exceeds 1%.

### 6. Common Challenges and Solutions
- **Challenge:** "This setup is very complex."
- **Solution:** It is. This is an enterprise-grade monitoring architecture. For simpler needs, the built-in monitoring dashboards in the Dataiku API Deployer are often sufficient. You should only implement this level of integration if you have a requirement to centralize all your company's monitoring in one place.
- **Challenge:** "My logs are not appearing in Kibana."
- **Solution:** This requires step-by-step debugging of the logging pipeline. Is Filebeat running? Can it connect to Logstash? Is Logstash successfully parsing the logs and sending them to Elasticsearch? Check the logs of each component in the ELK stack.
`},{id:574,slug:"logging-tracing-requests-for-observability",question:"How to get started with logging and tracing requests to Dataiku-hosted APIs for observability.",answer:`
### 1. Introduction/Overview
Observability goes beyond simple monitoring. It's about being able to ask arbitrary questions about your system's behavior. For an ML API, this means having detailed logs for every request and, for advanced use cases, distributed tracing to see how a request flows through multiple services.

### 2. Prerequisites
- **A deployed Dataiku API endpoint.**
- **A centralized logging platform** (like the ELK stack, Datadog, or Splunk).

### 3. Step-by-Step Instructions

#### Part 1: Centralized Logging
1.  **Understand the Default Logs:** The Dataiku API node automatically generates logs for every request. These logs are typically in a structured JSON format and contain key information like the timestamp, endpoint called, response status code, and latency.
2.  **Ship the Logs:** The most important step is to get these logs off the individual API node servers and into a centralized platform.
    *   Install a **log shipping agent** (like Filebeat or Fluentd) on each API node VM or as a sidecar container in Kubernetes.
    *   Configure this agent to watch the API node's log file and send new log entries to your central logging system (e.g., Elasticsearch, Datadog).
3.  **Use the Centralized Logs:**
    *   Now, you can use your logging platform's UI (e.g., Kibana) to search, filter, and analyze all your API requests.
    *   **Troubleshooting:** When a customer reports an error, you can search for their specific request ID to see the exact log entry and error message.
    *   **Analytics:** You can create dashboards to visualize trends like error rates or response times per API version.

#### Part 2: Distributed Tracing (Advanced)
1.  **When to Use:** When a single user request involves calls to multiple microservices, including your Dataiku API. Tracing allows you to see the full journey of that request.
2.  **Instrument Your Code:** This requires custom code.
    *   Your calling application must start a "trace" and generate a unique "trace ID".
    *   It must then pass this trace ID in an HTTP header (e.g., \`X-Trace-ID\`) when it calls the Dataiku API.
3.  **Propagate the Trace ID:**
    *   Inside your **Python API endpoint code** in Dataiku, you must read this header.
    *   Add the trace ID to your structured logs.
    *   If your Dataiku API in turn calls another service, it must pass the trace ID along in the headers of that downstream call.
4.  **Visualize the Traces:** A distributed tracing platform (like Jaeger or Datadog APM) can then ingest all these logs and use the common trace ID to stitch them together, creating a visual "flame graph" of the entire request lifecycle.

### 4. Resources and Tools
- **A Log Shipping Agent (Filebeat, etc.).**
- **A Centralized Logging Platform (ELK, Datadog, etc.).**
- **A Distributed Tracing Platform (Jaeger, OpenTelemetry, etc.)** for the advanced use case.

### 5. Next Steps and Progression
- **Custom Logging:** Add more detailed, application-specific logging within your Python API endpoint code to record business-relevant information for each prediction.

### 6. Common Challenges and Solutions
- **Challenge:** "My logs are in different formats and hard to parse."
- **Solution:** You need to configure a parsing pipeline, typically in Logstash or your logging platform's equivalent. This will parse the different log formats into a single, standardized JSON structure before they are indexed.
- **Challenge:** "Implementing distributed tracing is complicated."
- **Solution:** It is. This is a very advanced observability pattern. You should only undertake it if you have a complex microservices architecture where it's genuinely difficult to troubleshoot request flows. For many use cases, centralized logging is sufficient.
`},{id:575,slug:"setting-up-autoscaling-rules-for-api-services",question:"How to get started with setting up auto-scaling rules for Dataiku API services based on load.",answer:`
### 1. Introduction/Overview
Manually adding or removing servers to handle traffic spikes is inefficient and slow. Autoscaling is the practice of automatically adjusting the number of compute resources allocated to a service based on its current load. For a Dataiku API service deployed on Kubernetes, this is achieved using a **Horizontal Pod Autoscaler (HPA)**.

### 2. Prerequisites
- **Your Dataiku API service deployed on a Kubernetes cluster.** This means you have a Kubernetes **Deployment** that manages your model's pods.
- **A metrics server installed** in your Kubernetes cluster. Most managed Kubernetes services (EKS, AKS, GKE) include this by default. The HPA needs this to get CPU and memory metrics from the pods.

### 3. Step-by-Step Instructions
1.  **Define Resource Requests for Your Pods:**
    *   For autoscaling to work effectively, the pods in your API service's Deployment must have **CPU resource requests** defined.
    *   This tells Kubernetes how much CPU the pod needs to function, which is used as the baseline for calculating utilization percentage.
2.  **Create a Horizontal Pod Autoscaler (HPA) Object:**
    *   You define an HPA as a Kubernetes YAML manifest file.
    *   In the manifest, you specify:
        *   The **target Deployment** you want to scale.
        *   The **minimum and maximum number of replicas** (pods) you want to have.
        *   The **scaling metric and target**. The most common metric is CPU utilization.
3.  **Example HPA Manifest:**
    > \`\`\`yaml
    > apiVersion: autoscaling/v2
    > kind: HorizontalPodAutoscaler
    > metadata:
    >   name: my-model-api-hpa
    > spec:
    >   scaleTargetRef:
    >     apiVersion: apps/v1
    >     kind: Deployment
    >     name: my-model-api-deployment # The name of your API's Deployment
    >   minReplicas: 2   # Always keep at least 2 pods for high availability
    >   maxReplicas: 10  # Don't scale beyond 10 pods
    >   metrics:
    >   - type: Resource
    >     resource:
    >       name: cpu
    >       target:
    >         type: Utilization
    >         averageUtilization: 70 # Target 70% CPU utilization
    > \`\`\`
4.  **Apply the Manifest:**
    *   Use \`kubectl apply -f your-hpa-file.yaml\` to create the HPA in your cluster.
5.  **How It Works:**
    *   The HPA controller will now periodically check the average CPU utilization across all the pods in your deployment.
    *   If the average utilization rises above 70%, the HPA will automatically increase the number of replicas in the Deployment to bring the average back down.
    *   If the load decreases and the utilization falls below 70%, it will remove pods (but never going below \`minReplicas\`).

### 4. Resources and Tools
- **Kubernetes Horizontal Pod Autoscaler (HPA):** The core Kubernetes object for autoscaling.
- **\`kubectl\`:** The command-line tool for applying the HPA manifest.
- **Monitoring Tools (e.g., Prometheus/Grafana):** To observe your pod's CPU usage and decide on an appropriate target utilization percentage.

### 5. Next Steps and Progression
- **Custom Metrics:** You can configure the HPA to scale based on custom metrics, such as requests per second or queue depth, which can sometimes be more responsive than CPU usage. This requires a more advanced monitoring setup.
- **Cluster Autoscaling:** The HPA scales the number of *pods*. If you run out of room on your *nodes*, you also need a **Cluster Autoscaler** enabled. This will automatically add more VM nodes to your Kubernetes cluster when the HPA needs to schedule more pods.

### 6. Common Challenges and Solutions
- **Challenge:** "The HPA is not scaling up my deployment."
- **Solution:** Use \`kubectl describe hpa my-model-api-hpa\` to see the HPA's status and events. A common issue is that the metrics server is not running or the HPA can't fetch metrics from the pods. Another reason could be that the CPU utilization has not actually crossed your target threshold.
- **Challenge:** "The pods are scaling up, but the API is still slow."
- **Solution:** This might mean that your bottleneck is not CPU. It could be a downstream dependency (like a slow database) or that your model's code is not efficient and needs to be optimized.
`},{id:576,slug:"versioning-api-endpoints-v1-v2-deployments",question:"How to get started with versioning Dataiku API endpoints (e.g. v1, v2 deployments) in production.",answer:`
### 1. Introduction/Overview
When you update a production API, you can't just replace the old one. You need a strategy for versioning that allows you to roll out changes safely without breaking existing client applications. The Dataiku API Deployer supports this by allowing you to have multiple versions of an endpoint deployed simultaneously and managing the traffic between them.

### 2. Prerequisites
- **An existing, deployed API service** in the API Deployer.
- **A new version of your model** or Python function that you want to deploy.

### 3. Step-by-Step Instructions: The Versioning Workflow

1.  **Create a New Model/Code Version:**
    *   In your Dataiku design project, create the new version of your asset.
    *   For a model, you would retrain it and deploy it to your **Saved Model** object. This automatically creates a new, numbered version (e.g., v2).
2.  **Update the API Service Definition:**
    *   In the **API Designer**, open your API service.
    *   Edit your endpoint. You can now change the model version it points to from v1 to your new v2.
3.  **Deploy the New Version:**
    *   Click **Deploy**.
    *   The API Deployer will now package and deploy this new version of your API service.
4.  **Manage Traffic Between Versions:**
    *   Navigate to the API Deployer UI. You will now see both the old and new versions of your service deployed.
    *   You have several strategies for managing the rollout:
        *   **Blue-Green Deployment:** Deploy the new version (v2) but send 0% of traffic to it. You can test it internally. When you are confident, you can instantly switch 100% of the traffic from v1 to v2. If there's a problem, you can instantly switch back.
        *   **Canary Release / Traffic Splitting:** Configure the endpoint to send a small percentage of live traffic (e.g., 5%) to the new v2, while the majority (95%) still goes to the stable v1. You can monitor the performance and error rate of v2 with a small subset of users before gradually increasing its traffic share.
5.  **Decommission the Old Version:** Once the new version is stable and receiving 100% of the traffic, you can safely undeploy the old version from the API Deployer to clean up your infrastructure.

### 4. Resources and Tools
- **Dataiku API Deployer:** The service for managing the lifecycle of your deployed API versions.
- **Saved Model Versioning:** The feature that automatically versions your models when you retrain them.
- **Traffic Splitting / Canary Release:** The deployment strategy for safely rolling out new versions.

### 5. Next Steps and Progression
- **Semantic Versioning in URL:** For major, breaking changes, you might choose to include the version in the URL path itself (e.g., \`/api/v1/predict\` and \`/api/v2/predict\`). This is done by creating a completely new endpoint in the API Designer for the new version. This allows clients to opt-in to the new version on their own schedule.

### 6. Common Challenges and Solutions
- **Challenge:** "I deployed a new version, and it broke the client application."
- **Solution:** You did not perform a safe rollout. You should have used a canary release to test the new version with a small amount of traffic first. Immediately roll back by reactivating the previous stable version in the API Deployer to restore service.
- **Challenge:** "How do I know if the new version is performing well?"
- **Solution:** The API Deployer's monitoring dashboards will show you the latency and error rates for each version separately. This allows you to directly compare their technical performance in a live environment.
`},{id:577,slug:"integrating-apis-with-api-gateways",question:"How to get started with integrating Dataiku APIs with API gateways (AWS API Gateway, Azure API Mgmt) for scaling.",answer:`
### 1. Introduction/Overview
An API Gateway is a management tool that sits in front of your backend API services. Integrating your Dataiku API with a gateway (like AWS API Gateway or Azure API Management) provides a centralized way to handle cross-cutting concerns like authentication, rate limiting, and request routing, adding an extra layer of security and control.

### 2. Prerequisites
- **A deployed Dataiku API endpoint** running on an API node. This endpoint should be accessible within your private network (VPC).
- **An API Gateway service** configured in your cloud provider.
- **Networking knowledge** to configure routing between the gateway and your Dataiku API node.

### 3. Step-by-Step Instructions
1.  **Deploy Your Dataiku API:** Deploy your model as a standard API service on a Dataiku API node. Make a note of its private IP address and port.
2.  **Configure the API Gateway:**
    *   In your cloud provider's console, create a new API in your API Gateway service.
3.  **Define a Route and Integration:**
    *   Create a new route (e.g., \`/predict/churn\`).
    *   Create an **integration** for this route. The integration tells the gateway where to send the request.
    *   Configure the integration to be a "private integration" that forwards the request to the **private IP address and port** of your Dataiku API node.
4.  **Configure Authentication at the Gateway:**
    *   You can now configure the gateway to handle authentication. For example, you can require all clients to present a specific API key to the *gateway*.
    *   If the key is valid, the gateway then forwards the request to the Dataiku API node. The connection between the gateway and Dataiku can be on a trusted private network, so the Dataiku endpoint itself might not need a separate authentication method.
5.  **Configure Rate Limiting and Throttling:**
    *   Use the gateway's features to set usage plans. You can define rules like "allow a maximum of 100 requests per second" for a specific client. The gateway will automatically reject requests that exceed this limit, protecting your backend Dataiku API from being overloaded.
6.  **Clients Call the Gateway URL:** Your client applications no longer call the Dataiku API node directly. They call the public URL of the API Gateway endpoint.

### 4. Resources and Tools
- **Your Cloud Provider's API Gateway Service (AWS API Gateway, etc.).**
- **Dataiku API Deployer:** For running the backend model service.
- **Private Networking (VPCs, VNETs):** To ensure secure communication between the gateway and Dataiku.

### 5. Next Steps and Progression
- **Request/Response Transformation:** API Gateways can modify requests and responses. You could use this to transform a client's request into the format your Dataiku model expects, or to reformat the model's JSON response before sending it back to the client.
- **Canary Releases:** Use the API Gateway's traffic splitting or canary release features to route a percentage of traffic to a new version of your Dataiku model for testing.

### 6. Common Challenges and Solutions
- **Challenge:** "The gateway is returning a 504 Gateway Timeout error."
- **Solution:** This means the API Gateway was able to receive the request, but it could not get a response from your backend Dataiku API node in time. This is usually a network connectivity issue. Check your VPC routing, security groups, and firewall rules to ensure the gateway can reach the private IP of the Dataiku node.
- **Challenge:** "Which authentication method should I use?"
- **Solution:** It depends on your security model. If the gateway handles authentication, the communication between the gateway and Dataiku might not need additional auth. However, for a "defense-in-depth" approach, you could have the gateway authenticate clients, and also have the gateway present a specific, trusted API key to the Dataiku backend.
`},{id:578,slug:"documenting-generated-apis-with-openapi-swagger",question:"How to get started with documenting Dataiku-generated APIs using OpenAPI/Swagger for clients.",answer:`
### 1. Introduction/Overview
When you create an API, you must provide clear documentation so that client application developers know how to call it. The industry standard for documenting REST APIs is the **OpenAPI Specification** (formerly known as Swagger). Dataiku automatically generates an OpenAPI spec for any API service you create, making documentation easy.

### 2. Prerequisites
- **An API service** created in the Dataiku API Designer.

### 3. Step-by-Step Instructions
1.  **Find the API Specification:**
    *   After you have deployed your API service to the **API Deployer**, navigate to the service's page in the API Deployer UI.
    *   You will find a section or a button labeled **API Spec** or **OpenAPI**.
2.  **Access the Specification:** You have two ways to use the spec:
    *   **View the Interactive UI (Swagger UI):** The API Deployer hosts a live, interactive Swagger UI for your service. This UI provides a readable list of all your endpoints, shows their expected input parameters and JSON format, and displays the schema of the expected response. You can even use this UI to make test calls to the live API.
    *   **Download the JSON/YAML file:** You can download the raw OpenAPI specification as a \`.json\` or \`.yaml\` file.
3.  **Share with Developers:**
    *   Provide the link to the interactive Swagger UI to your client application developers. This is often the only documentation they will need to successfully integrate with your API.
    *   Alternatively, you can give them the OpenAPI JSON file, which they can use to automatically generate client-side code in their preferred programming language using OpenAPI generator tools.

### 4. Resources and Tools
- **API Deployer:** Hosts the automatically generated documentation.
- **OpenAPI Specification (Swagger):** The industry standard for defining REST APIs.
- **Swagger UI:** The interactive documentation tool.

### 5. Next Steps and Progression
- **Add Descriptions:** The quality of the generated documentation depends on you. In the **API Designer** in your project, add clear **descriptions** to your service, endpoints, and parameters. These descriptions will automatically appear in the generated OpenAPI documentation.
- **Custom Documentation:** You can import the downloaded OpenAPI JSON file into other documentation tools (like Postman or ReadMe.com) to create more elaborate, customized API portals.

### 6. Common Challenges and Solutions
- **Challenge:** The documentation is missing details about what the input fields mean.
- **Solution:** You need to add this information yourself. In the API Designer, when you are configuring your endpoint, you can add descriptions for each of the input parameters. This is essential for good documentation.
- **Challenge:** Our client developers need a client library in Java.
- **Solution:** They can use a tool like the **OpenAPI Generator**. They provide your API's OpenAPI JSON file to this tool, and it will automatically generate a client-side SDK in Java (or dozens of other languages), complete with all the necessary classes and methods for calling your API.
`},{id:579,slug:"gathering-usage-metrics-latency-error-rate",question:"How to get started with gathering usage metrics (latency, error rate) for Dataiku APIs to improve reliability.",answer:`
### 1. Introduction/Overview
To ensure your deployed API is reliable and performant, you must monitor its key usage metrics. The Dataiku **API Deployer** provides built-in monitoring dashboards that automatically track the most important metrics, such as latency (how fast the API is) and error rate (how often it fails).

### 2. Prerequisites
- **An API service** deployed and running on the API Deployer.
- **Live traffic:** The API must be receiving prediction requests to generate metrics.

### 3. Step-by-Step Instructions
1.  **Navigate to the API Deployer:** Open the API Deployer UI, which is separate from the main Dataiku design instance.
2.  **Select Your Deployed Service:** Find and click on the API service you want to monitor.
3.  **Go to the Monitoring Tab:** Navigate to the **Monitoring** tab for your service.
4.  **Analyze the Built-in Dashboards:** The monitoring page provides several key charts and metrics out-of-the-box:
    *   **Requests:** A chart showing the number of requests over time (throughput). This helps you understand your traffic patterns.
    *   **Latency:** A chart showing the average and percentile (e.g., p95, p99) response times. This tells you how fast your API is for your users.
    *   **Errors:** A chart showing the number of requests that resulted in an error (e.g., an HTTP 500 server error). This is a critical reliability metric.
5.  **Filter and Drill Down:**
    *   You can filter the view by a specific time range.
    *   If you have multiple versions of a model deployed (e.g., for an A/B test), you can often view the metrics for each version separately to compare their performance.
6.  **Use Metrics to Drive Improvements:**
    *   **High Latency?** This may indicate that your model is too complex or the API node needs more CPU/memory resources.
    *   **High Error Rate?** You need to investigate the API service logs to find the root cause of the errors and deploy a fix.

### 4. Resources and Tools
- **API Deployer Monitoring Tab:** Your primary source for built-in usage metrics.
- **API Service Logs:** Provide detailed error messages for failed requests.

### 5. Next Steps and Progression
- **External Monitoring:** For more advanced monitoring and alerting, you can export these metrics from Dataiku to a dedicated monitoring system like **Prometheus** and build customized dashboards in **Grafana**.
- **Alerting:** In your external monitoring system, you can set up alerts based on these metrics. For example: "If the p99 latency exceeds 500ms for more than 5 minutes, send an alert to the on-call engineer."
- **Service Level Objectives (SLOs):** Define formal SLOs for your API (e.g., "99.9% of requests will have a latency under 200ms"). Use your gathered metrics to track your performance against these objectives.

### 6. Common Challenges and Solutions
- **Challenge:** The monitoring dashboard is not showing any data.
- **Solution:** This means your API is not receiving any traffic. Use a tool like \`curl\` or Postman to send a test request to the endpoint to ensure it's live and accessible.
- **Challenge:** There is a spike in errors at a certain time.
- **Solution:** Use the time filter on the monitoring dashboard to zoom in on the time of the spike. Then, go to the API service logs and look at the logs from that specific time period to find the error messages that correspond to the failed requests.
`},{id:580,slug:"monitoring-job-resource-usage-cpu-memory",question:"How to get started with monitoring Dataiku job resource usage (CPU, memory) to identify performance bottlenecks.",answer:`
### 1. Introduction/Overview
When a job is slow, the cause is often a resource bottleneck—either CPU or memory. Monitoring the resource usage of your jobs is key to identifying these bottlenecks and making informed decisions about how to optimize your pipelines and infrastructure.

### 2. Prerequisites
- **A Dataiku job** that you want to monitor.
- **Access to monitoring tools.** This could be as simple as command-line tools on the server, or a sophisticated platform like Prometheus and Grafana.

### 3. Step-by-Step Instructions: Where to Look

The tool you use depends on where your job is running.

#### 1. For In-Memory Jobs (on the main Dataiku server)
- **What to do:** An administrator needs to monitor the Dataiku server itself while the job is running.
- **Tools:**
    - **Command Line:** Use tools like \`top\`, \`htop\`, or \`ps\` on the server to see the CPU and memory usage of the main Dataiku Java process and any Python processes it spawns.
    - **Infrastructure Monitoring:** Use your standard server monitoring tool (e.g., Datadog, Nagios) to view CPU and memory charts for the Dataiku server.
- **What to look for:** If you see the server's CPU hitting 100% or its memory usage climbing until the job fails, you've found a bottleneck.

#### 2. For Containerized Jobs (on Kubernetes)
- **What to do:** Use Kubernetes' built-in monitoring tools.
- **Tools:**
    - **\`kubectl top pod ...\`:** This command-line tool will show you the current CPU and memory usage of a specific pod running your job.
    - **Prometheus/Grafana:** If you have a monitoring stack set up for your cluster, you can view detailed time-series graphs of the resource consumption for your job's pod.
- **What to look for:** If a pod's memory usage hits its defined limit, Kubernetes will kill it (an "OOMKilled" error). If its CPU usage is constantly at its limit, it's CPU-bound and will be slow.

#### 3. For Pushed-down Jobs (on a Database/Spark)
- **What to do:** You need to use the native monitoring tools for that external engine.
- **Tools:**
    - **For Databases (Snowflake, BigQuery, etc.):** Use the database's own query history or system monitoring views. These will show you the CPU time, data scanned, and memory used by the query that Dataiku submitted.
    - **For Spark:** Use the **Spark UI**. This provides extremely detailed information on the resource usage of every executor and task in your Spark application.

### 4. Resources and Tools
- **The Job Inspector in Dataiku:** To see the duration of each recipe.
- **Your infrastructure's native monitoring tools:** This is where you will see the actual resource consumption.

### 5. Next Steps and Progression
- **Optimization:** Once you've identified the bottleneck, you can take action.
    - **Memory Bottleneck?** Refactor the job to be more memory-efficient or increase the memory allocated to it (e.g., in the container configuration).
    - **CPU Bottleneck?** Optimize the code or provide more CPU resources.
    - **The Best Optimization:** If the bottleneck is an in-memory job, the best solution is often to refactor it to **push down the computation** to a more powerful engine like a database or Spark cluster.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't have access to the server or the database monitoring tools."
- **Solution:** You need to collaborate with your platform administrators or DBAs. Explain which job you are running and ask them to monitor its resource consumption on the backend system.
`},{id:581,slug:"sizing-tuning-cluster-nodes",question:"How to get started with sizing and tuning Dataiku cluster nodes to balance cost and performance.",answer:`
### 1. Introduction/Overview
Sizing and tuning the nodes in your Dataiku cluster is a critical administrative task for balancing performance, stability, and cost. "Sizing" refers to choosing the right instance types (CPU/memory), and "tuning" involves configuring the software to use those resources effectively. This is an iterative process based on monitoring your specific workloads.

### 2. Prerequisites
- **A multi-node Dataiku deployment** (e.g., on-premise or in the cloud).
- **Administrator access** to the Dataiku servers and, if applicable, your cloud provider console.
- **Monitoring tools** to observe resource usage.

### 3. Step-by-Step Instructions: A Sizing and Tuning Strategy

1.  **Understand Your Node Types:** A typical production Dataiku deployment has different types of nodes with different resource needs.
    *   **Design Node:** Hosts the main web interface. Needs good memory and moderate CPU.
    *   **Automation Node:** Runs scenarios. Needs will vary based on the jobs being run.
    *   **API Nodes:** Serve real-time models. Need low-latency CPU and memory based on model complexity and traffic.
    *   **Execution Nodes (Kubernetes):** The worker nodes where containerized jobs run. Need a balance of CPU and memory.
2.  **Start with Dataiku's Recommendations:**
    *   The Dataiku installation documentation provides sizing recommendations for different scales of usage.
    *   Use these recommendations as your starting point when first provisioning your infrastructure.
3.  **Monitor Your Workloads:**
    *   Once the platform is in use, you must **monitor** the actual resource utilization of each node type.
    *   Use your infrastructure monitoring tools (e.g., AWS CloudWatch, Grafana) to track CPU Utilization, Memory Usage, and I/O over time.
4.  **Tune and Iterate:** Based on your monitoring data, adjust your sizing.
    *   **Is a node consistently at 90% CPU?** It's undersized. You need to choose a larger instance type with more CPU cores.
    *   **Is a node consistently using only 10% of its memory?** It's oversized. You can choose a smaller instance type to save costs.
    *   **Are jobs failing with Out Of Memory errors?** You need to increase the memory on your execution nodes or for a specific container configuration.
5.  **Leverage Autoscaling (for Cloud Deployments):**
    *   For your execution and API nodes (if on Kubernetes), use **autoscaling groups** or **Horizontal Pod Autoscalers**.
    *   This allows the cluster to automatically add or remove nodes based on the current workload, which is the most cost-effective way to manage resources.

### 4. Resources and Tools
- **Dataiku's Sizing Guidelines:** Your starting point.
- **Cloud Provider Monitoring Tools (CloudWatch, etc.).**
- **Kubernetes Autoscalers (HPA, Cluster Autoscaler).**

### 5. Next Steps and Progression
- **Load Testing:** Before deploying a new, critical API, use a load testing tool like JMeter or Locust to simulate high traffic. Monitor the API nodes during the test to see how they perform and to fine-tune your autoscaling rules.
- **Cost Analysis:** Use your cloud provider's cost management tools to analyze the cost of your Dataiku infrastructure. Use this data to identify opportunities for optimization (e.g., moving stateless workloads to cheaper spot instances).

### 6. Common Challenges and Solutions
- **Challenge:** "How do I know the right size before we even start?"
- **Solution:** You can't know it perfectly. Sizing is an educated guess to start, followed by an iterative process of monitoring and adjusting. It's often better to start slightly oversized and then scale down once you have real usage data.
- **Challenge:** "Our costs are too high."
- **Solution:** This is a sign that your nodes are oversized or you are not using autoscaling effectively. Analyze your monitoring data to find which nodes are idle or underutilized and shrink them. Ensure your autoscaling rules are configured to scale down aggressively during idle periods.
`},{id:582,slug:"using-kubernetes-to-horizontally-scale-services",question:"How to get started with using Kubernetes to horizontally scale Dataiku services under load.",answer:`
### 1. Introduction/Overview
Horizontal scaling (scaling out) means adding more machines or containers to handle increased load, as opposed to vertical scaling (scaling up), which means making a single machine more powerful. Kubernetes is designed for horizontal scaling, making it the ideal platform for running scalable, resilient Dataiku services like API nodes and job execution engines.

### 2. Prerequisites
- **A Dataiku deployment on a Kubernetes cluster.**
- **An understanding of core Kubernetes concepts:** Pods, Deployments, Services, and Horizontal Pod Autoscalers (HPA).

### 3. Step-by-Step Instructions

#### Scaling Real-time APIs
1.  **The Deployment Object:** When you deploy a model to the API Deployer on a K8s infrastructure, it creates a Kubernetes **Deployment**. This Deployment's job is to ensure that a specified number of identical **pods** (containers running your model) are always running.
2.  **Manual Scaling:** The simplest way to scale is to manually change the number of desired pods.
    > \`kubectl scale deployment/my-model-api --replicas=5\`
    *   Kubernetes will now create new pods until there are 5 running, and the Service will automatically start load balancing traffic across all of them.
3.  **Automatic Scaling (The Best Practice):**
    *   Create a **Horizontal Pod Autoscaler (HPA)** object for your Deployment.
    *   The HPA will watch the CPU or memory usage of your pods.
    *   If the average CPU exceeds a target you define (e.g., 70%), the HPA will automatically scale the Deployment by increasing the replica count. When the load decreases, it will scale back down.

#### Scaling Batch Jobs (Recipes)
1.  **Containerized Execution:** You must be using containerized execution for your recipes, targeting your Kubernetes cluster.
2.  **How it Works:** When multiple users run containerized jobs at the same time, Dataiku submits each one as a new pod to Kubernetes.
3.  **Cluster Autoscaling:** If there are not enough resources on the existing nodes to run all the new pods, the **Cluster Autoscaler** (a component of your managed K8s service) will kick in. It will automatically provision and add new worker nodes to the cluster to handle the load. When the jobs are finished and the nodes are no longer needed, the Cluster Autoscaler will terminate them to save costs.

### 4. Resources and Tools
- **Kubernetes Deployments:** The object that manages your application's pods.
- **Horizontal Pod Autoscaler (HPA):** For autoscaling the number of pods.
- **Cluster Autoscaler:** For autoscaling the number of underlying VMs (nodes).

### 5. Next Steps and Progression
- **Monitoring:** Use Prometheus and Grafana to monitor your pod and node resources to fine-tune your autoscaling rules.
- **Node Pools:** Configure different node pools in your cluster (e.g., some with GPUs) and use Kubernetes taints and tolerations to ensure that specific Dataiku jobs run on the appropriate type of hardware.

### 6. Common Challenges and Solutions
- **Challenge:** "My pods are scaling up, but the service is still slow."
- **Solution:** Horizontal scaling only helps if your application is stateless and can handle requests in parallel. If the bottleneck is an external dependency, like a slow database that all your pods are hitting, adding more pods won't solve the problem. You need to fix the downstream bottleneck.
- **Challenge:** "My cluster is not scaling up to add more nodes."
- **Solution:** Check the configuration of your Cluster Autoscaler. Ensure it's enabled and correctly configured for your node groups. Also, check your cloud provider account quotas to make sure you haven't hit a limit on the number of VMs you are allowed to create.
`},{id:583,slug:"setting-job-quotas-priorities",question:"How to get started with setting Dataiku job quotas and priorities to control resource consumption.",answer:`
### 1. Introduction/Overview
In a shared, multi-tenant Dataiku environment, you need mechanisms to ensure that a single user or a single large job doesn't consume all the available compute resources, starving other users. This is achieved by setting quotas and priorities, typically at the infrastructure level (e.g., in Kubernetes or a database).

### 2. Prerequisites
- **A shared Dataiku instance** used by multiple teams or projects.
- **Administrator access** to the underlying compute infrastructure (e.g., Kubernetes, a database).

### 3. Step-by-Step Instructions: Methods for Control

#### Method 1: Kubernetes Resource Quotas
- **When to Use:** When your jobs are running as containers on a shared Kubernetes cluster.
- **How it Works:**
    1.  Create separate **namespaces** in Kubernetes for different teams or project types (e.g., \`team-finance\`, \`team-marketing\`).
    2.  For each namespace, create a **ResourceQuota** object.
    3.  In the ResourceQuota, you can define hard limits on the total amount of CPU and memory that all pods in that namespace can consume *in aggregate*.
    4.  You can also set limits on the number of pods or other objects that can be created.
- **Effect:** The Finance team can run as many jobs as they want, but collectively they cannot exceed their allocated CPU/memory quota, preventing them from impacting the Marketing team.

#### Method 2: Database Workload Management
- **When to Use:** When your jobs are pushed down to a shared data warehouse like Snowflake or Redshift.
- **How it Works:**
    1.  Create different **users** or **roles** in the database for different Dataiku user groups.
    2.  Use the database's built-in **Workload Management (WLM)** features.
    3.  You can assign different priorities to different user roles, ensuring that queries from a high-priority user (like a CEO dashboard) will get resources before a large, low-priority ETL job.
    4.  In Snowflake, you can assign different roles to different virtual warehouses of varying sizes, effectively giving them different resource quotas.

#### Method 3: Dataiku-level Concurrency Limits
- **What it is:** A simpler, built-in Dataiku control.
- **How it Works (Admin Task):**
    1.  In **Administration > Settings > Flow build**, you can set a global limit on the **maximum number of concurrent activities** (recipes running at the same time).
- **Effect:** This provides a basic, instance-wide form of priority management by creating a queue. It doesn't provide fine-grained control per user or project, but it can prevent the server from being completely overloaded.

### 4. Resources and Tools
- **Kubernetes Namespaces and ResourceQuotas.**
- **Your Data Warehouse's Workload Management features.**
- **Dataiku's global concurrency settings.**

### 5. Next Steps and Progression
- **Showback/Chargeback:** Once you have quotas and are tracking resource usage (e.g., via Kubernetes monitoring or Snowflake query history), you can implement a showback or chargeback model, where you report on (or even bill) each team for their share of the infrastructure costs.

### 6. Common Challenges and Solutions
- **Challenge:** "A user's job is stuck in 'pending' in Kubernetes."
- **Solution:** This could be because their team's namespace has hit its ResourceQuota limit. They will have to wait for other jobs in their namespace to finish before theirs can be scheduled.
- **Challenge:** "How do we decide on the right quotas?"
- **Solution:** This is a business and resource planning decision. It requires understanding each team's needs and balancing them against the total available capacity and budget. Start with a reasonable allocation and adjust it over time based on actual usage and business priorities.
`},{id:584,slug:"tracking-cloud-costs-of-resources",question:"How to get started with tracking cloud costs of Dataiku-related resources (EC2 instances, S3 storage) over time.",answer:`
### 1. Introduction/Overview
When running Dataiku in the cloud, understanding and tracking your costs is essential for managing budgets and demonstrating ROI. All major cloud providers offer powerful cost management tools that, when combined with a disciplined tagging strategy, can give you a detailed view of your Dataiku-related spending.

### 2. Prerequisites
- **Dataiku deployed on a cloud provider** (AWS, Azure, or GCP).
- **Access to your cloud provider's billing and cost management console.**

### 3. Step-by-Step Instructions

1.  **Implement a Tagging Strategy (Crucial First Step):**
    *   **What it is:** Tags are key-value labels that you can attach to every resource you create in the cloud.
    *   **Your Strategy:** Define a standard set of tags that you will apply to all resources associated with your Dataiku deployment.
        *   \`service: dataiku\`
        *   \`environment: prod\` (or \`dev\`, \`qa\`)
        *   \`owner: data-science-team\`
    *   Apply these tags to your EC2 instances, S3 buckets, databases, Kubernetes clusters, etc.
2.  **Use the Cloud Cost Management Tool:**
    *   Navigate to your cloud provider's cost management service:
        *   **AWS:** Cost Explorer
        *   **Azure:** Cost Management + Billing
        *   **GCP:** Cloud Billing Reports
3.  **Filter by Tags:**
    *   These tools allow you to filter your spending based on the tags you applied.
    *   Create a report that filters for all resources with the tag \`service: dataiku\`.
    *   This will now show you a dashboard with the total cost of your Dataiku platform over time.
4.  **Analyze the Costs:**
    *   You can group the costs by service (to see how much you're spending on compute vs. storage vs. database) or by other tags (to see the cost of your prod vs. dev environments).
    *   Look at the cost trends over time. Is your spending increasing? This might be expected as usage grows, but it's important to track.

### 4. Resources and Tools
- **Tagging:** The fundamental mechanism for cost allocation.
- **Cloud Provider Cost Management Tools:** (e.g., AWS Cost Explorer).

### 5. Next Steps and Progression
- **Create Budgets and Alerts:**
    *   In your cloud provider's billing console, you can create a **budget** for your Dataiku service (based on your tags).
    *   You can then configure **alerts** to be sent to you automatically if your spending is forecasted to exceed your budget. This is essential for preventing unexpected cost overruns.
- **Granular Cost Allocation:** For even more detail, you can use a more granular tagging strategy. For example, have a Dataiku scenario that, when it spins up a temporary cluster for a specific project's job, it applies a tag for that project's key (e.g., \`project: churn_model\`). This allows you to track the compute cost of individual projects.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't see all my costs; the report seems incomplete."
- **Solution:** This means your tagging is inconsistent. You have likely missed applying the standard tags to some of your resources. You need to perform an audit of all your cloud resources and ensure everything related to Dataiku is tagged correctly. Many organizations use automated scripts or policies to enforce tagging.
- **Challenge:** "Our costs are higher than expected."
- **Solution:** Use the cost analysis tool to drill down and see which specific service is responsible for the high cost. Is it an oversized, idle database? Is it a Kubernetes cluster that isn't scaling down properly? The cost report will point you to where you need to optimize.
`},{id:585,slug:"tagging-cloud-resources-by-project-for-cost-allocation",question:"How to get started with tagging cloud resources by Dataiku project for granular cost allocation.",answer:`
### 1. Introduction/Overview
For large, multi-team Dataiku deployments, you often need to allocate infrastructure costs back to the individual projects or business units that consumed the resources. This requires a granular cost allocation strategy, which is achieved by dynamically **tagging** cloud resources with the Dataiku project key that is using them.

### 2. Prerequisites
- **Dataiku running on the cloud,** typically using containerized execution on Kubernetes for dynamic workloads.
- **A mechanism for dynamic tagging.** This is an advanced setup that usually requires custom scripting.

### 3. Step-by-Step Instructions: A Dynamic Tagging Workflow

This pattern is most common for containerized jobs that run on dynamically created resources.

1.  **Create a Wrapper Script or Plugin:**
    *   You need a process that sits between Dataiku and the compute engine. This could be a custom plugin in Dataiku or an external script that is called by a Dataiku scenario.
2.  **Extract the Project Key:**
    *   When a job is run, the script can use the Dataiku API to get the context of the job, including the **project key** of the project it belongs to.
3.  **Provision the Resources with Tags:**
    *   Instead of using a static compute cluster, the script will programmatically provision the resources needed for the job. For example, it could use the AWS SDK (Boto3) to create a new, temporary EMR cluster.
    *   When creating this cluster, the script will apply a **tag** to all its resources with the project key it extracted in the previous step.
    > \`Tag: { "Key": "dss_project", "Value": "FINANCE_REPORTING" }\`
4.  **Run the Job:** The script then submits the Dataiku job to this new, dedicated cluster.
5.  **Terminate Resources:** After the job is complete, the script terminates the temporary cluster.
6.  **Analyze Costs:** In your cloud provider's cost management tool (e.g., AWS Cost Explorer), you can now **filter and group your spending by the \`dss_project\` tag**. This will show you exactly how much compute cost was consumed by the "FINANCE_REPORTING" project, allowing for direct cost allocation.

### 4. Resources and Tools
- **Cloud Provider SDKs (e.g., Boto3):** For programmatically creating and tagging resources.
- **Dataiku API:** For getting the project context of a running job.
- **Cloud Cost Management Tools:** For filtering and reporting on the tags.

### 5. Next Steps and Progression
- **Showback Dashboards:** Create a dashboard (either in Dataiku or your BI tool) that uses the cost data to create a "showback" report, detailing the infrastructure costs incurred by each project or team.

### 6. Common Challenges and Solutions
- **Challenge:** "This is very complex to set up."
- **Solution:** It is. This is a very advanced MLOps and FinOps pattern. It should only be implemented if you have a strong business requirement for granular cost chargeback. For many organizations, a simpler model of tagging resources by environment (\`prod\`, \`dev\`) is sufficient.
- **Challenge:** "How do I tag the storage costs?"
- **Solution:** This is more difficult, as storage is often shared. You could implement a process where each project writes its data to a specific "prefix" or folder within a central S3 bucket. You can then use storage analytics tools to get the size of each prefix and allocate the total storage cost proportionally.
`},{id:586,slug:"using-spot-preemptible-instances-for-cost-reduction",question:"How to get started with using spot/preemptible instances or scheduling off-peak runs to reduce Dataiku compute costs.",answer:`
### 1. Introduction/Overview
A significant portion of cloud costs comes from compute instances (VMs). There are two powerful strategies for reducing these costs: using cheaper, interruptible **spot/preemptible instances** for non-critical workloads, and **scheduling** large jobs to run during off-peak hours when compute may be cheaper or more available.

### 2. Prerequisites
- **Dataiku running on the cloud.**
- **An understanding of which of your workloads are critical versus non-critical.**

### 3. Step-by-Step Instructions

#### Strategy 1: Using Spot/Preemptible Instances
1.  **What they are:** Spot (AWS) or Preemptible (GCP) instances are a cloud provider's spare compute capacity, which they sell at a massive discount (up to 90% off). The catch is that the provider can reclaim this capacity at any time with very little warning.
2.  **When to use them:** They are perfect for workloads that are **fault-tolerant** and **not time-critical**. This includes:
    *   Non-urgent development and testing jobs.
    *   Large data exploration tasks.
    *   Certain types of model training that can be checkpointed and resumed.
3.  **How to use with Dataiku:**
    *   If you are using Kubernetes for job execution, you can create a dedicated **node pool** that is configured to use only spot instances.
    *   You can then use Kubernetes taints and tolerations to direct your non-critical Dataiku jobs to run on this spot instance node pool.
4.  **Do NOT use for:** Production API nodes or critical, time-sensitive data pipelines, as the termination of the instances could cause failures.

#### Strategy 2: Scheduling Off-Peak Runs
1.  **What it is:** A simple but effective strategy of running your most resource-intensive jobs at night or on the weekend when there is less contention for shared resources.
2.  **How to implement:**
    *   Identify your largest, longest-running Dataiku pipelines.
    *   Create a **Scenario** to automate these pipelines.
    *   In the scenario's **Settings > Triggers**, add a **Time-based** trigger.
    *   Schedule the trigger to run during off-peak hours (e.g., every day at 2 AM).
3.  **Benefits:**
    *   Reduces resource contention with interactive users during business hours.
    *   Can sometimes result in lower costs if your cloud provider has variable pricing or if you can leverage autoscaling to shrink your clusters during the day.

### 4. Resources and Tools
- **Cloud Provider Instance Types:** Understanding the difference between on-demand, reserved, and spot instances.
- **Kubernetes Node Pools:** A way to segregate different types of hardware in your cluster.
- **Dataiku Scenarios and Triggers:** The tool for off-peak scheduling.

### 5. Next Steps and Progression
- **Mixed-Instance Clusters:** You can configure a Spark or Kubernetes cluster to use a mix of instance types: a few stable on-demand instances for critical services, and a large number of spot instances for the worker nodes to provide cheap, scalable compute power.

### 6. Common Challenges and Solutions
- **Challenge:** "My job running on spot instances failed because the node was terminated."
- **Solution:** This is the expected behavior and risk of using spot instances. You must design your job to be fault-tolerant. For example, Spark is naturally resilient to losing a worker node, as it can recompute the lost data partitions on other nodes. Ensure your job is not time-critical if using spot instances.
`},{id:587,slug:"archiving-deleting-old-artifacts-for-storage-fees",question:"How to get started with archiving or deleting old Dataiku artifacts (models, datasets) to save storage fees.",answer:`
### 1. Introduction/Overview
Over time, a Dataiku instance can accumulate a large number of old datasets, model versions, and project exports that consume valuable and costly storage. Implementing a regular cleanup and archiving process is a crucial housekeeping task for managing storage costs and keeping your instance tidy.

### 2. Prerequisites
- **A Dataiku instance that has been in use for some time.**
- **A defined data retention policy:** You need to know how long different types of data need to be kept.
- **A cheap, long-term archive storage location** (e.g., AWS S3 Glacier, Azure Archive Storage).

### 3. Step-by-Step Instructions: The Cleanup Process

1.  **Identify Obsolete Artifacts:**
    *   **Old Datasets:** Look for intermediate datasets in flows that are no longer being rebuilt or used. The "Last modified" date is a good indicator.
    *   **Old Model Versions:** In a "Saved Model", you might have dozens of old, inactive versions from previous retraining runs.
    *   **Old Project Exports/Bundles:** Check the server for old project \`.zip\` exports that are no longer needed.
2.  **Decide: Archive or Delete?**
    *   **Archive:** If you need to keep the artifact for compliance or potential future use, but don't need it for active operations.
    *   **Delete:** If the artifact is truly temporary and has no further value.
3.  **Implement the Process:**

    *   **For Datasets:**
        1.  **Archive:** Use an **Export** recipe to save the dataset to your long-term archive storage location.
        2.  **Delete:** Once archived, use the **Delete** action in the Flow to remove the dataset and free up its active storage.
    *   **For Model Versions:**
        1.  In the "Saved Model" view, you can select and delete old, inactive versions.
    *   **For Partitions:**
        1.  On a partitioned dataset, you can use a Python recipe with the Dataiku API to list all partitions and delete any that are older than your retention policy (e.g., delete partitions older than 2 years).

4.  **Automate with a Scenario:**
    *   Create a dedicated "Cleanup" scenario.
    *   In this scenario, have a **Python step** that uses the Dataiku API to programmatically find and delete old artifacts based on your rules.
    *   Schedule this scenario to run periodically (e.g., monthly).

### 4. Resources and Tools
- **Dataiku API:** Essential for automating the identification and deletion of old artifacts.
- **Export Recipe:** For archiving data before deletion.
- **A defined retention policy.**

### 5. Next Steps and Progression
- **Storage Lifecycle Policies:** Use your cloud provider's storage lifecycle policies. For example, you can set a rule on an S3 bucket to automatically move any object that hasn't been accessed in 90 days to a cheaper storage class like Glacier.

### 6. Common Challenges and Solutions
- **Challenge:** "I accidentally deleted something I needed."
- **Solution:** This is the primary risk. **Never delete anything without being certain it's no longer needed.** Before deleting a dataset, always use the "View downstream dependencies" feature to check if it's being used. Have a backup and restore plan in place for your Dataiku instance.
- **Challenge:** "The cleanup script is complex to write."
- **Solution:** Start simple. Your first script could just identify and report on potential candidates for deletion, writing the list to a dataset. A human can then review this list and perform the deletion manually. You can add the automatic deletion logic later once you are confident in the script.
`},{id:588,slug:"analyzing-cluster-usage-logs-for-redundant-workloads",question:"How to get started with analyzing Dataiku cluster usage logs to spot unnecessary or redundant workloads.",answer:`
### 1. Introduction/Overview
A common source of wasted compute resources and unnecessary cost is redundant work—jobs that are running more often than needed or are rebuilding the same data multiple times. Analyzing Dataiku's job logs and monitoring your cluster's usage patterns can help you spot and eliminate this inefficiency.

### 2. Prerequisites
- **A shared Dataiku instance** with multiple projects and scenarios.
- **Access to the Dataiku Jobs monitoring page** and/or your cluster's monitoring tools (e.g., Spark UI, YARN ResourceManager).

### 3. Step-by-Step Instructions
1.  **Review the Job Monitoring Page:**
    *   In Dataiku, go to **Administration > Monitoring > Jobs**.
    *   This page shows a list of all jobs that have run on the instance.
2.  **Look for Redundant Builds:**
    *   Filter the job list for "Build" jobs on a specific dataset.
    *   Do you see the same dataset being rebuilt by multiple different scenarios at different times? This is a red flag. It might mean two different teams have independently built pipelines to create the same data.
    *   **Solution:** Consolidate the work. Create a single, authoritative pipeline in a shared project that builds this dataset once. Have all other downstream processes consume this "golden" dataset instead of rebuilding it themselves.
3.  **Look for Inefficient Scheduling:**
    *   Filter the jobs by trigger type and name.
    *   Is a very large, expensive job scheduled to run every hour when the business users only look at the report once a day?
    *   **Solution:** Work with the business owner to change the scenario's trigger to run less frequently, matching the actual business need.
4.  **Analyze Cluster Logs (for Spark/Hadoop):**
    *   Use the YARN or Spark UI to look at the applications that are running.
    *   Are there jobs that are consistently reading a huge amount of data just to filter it down to a tiny subset?
    *   **Solution:** The upstream data source should be **partitioned**. This would allow the job to read only the specific partition it needs, avoiding the wasteful full table scan.

### 4. Resources and Tools
- **Dataiku's Job Monitoring Page:** Your primary tool for seeing what is running and when.
- **Your Cluster's Native UI (Spark UI, YARN UI):** For deeper analysis of compute jobs.

### 5. Next Steps and Progression
- **Create a "Cost Optimization" Dashboard:** Create a Dataiku project that uses the API to fetch the job history, analyze it for potential redundancies, and present a dashboard highlighting the top candidates for optimization.
- **Implement Showback:** Create a process to show teams the compute resources their jobs are consuming. When they see the cost, they are often more motivated to find and eliminate redundant work.

### 6. Common Challenges and Solutions
- **Challenge:** "I see two teams building the same dataset, but they refuse to use a shared version."
- **Solution:** This is an organizational and governance challenge. A "Center of Excellence" or a data governance council may need to step in to establish the principle of a "single source of truth". The goal is to create one certified, trusted dataset that everyone agrees to use.
- **Challenge:** "I don't know what all these jobs are."
- **Solution:** This points to a lack of documentation. Every project and scenario should have a clear owner and a description explaining its purpose. Without this, it's very difficult to manage a shared instance effectively.
`},{id:589,slug:"using-cloud-budget-alerts-for-monitoring-spending",question:"How to get started with using cloud budget alerts (AWS Budgets, Azure Cost Alerts) to monitor Dataiku spending.",answer:`
### 1. Introduction/Overview
When running Dataiku and its associated compute infrastructure in the cloud, costs can escalate quickly if not monitored. Cloud budget alerts are a simple but powerful FinOps tool. They act as a safety net, automatically notifying you when your spending exceeds a predefined threshold, which helps prevent unexpected bill shocks.

### 2. Prerequisites
- **Dataiku running on a cloud provider** (AWS, Azure, GCP).
- **A disciplined tagging strategy:** All your Dataiku-related resources must be consistently tagged.
- **Access to your cloud provider's billing and cost management console.**

### 3. Step-by-Step Instructions

#### For AWS (using AWS Budgets)
1.  **Navigate to AWS Budgets:** In the AWS Management Console, go to the "Billing" service and find "Budgets".
2.  **Create a Budget:**
    *   Click **Create budget**.
    *   Choose a **Cost Budget**.
    *   **Set your budget details:**
        *   Give it a name (e.g., \`dataiku-prod-monthly-budget\`).
        *   Set the period (e.g., Monthly).
        *   Enter the budgeted amount (e.g., $5000).
    *   **Scope the budget using tags:** In the "Filtering" section, choose **Tag** and select the tag key and value you use for your Dataiku resources (e.g., Key=\`service\`, Value=\`dataiku\`). This is the most important step.
3.  **Configure Alerts:**
    *   Add an **Alert threshold**. For example, "Alert me when actual spend reaches 80% of the budgeted amount."
    *   Enter the email addresses or SNS topic to send the notification to.
4.  **Create:** Create the budget. AWS will now monitor your spending against the budget and send an alert if the threshold is breached.

#### For Azure (using Budgets in Cost Management)
- The process is very similar. You go to **Cost Management + Billing > Budgets**, create a new budget, set the amount, and then use a **Filter** to scope the budget to the resource group or the specific tags associated with your Dataiku deployment. You then configure "Alert conditions" to send emails when a percentage of the budget is met.

### 4. Resources and Tools
- **Your Cloud Provider's Cost Management and Billing Console.**
- **A consistent tagging strategy.**

### 5. Next Steps and Progression
- **Multiple Budgets:** You can create multiple budgets for more granular tracking. For example, a budget for your dev environment and a separate, larger budget for your prod environment.
- **Actionable Alerts:** When you receive a budget alert, you must investigate. Use the cost analysis tools to see what caused the spending spike. Was it an inefficient query that ran for too long? Or did a user spin up a large, expensive cluster and forget to turn it off?

### 6. Common Challenges and Solutions
- **Challenge:** "The budget alert fired, but the amount seems wrong."
- **Solution:** This is almost always caused by an inconsistent tagging strategy. There is likely a resource that is part of your Dataiku deployment that you forgot to tag, so it's not being included in the budget's scope. You need to perform an audit of all your resources and ensure all of them are correctly tagged. Many organizations use automated scripts or policies to enforce tagging.
- **Challenge:** "We get alerts every month. What should we do?"
- **Solution:** If you are consistently exceeding your budget, it means either your budget is unrealistically low, or your platform is becoming more popular and using more resources (which can be a good thing!). You need to analyze the spending trend and either optimize your workloads to reduce costs or work with your finance department to get the budget increased to reflect the actual usage.
`},{id:590,slug:"customizing-dashboards-visual-reports-for-ml-pipeline-kpis",question:"How to get started with customizing Dataiku dashboards and visual reports for ML pipeline KPIs.",answer:`
### 1. Introduction/Overview
Monitoring the health and performance of your MLOps pipelines is essential. A dedicated Dataiku Dashboard can serve as a central "control tower," visualizing the Key Performance Indicators (KPIs) of your ML pipelines, such as model accuracy, data drift scores, and job runtimes.

### 2. Prerequisites
- **An automated ML pipeline** running as a Dataiku Scenario.
- **A process for generating the KPI data:** Your scenarios should be creating datasets that contain the metrics you want to track.

### 3. Step-by-Step Instructions
1.  **Create a Monitoring Project or Zone:**
    *   It's a good practice to have a dedicated project or a "Monitoring" Flow Zone whose job is to track the performance of your other pipelines.
2.  **Generate KPI Datasets:**
    *   Your production scenarios should output their results as datasets. For example:
        *   Your **model evaluation** step should output a dataset containing accuracy, AUC, F1-score, etc., for each run.
        *   Your **data drift** analysis step should output a dataset with the drift scores.
        *   You can use the Dataiku API to create a dataset of **scenario run times** and outcomes.
3.  **Build the Dashboard:**
    *   In your monitoring project, create a new **Dashboard**.
4.  **Create KPI Visualizations:**
    *   For each KPI, create a chart on its corresponding dataset and add it to the dashboard.
    *   **Model Performance Trend:** Create a **line chart** showing how your model's AUC or accuracy has changed over time (with each retraining run).
    *   **Data Drift History:** Create a line chart showing the data drift score over time.
    *   **Pipeline Health:** Create a **bar chart** showing the count of "SUCCESS" vs. "FAILED" scenario runs over the last 30 days.
    *   **Latest Runtimes:** Use a **table view** to show the duration of the last few scenario runs.
5.  **Add Context:** Use **Text** tiles to add headers and explanations for each section of your MLOps dashboard.

### 4. Resources and Tools
- **Dataiku Dashboards:** The visualization tool.
- **Dataiku Scenarios:** To generate the underlying KPI data.
- **The Charts tab** on datasets.

### 5. Next Steps and Progression
- **Alerting:** While the dashboard is for visualization, your scenarios should still have active **Reporters** to send immediate alerts when a critical KPI (like a failed job or a drop in model accuracy) crosses a threshold.
- **Share with Stakeholders:** Share this dashboard with the MLOps team, data scientists, and relevant product managers to provide a transparent, unified view of the health of your production ML systems.

### 6. Common Challenges and Solutions
- **Challenge:** "My KPI charts are not updating."
- **Solution:** You need to ensure your scenarios are correctly building the underlying KPI datasets and that the dashboard's caches are being refreshed. Your main monitoring scenario should have a final step to "Refresh dashboard caches."
- **Challenge:** "The dashboard is too cluttered."
- **Solution:** Focus on the most important KPIs. You might create separate dashboards for different concerns: one for model performance, another for pipeline operational health, and a third for infrastructure costs.
`},{id:591,slug:"developing-custom-plugin-for-environment-provisioning",question:"How to get started with developing a custom Dataiku plugin to automate environment provisioning.",answer:`
### 1. Introduction/Overview
This is a very advanced MLOps automation pattern. A custom Dataiku plugin can be developed to provide a simple UI that allows an administrator or a user to automatically provision a new, standardized Dataiku project or environment by clicking a button. This abstracts away the complexity of the underlying API calls and scripts.

### 2. Prerequisites
- **Expert-level knowledge of the Dataiku Python API.**
- **Administrator rights** and filesystem access to a dev instance for plugin development.
- **A clear, standardized process** for what needs to be created when a new project is provisioned.

### 3. Step-by-Step Instructions: The Plugin's Logic
The goal is to create a custom **Macro** within a plugin.

1.  **Define the Macro's UI:**
    *   In your plugin's definition files, you will define the UI that the user will see when they run the macro.
    *   This could include input fields for the "New Project Name" and a dropdown to select a "Project Template".
2.  **Write the Python Backend:**
    *   The core of the macro is a Python script that runs when the user clicks the "Run" button.
    *   This script will use the **Dataiku Python API client** (\`dataiku.api_client()\`) to perform a series of administrative actions:
        1.  **Get user inputs:** Read the values from the macro's UI (the new project name).
        2.  **Duplicate a template project:** Use the API to find your standard project template and duplicate it to create the new project.
        3.  **Set permissions:** Use the API to set the default group permissions on the new project.
        4.  **Create connections (if needed):** The script could even call cloud provider SDKs to provision a new database schema for the project.
        5.  **Log the action:** The script should log that a new project was provisioned, by whom, and when.
3.  **Expose the Macro:**
    *   Once the plugin is developed and installed, the new macro will be available to run from a Dataiku Dashboard or from a Scenario. An administrator could create a "Project Vending Machine" dashboard with a button to run this macro.

### 4. Resources and Tools
- **Dataiku Developer Guide:** The official documentation on how to build custom plugins and macros.
- **Dataiku Python API:** The API client is essential for performing the administrative actions.
- **A "Template" Project:** The plugin will use this as the base for creating new projects.

### 5. Next Steps and Progression
- **Webapp Interface:** You could build a full Dataiku Webapp as the user interface for this provisioning process, providing a more guided and user-friendly experience than a simple macro.

### 6. Common Challenges and Solutions
- **Challenge:** "Plugin development is too complex."
- **Solution:** It is the most advanced form of customization in Dataiku. This pattern should only be considered by organizations with a mature MLOps practice and a strong need to automate the provisioning of dozens or hundreds of projects in a standardized way.
- **Challenge:** "The script fails with a permissions error."
- **Solution:** The user or service account running the macro needs to have the appropriate global permissions in Dataiku to perform the actions in the script, such as "Create projects".
`},{id:592,slug:"securing-scenarios-by-rotating-credentials-secrets",question:"How to get started with securing Dataiku scenarios by rotating credentials and managing secrets.",answer:`
### 1. Introduction/Overview
If your scenarios interact with external systems, they will need credentials. Hardcoding these secrets is a major security risk. Securing your scenarios involves storing secrets in a managed location and, for high-security environments, integrating with a system that can automatically rotate these credentials.

### 2. Prerequisites
- **A scenario that needs to use a secret** (e.g., a database password or an API key).
- **Administrator access** to Dataiku to configure secret management.

### 3. Step-by-Step Instructions: A Tiered Approach to Security

#### Level 1: Use Dataiku's Built-in Secret Management (Good)
- **What it is:** Using **Project Variables** of the "Password" type.
- **How:**
    1.  In your project, go to **... > Variables**.
    2.  Create a variable for your secret (e.g., \`MY_API_KEY\`).
    3.  Set its type to **Password**. This encrypts it and masks it in the UI.
- **Pros:** Simple, built-in, and much better than hardcoding.
- **Cons:** The secret is still static; it does not rotate automatically.

#### Level 2: Use an External Secrets Vault (Better)
- **What it is:** Integrating Dataiku with a dedicated secrets management tool like **HashiCorp Vault** or **Azure Key Vault**.
- **How (Admin Task):**
    1.  An administrator configures the integration in **Administration > Settings**.
    2.  Now, when you create a Project Variable or a Connection, you will have a new option to fetch the value from the external vault instead of typing it in.
- **Pros:** Centralized management of secrets. The secrets live in a dedicated, highly secure system.
- **Cons:** Still often relies on a static token to connect to the vault itself.

#### Level 3: Dynamic, Rotated Credentials (Best)
- **What it is:** The external vault is configured to automatically generate new, temporary credentials on a regular basis (e.g., every 24 hours).
- **How it Works:**
    1.  This uses the integration from Level 2.
    2.  When your Dataiku scenario runs, it asks the vault for the database password.
    3.  The vault provides the current, valid password.
    4.  Later, the vault automatically rotates the password in the database.
    5.  The next time your scenario runs, it gets the *new* password from the vault.
- **Pros:** Highly secure. The credentials are short-lived, dramatically reducing the window of opportunity if one is compromised.

### 4. Resources and Tools
- **Project Variables (Password type).**
- **External Secrets Management tools (HashiCorp Vault, etc.).**
- **IAM Roles (for cloud):** The best way to avoid static secrets for cloud resources is to use temporary credentials from an IAM role.

### 5. Next Steps and Progression
- **Audit Secret Access:** Your external vault will provide a detailed audit log of which service or user requested which secret and when.

### 6. Common Challenges and Solutions
- **Challenge:** "Setting up a secrets vault is too complicated."
- **Solution:** It is an advanced infrastructure setup. For many use cases, using Dataiku's built-in "Password" type variables (Level 1) provides a sufficient level of security and is a huge improvement over hardcoding secrets in scripts.
`},{id:593,slug:"using-backup-restore-for-disaster-recovery",question:"How to get started with using Dataiku’s backup and restore features as part of a disaster recovery plan.",answer:`
### 1. Introduction/Overview
A Disaster Recovery (DR) plan is essential for any critical system. It outlines how you will restore service in the event of a major failure (e.g., a server crash, data corruption). For Dataiku, the core of a DR plan is a regular, automated backup of the Dataiku instance configuration and data, and a clear, tested process for restoring it. This is an administrator-level responsibility.

### 2. Prerequisites
- **A production Dataiku instance.**
- **Administrator access** to the Dataiku server's command line.
- **A separate, secure storage location** for your backups (e.g., a different server, a cloud storage bucket).

### 3. Step-by-Step Instructions

#### Part 1: Performing the Backup
1.  **Understand What to Back Up:** A full Dataiku backup consists of two parts:
    *   **The Dataiku Data Directory:** This folder contains all your projects, recipes, model definitions, configurations, etc.
    *   **The Backend Database:** If you are using an external PostgreSQL database for Dataiku's configuration, you must back up this database as well.
2.  **Use the Backup Script:** Dataiku provides a command-line script to simplify the backup process.
    > \`[DATAİKU_DATA_DIR]/bin/backup-master\`
3.  **Automate the Backup:**
    *   Create a shell script that runs this backup command.
    *   The script should also copy the resulting backup \`.zip\` file to your secure, off-site storage location.
    *   Use a system scheduler (like \`cron\` on Linux) to run this backup script automatically on a regular basis (e.g., every night).

#### Part 2: Performing the Restore
1.  **Prepare a New Server:** In a disaster scenario, you would first provision a new, clean server for Dataiku.
2.  **Install Dataiku:** Install the exact same version of the Dataiku software on the new server.
3.  **Retrieve the Backup:** Copy your latest backup \`.zip\` file from your archive storage to the new server.
4.  **Use the Restore Script:** Dataiku provides a restore script.
    > \`[NEW_DATAİKU_DATA_DIR]/bin/restore-master --file /path/to/backup.zip\`
5.  **Start Dataiku:** After the restore is complete, you can start the Dataiku service on the new server. It will be restored to the state it was in at the time of the backup.

### 4. Resources and Tools
- **The Dataiku Backup and Restore scripts:** Located in the \`bin\` directory of your installation.
- **A system scheduler (\`cron\`).**
- **A secure, remote location** for storing your backups.

### 5. Next Steps and Progression
- **Test Your Restore Process:** A backup is useless if you can't restore it. You must periodically (e.g., quarterly) test your disaster recovery plan by performing a full restore to a temporary, non-production environment. This ensures your backups are valid and your process works.
- **High Availability (HA):** For near-zero downtime, you can implement a High Availability architecture with a multi-node setup and a replicated backend database. This provides automatic failover but is a more complex and expensive setup.

### 6. Common Challenges and Solutions
- **Challenge:** "The restore failed."
- **Solution:** This is why you test your backups. A common reason for failure is trying to restore a backup from one version of Dataiku onto a different version. The versions must match.
- **Challenge:** "We lost a day of work because the backup was from last night."
- **Solution:** This is the trade-off. The frequency of your backups determines your Recovery Point Objective (RPO). A nightly backup means you could potentially lose up to 24 hours of work. For more critical systems, you might need to run backups more frequently.
`},{id:594,slug:"auditing-logs-with-elk-stack",question:"How to get started with auditing Dataiku logs using an ELK stack for troubleshooting and compliance.",answer:`
### 1. Introduction/Overview
The ELK Stack (Elasticsearch, Logstash, Kibana) is a popular open-source platform for centralized logging and analysis. Integrating Dataiku with ELK allows you to collect, search, and visualize all your Dataiku logs (from the backend, jobs, audits, etc.) in one place, which is powerful for troubleshooting and compliance auditing.

### 2. Prerequisites
- **A running ELK stack.**
- **Administrator access** to the Dataiku server(s) to install a log shipper.
- **Network connectivity** between the Dataiku servers and your Logstash instance.

### 3. Step-by-Step Instructions: The Logging Pipeline

1.  **Install a Log Shipper (Filebeat):**
    *   On every server that hosts a Dataiku component (backend, API node, etc.), you need to install a log shipping agent. **Filebeat** is a lightweight and common choice from the Elastic stack.
2.  **Configure Filebeat:**
    *   You need to configure Filebeat by editing its \`filebeat.yml\` file.
    *   You will tell it which Dataiku log files to monitor. Key log files include:
        *   \`[DATAİKU_DATA_DIR]/run/backend.log\`
        *   \`[DATAİKU_DATA_DIR]/run/jobs.log\`
        *   \`[DATAİKU_DATA_DIR]/run/audit.log\`
    *   You also configure the "output" to point to the address of your Logstash server.
3.  **Configure Logstash (The Parser):**
    *   Logstash receives the raw log lines from Filebeat. Its job is to parse and structure them.
    *   Dataiku logs are in a structured JSON format, which makes them easy to parse. You will use a Logstash configuration file to define a pipeline that uses a "json" filter to parse the log message.
    *   You can also use Logstash to enrich the logs, for example, by adding metadata about the source server.
4.  **Configure the Logstash Output:** The final step in the Logstash pipeline is to send the processed, structured JSON to your Elasticsearch cluster for indexing.
5.  **Visualize and Audit in Kibana:**
    *   Now that the logs are in Elasticsearch, you can use **Kibana**.
    *   In Kibana, you can create dashboards to visualize log data (e.g., a chart of errors over time).
    *   Most importantly, you can use Kibana's powerful search and filtering capabilities to troubleshoot issues (e.g., "show me all logs for job_id X that contain the word 'error'").

### 4. Resources and Tools
- **The ELK Stack:** Elasticsearch, Logstash, Kibana.
- **Filebeat:** The log shipping agent.

### 5. Next Steps and Progression
- **Create Alerts:** In Kibana, you can create alerts that will automatically notify you if certain conditions are met in your logs (e.g., "alert me if we see more than 10 critical errors in the last 5 minutes").
- **Correlate with Other Logs:** The real power of a central logging system is the ability to correlate Dataiku logs with logs from your other applications and infrastructure, giving you a complete view of your system's behavior.

### 6. Common Challenges and Solutions
- **Challenge:** "I don't see my logs in Kibana."
- **Solution:** You need to debug the pipeline step-by-step. Is Filebeat running and can it read the log files? Can Filebeat connect to Logstash (check firewalls)? Is Logstash successfully parsing the logs and sending them to Elasticsearch? Check the logs of each component in the ELK stack.
- **Challenge:** "The logs are not parsed correctly; they just show up as a single long message."
- **Solution:** Your Logstash parsing configuration is incorrect. You need to ensure you are using the "json" filter to correctly parse the structured log entries that Dataiku produces.
`},{id:595,slug:"integrating-with-big-data-tools-hadoop-hdfs-hive",question:"How to get started with integrating Dataiku with Big Data tools (Hadoop HDFS, Hive) for large-scale pipelines.",answer:`
### 1. Introduction/Overview
Dataiku is designed for the Big Data ecosystem. It can be tightly integrated with a Hadoop cluster, using HDFS as its primary storage backend and submitting computation to engines like Spark and Hive. This allows Dataiku to act as a user-friendly, high-level interface for your powerful but complex big data infrastructure.

### 2. Prerequisites
- **A running Hadoop cluster** with HDFS, YARN, and Hive.
- **Dataiku installed on an "edge node"** of the cluster. This is critical for network connectivity and access to client configurations.
- **Administrator access** to both Dataiku and the Hadoop cluster.

### 3. Step-by-Step Instructions: The Integration Process

1.  **Installation on an Edge Node:** The Dataiku server software must be installed on a node that is part of the Hadoop cluster and has all the necessary Hadoop client libraries and configuration files (\`core-site.xml\`, \`hdfs-site.xml\`, etc.) available.
2.  **Hadoop Configuration:**
    *   During the installation of Dataiku, the setup script will detect the local Hadoop installation and prompt you to configure the integration.
    *   You will point Dataiku to the Hadoop configuration directory.
    *   Dataiku will then automatically configure itself to use HDFS for storage and YARN for resource management.
3.  **Hive Integration:**
    *   In **Administration > Settings > Hive**, you can configure the connection to the Hive metastore and set the execution engine.
4.  **Using the Integrated Environment:**
    *   **For Storage:** You can now create datasets that read from and write to HDFS paths directly. When you create a new dataset, HDFS will be the default storage location.
    *   **For Compute:** In any visual recipe, you can now select **Hive** as an **Execution engine**. Dataiku will translate the visual steps into a HiveQL query and execute it on the cluster. You can also create **Hive recipes** to write HiveQL code directly.

### 4. Resources and Tools
- **The Dataiku Installation Guide:** Provides detailed instructions for different Hadoop distributions.
- **The Recipe Execution Engine Dropdown:** The UI for choosing Hive (or Spark) as the compute engine.
- **Hive Recipes:** For writing custom HiveQL queries.

### 5. Next Steps and Progression
- **Spark on YARN:** The same integration allows you to run Spark jobs. You can set the execution engine for recipes to **Spark** and create **PySpark** recipes. Dataiku will submit these jobs to the YARN resource manager on your cluster.
- **Security (Kerberos):** If your Hadoop cluster is secured with Kerberos, Dataiku can be configured to work in this environment. This is a complex setup that requires careful configuration of keytabs and principals.

### 6. Common Challenges and Solutions
- **Challenge:** "Dataiku can't write to HDFS; I'm getting a permissions error."
- **Solution:** The user account that the Dataiku service is running as on the server needs to have the correct permissions (read, write, execute) on the HDFS directories it's trying to use. You may need to work with your Hadoop administrator to set up the HDFS permissions correctly.
- **Challenge:** "My Hive job is failing."
- **Solution:** Use the YARN ResourceManager UI to find your failed job and look at its logs. The detailed error message from the Hive execution will be there. Common issues include syntax errors in the HiveQL or resource issues on the cluster.
`},{id:596,slug:"using-web-application-node-to-build-deploy-ml-apps",question:"How to get started with using Dataiku’s Web Application node to quickly build and deploy ML apps.",answer:`
### 1. Introduction/Overview
A Dataiku Webapp is a simple, interactive web application that you can build inside your project. It's a fantastic way to create a user-friendly "front-end" for your data pipeline or machine learning model, allowing non-technical users to interact with your work without seeing the underlying complexity.

### 2. Prerequisites
- **A Dataiku pipeline or model** that you want to expose through a UI.
- **A clear idea of what you want the user to do** in the app.

### 3. Step-by-Step Instructions: Building a Simple App

1.  **Create a New Webapp:** In your project's top navigation bar, go to **... > Webapps**. Click **+ NEW WEBAPP**.
2.  **Choose an App Template:** Dataiku offers several templates.
    *   **Standard:** Best for creating simple, "wizard-like" apps with a few inputs and outputs.
    *   **Dash/Bokeh/Streamlit:** For creating more complex, interactive dashboards using Python web frameworks. For a first app, **Standard** is a good choice.
3.  **Design the UI:**
    *   The editor for a Standard webapp is slide-based.
    *   **Add an Input Slide:** Add a slide with input widgets. For example, add a **Dropdown menu** to let the user select a value.
    *   **Link Inputs to Variables:** Connect the input widget to a **Project Variable**. When the user makes a selection, it will update the variable's value.
    *   **Add an Action Button:** Add a button that the user can click. Configure this button to run a **Scenario**. This scenario should be parameterized to use the project variable that the user just set.
    *   **Add an Output Slide:** Add a final slide to display the results. You can add a **Chart**, a **Dataset table**, or other widgets that show the output of the scenario run.
4.  **Test and Share:**
    *   Use the "View" mode to test your app.
    *   Once it's working, you can share a direct link to the webapp with your business users. They will see the simple UI you created, not the complex Flow.

### 4. Resources and Tools
- **Dataiku Webapps:** The core feature for building the app.
- **Project Variables:** The mechanism for passing information from the user's input in the app to your backend pipeline.
- **Scenarios:** The engine that runs the backend logic when the user clicks a button.

### 5. Next Steps and Progression
- **Python-backed Apps:** For full interactivity, build a webapp using the **Streamlit** or **Dash** templates. This lets you write Python code to create a dynamic UI that can react instantly to user input without needing to run a full scenario.
- **Embed in a Dashboard:** You can embed your finished webapp as a tile in a standard Dataiku Dashboard.

### 6. Common Challenges and Solutions
- **Challenge:** "The user clicks the button, but the results don't update."
- **Solution:** Your scenario needs to run and complete successfully. You may need to add logic that explicitly refreshes the output tile after the scenario is done. For Python-backed apps, you need to write callback functions to handle the updates.
- **Challenge:** "The app UI is confusing."
- **Solution:** Keep it simple. A good webapp should have a very clear, single purpose. Use **Text** widgets to provide clear instructions to the user on every slide. Guide them through the process.
`},{id:597,slug:"implementing-scenario-based-testing",question:"How to get started with implementing scenario-based testing (data drift tests, regression tests) in Dataiku.",answer:`
### 1. Introduction/Overview
Scenario-based testing is the practice of creating a dedicated Dataiku Scenario whose sole purpose is to run a suite of automated tests against your pipeline. This is a core component of CI/CD and MLOps, as it allows you to automatically validate your project's quality and correctness.

### 2. Prerequisites
- **A Dataiku project** with a pipeline you want to test.
- **A clear definition of your test cases.**

### 3. Step-by-Step Instructions: Building a Test Scenario

1.  **Create a "Test" Scenario:**
    *   In your project, go to **Scenarios** and create a new scenario. Name it descriptively, like \`Run_Project_Tests\`.
2.  **Add Test Steps:** This scenario will not build your main outputs. Instead, it will contain steps that perform checks.
    *   **Step 1: Data Quality Checks.**
        *   Add a **Run checks** step.
        *   Configure it to run the predefined quality checks on your key input and output datasets. This tests for things like nulls, valid ranges, etc.
    *   **Step 2: Data Drift Checks.**
        *   Add another **Run checks** step.
        *   This time, point it to your **Saved Model** object. This will execute the drift analysis you have configured on the model, checking if the input data has changed significantly.
    *   **Step 3: Model Regression Tests.**
        *   Add a final **Run checks** step on your Saved Model.
        *   This can run a check to ensure the model's performance (e.g., AUC) on a test set has not dropped below a certain threshold.
    *   **Step 4: Unit Tests (for code).**
        *   If you have written unit tests for your Python libraries, you can add a Python recipe that runs these tests and have this step build that recipe. The recipe should be written to fail if any unit test fails.
3.  **How it Works:**
    *   If any of the "Run checks" steps encounters a check that fails at the "Error" severity level, the step will fail, which in turn causes the entire test scenario to fail.
4.  **Integrate with CI/CD:**
    *   The primary use of this test scenario is to be triggered by your CI/CD pipeline.
    *   When a developer opens a pull request, the CI pipeline runs this test scenario. If it fails, the build is marked as "failed," and the developer knows they have introduced a regression that they need to fix before the code can be merged.

### 4. Resources and Tools
- **Scenarios:** The orchestration engine for your tests.
- **Run Checks Step:** The key step for executing your data and model validation rules.
- **A CI/CD tool (Jenkins, GitHub Actions, etc.).**

### 5. Next Steps and Progression
- **Test on a Sample:** For your CI pipeline, your test scenario should ideally run on a small, static sample of the data. This ensures the tests run quickly and are repeatable. The goal is to test the pipeline's *logic*, not to process large volumes of data.

### 6. Common Challenges and Solutions
- **Challenge:** "My tests are flaky; they sometimes pass and sometimes fail."
- **Solution:** This usually means your checks are too sensitive or your test data is not static. For example, if you have a check on the exact row count, and the source data changes, the test will fail. Your tests should be designed to run against a fixed, known set of data to be reliable.
- **Challenge:** "The test scenario takes too long to run."
- **Solution:** You are likely running the tests on too much data. Create a small, representative sample of your data and have your test scenario run against that sample instead.
`},{id:598,slug:"linking-to-feature-store-for-reuse",question:"How to get started with linking Dataiku to a feature store to reuse features across projects.",answer:`
### 1. Introduction/Overview
A Feature Store is a central repository for storing, sharing, and managing curated features for machine learning models. Integrating Dataiku with a feature store allows data scientists to easily discover and reuse high-quality features built by other teams, which accelerates model development and ensures consistency.

### 2. Prerequisites
- **A Feature Store platform:** This could be an open-source tool like Feast or a managed service from a cloud provider.
- **A connection** from Dataiku to the feature store's underlying database (e.g., a Redis or SQL database).

### 3. Step-by-Step Instructions: The Integration Workflow

1.  **Connecting to the Feature Store (as a Data Source):**
    *   The feature store itself will store its data in a standard database (online stores often use a key-value store like Redis, while offline stores use a data warehouse like BigQuery).
    *   In Dataiku, an administrator creates a **Connection** to this underlying database.
2.  **Using Features from the Store (Feature Consumption):**
    *   A data scientist can now create a **Dataset** in their Dataiku project that points to a specific feature table in the feature store, using the connection created in the previous step.
    *   They can use this dataset as an input to their model training flow, joining the features with their specific target data.
3.  **Writing New Features to the Store (Feature Creation):**
    *   A data scientist or engineer can build a pipeline in Dataiku to create a new, valuable feature.
    *   The final step of this pipeline would be an **Export** recipe.
    *   This recipe would write the new feature data (e.g., a dataset with columns for \`user_id\`, \`feature_name\`, \`feature_value\`, \`timestamp\`) to a new table in the feature store's database. This makes the new feature available for others to use.

### 4. Resources and Tools
- **A Feature Store Platform:** The central service for managing features.
- **Dataiku Database Connectors:** The tool for reading from and writing to the feature store's underlying storage.
- **Dataiku Recipes (Join, Export):** The tools for consuming and producing features.

### 5. Next Steps and Progression
- **Plugin for Tighter Integration:** For a more seamless user experience, a developer could create a custom Dataiku plugin. This plugin could provide a custom "Feature Store" dataset connector that allows users to browse and select features by name, hiding the underlying database tables.
- **Real-time Features:** The feature store's online component can be used to serve features with low latency to your real-time model APIs deployed on Dataiku.

### 6. Common Challenges and Solutions
- **Challenge:** "How do I know which features are available in the store?"
- **Solution:** A good feature store has a UI or a registry that serves as a searchable catalog of all available features, along with their definitions, owners, and quality metrics.
- **Challenge:** "The data in the feature store is stale."
- **Solution:** The team that owns a feature is responsible for creating an automated Dataiku scenario that periodically recomputes and updates that feature in the store, ensuring it remains fresh.
`},{id:599,slug:"orchestrating-jobs-with-cloud-native-schedulers-kubernetes-executors",question:"How to get started with orchestrating Dataiku jobs using cloud-native schedulers (Cloud Composer/Airflow) with Kubernetes executors.",answer:`
### 1. Introduction/Overview
This is a modern, powerful, and cloud-native approach to orchestration. It involves using a managed scheduler like Google's Cloud Composer (which is a managed Apache Airflow) to define your workflows, and having Airflow execute tasks as pods on a Kubernetes cluster. This allows you to orchestrate Dataiku jobs as just one step in a much larger, multi-system workflow.

### 2. Prerequisites
- **A Kubernetes cluster** (e.g., GKE).
- **A managed Airflow instance** (e.g., Cloud Composer) that is configured to use the Kubernetes cluster for its executors.
- **A Dataiku instance.**
- **Network connectivity** between the Kubernetes cluster and the Dataiku instance.

### 3. Step-by-Step Instructions: The Workflow
1.  **Set up the Airflow Kubernetes Executor:**
    *   Configure your Airflow instance to use the \`KubernetesExecutor\`. This means that for every task in a DAG run, the Airflow scheduler will create a new pod on your Kubernetes cluster to execute that task.
2.  **Create a Custom Docker Image:**
    *   Create a Docker image that contains all the tools your Airflow tasks will need. This should include the Python library for your cloud provider and the \`requests\` library for calling the Dataiku API.
3.  **Write the Airflow DAG:**
    *   In your DAG file, you will define your pipeline.
    *   You will use the \`KubernetesPodOperator\` to run your tasks.
4.  **The Dataiku Task:**
    *   One of the tasks in your DAG will be responsible for triggering the Dataiku job.
    *   This task will use the \`KubernetesPodOperator\` to run a pod using your custom Docker image.
    *   The command for the pod will be a simple shell script that uses \`curl\` to make a **REST API call** to the Dataiku API endpoint to run a specific scenario.
    *   This script should also poll for the job's completion and check its status.
5.  **How it Works:** Airflow orchestrates the end-to-end pipeline. When it gets to the Dataiku step, it creates a pod on K8s. That pod's only job is to call the Dataiku API, effectively delegating the data processing work to the Dataiku platform.

### 4. Resources and Tools
- **Managed Airflow (Cloud Composer, etc.).**
- **Kubernetes and the KubernetesPodOperator.**
- **A custom Docker image.**
- **The Dataiku REST API.**

### 5. Next Steps and Progression
- **Passing Data:** You can pass information from one Airflow task to another using **XComs**. A task could pull some data, and then pass a parameter via XComs to the next task, which would then include that parameter in its API call to Dataiku.
- **Dynamic DAGs:** You can create Airflow DAGs that are dynamically generated, allowing you to build very complex and flexible orchestration pipelines.

### 6. Common Challenges and Solutions
- **Challenge:** "My task pod can't connect to Dataiku."
- **Solution:** This is a Kubernetes networking issue. The pods running your Airflow tasks need to be able to resolve and connect to the Dataiku server's address. This might require configuring DNS, VPC peering, or other network policies.
- **Challenge:** "How do I handle my Dataiku API key securely?"
- **Solution:** Use the secrets management system provided by your orchestrator and your cloud provider. In Airflow, you can store it as a Connection. In Kubernetes, you would store it as a K8s Secret and mount it into the task pod at runtime. **Do not** hardcode the key in your Docker image or your DAG file.
`},{id:600,slug:"managing-multi-region-multi-cloud-setup",question:"How to get started with managing Dataiku in a multi-region or multi-cloud setup for high availability.",answer:`
### 1. Introduction/Overview
For mission-critical applications that require very high availability and disaster recovery, you can deploy Dataiku in a multi-region or even a multi-cloud architecture. This is an advanced setup that provides resilience against the failure of an entire cloud region or provider.

### 2. Prerequisites
- **Presence in multiple cloud regions or multiple cloud providers.**
- **A deep understanding of cloud networking, data replication, and DNS.**
- **Administrator-level expertise** in Dataiku and cloud infrastructure.

### 3. Step-by-Step Instructions: Common Architectures

#### Pattern 1: Active-Passive for Disaster Recovery (DR)
- **What it is:** You have a primary, "active" Dataiku instance in one region, and a secondary, "passive" instance in another region that is kept as a cold or warm standby.
- **How it works:**
    1.  **Replication:** You must have a process to regularly replicate your Dataiku backups (both the Dataiku data directory and the backend database) from the active region to the passive region.
    2.  **DNS:** Use a DNS service (like AWS Route 53) with a failover routing policy. Normally, your Dataiku URL points to the active instance.
    3.  **Failover:** If the primary region fails, an administrator manually triggers a DNS failover. This points the URL to the passive instance. They then restore the latest backup to the passive instance, making it the new active one.
- **This provides disaster recovery, but the failover is not instantaneous.**

#### Pattern 2: Active-Active for High Availability (HA)
- **What it is:** You have two or more fully active Dataiku instances running in different regions, with a load balancer distributing traffic between them.
- **How it works:**
    1.  **Global Load Balancer:** Use a global load balancing service to direct users to the nearest or healthiest Dataiku instance.
    2.  **Project and Data Replication:** This is the hardest part. You need a robust, near real-time process to keep the projects and data on all active instances in sync. This can be extremely complex and often requires a custom-built replication solution.
    3.  **Centralized Backend:** The backend database for Dataiku would need to be a globally replicated database service (like Amazon Aurora Global Database or Google Cloud Spanner).
- **This provides near-zero downtime but is very complex and expensive to set up and maintain.**

### 4. Resources and Tools
- **Cloud Provider Global Load Balancers.**
- **DNS Failover Services.**
- **Data Replication Tools** (for databases and file storage).
- **Dataiku's Backup and Restore scripts.**

### 5. Next Steps and Progression
- **Start with DR:** For most organizations, a solid, well-tested Active-Passive disaster recovery plan is sufficient. An Active-Active setup is only justified for the most critical applications with extreme uptime requirements.

### 6. Common Challenges and Solutions
- **Challenge:** Keeping projects and data in sync in an Active-Active setup is a nightmare.
- **Solution:** It is. This is a major data engineering challenge. You need to handle concurrent writes and resolve conflicts. This often leads to an "eventually consistent" model, which may not be suitable for all use cases. The complexity of this problem is why most organizations opt for a simpler Active-Passive DR strategy.
- **Challenge:** "Our failover test took hours."
- **Solution:** Your Recovery Time Objective (RTO) was not met. You need to optimize your DR plan. Can the restore process be automated with scripts? Is the passive instance a "warm" standby (already running) to speed up the process? You must regularly test and refine your DR plan to ensure it's effective.
`},{id:601,slug:"establishing-slo-based-monitoring-alerting",question:"How to get started with establishing SLO-based monitoring and alerting for Dataiku-driven ML services.",answer:`
### 1. Introduction/Overview
Moving beyond simple threshold-based alerts (e.g., "CPU > 90%"), Service Level Objective (SLO)-based monitoring is a more mature and user-centric approach. An SLO is a target for a specific metric (e.g., "99.9% of API requests should be served in under 200ms"). This framework helps you balance reliability with the pace of innovation.

### 2. Prerequisites
- **A production ML service** (e.g., a Dataiku API endpoint).
- **A monitoring system** that can track metrics over time (e.g., Prometheus and Grafana).
- **Agreement with your business stakeholders** on what constitutes "good enough" performance.

### 3. Step-by-Step Instructions
1.  **Identify Your Service Level Indicators (SLIs):**
    *   An SLI is the thing you actually measure. For a real-time ML API, common SLIs are:
        *   **Latency:** The time it takes to return a prediction.
        *   **Availability:** The percentage of requests that return a successful response (not a 5xx error).
        *   **Quality:** The accuracy or business impact of the model's predictions.
2.  **Define Your SLOs:**
    *   An SLO is a target for an SLI over a specific time window.
    *   Work with your product manager and stakeholders to define these targets.
    *   **Example SLOs:**
        *   "99% of prediction requests over a rolling 28-day window will have a latency of less than 200ms."
        *   "99.95% of prediction requests over a rolling 28-day window will be successful (return a non-500-level error)."
3.  **Measure Your SLIs:**
    *   Use your monitoring stack (e.g., Prometheus) to collect the raw data for your SLIs. You need to be logging the latency and status code for every single request.
4.  **Calculate and Visualize Your SLOs:**
    *   In your dashboarding tool (e.g., Grafana), create queries that calculate your SLO compliance.
    *   A key concept is the **Error Budget**. If your availability SLO is 99.95%, your error budget is 0.05%. This is the amount of "unreliability" you are allowed to have over the time window.
    *   Create a dashboard showing your current SLO compliance and how much of your error budget you have remaining.
5.  **Alert on Error Budget Burn Rate:**
    *   Do not alert when a single request is slow. Alert when you are burning through your error budget too quickly.
    *   For example: "If we continue at this rate of errors, we will exhaust our entire monthly error budget in the next 2 days. Page the on-call engineer." This approach reduces alert fatigue and focuses on what really matters to the user experience.

### 4. Resources and Tools
- **Prometheus and Grafana:** A common stack for SLI/SLO monitoring.
- **The Google SRE Handbook:** The definitive source for learning about the theory and practice of SLOs.

### 5. Next Steps and Progression
- **Balancing Reliability and Features:** The error budget becomes a data-driven tool for decision-making. If you have plenty of error budget left, it's safe to push a risky new feature. If your error budget is almost gone, you should freeze new deployments and focus on reliability improvements.

### 6. Common Challenges and Solutions
- **Challenge:** "We can't agree on the right SLO target."
- **Solution:** Start with a lenient, achievable target based on your current historical performance. You can always make the SLO stricter over time as you improve the service's reliability. The goal is to have a reasonable target that reflects user expectations.
- **Challenge:** "Measuring the 'quality' SLI is difficult."
- **Solution:** It is. Measuring model accuracy in real-time requires a fast feedback loop to get the ground truth. For many systems, you may start by only having SLOs for latency and availability, and monitor quality with a slower, offline process.
`},{id:602,slug:"applying-devops-practices-to-python-sql-code",question:"How to get started with applying DevOps practices (linting, unit tests) to Dataiku Python and SQL code.",answer:`
### 1. Introduction/Overview
DevOps is a set of practices that combines software development and IT operations, aiming to shorten the development lifecycle and provide continuous delivery with high quality. You can apply core DevOps practices like automated testing and style checking to the code within your Dataiku projects to dramatically improve its quality and maintainability.

### 2. Prerequisites
- **Dataiku projects that use code recipes** (Python or SQL).
- **A CI/CD pipeline** (e.g., in GitHub Actions or Jenkins) triggered by your project's Git repository.

### 3. Step-by-Step Instructions

#### Part 1: Linting (Automated Style Checking)
1.  **Define a Style Guide:** Agree on a team-wide style guide. For Python, this is **PEP 8**.
2.  **Choose a Linter:** Select a command-line tool that can enforce your style guide.
    *   **For Python:** \`flake8\` is a popular choice. \`black\` is an auto-formatter that can also be used.
    *   **For SQL:** Tools like \`sqlfluff\` are available.
3.  **Integrate into CI/CD:**
    *   In your CI/CD pipeline's script, add a new "Lint" stage that runs before any other tests.
    *   This stage will run the linter command on the code files in your project.
    *   If the linter finds any style violations, it should exit with an error code, which will fail the CI build and prevent the code from being merged.

#### Part 2: Unit Testing
1.  **Write Testable Code:** In your Dataiku project's **Library**, write your core logic as pure Python functions that can be tested in isolation.
2.  **Use a Testing Framework:** Use a standard Python testing framework like \`pytest\`.
3.  **Write Unit Tests:** Create test files (e.g., \`test_my_utils.py\`) that import your functions and use \`assert\` statements to check that they produce the expected output for a given input.
4.  **Integrate into CI/CD:**
    *   Add a "Unit Test" stage to your CI/CD pipeline script.
    *   This stage runs the \`pytest\` command.
    *   If any test fails, \`pytest\` will exit with an error code, which will fail the CI build.

### 4. Resources and Tools
- **Linters:** \`flake8\`, \`black\`, \`sqlfluff\`.
- **Testing Frameworks:** \`pytest\`.
- **Your CI/CD tool (GitHub Actions, etc.).**

### 5. Next Steps and Progression
- **Pre-Commit Hooks:** You can set up Git pre-commit hooks that automatically run the linter on a developer's machine *before* they are even allowed to commit their code. This catches style issues at the earliest possible moment.
- **Test Coverage:** Use tools like \`pytest-cov\` to measure what percentage of your code is covered by unit tests. Aim for high coverage on your most critical and complex functions.

### 6. Common Challenges and Solutions
- **Challenge:** "Developers are complaining that the linter is too strict."
- **Solution:** A consistent style is important for readability. However, most linters can be configured. You can have a team discussion to decide if a specific rule should be disabled or adjusted.
- **Challenge:** "Our CI pipeline is failing because of a unit test failure."
- **Solution:** This is a good thing! The CI pipeline has done its job. It has automatically caught a regression or a bug in your code before it could be merged and deployed to production. The developer needs to fix their code to make the test pass.
`},{id:603,slug:"leveraging-project-sharing-home-folder-structure",question:"How to get started with leveraging Dataiku’s project sharing and Home folder structure for multi-team environments.",answer:`
### 1. Introduction/Overview
In a large organization with many teams using a single Dataiku instance, keeping the projects organized is essential for security and navigability. Dataiku provides two main features for this: **Project Folders** to organize the homepage, and a **Shared Objects** model for cross-project collaboration.

### 2. Prerequisites
- **A multi-team Dataiku instance.**
- **Administrator rights** to create project folders and manage permissions.

### 3. Step-by-Step Instructions

#### Part 1: Organizing with Project Folders
1.  **Plan Your Folder Structure:** Design a folder hierarchy that makes sense for your organization. A common structure is to have top-level folders for each major department or business unit.
    *   \`/Finance\`
    *   \`/Marketing\`
    *   \`/Shared_Services\`
2.  **Create the Folders (Admin Task):** An administrator can create these folders directly from the Dataiku homepage.
3.  **Set Permissions on Folders:**
    *   For each folder, you can set permissions.
    *   For example, you can set the permissions on the \`/Finance\` folder so that only members of the "Finance Team" group can even *see* it.
    *   This is a powerful way to control project visibility at a high level.
4.  **Move Projects into Folders:** Users can then create new projects within, or move existing projects into, the appropriate folder.

#### Part 2: Sharing Between Projects
1.  **Create a "Shared" Project:**
    *   Create a dedicated project called \`SHARED_DATASETS\` or \`GOLDEN_DATA\`.
    *   This project's purpose is to contain the authoritative, cleaned, "golden" datasets that many other projects will need (e.g., your master customer table).
2.  **Set Permissions on the Shared Project:**
    *   Grant **Reader** access to this shared project to all the developer groups.
3.  **How to Use the Shared Data:**
    *   Now, in any other project, a developer can click **+ DATASET > Import Dataset from another project**.
    *   They can then select a dataset from the \`SHARED_DATASETS\` project to use as a read-only input in their own Flow.
    *   This ensures that all teams are building on the same, consistent, single source of truth.

### 4. Resources and Tools
- **Project Folders:** The tool for organizing the Dataiku homepage.
- **Folder Permissions:** For controlling project visibility.
- **Shared Projects and the "Import Dataset" feature:** For reusing data assets across projects.

### 5. Next Steps and Progression
- **Shared Code Libraries:** You can use the same pattern to create a shared project for reusable Python functions, and have other projects add it as a dependency.

### 6. Common Challenges and Solutions
- **Challenge:** "A user can't find a project I shared with them."
- **Solution:** They may not have permission to see the **folder** the project is in. You need to ensure they have at least "Read" access to the entire folder path.
- **Challenge:** "A user changed a dataset in the shared project and broke everyone else's flow."
- **Solution:** This is why you should grant only **Reader** permissions on the shared project to most users. Only a small, designated "data steward" or "data engineering" team should have contributor rights to modify the golden datasets.
`},{id:604,slug:"ensuring-auditability-with-logging-version-snapshots",question:"How to get started with ensuring auditability of Dataiku workflows by enabling detailed logging and version snapshots.",answer:`
### 1. Introduction/Overview
Auditability—the ability to trace and prove how a result was produced—is a critical requirement for any governed analytics platform. Dataiku provides a powerful, multi-layered audit trail automatically. The key is to combine Dataiku's built-in logging with disciplined use of version control.

### 2. Prerequisites
- **A Dataiku project.**
- **Git integration** set up for your project.

### 3. Step-by-Step Instructions: The Pillars of Auditability

#### Pillar 1: Detailed Logging (Automatic)
- **What it is:** Dataiku automatically logs every job run and every significant user action.
- **How to use:**
    - **Job Logs:** For any scenario or recipe run, you can go to the **Jobs** menu and find a detailed log showing what was executed, how long it took, and any errors.
    - **Project Timeline:** In your project, go to **... > Timeline** to see a high-level, human-readable log of all changes made to the project (who created what, and when).
    - **Global Audit Log (Admin):** An admin can see a log of all security-related events for the entire instance.

#### Pillar 2: Version Snapshots of Logic (Git)
- **What it is:** This provides an immutable, point-in-time history of your project's *logic* (recipes, schemas, etc.).
- **How to use:**
    - Connect your project to a **Git** repository.
    - As you make changes, **commit** them with clear messages explaining the change.
    - The **commit history** in your Git provider (e.g., GitHub) is now your definitive audit trail for all code and configuration changes. You can see exactly what was changed, by whom, and when. You can also revert to a previous version if needed.

#### Pillar 3: Version Snapshots of Data (Partitioning)
- **What it is:** This provides a point-in-time history of your *data*.
- **How to use:**
    - For any time-based dataset, use Dataiku's **Partitioning** feature (e.g., partition by day).
    - Each partition is an immutable snapshot of the data for that specific day.
    - This allows you to go back and reproduce a result from a specific point in time using the exact data from that day.

### 4. Resources and Tools
- **Dataiku's built-in logging features.**
- **Git Integration:** For versioning your project logic.
- **Partitioning:** For versioning your data.
- **The Lineage Graph:** A visual tool that complements the logs, showing the data flow for any given output.

### 5. Next Steps and Progression
- **Formal Documentation:** For a formal audit, supplement the automated trails with a written document in the **Project Wiki** that explains the business logic and how it complies with relevant policies.
- **Automated Log Analysis:** For advanced auditing, you can ship all the Dataiku logs to an external system like Splunk or ELK for centralized analysis and alerting.

### 6. Common Challenges and Solutions
- **Challenge:** "An auditor wants to know how a specific number was calculated."
- **Solution:** This is the perfect use case for the **column-level lineage** graph. You can visually trace the calculation from the final number back to the raw source data, providing definitive proof.
- **Challenge:** "Our Git commit history is a mess."
- **Solution:** This is a team discipline issue. You must train and enforce the practice of writing clear, descriptive commit messages that explain the business reason for a change. A good commit message is a crucial part of the audit trail.
`}]}};