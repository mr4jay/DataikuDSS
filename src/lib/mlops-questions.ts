
export interface Question {
  id: number;
  slug: string;
  question: string;
  answer: string;
}

export const questions: Question[] = [
    {
    id: 501,
    slug: 'deploying-ml-models-as-rest-apis',
    question: 'How to get started with deploying machine learning models as REST APIs in Dataiku using the Dataiku Model API.',
    answer: 'First, train and save a model in your Dataiku Flow. Then, navigate to the API Designer and create a new API service. Add an endpoint that uses your saved model. Dataiku will automatically generate a REST endpoint that you can call with new data to get real-time predictions.',
  },
  {
    id: 502,
    slug: 'automating-model-retraining-tuning-deployment',
    question: 'How to get started with automating model retraining, tuning, and deployment in Dataiku DSS.',
    answer: 'Create a scenario. Add a step to rebuild your training dataset, followed by a step to "Train" your saved model. You can configure this step to also search for new best hyperparameters. Finally, add a step to deploy the newly trained model version to the API service.',
  },
  {
    id: 503,
    slug: 'performing-champion-challenger-workflows',
    question: 'How to get started with performing champion/challenger and version-comparison workflows in Dataiku for model updates.',
    answer: 'In the API Deployer, you can deploy multiple versions of a model to the same endpoint. Configure the endpoint to split traffic (e.g., 90% to the champion, 10% to the challenger). You can then monitor the performance of both versions in a live environment to decide which is better.',
  },
  {
    id: 504,
    slug: 'monitoring-deployed-model-performance-data-drift',
    question: 'How to get started with monitoring deployed model performance and data drift using Dataiku’s unified monitoring features.',
    answer: 'On your saved model in the Flow, use the "Model Views" feature. Enable drift analysis to track changes in the statistical distribution of your input data. Create a "Model Evaluation" recipe to compare predictions against actuals and monitor accuracy over time.',
  },
  {
    id: 505,
    slug: 'integrating-prometheus-grafana-for-metrics',
    question: 'How to get started with integrating Prometheus/Grafana to visualize Dataiku flow and model metrics.',
    answer: 'Dataiku can expose its internal metrics (like scenario run times or model performance) through a JMX endpoint. You can use a tool like JMX Exporter to make these metrics available to Prometheus, and then build dashboards in Grafana to visualize them.',
  },
  {
    id: 506,
    slug: 'configuring-alerting-for-model-accuracy-failures',
    question: 'How to get started with configuring alerting for model accuracy or pipeline failures in Dataiku scenarios.',
    answer: 'In your scenario, add a "Run checks" step after your model evaluation. The check can assert that the model\'s accuracy is above a certain threshold. If the check fails, the scenario will fail, and you can configure a "Reporter" to send an email or Slack alert.',
  },
  {
    id: 507,
    slug: 'containerizing-flows-and-models-with-docker-kubernetes',
    question: 'How to get started with containerizing Dataiku flows and models with Docker or Kubernetes for portable deployment.',
    answer: 'Dataiku can run recipes and models in containerized environments. In Administration > Containerized Execution, configure a connection to your Kubernetes cluster. Then, in a recipe\'s settings, you can choose to run it on this containerized infrastructure.',
  },
  {
    id: 508,
    slug: 'scaling-scoring-apis-on-kubernetes',
    question: 'How to get started with scaling Dataiku scoring APIs by deploying them on Kubernetes clusters.',
    answer: 'When you deploy an API service in the API Deployer, you can configure its infrastructure to run on a Kubernetes cluster. This allows you to easily scale the number of API replicas up or down to handle changes in prediction request volume.',
  },
  {
    id: 509,
    slug: 'using-flow-mode-batch-scoring-vs-real-time-api',
    question: 'How to get started with using Dataiku Flow mode for batch scoring versus real-time API deployment.',
    answer: 'Use batch scoring (a "Score" recipe in the Flow) when you need to make predictions on a large dataset at once. Use a real-time API deployment when your applications need to get predictions for single records on-demand with low latency.',
  },
  {
    id: 510,
    slug: 'packaging-versioning-python-r-environments',
    question: 'How to get started with packaging and version-controlling Dataiku Python/R environments for reproducible model builds.',
    answer: 'In Administration > Code Envs, create an environment for your project. Specify the exact versions of all the packages you need. This environment definition can be exported as a JSON file and checked into Git, ensuring that you can perfectly recreate it later.',
  },
  {
    id: 511,
    slug: 'managing-code-environments-for-reproducibility',
    question: 'How to get started with managing Dataiku code environments (Python/R) to pin package versions and ensure reproducible pipelines.',
    answer: 'Create a dedicated code environment for each project. In the package list, specify exact versions for all libraries (e.g., `pandas==1.3.5`). This prevents unexpected behavior caused by package updates and is crucial for reproducibility.',
  },
  {
    id: 512,
    slug: 'containerizing-workflows-for-sharing-environments',
    question: 'How to get started with containerizing the entire Dataiku workflow (code, data, dependencies) to capture and share environments.',
    answer: 'Dataiku can be run within a Docker container. You can create a custom Docker image that includes the Dataiku software and all your specific OS-level dependencies. This container can then be shared with others to ensure they have the exact same environment.',
  },
  {
    id: 513,
    slug: 'exporting-importing-projects-between-instances',
    question: 'How to get started with exporting and importing Dataiku projects between DSS instances to replicate environments.',
    answer: 'From your project homepage, click the "Export" button to create a `.zip` bundle of your project. This bundle contains all recipes, datasets, and settings. You can then import this bundle into another Dataiku instance to recreate the project.',
  },
  {
    id: 514,
    slug: 'defining-standardized-project-templates',
    question: 'How to get started with defining standardized Dataiku project templates for consistent pipeline setup across teams.',
    answer: 'Create a "Template" project that includes your standard Flow Zone structure, naming conventions, and common code libraries. Other teams can then duplicate this project to kickstart their work with a consistent and governed setup.',
  },
  {
    id: 515,
    slug: 'using-time-travel-snapshots-for-reproducibility',
    question: 'How to get started with using Dataiku’s managed “time travel” snapshots or project backups to reproduce past runs.',
    answer: 'Dataiku does not have a native "time travel" feature for code, which is why Git integration is essential. For data, you can use a Python recipe to create versioned copies of datasets at specific points in time if you need to reproduce a past result.',
  },
  {
    id: 516,
    slug: 'sharing-versioning-notebooks-sql-recipes',
    question: 'How to get started with sharing and versioning Dataiku notebooks and SQL recipes across projects for reproducibility.',
    answer: 'The best way is through Git integration. All notebooks and recipes are stored as files within the project, so they can be versioned with Git. For sharing, you can create a project that serves as a library of common notebooks and recipes that other projects can refer to.',
  },
  {
    id: 517,
    slug: 'integrating-projects-with-git-for-tracking-changes',
    question: 'How to get started with integrating Dataiku projects with Git to track code changes and environment changes.',
    answer: 'In the project settings, link your project to a remote Git repository. This allows you to commit all your project assets. When you export the definition of your code environment, you can also commit this JSON file to Git to version your environment.',
  },
  {
    id: 518,
    slug: 'automating-documentation-for-reproducibility-compliance',
    question: 'How to get started with automating documentation of Dataiku flows and environments to enforce reproducibility and compliance.',
    answer: 'Use a Python scenario step. The script can use the Dataiku API to loop through all datasets and recipes in the project, extract their descriptions and metadata, and generate a markdown or HTML documentation file automatically.',
  },
  {
    id: 519,
    slug: 'creating-dev-test-prod-instances',
    question: 'How to get started with creating separate dev/test/prod Dataiku instances and migrating projects through them.',
    answer: 'This is the standard MLOps architecture. You have separate Dataiku servers for each environment. Development happens on the dev instance. When ready, you export the project as a bundle and import it into the test instance for QA. After validation, you deploy the bundle to the production instance.',
  },
  {
    id: 520,
    slug: 'connecting-projects-to-git-for-mlops-workflows',
    question: 'How to get started with connecting Dataiku projects to a Git repository and managing branches for MLOps workflows.',
    answer: 'In Project Settings, go to the Git tab. Configure the connection to your remote repository (e.g., on GitHub). Use a standard branching strategy like GitFlow. Create feature branches for new development, then use pull requests to merge them into a main or develop branch.',
  },
  {
    id: 521,
    slug: 'using-native-git-integration-for-team-collaboration',
    question: 'How to get started with using Dataiku’s native Git integration to enable team collaboration on data pipelines.',
    answer: 'With Git integration, team members can work on the same project simultaneously. Each can work on a separate branch. When they are ready to integrate their changes, they can push their branch and create a pull request for review.',
  },
  {
    id: 522,
    slug: 'setting-up-jenkins-pipelines-with-dataiku-api',
    question: 'How to get started with setting up Jenkins pipelines that use the Dataiku Python API to run and deploy projects.',
    answer: 'In your Jenkins pipeline script, you will need to make REST API calls to Dataiku. First, call the API to update the project from Git. Then, call the endpoint to run a scenario that executes your tests. If the tests pass, call the API to create a project bundle for deployment.',
  },
  {
    id: 523,
    slug: 'building-github-actions-gitlab-ci-workflows',
    question: 'How to get started with building GitHub Actions or GitLab CI workflows to automate Dataiku project deployment.',
    answer: 'In your Git repository, create a workflow file (e.g., in `.github/workflows`). This YAML file defines the CI/CD pipeline. On a push to the main branch, the workflow can execute a script that uses the Dataiku REST API to deploy the project bundle to the production instance.',
  },
  {
    id: 524,
    slug: 'enabling-continuous-delivery-with-cicd-tools',
    question: 'How to get started with enabling continuous delivery of Dataiku flows using Bamboo, Jenkins, or Bitbucket pipelines.',
    answer: 'The principle is the same across tools. Your CI/CD pipeline is triggered by a Git commit. The pipeline script then interacts with Dataiku via the REST API to perform a sequence of actions: update project, run tests, build artifacts (bundles), and deploy to the next environment.',
  },
  {
    id: 525,
    slug: 'integrating-infrastructure-as-code-for-provisioning',
    question: 'How to get started with integrating infrastructure-as-code (e.g. Terraform, Azure DevOps) to provision Dataiku infrastructure.',
    answer: 'If you are running Dataiku on the cloud, you can use Terraform or similar tools to define your infrastructure (like the virtual machine for the Dataiku server, the database for its backend) in code. This allows you to create and destroy environments in an automated and repeatable way.',
  },
  {
    id: 526,
    slug: 'adding-scenario-runs-validation-tests-to-ci-pipeline',
    question: 'How to get started with adding Dataiku scenario runs or validation tests into your CI pipeline as build steps.',
    answer: 'In your CI/CD pipeline script (e.g., Jenkinsfile), after checking out the code, add a step that makes a REST API call to run a specific "test" scenario in Dataiku. The script should wait for the scenario to complete and check its outcome before proceeding.',
  },
  {
    id: 527,
    slug: 'creating-automated-tests-for-ci',
    question: 'How to get started with creating automated tests (unit or integration) for Dataiku projects as part of CI.',
    answer: 'Create a dedicated "test" scenario in your Dataiku project. This scenario should include steps to "Run checks" on your key datasets (for data quality) and to run unit tests on your Python recipes. Your CI pipeline will then trigger this specific scenario.',
  },
  {
    id: 528,
    slug: 'migrating-projects-between-environments-with-scripts-apis',
    question: 'How to get started with migrating Dataiku projects between development and production environments using automated scripts or APIs.',
    answer: 'Write a script (e.g., in Python) that orchestrates the deployment. The script would use the Dataiku REST API to: 1. Create a bundle from the dev project. 2. Download the bundle. 3. Upload the bundle to the prod instance. 4. Import the bundle to create/update the project on prod.',
  },
  {
    id: 529,
    slug: 'triggering-scenarios-from-ci-cd-on-new-code-data',
    question: 'How to get started with triggering Dataiku scenarios from a CI/CD pipeline whenever new code or data is available.',
    answer: 'Configure a webhook in your Git repository that triggers your CI/CD pipeline on every commit. The pipeline script then runs the necessary Dataiku scenarios via the API. For new data, you can have an external system that drops the data also make an API call to trigger the Dataiku job.',
  },
  {
    id: 530,
    slug: 'configuring-dataiku-with-aws-sagemaker-lambda-s3',
    question: 'How to get started with configuring Dataiku to use AWS SageMaker, Lambda, and S3 for model training and inference.',
    answer: 'You can use a Python recipe to call the AWS SDK (Boto3). From Dataiku, you can start a SageMaker training job, invoke a Lambda function for a specific task, or read/write data from S3. This allows you to integrate these AWS services into your Dataiku flows.',
  },
  {
    id: 531,
    slug: 'integrating-with-azure-ml-services-storage',
    question: 'How to get started with integrating Dataiku DSS with Azure ML services and Azure Storage for end-to-end pipelines.',
    answer: 'Set up a connection to Azure Blob Storage in Dataiku to read/write data. From a Python recipe, use the Azure ML SDK to submit training jobs to Azure ML or to deploy models as Azure ML endpoints.',
  },
  {
    id: 532,
    slug: 'connecting-to-gcp-bigquery-vertex-ai',
    question: 'How to get started with connecting Dataiku to Google Cloud BigQuery and Vertex AI for training and prediction.',
    answer: 'Dataiku has a native connector for BigQuery, allowing you to use it as a data source and for computation push-down. For Vertex AI, you can use a Python recipe with the Google Cloud SDK to submit training jobs or call deployed models for prediction.',
  },
  {
    id: 533,
    slug: 'deploying-workloads-on-emr-hdinsight-dataproc',
    question: 'How to get started with deploying Dataiku workloads on AWS EMR, Azure HDInsight, or GCP Dataproc for scalable Spark processing.',
    answer: 'Dataiku can be configured to use these managed Spark services. In Administration, you set up a connection to the cluster. Then, in your recipes, you can select that cluster as the execution engine for your Spark jobs.',
  },
  {
    id: 534,
    slug: 'managing-credentials-via-aws-iam-roles-azure-service-principals',
    question: 'How to get started with managing Dataiku credentials via AWS IAM roles or Azure service principals for secure access.',
    answer: 'This is the most secure method. When deploying Dataiku on a cloud VM, assign an IAM role or service principal to the VM. Then, in the Dataiku connection settings, you can specify that it should use the role to authenticate, avoiding the need to store static credentials.',
  },
  {
    id: 535,
    slug: 'linking-to-cloud-data-warehouses-snowflake-redshift-bigquery',
    question: 'How to get started with linking Dataiku to cloud data warehouses (Snowflake, Redshift, BigQuery) for large-scale data access.',
    answer: 'In Administration > Connections, create a new connection for your data warehouse. Provide the necessary credentials. This single connection can then be reused by all projects, and Dataiku will leverage it for push-down computation.',
  },
  {
    id: 536,
    slug: 'optimizing-spark-compute-on-cloud-clusters-autoscaling',
    question: 'How to get started with optimizing Dataiku Spark compute on cloud clusters and autoscaling for cost-efficiency.',
    answer: 'When using a managed Spark service like EMR or Dataproc, configure it to use autoscaling. This will automatically add or remove nodes from the cluster based on the workload, ensuring you have enough power for large jobs without paying for idle resources.',
  },
  {
    id: 537,
    slug: 'setting-up-hybrid-architecture-on-prem-and-cloud',
    question: 'How to get started with setting up Dataiku in a hybrid architecture (on-prem DSS accessing cloud resources).',
    answer: 'Your Dataiku instance can be on-premise, but it can still connect to cloud resources. You will need to ensure there is network connectivity between your on-premise server and the cloud services (e.g., via a VPN or direct connect), and that firewall rules allow the traffic.',
  },
  {
    id: 538,
    slug: 'integrating-cloud-event-triggers',
    question: 'How to get started with integrating cloud event triggers (S3 file arrival, Azure Event Grid) to kick off Dataiku scenarios.',
    answer: 'Set up a cloud function (like AWS Lambda) that is triggered by the cloud event (e.g., a new file in S3). The code in this function will then make a REST API call to Dataiku to run the scenario that processes the new file.',
  },
  {
    id: 539,
    slug: 'using-kubernetes-cloud-deployment-templates',
    question: 'How to get started with using Dataiku’s Kubernetes or cloud deployment templates to run DSS on Azure Kubernetes Service, EKS, or GKE.',
    answer: 'Dataiku provides official Helm charts and other templates for deploying on managed Kubernetes services. These templates handle the complexity of setting up the Dataiku pods, services, and storage, making it easier to get a scalable instance running.',
  },
  {
    id: 540,
    slug: 'creating-scheduling-scenarios-for-end-to-end-workflows',
    question: 'How to get started with creating and scheduling Dataiku Scenarios to automate end-to-end ML workflows.',
    answer: 'Go to the "Scenarios" page. Create a new scenario. Add steps to build your final datasets and models. In the "Settings" tab, add a "Time-based" trigger to schedule it to run on a recurring basis (e.g., daily).',
  },
  {
    id: 541,
    slug: 'using-event-based-triggers-for-orchestration',
    question: 'How to get started with using event-based triggers in Dataiku Scenarios (e.g., file drop or schedule) for orchestration.',
    answer: 'In a scenario\'s settings, you can add a "Dataset change" trigger. This will automatically run the scenario whenever a specific input dataset is updated. This is useful for creating event-driven pipelines.',
  },
  {
    id: 542,
    slug: 'calling-external-scripts-apis-from-scenarios',
    question: 'How to get started with calling external scripts or APIs from Dataiku Scenarios to extend automation.',
    answer: 'Add a "Execute Python code" step to your scenario. In this script, you can use libraries like `requests` to call external APIs or `subprocess` to run a shell script, allowing you to integrate with other systems.',
  },
  {
    id: 543,
    slug: 'integrating-airflow-to-orchestrate-dataiku-tasks',
    question: 'How to get started with integrating Apache Airflow to orchestrate pipelines that include Dataiku tasks.',
    answer: 'You can use the `DataikuScenarioOperator` in Airflow. This operator allows you to define a task in your Airflow DAG that triggers a Dataiku scenario and waits for its completion. This is useful for orchestrating complex pipelines that span multiple systems.',
  },
  {
    id: 544,
    slug: 'using-rest-api-to-trigger-runs-from-external-orchestrator',
    question: 'How to get started with using the Dataiku REST API to trigger project runs from an external orchestrator.',
    answer: 'From your external tool (like Airflow or your own application), make a POST request to the Dataiku API endpoint for running a scenario. You will need to provide the project key and scenario ID. The API call will trigger the job in Dataiku.',
  },
  {
    id: 545,
    slug: 'chaining-projects-by-invoking-scenarios',
    question: 'How to get started with chaining Dataiku projects by invoking one project’s scenario from another’s.',
    answer: 'In a scenario in Project A, you can add a Python step that uses the Dataiku API to trigger a scenario in Project B. This allows you to create dependencies between different projects.',
  },
  {
    id: 546,
    slug: 'handling-errors-retries-in-scenarios',
    question: 'How to get started with handling errors and retries in Dataiku Scenarios for robust pipeline automation.',
    answer: 'For error handling, configure "Reporters" to send alerts on failure. For retries, you need to implement custom logic in a Python scenario step. The script can check the outcome of a job and, if it failed, trigger it again up to a specified retry limit.',
  },
  {
    id: 547,
    slug: 'building-complex-multi-step-pipelines-with-flow-scenario-logic',
    question: 'How to get started with building complex multi-step pipelines in Dataiku by combining Flow and Scenario logic.',
    answer: 'Use the Flow to define the "what" (the sequence of data transformations). Use Scenarios to define the "how" and "when" (the automation, scheduling, and error handling). This separation of concerns is key to building maintainable pipelines.',
  },
  {
    id: 548,
    slug: 'integrating-streaming-sources-kafka',
    question: 'How to get started with integrating streaming sources (e.g., Kafka) to continuously update Dataiku flows.',
    answer: 'Dataiku has a "Streaming" capability. You can create a streaming endpoint that connects to a source like Kafka. Then, you can apply real-time recipes to transform the streaming data and write it to a sink, such as another Kafka topic or a dataset.',
  },
  {
    id: 549,
    slug: 'creating-automated-data-quality-checks-for-orchestration',
    question: 'How to get started with creating automated data quality checks in Dataiku as part of orchestration.',
    answer: 'Define data quality rules on your datasets using the "Metrics & Checks" feature. Then, in your main scenario, add a "Run checks" step. If the data fails the quality checks, the scenario will fail, preventing bad data from being processed.',
  },
  {
    id: 550,
    slug: 'setting-up-user-roles-permissions',
    question: 'How to get started with setting up Dataiku user roles and permissions for project access control.',
    answer: 'In Administration > Security, create groups corresponding to your user roles (e.g., "Analyst", "Developer"). Then, in each project\'s settings, grant these groups the appropriate level of access (e.g., "Reader", "Contributor").',
  },
  {
    id: 551,
    slug: 'using-policy-governance-features-lineage-tags',
    question: 'How to get started with using Dataiku’s policy and governance features (data lineage, tags) to enforce compliance.',
    answer: 'Establish a tagging policy for all datasets (e.g., to identify PII). Use the automated lineage graph to trace data flows for audits. Dataiku\'s governance features provide the visibility needed to ensure compliance.',
  },
  {
    id: 552,
    slug: 'configuring-ldap-sso-integration',
    question: 'How to get started with configuring Dataiku’s LDAP/SSO integration for enterprise user management.',
    answer: 'In Administration > Security, you can configure Dataiku to connect to your corporate LDAP or Active Directory for user authentication. You can also set up Single Sign-On (SSO) with providers like Okta for a seamless login experience.',
  },
  {
    id: 553,
    slug: 'enabling-audit-logging-for-lineage-changes',
    question: 'How to get started with enabling audit logging in Dataiku to track data lineage and project changes.',
    answer: 'Audit logging is enabled by default. A Dataiku administrator can access the audit logs, which provide a detailed record of all user activities, including who made changes to which project and when.',
  },
  {
    id: 554,
    slug: 'storing-credentials-securely-secret-management',
    question: 'How to get started with storing credentials securely in Dataiku (secret management) and encrypting sensitive data.',
    answer: 'When setting up connections, use the built-in credential store. For higher security, integrate Dataiku with a dedicated secrets management tool like HashiCorp Vault or use IAM roles if in a cloud environment.',
  },
  {
    id: 555,
    slug: 'tagging-datasets-for-sensitive-information-pii-phi',
    question: 'How to get started with tagging datasets in Dataiku to manage sensitive information (PII/PHI) for compliance.',
    answer: 'On any dataset, go to the "Summary" tab and add tags like `PII` or `Confidential`. You can then search for datasets with these tags and apply more restrictive permissions to them.',
  },
  {
    id: 556,
    slug: 'using-api-to-automate-governance-checks',
    question: 'How to get started with using Dataiku’s API to automate governance checks and policy enforcement.',
    answer: 'Create a scenario with a Python step. The script can use the Dataiku API to loop through all projects, check if they are compliant with your tagging policy, and generate a governance report.',
  },
  {
    id: 557,
    slug: 'implementing-role-based-access-control-rbac',
    question: 'How to get started with implementing role-based access control (RBAC) in Dataiku for different teams.',
    answer: 'Define user groups based on roles. Assign permissions to these groups at the project level. This ensures that users can only see and edit the projects and data relevant to their role.',
  },
  {
    id: 558,
    slug: 'maintaining-documentation-catalogs-for-audits',
    question: 'How to get started with maintaining documentation and catalogs in Dataiku for regulatory audits.',
    answer: 'Use the project Wiki for detailed documentation. Use descriptions on every object. The combination of the lineage graph and this documentation provides auditors with a clear picture of your data processes.',
  },
  {
    id: 559,
    slug: 'collaborating-with-security-compliance-teams',
    question: 'How to get started with collaborating with security and compliance teams to align Dataiku configurations with company policies.',
    answer: 'Hold regular meetings with these teams. Give them read-only access to Dataiku so they can see the governance features in action. Use their feedback to refine your security configurations and tagging policies.',
  },
  {
    id: 560,
    slug: 'enabling-collaborative-development-with-project-flow-sharing',
    question: 'How to get started with enabling collaborative development in Dataiku using project and Flow sharing.',
    answer: 'The primary method is to have multiple users as contributors on the same project. If using Git, they can work on separate branches. The visual Flow acts as a common language for both data scientists and analysts to collaborate.',
  },
  {
    id: 561,
    slug: 'setting-up-code-review-processes-for-python-r-scripts',
    question: 'How to get started with setting up code review processes for Dataiku Python/R scripts (e.g., via pull requests).',
    answer: 'Integrate your project with Git. Require all changes to be made in feature branches. Use your Git provider\'s pull request feature to conduct code reviews before merging any changes into the main branch.',
  },
  {
    id: 562,
    slug: 'creating-project-templates-shared-libraries',
    question: 'How to get started with creating Dataiku project templates and shared libraries for reuse across teams.',
    answer: 'Create a project to serve as a template, including standard flows and code libraries. For shared code, you can also create a separate "Library" project containing reusable Python functions that other projects can access.',
  },
  {
    id: 563,
    slug: 'using-flow-view-bookmarks-to-communicate-pipeline-design',
    question: 'How to get started with using the Dataiku Flow view and bookmarks to communicate pipeline design to stakeholders.',
    answer: 'The Flow is a powerful communication tool. Organize it with Flow Zones. Use the "Description" field on recipes to explain their purpose. You can also create bookmarks to quickly navigate to important parts of the Flow during a presentation.',
  },
  {
    id: 564,
    slug: 'integrating-project-management-with-jira-trello',
    question: 'How to get started with integrating Dataiku project management with tools like JIRA or Trello for tracking tasks.',
    answer: 'You can use the Dataiku API to create integrations. For example, a Python scenario step could create a JIRA ticket when a data quality check fails. This requires custom scripting using the JIRA REST API.',
  },
  {
    id: 565,
    slug: 'structuring-projects-for-handover-between-scientists-engineers',
    question: 'How to get started with structuring Dataiku projects to hand off between data scientists and MLOps engineers.',
    answer: 'Use Flow Zones to separate concerns. The data scientist can work in a "Modeling" zone. When the model is ready, the MLOps engineer can take over, building the deployment and monitoring logic in a separate "Deployment" zone.',
  },
  {
    id: 566,
    slug: 'using-notebooks-for-collaborative-exploration',
    question: 'How to get started with using Dataiku notebooks for collaborative data exploration and sharing results.',
    answer: 'Notebooks can be shared between users in the same project. Use Markdown cells to document your analysis. The results of notebook cells are saved, so a colleague can see your work without having to re-run the code.',
  },
  {
    id: 567,
    slug: 'managing-concurrent-changes-to-avoid-conflicts',
    question: 'How to get started with managing concurrent changes in Dataiku (e.g., branches or shared projects) to avoid conflicts.',
    answer: 'The best practice is to use Git and have each developer work on their own branch. This isolates their changes. When merging, Dataiku provides a visual diff tool to help resolve any conflicts that may have occurred.',
  },
  {
    id: 568,
    slug: 'documenting-projects-wiki-comments',
    question: 'How to get started with documenting Dataiku projects (wiki pages, comments) to align data scientists and engineers.',
    answer: 'Use the project Wiki for high-level documentation. Use the "Description" on every recipe and dataset for detailed context. Use the "Discussions" feature for Q&A. This multi-layered approach ensures the project is well-documented for everyone.',
  },
  {
    id: 569,
    slug: 'training-onboarding-team-members-on-best-practices',
    question: 'How to get started with training and onboarding team members on Dataiku best practices and workflows.',
    answer: 'Create a dedicated "Onboarding" project with examples. Point them to the Dataiku Academy. Hold regular "brown bag" sessions to share knowledge. Enforce best practices through peer reviews of their work.',
  },
  {
    id: 570,
    slug: 'exposing-workflows-as-restful-apis',
    question: 'How to get started with exposing Dataiku workflows as RESTful APIs for applications to consume.',
    answer: 'You can deploy a saved model as an API endpoint. You can also deploy a Python function as an endpoint. This is done via the API Designer, where you define the endpoint and link it to your Dataiku asset.',
  },
  {
    id: 571,
    slug: 'deploying-api-nodes-in-docker-kubernetes',
    question: 'How to get started with deploying Dataiku API nodes in a Docker or Kubernetes environment for production.',
    answer: 'The API node is a separate component of Dataiku. It can be installed on its own server or, for scalability, deployed as a set of containers in Kubernetes. Dataiku provides templates for this type of deployment.',
  },
  {
    id: 572,
    slug: 'securing-apis-with-oauth-api-tokens',
    question: 'How to get started with securing Dataiku APIs using OAuth or API tokens.',
    answer: 'In the API Deployer, you can configure the security for your API service. You can require authentication via a static API key or integrate with an OAuth provider for more advanced security.',
  },
  {
    id: 573,
    slug: 'monitoring-api-endpoints-with-prometheus-elk-stack',
    question: 'How to get started with monitoring Dataiku API endpoints using tools like Prometheus or ELK Stack.',
    answer: 'The API node exposes logs and metrics. You can configure a log shipper (like Filebeat) to send the API logs to an ELK stack for analysis. Similarly, you can use an exporter to send performance metrics to Prometheus.',
  },
  {
    id: 574,
    slug: 'logging-tracing-requests-for-observability',
    question: 'How to get started with logging and tracing requests to Dataiku-hosted APIs for observability.',
    answer: 'The API node logs every request. For more advanced tracing, you would need to add custom logging within your Python API endpoint code to record specific information about each request.',
  },
  {
    id: 575,
    slug: 'setting-up-autoscaling-rules-for-api-services',
    question: 'How to get started with setting up auto-scaling rules for Dataiku API services based on load.',
    answer: 'If your API node is deployed on Kubernetes, you can configure a Horizontal Pod Autoscaler (HPA). The HPA will automatically increase or decrease the number of API node replicas based on CPU or memory usage.',
  },
  {
    id: 576,
    slug: 'versioning-api-endpoints-v1-v2-deployments',
    question: 'How to get started with versioning Dataiku API endpoints (e.g. v1, v2 deployments) in production.',
    answer: 'In the API Deployer, you can have multiple versions of an endpoint deployed simultaneously. When you update your endpoint (e.g., with a new model version), you can deploy it alongside the old one. You can then use traffic splitting to gradually move users to the new version.',
  },
  {
    id: 577,
    slug: 'integrating-apis-with-api-gateways',
    question: 'How to get started with integrating Dataiku APIs with API gateways (AWS API Gateway, Azure API Mgmt) for scaling.',
    answer: 'You can put an API Gateway in front of your Dataiku API node. The gateway can handle tasks like authentication, rate limiting, and request routing, providing an extra layer of management and security.',
  },
  {
    id: 578,
    slug: 'documenting-generated-apis-with-openapi-swagger',
    question: 'How to get started with documenting Dataiku-generated APIs using OpenAPI/Swagger for clients.',
    answer: 'Dataiku automatically generates an OpenAPI/Swagger specification for your API endpoints. You can access this specification from the API Deployer and provide it to client application developers as documentation.',
  },
  {
    id: 579,
    slug: 'gathering-usage-metrics-latency-error-rate',
    question: 'How to get started with gathering usage metrics (latency, error rate) for Dataiku APIs to improve reliability.',
    answer: 'The API node provides built-in monitoring that shows the number of requests, average latency, and error rates for your endpoints. Analyzing these metrics is key to understanding and improving the reliability of your API.',
  },
  {
    id: 580,
    slug: 'monitoring-job-resource-usage-cpu-memory',
    question: 'How to get started with monitoring Dataiku job resource usage (CPU, memory) to identify performance bottlenecks.',
    answer: 'A Dataiku administrator can monitor the resource usage of the Dataiku server. The "Job Inspector" also provides some information on the resources consumed by individual recipes, especially when running on containerized infrastructure.',
  },
  {
    id: 581,
    slug: 'sizing-tuning-cluster-nodes',
    question: 'How to get started with sizing and tuning Dataiku cluster nodes to balance cost and performance.',
    answer: 'This is an iterative process. Start with a recommended node size. Monitor the resource usage. If jobs are failing due to memory or CPU constraints, increase the size or number of nodes. If nodes are consistently underutilized, you can downsize to save costs.',
  },
  {
    id: 582,
    slug: 'using-kubernetes-to-horizontally-scale-services',
    question: 'How to get started with using Kubernetes to horizontally scale Dataiku services under load.',
    answer: 'By deploying Dataiku on Kubernetes, you can easily scale its different components. For example, you can increase the number of "execution" pods to handle more concurrent recipe runs, or increase the "API node" pods to handle more prediction requests.',
  },
  {
    id: 583,
    slug: 'setting-job-quotas-priorities',
    question: 'How to get started with setting Dataiku job quotas and priorities to control resource consumption.',
    answer: 'In a containerized deployment, you can set resource quotas and priorities at the Kubernetes level. This can prevent a single user or project from consuming all the available cluster resources.',
  },
  {
    id: 584,
    slug: 'tracking-cloud-costs-of-resources',
    question: 'How to get started with tracking cloud costs of Dataiku-related resources (EC2 instances, S3 storage) over time.',
    answer: 'Use your cloud provider\'s cost management tools (like AWS Cost Explorer or Azure Cost Management). Apply tags to all the resources associated with your Dataiku deployment so you can filter and see exactly how much it is costing.',
  },
  {
    id: 585,
    slug: 'tagging-cloud-resources-by-project-for-cost-allocation',
    question: 'How to get started with tagging cloud resources by Dataiku project for granular cost allocation.',
    answer: 'This requires advanced integration. You could have a Dataiku scenario that, when it runs a job on a dynamic cluster, applies a tag to that cluster corresponding to the Dataiku project key. This allows for very granular cost tracking.',
  },
  {
    id: 586,
    slug: 'using-spot-preemptible-instances-for-cost-reduction',
    question: 'How to get started with using spot/preemptible instances or scheduling off-peak runs to reduce Dataiku compute costs.',
    answer: 'When configuring containerized execution, you can set up node pools that use cheaper spot instances. This is great for non-critical workloads. You can also schedule large, resource-intensive scenarios to run overnight during off-peak hours.',
  },
  {
    id: 587,
    slug: 'archiving-deleting-old-artifacts-for-storage-fees',
    question: 'How to get started with archiving or deleting old Dataiku artifacts (models, datasets) to save storage fees.',
    answer: 'Create a cleanup scenario. The scenario can have a Python step that uses the API to find old or unused datasets and models and then delete them. Schedule this to run periodically to keep your instance clean.',
  },
  {
    id: 588,
    slug: 'analyzing-cluster-usage-logs-for-redundant-workloads',
    question: 'How to get started with analyzing Dataiku cluster usage logs to spot unnecessary or redundant workloads.',
    answer: 'Review the job monitoring page in Dataiku. Look for scenarios that are building the same dataset multiple times. This could indicate a redundant workload that can be eliminated.',
  },
  {
    id: 589,
    slug: 'using-cloud-budget-alerts-for-monitoring-spending',
    question: 'How to get started with using cloud budget alerts (AWS Budgets, Azure Cost Alerts) to monitor Dataiku spending.',
    answer: 'In your cloud provider\'s console, set up a budget for your Dataiku-related resources (identified by tags). Configure an alert to be sent to you when the spending exceeds a certain percentage of the budget. This helps prevent unexpected cost overruns.',
  },
  {
    id: 590,
    slug: 'customizing-dashboards-visual-reports-for-ml-pipeline-kpis',
    question: 'How to get started with customizing Dataiku dashboards and visual reports for ML pipeline KPIs.',
    answer: 'Create a dataset that logs the performance of your ML pipeline (e.g., run time, model accuracy). Then, build a Dataiku dashboard with charts that visualize these KPIs over time. This gives you a central place to monitor the health of your MLOps process.',
  },
  {
    id: 591,
    slug: 'developing-custom-plugin-for-environment-provisioning',
    question: 'How to get started with developing a custom Dataiku plugin to automate environment provisioning.',
    answer: 'This is an advanced task. You would create a plugin with a custom recipe. The Python backend of this recipe could use cloud SDKs and the Dataiku API to automate the creation of a new project, setting up its connections, and creating a default flow.',
  },
  {
    id: 592,
    slug: 'securing-scenarios-by-rotating-credentials-secrets',
    question: 'How to get started with securing Dataiku scenarios by rotating credentials and managing secrets.',
    answer: 'Integrate Dataiku with a secrets management vault. The vault can handle the automatic rotation of credentials. In Dataiku, you configure the connection to retrieve the latest secret from the vault at runtime, rather than storing a static password.',
  },
  {
    id: 593,
    slug: 'using-backup-restore-for-disaster-recovery',
    question: 'How to get started with using Dataiku’s backup and restore features as part of a disaster recovery plan.',
    answer: 'A Dataiku administrator can perform a full backup of the Dataiku instance. This backup includes all projects, data, and configurations. It can be restored to a new server in case of a disaster, allowing for business continuity.',
  },
  {
    id: 594,
    slug: 'auditing-logs-with-elk-stack',
    question: 'How to get started with auditing Dataiku logs using an ELK stack for troubleshooting and compliance.',
    answer: 'Configure a log shipper (like Filebeat) on the Dataiku server to send the various Dataiku logs (backend, jobs, audit) to your Elasticsearch cluster. You can then use Kibana to search, analyze, and create dashboards on these logs.',
  },
  {
    id: 595,
    slug: 'integrating-with-big-data-tools-hadoop-hdfs-hive',
    question: 'How to get started with integrating Dataiku with Big Data tools (Hadoop HDFS, Hive) for large-scale pipelines.',
    answer: 'Dataiku can be installed on an edge node of a Hadoop cluster. The installation process configures it to read/write from HDFS and to submit jobs to engines like Hive or Spark. This allows Dataiku to act as a user-friendly interface to your big data ecosystem.',
  },
  {
    id: 596,
    slug: 'using-web-application-node-to-build-deploy-ml-apps',
    question: 'How to get started with using Dataiku’s Web Application node to quickly build and deploy ML apps.',
    answer: 'In your project, you can create a new "Webapp". Dataiku supports various backends like Python (Dash/Streamlit) or standard HTML/JS. This lets you build a simple user interface for your model, which can be used by business users to interact with it.',
  },
  {
    id: 597,
    slug: 'implementing-scenario-based-testing',
    question: 'How to get started with implementing scenario-based testing (data drift tests, regression tests) in Dataiku.',
    answer: 'Create a "test" scenario. It should contain steps to run your data quality checks, model drift analysis, and perhaps a Python step to compare the output of a recipe against a known "golden" output. This scenario can be triggered by your CI/CD pipeline.',
  },
  {
    id: 598,
    slug: 'linking-to-feature-store-for-reuse',
    question: 'How to get started with linking Dataiku to a feature store to reuse features across projects.',
    answer: 'A feature store is often a specialized database. You would set up a connection to this database in Dataiku. You can then create recipes that read from the feature store to get inputs for your models, and other recipes that write newly engineered features back to the store.',
  },
  {
    id: 599,
    slug: 'orchestrating-jobs-with-cloud-native-schedulers-kubernetes-executors',
    question: 'How to get started with orchestrating Dataiku jobs using cloud-native schedulers (Cloud Composer/Airflow) with Kubernetes executors.',
    answer: 'This is a modern MLOps pattern. Your orchestrator (like Airflow running on Cloud Composer) defines the DAG. When it needs to run a Dataiku job, it uses a Kubernetes operator to create a pod that runs a script. This script then calls the Dataiku API to trigger the specific job.',
  },
  {
    id: 600,
    slug: 'managing-multi-region-multi-cloud-setup',
    question: 'How to get started with managing Dataiku in a multi-region or multi-cloud setup for high availability.',
    answer: 'You can have active-passive or active-active setups. This typically involves having separate Dataiku instances in different regions or clouds. A load balancer can direct traffic, and you would need a process to replicate projects and data between the instances to keep them in sync.',
  },
  {
    id: 601,
    slug: 'establishing-slo-based-monitoring-alerting',
    question: 'How to get started with establishing SLO-based monitoring and alerting for Dataiku-driven ML services.',
    answer: 'Define Service Level Objectives (SLOs) for your deployed models, such as "99.9% of prediction requests should be served in under 100ms." Use monitoring tools like Prometheus to track these metrics and configure alerts to fire if you are in danger of violating your SLO.',
  },
  {
    id: 602,
    slug: 'applying-devops-practices-to-python-sql-code',
    question: 'How to get started with applying DevOps practices (linting, unit tests) to Dataiku Python and SQL code.',
    answer: 'Integrate linters (like `flake8` for Python) into your CI/CD pipeline. The pipeline should fail if the code does not meet the style guide. Similarly, the pipeline should run the unit tests you have defined for your Python recipes and fail if any test does not pass.',
  },
  {
    id: 603,
    slug: 'leveraging-project-sharing-home-folder-structure',
    question: 'How to get started with leveraging Dataiku’s project sharing and Home folder structure for multi-team environments.',
    answer: 'Dataiku supports a "Project Folder" structure. You can create folders for different departments (e.g., "Finance", "Marketing"). This helps organize the large number of projects on a shared instance and makes it easier for teams to find their relevant work.',
  },
  {
    id: 604,
    slug: 'ensuring-auditability-with-logging-version-snapshots',
    question: 'How to get started with ensuring auditability of Dataiku workflows by enabling detailed logging and version snapshots.',
    answer: 'For critical projects, ensure that all changes are versioned in Git. Use the project Wiki to document all major decisions and changes. The combination of Dataiku\'s automatic audit logs and the Git history provides a strong audit trail.',
  }
];

export const getQuestionBySlug = (slug: string): Question | undefined => {
  return questions.find(q => q.slug === slug);
}

export const getQuestionById = (id: number): Question | undefined => {
  return questions.find(q => q.id === id);
}
