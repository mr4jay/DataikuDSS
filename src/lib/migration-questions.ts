
export interface Question {
  id: number;
  slug: string;
  question: string;
  answer: string;
}

export const questions: Question[] = [
  {
    id: 201,
    slug: 'inventorying-existing-alteryx-workflows-for-migration',
    question: 'How to get started with inventorying existing Alteryx workflows for migration?',
    answer: 'Begin by using the Alteryx Server API or manually documenting all workflows. For each workflow, capture its name, purpose, owner, frequency of use, and complexity. A simple spreadsheet is a great tool for this initial inventory.',
  },
  {
    id: 202,
    slug: 'mapping-alteryx-macros-and-custom-tools-to-dataiku-recipes',
    question: 'How to get started with mapping Alteryx macros and custom tools to Dataiku recipes?',
    answer: 'Analyze each Alteryx macro to understand its logic. Simple macros often map to a Dataiku visual recipe (e.g., a macro for data cleaning can become a Prepare recipe). Complex macros may require a Python recipe, a custom plugin, or a reusable Flow Zone in Dataiku.',
  },
  {
    id: 203,
    slug: 'classifying-transformation-complexity-and-dependencies',
    question: 'How to get started with classifying transformation complexity and dependencies?',
    answer: 'Review each workflow and classify it as "Low", "Medium", or "High" complexity based on the number of tools and the types of transformations. Use the Alteryx workflow dependencies feature to identify all input/output data sources, which will need to be mapped in Dataiku.',
  },
  {
    id: 204,
    slug: 'assessing-business-critical-alteryx-pipelines',
    question: 'How to get started with assessing business-critical Alteryx pipelines?',
    answer: 'Work with business stakeholders to identify which workflows support critical business processes. These are your highest priority for migration. Consider the impact on the business if these workflows were to fail.',
  },
  {
    id: 205,
    slug: 'documenting-source-to-target-field-mappings',
    question: 'How to get started with documenting source‑to‑target field mappings?',
    answer: 'For each workflow, create a mapping document. This should list every input field, the transformations applied to it, and the corresponding output field. This documentation is crucial for validation after migration.',
  },
  {
    id: 206,
    slug: 'analyzing-performance-bottlenecks-in-legacy-etl',
    question: 'How to get started with analyzing performance bottlenecks in legacy ETL?',
    answer: 'Review the Alteryx job logs to identify which workflows and specific tools take the longest to run. This will highlight opportunities for optimization when you rebuild the pipeline in Dataiku, for example by using Spark or SQL push-down.',
  },
  {
    id: 207,
    slug: 'identifying-alteryx-specific-logic-that-needs-rewriting',
    question: 'How to get started with identifying Alteryx-specific logic that needs rewriting?',
    answer: 'Look for heavy use of custom macros, R-tools, or specific Alteryx functions that do not have a direct one-to-one equivalent in Dataiku. These will require custom Python or SQL code to be written in Dataiku recipes.',
  },
  {
    id: 208,
    slug: 'creating-migration-timelines-and-milestones',
    question: 'How to get started with creating migration timelines and milestones?',
    answer: 'Group the inventoried workflows into logical migration batches. Create a project plan with clear milestones, such as "Complete migration of all Finance workflows by end of Q2". This provides a roadmap for the migration project.',
  },
  {
    id: 209,
    slug: 'risk-analysis-and-fallback-planning-during-migration',
    question: 'How to get started with risk‑analysis and fallback planning during migration?',
    answer: 'For each critical workflow, define a fallback plan. This usually means keeping the original Alteryx workflow ready to run in parallel for a period after the Dataiku migration goes live. This allows you to switch back if any issues are found.',
  },
  {
    id: 210,
    slug: 'prioritizing-alteryx-flows-for-incremental-migration',
    question: 'How to get started with prioritizing Alteryx flows for incremental migration?',
    answer: 'Prioritize based on a combination of business criticality and complexity. Start with some "quick wins" – workflows that are important but not overly complex. This builds momentum and allows the team to learn the migration process.',
  },
  {
    id: 211,
    slug: 'translating-alteryx-data-cleansing-steps-into-dataiku-prepare-recipes',
    question: 'How to get started with translating Alteryx data cleansing steps into Dataiku Prepare recipes?',
    answer: 'The Dataiku Prepare recipe is the direct equivalent of many Alteryx data cleansing tools. For each step in Alteryx (e.g., trim, case change, filter), add a corresponding processor from the library in the Dataiku Prepare recipe.',
  },
  {
    id: 212,
    slug: 'reproducing-alteryx-joins-using-dataiku-join-recipes-or-sql-recipes',
    question: 'How to get started with reproducing Alteryx joins using Dataiku Join recipes or SQL recipes?',
    answer: 'Use the Dataiku "Join" visual recipe to replicate Alteryx joins. If the data is in a SQL database, a Dataiku SQL recipe is often more performant as it pushes the join operation down to the database.',
  },
  {
    id: 213,
    slug: 'replacing-alteryx-macros-by-dataiku-loops-and-scenario-logic',
    question: 'How to get started with replacing Alteryx macros by Dataiku loops and scenario logic?',
    answer: 'For iterative Alteryx macros, you can use a Dataiku scenario with a Python step. The Python code can loop through a list of items (e.g., regions or products) and run a parameterized job for each item.',
  },
  {
    id: 214,
    slug: 'reimplementing-predictive-macros-in-automl-or-code-recipes',
    question: 'How to get started with reimplementing predictive macros in AutoML or code recipes?',
    answer: 'If an Alteryx workflow uses predictive tools, you can rebuild the model using Dataiku\'s AutoML (Visual Analysis Lab). For custom models, use a Python recipe with libraries like scikit-learn to write the modeling code.',
  },
  {
    id: 215,
    slug: 'recreating-record-parsing-logic-as-python-recipes',
    question: 'How to get started with recreating record parsing logic as Python recipes?',
    answer: 'For complex text parsing that goes beyond Dataiku\'s visual processors, a Python recipe is the best solution. Use Python\'s regular expression libraries to parse text fields and extract the required information.',
  },
  {
    id: 216,
    slug: 'replicating-conditional-logic-if-case-in-dataiku-formulas',
    question: 'How to get started with replicating conditional logic (IF/CASE) in Dataiku formulas?',
    answer: 'In a Dataiku Prepare recipe, use the "Formula" processor. The formula language supports IF/THEN/ELSE statements, allowing you to replicate the conditional logic from Alteryx.',
  },
  {
    id: 217,
    slug: 'handling-alteryx-spatial-or-geocoding-transforms-using-python-or-plugins',
    question: 'How to get started with handling Alteryx spatial or geocoding transforms using Python or plugins?',
    answer: 'Dataiku has a "Geospatial" plugin with recipes for common operations. For more advanced spatial analysis, use a Python recipe with libraries like GeoPandas.',
  },
  {
    id: 218,
    slug: 'generating-pivot-or-cross-tab-transformations-in-dataiku',
    question: 'How to get started with generating pivot or cross‑tab transformations in Dataiku?',
    answer: 'Use the "Pivot" visual recipe in Dataiku. It allows you to transpose rows into columns, similar to the Alteryx Cross Tab tool, to create summary tables.',
  },
  {
    id: 219,
    slug: 'rebuilding-data-blending-routines-in-flow-zones',
    question: 'How to get started with rebuilding data blending routines in Flow Zones?',
    answer: 'Organize your data blending logic in a dedicated Flow Zone. Import all the necessary datasets into this zone and use a series of Join, Stack, and Prepare recipes to replicate the Alteryx data blending process.',
  },
  {
    id: 220,
    slug: 'automated-deduplication-logic-via-dataiku-rules',
    question: 'How to get started with automated deduplication logic via Dataiku rules?',
    answer: 'Use the "Distinct" visual recipe to keep only unique rows. For more complex deduplication based on a key, use a "Group" recipe, aggregate by your key, and select the first or last record from each group.',
  },
  {
    id: 221,
    slug: 'setting-up-dataiku-scenarios-to-mimic-legacy-scheduler-runs',
    question: 'How to get started with setting up Dataiku Scenarios to mimic legacy scheduler runs?',
    answer: 'Create a Dataiku scenario for each scheduled Alteryx job. In the scenario, add a step to build the final output dataset. Then, use a "Time-based" trigger in the scenario settings to replicate the original schedule (e.g., daily at 2 AM).',
  },
  {
    id: 222,
    slug: 'configuring-retry-logic-and-error-alerts-in-scenarios',
    question: 'How to get started with configuring retry logic and error alerts in Scenarios?',
    answer: 'In the "Reporters" tab of a scenario, add a "Mail" or "Slack" reporter to send an alert on failure. For retry logic, you can add a Python step that checks the outcome of a job and re-runs it if it failed, though this requires custom scripting.',
  },
  {
    id: 223,
    slug: 'parameterizing-flow-runs-for-daily-ingestion-cycles',
    question: 'How to get started with parameterizing flow runs for daily ingestion cycles?',
    answer: 'Use project variables for parameters like dates. In your recipes, filter using the variable (e.g., `date_col = \'${run_date}\'`). Your scenario can then be triggered with different values for the `run_date` variable.',
  },
  {
    id: 224,
    slug: 'orchestrating-multi-step-flows-across-project-zones',
    question: 'How to get started with orchestrating multi-step flows across project zones?',
    answer: 'A scenario can build items from any Flow Zone. You can create a master scenario that builds the final outputs. Dataiku\'s dependency graph will automatically ensure that all the necessary upstream steps, even in different zones, are built in the correct order.',
  },
  {
    id: 225,
    slug: 'triggering-dataiku-jobs-via-rest-api-from-legacy-systems',
    question: 'How to get started with triggering Dataiku jobs via REST API from legacy systems?',
    answer: 'If an external system needs to trigger a Dataiku job, use the Dataiku REST API. Generate an API key, find the endpoint for running a scenario, and have the external system make a POST request to that endpoint.',
  },
  {
    id: 226,
    slug: 'building-scheduling-daily-incremental-load-jobs-to-mimic-batch-patterns',
    question: 'How to build + scheduling daily incremental load jobs to mimic batch patterns?',
    answer: 'Use partitioning on your input and output datasets, typically by date. Create a scenario that runs daily. In the "Build" step of the scenario, specify that you only want to build the latest partition. This creates an efficient incremental load.',
  },
  {
    id: 227,
    slug: 'migrating-recursive-workflows-into-scenario-loops',
    question: 'How to get started with migrating recursive workflows into scenario loops?',
    answer: 'This is an advanced pattern. Use a Python scenario step to manage the recursion. The script would run a job, check a condition, and if the condition is not met, it would call the same scenario again with updated parameters.',
  },
  {
    id: 228,
    slug: 'integrating-dataiku-jobs-into-ci-cd-pipelines-using-git-hooks',
    question: 'How to get started with integrating Dataiku jobs into CI/CD pipelines using Git hooks?',
    answer: 'When you connect your Dataiku project to Git, your CI/CD tool (like Jenkins) can be configured to listen for new commits. When a change is pushed, the CI/CD pipeline can automatically trigger a Dataiku scenario to run tests on the new code.',
  },
  {
    id: 229,
    slug: 'archiving-intermediate-data-per-ingestion-cycle',
    question: 'How to get started with archiving intermediate data per ingestion cycle?',
    answer: 'Use a Python recipe at the end of your flow. The recipe can take the output dataset for the day, and copy it to a long-term storage location, such as a different folder in a cloud storage bucket, often with the date in the filename.',
  },
  {
    id: 230,
    slug: 'implementing-alerting-to-replace-alteryx-server-monitoring',
    question: 'How to get started with implementing alerting to replace Alteryx Server monitoring?',
    answer: 'Use scenario reporters. For every migrated workflow (now a scenario), configure a reporter to send an email or Slack message on failure. This provides the same level of monitoring as Alteryx Server notifications.',
  },
  {
    id: 231,
    slug: 'building-data-quality-checks-using-dataiku-metrics-and-checks',
    question: 'How to get started with building data quality checks using Dataiku Metrics and Checks?',
    answer: 'On your output dataset, go to the "Status" tab. In "Metrics", define data quality rules like checking for nulls, min/max values, or valid patterns. Then, in your scenario, add a "Run checks" step that will fail the job if these rules are not met.',
  },
  {
    id: 232,
    slug: 'validating-migrated-outputs-against-alteryx-run-results',
    question: 'How to get started with validating migrated outputs against Alteryx run results?',
    answer: 'For a period, run both the old Alteryx workflow and the new Dataiku flow. Import both output files into a new Dataiku project. Use a "Stack" recipe to combine them, then a "Group" recipe to compare row counts and sum numerical fields to ensure they match.',
  },
  {
    id: 233,
    slug: 'comparing-row-counts-and-hash-sums-for-equivalence',
    question: 'How to get started with comparing row‑counts and hash sums for equivalence?',
    answer: 'To validate a migration, you need to prove the output is identical. Comparing row counts is the first step. For a more robust check, use a Python recipe to calculate a hash (e.g., MD5) of each row in both the Alteryx and Dataiku outputs and ensure they match.',
  },
  {
    id: 234,
    slug: 'embedding-sanity-checks-in-flow-logic-to-auto-validate-transformations',
    question: 'How to get started with embedding sanity checks in flow logic to auto‑validate transformations?',
    answer: 'In your scenario, you can add Python steps that perform sanity checks. For example, after a build step, a Python step can get a handle on the output dataset, check that the number of rows is within an expected range, and fail the scenario if it is not.',
  },
  {
    id: 235,
    slug: 'implementing-anomaly-detection-to-flag-migration-errors',
    question: 'How to get started with implementing anomaly detection to flag migration errors?',
    answer: 'After migrating a workflow, use Dataiku\'s visual statistics to look for anomalies. For example, if the distribution of a numerical column in the Dataiku output looks drastically different from the Alteryx output, it could indicate an error in the transformation logic.',
  },
  {
    id: 236,
    slug: 'setting-up-automated-validation-scenarios-post-migration',
    question: 'How to get started with setting up automated validation scenarios post‑migration?',
    answer: 'Create a dedicated "Validation" scenario. This scenario would take the output from the new Dataiku flow and the old Alteryx flow, run a recipe to compare them, and use metrics and checks to assert that they are identical. This scenario can be run automatically after each main run.',
  },
  {
    id: 237,
    slug: 'generating-lineage-reports-to-trace-migrated-pipelines',
    question: 'How to get started with generating lineage reports to trace migrated pipelines?',
    answer: 'Dataiku automatically generates lineage. For any dataset, the "Lineage" tab provides a detailed, column-level graph showing exactly how it was produced. This is invaluable for documenting and auditing the migrated workflows.',
  },
  {
    id: 238,
    slug: 'versioning-migrated-datasets-and-recipe-logic',
    question: 'How to get started with versioning migrated datasets and recipe logic?',
    answer: 'Connect your Dataiku project to a Git repository. This allows you to commit versions of your recipe code and flow structure. For datasets, you can take snapshots or use partitioning to keep historical versions.',
  },
  {
    id: 239,
    slug: 'documenting-transformations-for-auditability',
    question: 'How to get started with documenting transformations for auditability?',
    answer: 'Use the "Description" field on every recipe to explain its purpose. In Prepare recipes, you can add comments to each individual transformation step. This creates a rich, self-documenting flow that is easy for auditors to understand.',
  },
  {
    id: 240,
    slug: 'ensuring-governance-by-attaching-metadata-to-flows',
    question: 'How to get started with ensuring governance by attaching metadata to flows?',
    answer: 'Use tags to categorize your datasets and flows (e.g., `source:alteryx`, `status:migrated`). Add custom metadata in the "Summary" tab of objects. This metadata helps in managing and governing the large number of assets created during a migration.',
  },
  {
    id: 241,
    slug: 'converting-alteryx-custom-python-tools-into-dataiku-python-recipes',
    question: 'How to get started with converting Alteryx custom Python tools into Dataiku Python recipes?',
    answer: 'Copy the Python code from the Alteryx tool into a new Dataiku Python recipe. You will need to replace the Alteryx-specific API calls for reading and writing data with the corresponding calls from the Dataiku Python API.',
  },
  {
    id: 242,
    slug: 'rewriting-alteryx-sql-statements-within-dataiku-sql-recipes',
    question: 'How to get started with rewriting Alteryx SQL statements within Dataiku SQL recipes?',
    answer: 'This is often a direct copy-paste. Take the SQL code from the Alteryx tool and place it in a Dataiku SQL recipe. You will just need to ensure the table names match the names of the input datasets in Dataiku.',
  },
  {
    id: 243,
    slug: 'building-reusable-python-functions-for-repeated-logic',
    question: 'How to get started with building reusable Python functions for repeated logic?',
    answer: 'If you have Python logic that is used in multiple migrated workflows, create a Python library in your project. Define your reusable functions in this library, and then you can import and call them from any Python recipe in the project.',
  },
  {
    id: 244,
    slug: 'creating-custom-plugins-to-simulate-rare-alteryx-functionality',
    question: 'How to get started with creating custom plugins to simulate rare Alteryx functionality?',
    answer: 'If there is a very specific Alteryx tool with no Dataiku equivalent, you can create a custom Dataiku plugin. This involves writing Python code for the backend logic and JSON configuration for the UI. This is an advanced topic but very powerful.',
  },
  {
    id: 245,
    slug: 'deploying-parameter-driven-saved-models-to-replace-predictive-macros',
    question: 'How to get started with deploying parameter-driven saved models to replace predictive macros?',
    answer: 'Rebuild the predictive model in Dataiku\'s Visual ML lab. Deploy it as a "Saved Model" to your flow. You can then use a "Score" recipe to apply this model to new data, replicating the functionality of the Alteryx predictive macro.',
  },
  {
    id: 246,
    slug: 'embedding-third-party-library-logic-for-specialized-data-transforms',
    question: 'How to get started with embedding third-party library logic for specialized data transforms?',
    answer: 'In a Python recipe, you can use any third-party Python library. Create a "code environment" for your project, add the library as a dependency, and then you can import and use it in your Python recipes.',
  },
  {
    id: 247,
    slug: 'scripting-api-calls-within-dataiku-to-external-services',
    question: 'How to get started with scripting API calls within Dataiku to external services?',
    answer: 'Use a Python recipe with a library like `requests`. You can make API calls to external services, retrieve the data, and then parse the JSON response into a Dataiku dataset.',
  },
  {
    id: 248,
    slug: 'looping-datasets-via-python-to-simulate-alteryx-iterative-actions',
    question: 'How to get started with looping datasets via Python to simulate Alteryx iterative actions?',
    answer: 'In a Python recipe, you can read an input dataset that contains a list of items to iterate over. Then, loop through the rows of this dataset, and for each row, perform an action, such as calling another recipe or an API.',
  },
  {
    id: 249,
    slug: 'building-audit-logs-within-recipes-for-traceability',
    question: 'How to get started with building audit logs within recipes for traceability?',
    answer: 'At the end of a recipe, you can write summary information (like row counts or processing time) to a separate "log" dataset. Appending to this log dataset on each run creates a custom audit trail for your workflow.',
  },
  {
    id: 250,
    slug: 'using-global-variables-and-project-parameters-for-dynamic-logic',
    question: 'How to get started with using global variables and project parameters for dynamic logic?',
    answer: 'Define variables in your project\'s "Variables" section. Reference them in your recipes with the `${variable_name}` syntax. This is key for managing differences between development and production environments without changing the recipe code.',
  },
  {
    id: 251,
    slug: 'connecting-dataiku-to-cloud-data-warehouses-replacing-legacy-outputs',
    question: 'How to get started with connecting Dataiku to cloud data warehouses replacing legacy outputs?',
    answer: 'Instead of writing to a file, configure your final "Export" recipe to write to a cloud data warehouse like Snowflake or BigQuery. You will first need to set up the connection in Administration > Connections.',
  },
  {
    id: 252,
    slug: 'moving-file-based-etl-feeds-into-managed-dataiku-connections',
    question: 'How to get started with moving file‑based ETL feeds into managed Dataiku connections?',
    answer: 'Instead of having Alteryx read from local files, set up a managed connection in Dataiku to the source file location (e.g., an S3 bucket or SFTP server). This makes the pipeline more robust and auditable.',
  },
  {
    id: 253,
    slug: 'managing-secrets-to-authenticate-legacy-data-sources',
    question: 'How to get started with managing secrets to authenticate legacy data sources?',
    answer: 'Use Dataiku\'s built-in secrets management. When you configure a connection to a database, enter the credentials there. Avoid hardcoding passwords in your recipes. For higher security, integrate with a vault system.',
  },
  {
    id: 254,
    slug: 'pulling-data-from-mdm-platforms-informatica-talend-into-dataiku',
    question: 'How to get started with pulling data from MDM platforms (e.g. Informatica, Talend) into Dataiku?',
    answer: 'MDM platforms usually expose their data via a SQL database connection. Set up a standard database connection in Dataiku to the MDM\'s underlying database to read the master data tables.',
  },
  {
    id: 255,
    slug: 'exporting-processed-datasets-to-bi-dashboards-tableau-power-bi',
    question: 'How to get started with exporting processed datasets to BI dashboards (Tableau, Power BI)?',
    answer: 'The final step of your migrated flow should be an "Export" recipe that writes the data to a table in a database that your BI tool is connected to. This allows for seamless integration with existing reporting.',
  },
  {
    id: 256,
    slug: 'syncing-upstream-changes-from-alteryx-outputs-for-dual-run-validation',
    question: 'How to get started with syncing upstream changes from Alteryx outputs for dual‑run validation?',
    answer: 'During the migration, you will need to compare the output of the old and new systems. Create a Dataiku dataset that reads the output file generated by the Alteryx workflow. This allows you to bring the Alteryx result into Dataiku for comparison.',
  },
  {
    id: 257,
    slug: 'interfacing-dataiku-with-message-queues-for-real-time-loads',
    question: 'How to get started with interfacing Dataiku with message queues for real‑time loads?',
    answer: 'Dataiku has a "Kafka" plugin that allows you to read from and write to Kafka topics. This can be used to migrate Alteryx workflows that were part of a more real-time data process.',
  },
  {
    id: 258,
    slug: 'replacing-flat-file-landing-zones-with-structured-dataiku-datasets',
    question: 'How to get started with replacing flat‑file landing zones with structured Dataiku datasets?',
    answer: 'The Alteryx practice of dropping files in folders can be improved. In Dataiku, read the raw data and immediately create a structured, typed dataset. This serves as the new, governed "landing zone" for your data.',
  },
  {
    id: 259,
    slug: 'using-jdbc-odbc-to-replace-legacy-extracts-into-dataiku',
    question: 'How to get started with using JDBC/ODBC to replace legacy extracts into Dataiku?',
    answer: 'Dataiku can connect to almost any database using a JDBC driver. In Administration > Connections, you can set up a generic JDBC connection to replicate the data source connections you had in Alteryx.',
  },
  {
    id: 260,
    slug: 'building-delta-load-patterns-for-incremental-ingestion-workflows',
    question: 'How to get started with building delta‑load patterns for incremental ingestion workflows?',
    answer: 'Use partitioning, typically on a date column. When your scenario runs, it should calculate the latest data to be loaded (the delta) and only build the corresponding new partitions, leaving the historical partitions untouched.',
  },
  {
    id: 261,
    slug: 'profiling-dataiku-runs-to-tune-recipe-performance-post-migration',
    question: 'How to get started with profiling Dataiku runs to tune recipe performance post‑migration?',
    answer: 'After migrating a flow, check the job logs. The timings for each recipe will show you where the bottlenecks are. A recipe that takes minutes to run when others take seconds is a candidate for optimization.',
  },
  {
    id: 262,
    slug: 'using-spark-push-down-or-spark-recipes-to-parallelize-heavy-lifts',
    question: 'How to get started with using Spark push-down or Spark recipes to parallelize heavy lifts?',
    answer: 'For slow recipes on large datasets, change the execution engine from "In-Memory" to "Spark". This will distribute the processing across a Spark cluster, leading to significant performance gains.',
  },
  {
    id: 263,
    slug: 'partitioning-datasets-to-shrink-compute-footprint',
    question: 'How to get started with partitioning datasets to shrink compute footprint?',
    answer: 'If your data is time-based, partition it by day or month. When you rebuild the data daily, downstream recipes can be configured to only process the latest partition, which is much more efficient than processing the entire dataset every time.',
  },
  {
    id: 264,
    slug: 'implementing-caching-strategies-to-avoid-redundant-recompute',
    question: 'How to get started with implementing caching strategies to avoid redundant recompute?',
    answer: 'Dataiku does this automatically. Every dataset is a cache. If the code or input data for an upstream recipe has not changed, Dataiku will not re-run it. You can optimize the format of this cache (e.g., use Parquet) for better performance.',
  },
  {
    id: 265,
    slug: 'trimming-intermediate-datasets-to-reduce-storage-and-runtime',
    question: 'How to get started with trimming intermediate datasets to reduce storage and runtime?',
    answer: 'After a Join or other recipe, you may have columns that are no longer needed. Add a Prepare recipe to remove these columns. This reduces the size of the intermediate dataset, which speeds up all downstream processing.',
  },
  {
    id: 266,
    slug: 'refactoring-complex-join-logic-for-push-down-execution',
    question: 'How to get started with refactoring complex join logic for push-down execution?',
    answer: 'If you have a chain of multiple Join recipes, try to combine them into a single SQL recipe. This allows the database to optimize the entire join plan at once, which is usually more efficient than Dataiku running them one by one.',
  },
  {
    id: 267,
    slug: 'tuning-resource-settings-on-api-nodes-and-job-execution',
    question: 'How to get started with tuning resource settings on API nodes and job execution?',
    answer: 'A Dataiku administrator can configure the resources available for jobs, such as the memory allocated to the Dataiku backend. For real-time APIs, the resources for the API node can be configured in the API Deployer.',
  },
  {
    id: 268,
    slug: 'archiving-or-deleting-obsolete-datasets-for-performance',
    question: 'How to get started with archiving or deleting obsolete datasets for performance?',
    answer: 'During a migration, you will create many temporary or validation datasets. Once a migrated flow is in production, clean up these unnecessary datasets to save storage space and reduce clutter in the project.',
  },
  {
    id: 269,
    slug: 'benchmarking-dataiku-vs-legacy-alteryx-runtimes',
    question: 'How to get started with benchmarking Dataiku vs legacy Alteryx runtimes?',
    answer: 'For a key workflow, record the end-to-end run time in Alteryx. Then, after migrating it, record the run time of the corresponding Dataiku scenario. This provides a clear metric for the performance improvement of the migration.',
  },
  {
    id: 270,
    slug: 'monitor-and-optimize-memory-usage-across-flow-stages',
    question: 'How to get started with monitor and optimize memory usage across flow stages?',
    answer: 'Avoid pulling large datasets into the in-memory engine. Where possible, use the SQL or Spark engines to push down computation. This minimizes the memory used by Dataiku itself and leverages the power of distributed systems.',
  },
  {
    id: 271,
    slug: 'training-users-on-dataiku-equivalents-to-alteryx-tools',
    question: 'How to get started with training users on Dataiku equivalents to Alteryx tools?',
    answer: 'Create a simple "cheat sheet" that maps common Alteryx tools to their Dataiku recipe equivalents (e.g., Alteryx "Join" -> Dataiku "Join"). Hold training sessions to walk Alteryx users through this mapping and build a simple flow in Dataiku.',
  },
  {
    id: 272,
    slug: 'creating-user-guides-for-migrated-workflows',
    question: 'How to get started with creating user guides for migrated workflows?',
    answer: 'Use the project Wiki. For each migrated workflow, create a Wiki page that explains its purpose, inputs, outputs, and how to run it via the scenario. This documentation is essential for user adoption.',
  },
  {
    id: 273,
    slug: 'building-self-service-applications-via-dataiku-apps',
    question: 'How to get started with building self‑service applications via Dataiku Apps?',
    answer: 'For workflows that required user input in Alteryx, you can build a Dataiku Web App. This provides a simple UI where a business user can enter parameters, click a button, and run the underlying flow without needing to see the code.',
  },
  {
    id: 274,
    slug: 'hosting-brown-bag-sessions-to-share-migration-learnings',
    question: 'How to get started with hosting brown‑bag sessions to share migration learnings?',
    answer: 'Organize informal lunch sessions where the migration team can present a recently migrated workflow. They can share the challenges they faced and the solutions they found. This helps spread knowledge across the organization.',
  },
  {
    id: 275,
    slug: 'setting-up-sandbox-projects-for-user-experimentation',
    question: 'How to get started with setting up sandbox projects for user experimentation?',
    answer: 'Provide a sandbox Dataiku instance where business users can have their own projects. They can import the migrated "golden" datasets and experiment with them without fear of breaking the production pipelines.',
  },
  {
    id: 276,
    slug: 'collecting-feedback-from-business-users-post-migration',
    question: 'How to get started with collecting feedback from business users post‑migration?',
    answer: 'After a workflow is migrated and users start consuming its output, schedule follow-up meetings to gather their feedback. Are they getting the data they expect? Is it timely? This is crucial for ensuring the migration is a success.',
  },
  {
    id: 277,
    slug: 'updating-coding-standards-based-on-migrated-practices',
    question: 'How to get started with updating coding standards based on migrated practices?',
    answer: 'As you migrate workflows, you will develop best practices for how to implement certain patterns in Dataiku. Document these in a central Wiki and make them part of your team\'s official coding standards.',
  },
  {
    id: 278,
    slug: 'building-a-central-wiki-of-migrated-logic-and-flows',
    question: 'How to get started with building a central wiki of migrated logic and flows?',
    answer: 'Create a dedicated Dataiku project to serve as a "Migration Hub". Use its Wiki to document every migrated workflow, linking to the new project where it lives. This creates a central catalog of your migrated assets.',
  },
  {
    id: 279,
    slug: 'enabling-self-service-reuse-of-dataiku-recipes',
    question: 'How to get started with enabling self‑service reuse of Dataiku recipes?',
    answer: 'If you build a particularly useful recipe (e.g., for address parsing), you can turn it into a custom plugin. This makes it appear as a new visual recipe in the UI, which other users can then easily reuse in their own flows.',
  },
  {
    id: 280,
    slug: 'mentoring-junior-team-members-on-dataiku-migration-patterns',
    question: 'How to get started with mentoring junior team members on Dataiku migration patterns?',
    answer: 'Pair a junior developer with a senior developer for a migration task. The senior developer can guide them through the process of analyzing the Alteryx workflow and rebuilding it in Dataiku, explaining the best practices along the way.',
  },
  {
    id: 281,
    slug: 'applying-access-control-roles-in-migrated-projects',
    question: 'How to get started with applying access control roles in migrated projects?',
    answer: 'In the new Dataiku project, go to Settings > Permissions. Add the user groups that should have access and assign them appropriate roles (e.g., "Reader" for business users, "Contributor" for developers).',
  },
  {
    id: 282,
    slug: 'tracking-lineage-from-alteryx-origin-to-dataiku-outputs',
    question: 'How to get started with tracking lineage from Alteryx origin to Dataiku outputs?',
    answer: 'In the description of your input datasets in Dataiku, document the original Alteryx data source. Dataiku\'s automatic lineage will then provide a complete map from that source all the way to the final output.',
  },
  {
    id: 283,
    slug: 'enforcing-regulatory-compliance-in-migrated-logic',
    question: 'How to get started with enforcing regulatory compliance in migrated logic?',
    answer: 'Use PII tagging on sensitive columns. Use access controls to restrict who can see this data. Document the transformations in the recipes to prove to auditors that you are handling the data in a compliant way.',
  },
  {
    id: 284,
    slug: 'capturing-transformation-metadata-within-dataiku',
    question: 'How to get started with capturing transformation metadata within Dataiku?',
    answer: 'Use the "Description" field on every recipe and dataset. In Prepare recipes, comment on individual steps. This builds a rich metadata layer that explains the "what" and "why" of your flow.',
  },
  {
    id: 285,
    slug: 'approving-deployments-from-dev-to-prod-migration-branches',
    question: 'How to get started with approving deployments from dev to prod migration branches?',
    answer: 'Use project bundles for deployment. A senior developer should review the development project before creating the bundle. The deployment to the production instance should be done by a limited number of authorized administrators.',
  },
  {
    id: 286,
    slug: 'maintaining-audit-trails-across-recipe-and-dataset-changes',
    question: 'How to get started with maintaining audit trails across recipe and dataset changes?',
    answer: 'The project "Timeline" automatically logs every change to the project. When using Git, the commit history provides an even more detailed audit trail of all code and configuration changes.',
  },
  {
    id: 287,
    slug: 'archiving-legacy-alteryx-projects-with-documented-equivalence',
    question: 'How to get started with archiving legacy Alteryx projects with documented equivalence?',
    answer: 'Once a workflow is successfully migrated and validated, the original Alteryx workflow can be archived. Store it in a designated location and include a "ReadMe" file that points to the new, equivalent Dataiku project.',
  },
  {
    id: 288,
    slug: 'embedding-privacy-logic-pii-masking-anonymization-in-recipes',
    question: 'How to get started with embedding privacy logic (PII masking, anonymization) in recipes?',
    answer: 'Use a Prepare recipe with the "Find and Replace" or "Formula" processors to mask sensitive data. For example, you could replace the last digits of a phone number with "X". The "Anonymize" plugin can also be used for this.',
  },
  {
    id: 289,
    slug: 'version-controlling-project-code-via-git',
    question: 'How to get started with version controlling project code via Git?',
    answer: 'Connect your project to a remote Git repository in the project settings. This allows you to commit all your flow logic, recipe code, and configurations, providing a full version history and enabling collaborative development.',
  },
  {
    id: 290,
    slug: 'watermarking-datasets-to-flag-migrated-artifacts',
    question: 'How to get started with watermarking datasets to flag migrated artifacts?',
    answer: 'In a final Prepare recipe, you can add a new column to your output dataset, for example `migration_status`, with a constant value of "Migrated from Alteryx". This makes it easy to identify the origin of the data.',
  },
  {
    id: 291,
    slug: 'defining-kpis-to-measure-migration-success',
    question: 'How to get started with defining KPIs to measure migration success?',
    answer: 'Key Performance Indicators for a migration project could include: reduction in run time, reduction in manual effort, number of workflows migrated, and user satisfaction scores. Track these in a dashboard.',
  },
  {
    id: 292,
    slug: 'benchmarking-cost-and-runtime-reduction-post-migration',
    question: 'How to get started with benchmarking cost and runtime reduction post‑migration?',
    answer: 'Measure the compute cost and run time of the original Alteryx workflow. After migrating to Dataiku, measure the new costs and run times. The difference demonstrates the ROI of the migration project.',
  },
  {
    id: 293,
    slug: 'tracking-number-of-workflows-migrated-per-sprint',
    question: 'How to get started with tracking number of workflows migrated per sprint?',
    answer: 'If you are using an Agile approach, use a simple burn-down chart to track the number of workflows migrated against your target for each sprint. This helps in managing the project and communicating progress.',
  },
  {
    id: 294,
    slug: 'monitoring-scenario-success-rates-post-cutover',
    question: 'How to get started with monitoring scenario success rates post‑cutover?',
    answer: 'After a workflow goes live in Dataiku, monitor its scenario run history. A high success rate indicates a stable and successful migration. A high failure rate indicates issues that need to be investigated.',
  },
  {
    id: 295,
    slug: 'capturing-feedback-to-refine-migration-templates',
    question: 'How to get started with capturing feedback to refine migration templates?',
    answer: 'After migrating a few workflows, hold a lessons-learned session with the team. Use their feedback to create or refine a project template for migrations. This template could include standard Flow Zones, naming conventions, and validation recipes.',
  },
  {
    id: 296,
    slug: 'assessing-user-adoption-rates-of-new-dataiku-pipelines',
    question: 'How to get started with assessing user adoption rates of new Dataiku pipelines?',
    answer: 'Check how many users are accessing the output datasets and dashboards from your migrated flows. You can see this in the usage logs. Low adoption may indicate a need for more training or communication.',
  },
  {
    id: 297,
    slug: 'tracking-improvements-in-data-freshness-or-throughput',
    question: 'How to get started with tracking improvements in data freshness or throughput?',
    answer: 'If the Alteryx workflow ran weekly, and the new Dataiku flow runs daily, you have improved data freshness. Measure and document these improvements as a key benefit of the migration.',
  },
  {
    id: 298,
    slug: 'reporting-migration-metrics-to-leadership-and-stakeholders',
    question: 'How to get started with reporting migration metrics to leadership and stakeholders?',
    answer: 'Create a Dataiku dashboard to report on your migration KPIs. Share this dashboard with stakeholders to provide them with a clear and up-to-date view of the project\'s progress and success.',
  },
  {
    id: 299,
    slug: 'iterating-migration-plans-based-on-feedback',
    question: 'How to get started with iterating migration plans based on feedback?',
    answer: 'A migration project is not static. Use the feedback from users and the lessons learned from early migrations to adjust your plan for the remaining workflows. You may need to re-prioritize or allocate more time for complex migrations.',
  },
  {
    id: 300,
    slug: 'planning-phase-2-or-next-wave-migrations-using-lessons-learned',
    question: 'How to get started with planning phase‑2 or next‑wave migrations using lessons learned?',
    answer: 'After completing the first phase of migration, use the experience you have gained to plan the next phase. You will have a much better understanding of the effort required, which will lead to more accurate timelines and a smoother process.',
  },
];

export const getQuestionBySlug = (slug: string): Question | undefined => {
  return questions.find(q => q.slug === slug);
}

export const getQuestionById = (id: number): Question | undefined => {
  return questions.find(q => q.id === id);
}
