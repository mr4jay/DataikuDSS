
export interface Question {
  id: number;
  slug: string;
  question: string;
  answer: string;
}

export const questions: Question[] = [
  {
    id: 401,
    slug: 'defining-onboarding-workflows-for-new-dataiku-users',
    question: 'How to get started with defining onboarding workflows for new Dataiku users?',
    answer: 'Begin by creating a dedicated "Onboarding" project in Dataiku. Structure it with Flow Zones representing different learning stages (e.g., "1. Connecting to Data," "2. Visual Recipes"). Each zone should contain simple, well-documented example flows that new users can explore and replicate.',
  },
  {
    id: 402,
    slug: 'assigning-datasets-and-flows-for-hands-on-learning',
    question: 'How to get started with assigning datasets and flows for hands-on learning?',
    answer: 'Provide new users with read-only access to a "Sandbox" project containing clean, representative datasets. Assign them specific tasks, documented in the project Wiki, such as "Replicate the sales-forecasting-prepare recipe" or "Build a dashboard to show customer demographics."',
  },
  {
    id: 403,
    slug: 'introducing-standard-naming-conventions-for-datasets-and-recipes',
    question: 'How to get started with introducing standard naming conventions for datasets and recipes?',
    answer: 'Create a page in your central Dataiku Wiki titled "Naming Conventions." Document clear rules, such as `project_source_name_status` for datasets (e.g., `FIN_SAP_CUSTOMERS_CLEANED`) and verb-based names for recipes (e.g., `prepare_customer_data`, `join_sales_and_customers`). Enforce this through peer reviews.',
  },
  {
    id: 404,
    slug: 'monitoring-new-joiners-progress-across-the-flow',
    question: 'How to get started with monitoring new joiners’ progress across the Flow?',
    answer: 'Use the project "Timeline" to see a log of a new user\'s activity. For a more structured approach, create a simple checklist in the Wiki for their onboarding tasks. Ask them to check off items as they complete them, and review their work in the Flow directly.',
  },
  {
    id: 405,
    slug: 'creating-checklist-based-training-modules-for-each-feature',
    question: 'How to get started with creating checklist-based training modules for each feature?',
    answer: 'Develop a set of learning modules in a shared space like a Confluence page or a central Dataiku Wiki. For each module (e.g., "Visual Recipes"), create a checklist of skills to master, like "Successfully use a Prepare recipe to filter data" or "Perform a Join on two datasets."',
  },
  {
    id: 406,
    slug: 'enforcing-documentation-discipline-early-in-the-project',
    question: 'How to get started with enforcing documentation discipline early in the project?',
    answer: 'Make it a mandatory part of the "definition of done" for any task. Before a user\'s work is considered complete, they must have added a clear description to every recipe and dataset they created. This builds good habits from day one.',
  },
  {
    id: 407,
    slug: 'building-learning-flows-that-cover-prepare-join-window-recipes',
    question: 'How to get started with building learning flows that cover Prepare, Join, Window recipes?',
    answer: 'Create a sample project with a simple, linear flow. Start with two raw datasets. The first recipe should be a "Prepare" recipe for cleaning. The next should be a "Join" recipe to combine them. The final should be a "Window" recipe to calculate a running total. Document each step clearly.',
  },
  {
    id: 408,
    slug: 'guiding-them-through-real-data-exploration-tasks',
    question: 'How to get started with guiding them through real data exploration tasks?',
    answer: 'Provide a new dataset and ask open-ended questions. For example, "Use the Charts tab to explore the distribution of customer ages" or "Profile this dataset and identify the columns with the most missing values." This encourages curiosity and teaches them to use Dataiku\'s exploratory features.',
  },
  {
    id: 409,
    slug: 'auditing-how-juniors-use-variables-metrics-and-flags',
    question: 'How to get started with auditing how juniors use variables, metrics, and flags?',
    answer: 'Periodically review their projects. Check if they are correctly using project variables for parameterization instead of hardcoding values. See if they are setting up metrics and checks on their key datasets to validate data quality. Provide feedback on best practices.',
  },
  {
    id: 410,
    slug: 'encouraging-best-practices-around-branching-and-git-sync',
    question: 'How to get started with encouraging best practices around branching and Git sync?',
    answer: 'If using Git, establish a clear branching strategy (e.g., feature branches). Instruct new users to always create a new branch for their work, commit their changes regularly with clear messages, and use pull requests for review before merging into the main branch.',
  },
  {
    id: 411,
    slug: 'defining-recipe-selection-frameworks-prepare-vs-python',
    question: 'How to get started with defining recipe selection frameworks (e.g., when to prefer “Prepare” vs “Python”)?',
    answer: 'Document a simple rule: "Use a Prepare recipe for anything that can be done with the built-in visual processors. Switch to a Python recipe only when you need custom logic, external libraries, or complex operations that are not available visually." This prioritizes maintainability.',
  },
  {
    id: 412,
    slug: 'standardizing-the-transformation-rules-in-prepare-steps',
    question: 'How to get started with standardizing the transformation rules in “Prepare” steps?',
    answer: 'Create a shared library of Prepare recipe steps. You can copy a set of standardized cleaning steps (e.g., for handling nulls, trimming whitespace, and standardizing case) and share the JSON with the team so they can paste it into their own recipes for consistency.',
  },
  {
    id: 413,
    slug: 'setting-ground-rules-for-using-stack-vs-join-recipes',
    question: 'How to get started with setting ground rules for using Stack vs Join recipes?',
    answer: 'Clearly define the use cases. "Use Join to add new columns to a dataset from another dataset based on a common key. Use Stack to add new rows from another dataset that has the same columns." Visual aids in the Wiki can help illustrate this.',
  },
  {
    id: 414,
    slug: 'using-pivot-and-unpivot-recipes-for-reporting-logic',
    question: 'How to get started with using Pivot and Unpivot recipes for reporting logic?',
    answer: 'Showcase a clear example. Use a "Pivot" recipe to turn a long transaction table into a wide summary table for reporting. Then, show how an "Unpivot" recipe can be used to normalize a wide table (like from a spreadsheet) into a long format suitable for database storage.',
  },
  {
    id: 415,
    slug: 'preempting-edge-cases-in-window-and-top-n-recipes',
    question: 'How to get started with preempting edge cases in Window and Top-N recipes?',
    answer: 'When teaching these recipes, always cover the "Partitioning" and "Ordering" aspects. For the Top-N recipe, explain the different options for handling ties, as this is a common source of confusion and unexpected results.',
  },
  {
    id: 416,
    slug: 'resolving-issues-when-visual-recipes-get-too-complex',
    question: 'How to get started with resolving issues when visual recipes get too complex?',
    answer: 'Advise developers to break down a very long Prepare recipe (e.g., with 50+ steps) into multiple, smaller Prepare recipes. This improves readability and makes it easier to debug. Each recipe should have a clear, single purpose.',
  },
  {
    id: 417,
    slug: 'validating-recipe-chains-for-schema-consistency',
    question: 'How to get started with validating recipe chains for schema consistency?',
    answer: 'Encourage the use of the "Propagate schema changes" feature when modifying upstream recipes. After running a job, teach them to quickly check the schema of the output dataset to ensure column names and types are as expected.',
  },
  {
    id: 418,
    slug: 'enforcing-test-datasets-for-each-visual-recipe-module',
    question: 'How to get started with enforcing test datasets for each visual recipe module?',
    answer: 'For critical visual recipes, create a separate small, static dataset that contains known edge cases. Create a branch of the flow where the recipe takes this test dataset as input. The developer can then validate that their changes handle the edge cases correctly.',
  },
  {
    id: 419,
    slug: 'mentoring-on-when-to-add-intermediate-sync-recipes',
    question: 'How to get started with mentoring on when to add intermediate Sync recipes?',
    answer: 'Explain that a "Sync" recipe is primarily for moving data between different storage connections (e.g., from a file system to a SQL database). It is a good practice to use a Sync recipe to materialize the results of a complex set of steps before the final reporting phase.',
  },
  {
    id: 420,
    slug: 'embedding-qa-tags-within-recipe-descriptions',
    question: 'How to get started with embedding QA tags within recipe descriptions?',
    answer: 'Establish a simple tagging system for recipe descriptions, such as `[QA-Status: Pending]` or `[Reviewer: John Doe]`. This makes it easy to see the status of a recipe directly in the Flow without having to look elsewhere.',
  },
  {
    id: 421,
    slug: 'establishing-reusable-python-templates-for-data-transformation',
    question: 'How to get started with establishing reusable Python templates for data transformation?',
    answer: 'Create a "Template" project. In its library, create a Python file with a template class or function that includes standard sections for reading inputs, logging, applying logic, and writing outputs. Developers can copy this template to start new Python recipes.',
  },
  {
    id: 422,
    slug: 'managing-libraries-across-code-environments',
    question: 'How to get started with managing libraries across code environments?',
    answer: 'Centralize code environment management. Define a set of standard environments (e.g., "General Purpose," "Geospatial," "Deep Learning") with specific package lists. Instruct developers to choose from these standard environments rather than creating new ones for every project.',
  },
  {
    id: 423,
    slug: 'documenting-logic-using-inline-markdown-in-jupyter-recipes',
    question: 'How to get started with documenting logic using inline markdown in Jupyter recipes?',
    answer: 'Encourage the use of Markdown cells in notebooks to explain the "why" behind the code. A good notebook should read like a story, with text cells explaining the approach, followed by code cells that implement it.',
  },
  {
    id: 424,
    slug: 'validating-sql-logic-using-profiling-not-just-preview',
    question: 'How to get started with validating SQL logic using profiling, not just preview?',
    answer: 'Teach developers that the preview pane in a SQL recipe only runs on a sample. To validate the logic on the full dataset, they must run the recipe and then use the "Profile" feature on the output dataset to check distributions and statistics.',
  },
  {
    id: 425,
    slug: 'reviewing-memory-intensive-operations-in-python-code',
    question: 'How to get started with reviewing memory-intensive operations in Python code?',
    answer: 'During code reviews, look for signs that a large dataset is being loaded entirely into a Pandas DataFrame. If this is the case, guide the developer to either use an iterator to process the data in chunks or to switch to a PySpark recipe for distributed processing.',
  },
  {
    id: 426,
    slug: 'enforcing-parameterization-and-modular-code',
    question: 'How to get started with enforcing parameterization and modular code?',
    answer: 'Mandate that any hardcoded value (like a threshold or a filename) should be replaced with a project variable. For repeated code blocks, require them to be moved into a function in the project\'s Python library for reusability.',
  },
  {
    id: 427,
    slug: 'standardizing-code-recipe-structure',
    question: 'How to get started with standardizing code recipe structure (imports, logging, outputs)?',
    answer: 'Provide a template for all Python recipes. It should have a clear structure: 1. Imports, 2. Main function, 3. Helper functions, 4. Main execution block that calls the main function. This consistency makes code much easier to review and maintain.',
  },
  {
    id: 428,
    slug: 'designing-code-recipes-to-follow-lineage-traceability',
    question: 'How to get started with designing code recipes to follow lineage traceability?',
    answer: 'Ensure that code recipes explicitly declare their inputs and outputs. Avoid using the Dataiku API to read datasets that are not formal inputs, as this breaks the visible lineage in the Flow.',
  },
  {
    id: 429,
    slug: 'enforcing-use-of-the-dataiku-api-to-read-write-datasets',
    question: 'How to get started with enforcing use of the Dataiku API to read/write datasets?',
    answer: 'During code reviews, check that developers are using `dataiku.Dataset("...").get_dataframe()` to read data and `output_dataset.write_with_schema(...)` to write data. This ensures that Dataiku is managing the data and tracking lineage correctly.',
  },
  {
    id: 430,
    slug: 'ensuring-each-code-recipe-is-productionizable',
    question: 'How to get started with ensuring each code recipe is productionizable?',
    answer: 'A recipe is productionizable if it is robust. This means it should include error handling (try/except blocks), be well-documented, be parameterized with variables, and have its dependencies managed through a code environment.',
  },
  {
    id: 431,
    slug: 'defining-flow-zones-based-on-data-maturity',
    question: 'How to get started with defining Flow Zones based on data maturity?',
    answer: 'Structure your flows with standard zones: "1. Raw Data" (for source datasets), "2. Staging" (for cleaned and joined data), "3. Feature Engineering" (for model-ready data), and "4. Reporting" (for final outputs). This creates a clear, logical data flow.',
  },
  {
    id: 432,
    slug: 'enabling-cross-team-dataset-reusability',
    question: 'How to get started with enabling cross-team dataset reusability?',
    answer: 'Create a "Shared" or "Golden" project. This project contains key, certified datasets that are used by multiple teams. Grant other projects read-only access to these shared datasets. This prevents data duplication and ensures consistency.',
  },
  {
    id: 433,
    slug: 'identifying-flow-bottlenecks-before-scaling',
    question: 'How to get started with identifying Flow bottlenecks before scaling?',
    answer: 'Use the "Job Inspector" to review the run times of your main scenarios. A flow diagram will show the duration of each recipe. Any recipe that takes significantly longer than others is a bottleneck that needs to be optimized before the project is scaled up.',
  },
  {
    id: 434,
    slug: 'labeling-datasets-for-ownership-and-expiry',
    question: 'How to get started with labeling datasets for ownership and expiry?',
    answer: 'Use a combination of tags and custom metadata. Establish tags like `owner:finance-team` and `data-tier:critical`. Add a custom metadata field called "Expiry Date" to datasets that should be archived or deleted after a certain time.',
  },
  {
    id: 435,
    slug: 'organizing-datasets-to-align-with-domain-logic',
    question: 'How to get started with organizing datasets to align with domain logic?',
    answer: 'Flow Zones are the key. Create zones that match your business domains, such as "Customer Data," "Sales Data," and "Product Data." This makes the Flow intuitive for business stakeholders to understand.',
  },
  {
    id: 436,
    slug: 'enforcing-metadata-capture-for-each-recipe',
    question: 'How to get started with enforcing metadata capture for each recipe?',
    answer: 'Make the "Description" field on all recipes mandatory. The description should explain the business purpose of the recipe. This documentation is essential for maintainability and governance.',
  },
  {
    id: 437,
    slug: 'setting-up-documentation-expectations-per-flow-block',
    question: 'How to get started with setting up documentation expectations per flow block?',
    answer: 'For each Flow Zone, create a text box on the Flow itself that describes the purpose of that zone. For individual recipes, the expectation should be a clear, one-sentence description of what it does.',
  },
  {
    id: 438,
    slug: 'creating-dummy-flows-for-mock-practice',
    question: 'How to get started with creating dummy flows for mock practice?',
    answer: 'Create a sandbox project with simple, non-sensitive datasets. Build a representative but simplified version of a real production flow. This allows new users to practice and experiment without any risk to real data.',
  },
  {
    id: 439,
    slug: 'managing-lineage-updates-during-major-changes',
    question: 'How to get started with managing lineage updates during major changes?',
    answer: 'When you make a significant change to an upstream recipe, use the "Propagate schema changes" feature to automatically update downstream recipes. After the change, you must rebuild all downstream datasets to ensure the data reflects the new logic.',
  },
  {
    id: 440,
    slug: 'preparing-flows-for-handover-between-teams',
    question: 'How to get started with preparing flows for handover between teams?',
    answer: 'Ensure the project is thoroughly documented in the Wiki. Check that all recipes have clear descriptions. Organize a handover meeting to walk the new team through the Flow, scenarios, and dashboards. Finally, update the project permissions to give the new team ownership.',
  },
  {
    id: 441,
    slug: 'setting-up-standard-build-all-scenarios',
    question: 'How to get started with setting up standard “build all” scenarios?',
    answer: 'Create a main scenario for each project. In the "Steps" tab, add a single "Build / Train" step. In this step, select only the final output datasets of your flow. Dataiku\'s dependency engine will automatically build all the necessary upstream recipes in the correct order.',
  },
  {
    id: 442,
    slug: 'defining-escalation-protocols-on-scenario-failure',
    question: 'How to get started with defining escalation protocols on scenario failure?',
    answer: 'In the scenario\'s "Reporters" tab, configure a mail reporter. Set it to trigger on failure. The email should be sent to a distribution list that includes the project owner and the on-call support team. The email body should contain a link to the failed job log.',
  },
  {
    id: 443,
    slug: 'parameterizing-scenarios-for-staging-vs-production',
    question: 'How to get started with parameterizing scenarios for staging vs production?',
    answer: 'Use project variables to define environment-specific settings like database names or file paths. You can override these variables for a specific scenario run. For example, you can have a "Run_Staging" scenario that sets the `env` variable to "staging".',
  },
  {
    id: 444,
    slug: 'scheduling-health-check-scenarios-dataset-freshness',
    question: 'How to get started with scheduling health check scenarios (e.g., dataset freshness)?',
    answer: 'Create a scenario with a Python step. The script can use the Dataiku API to get the last modification time of a key dataset. If the time is older than a defined threshold (e.g., 24 hours), the script can fail the scenario, triggering an alert.',
  },
  {
    id: 445,
    slug: 'building-alert-scenarios-that-notify-different-roles',
    question: 'How to get started with building alert scenarios that notify different roles?',
    answer: 'You can have multiple reporters on a single scenario. For example, on failure, one reporter could send a technical alert with logs to the developers, while another sends a high-level business alert to the project manager.',
  },
  {
    id: 446,
    slug: 'documenting-business-logic-per-scenario-step',
    question: 'How to get started with documenting business logic per scenario step?',
    answer: 'Give each step in your scenario a clear, descriptive name. For example, instead of "Build", name the step "Build Final Customer Reporting Table". This makes the scenario\'s purpose clear to anyone who looks at it.',
  },
  {
    id: 447,
    slug: 'automating-cleanup-scenarios-for-temp-datasets',
    question: 'How to get started with automating cleanup scenarios for temp datasets?',
    answer: 'Create a scenario with a step to "Clear" a dataset. This will delete the data but keep the schema. Schedule this scenario to run periodically on temporary or intermediate datasets to save storage space.',
  },
  {
    id: 448,
    slug: 'introducing-time-window-checks-into-scenarios',
    question: 'How to get started with introducing time-window checks into scenarios?',
    answer: 'Use a Python step at the beginning of a scenario. The script can check the current time. If it is outside of an approved run window (e.g., a batch window at night), the script can exit, preventing the scenario from running at the wrong time.',
  },
  {
    id: 449,
    slug: 'writing-scenario-scripts-to-restart-specific-failed-recipes',
    question: 'How to get started with writing scenario scripts to restart specific failed recipes?',
    answer: 'This is an advanced recovery pattern. A Python scenario step can use the API to get the status of a job. If it failed, the script can identify the specific recipe that failed from the logs and then trigger a new job that only rebuilds that single recipe.',
  },
  {
    id: 450,
    slug: 'benchmarking-performance-of-scheduled-flows',
    question: 'How to get started with benchmarking performance of scheduled flows?',
    answer: 'The "Monitoring" section of Dataiku provides a view of all job runs. You can filter by scenario and see the historical run times. A line chart of the run duration can quickly show if the performance of a scheduled flow is degrading over time.',
  },
  {
    id: 451,
    slug: 'defining-dashboarding-strategy-across-projects',
    question: 'How to get started with defining dashboarding strategy across projects?',
    answer: 'Create a strategy document in your central Wiki. It should define the types of dashboards to be used (e.g., Operational, Analytical, Strategic), the target audience for each, and the standard layout and branding to be applied for consistency.',
  },
  {
    id: 452,
    slug: 'building-reusable-dashboard-layouts-per-team',
    question: 'How to get started with building reusable dashboard layouts per team?',
    answer: 'Create a template dashboard in a shared project. This dashboard can have a standard header, layout grid, and placeholders for charts and KPIs. Teams can then duplicate this dashboard into their own projects to get a consistent starting point.',
  },
  {
    id: 453,
    slug: 'training-teams-to-use-metrics-and-checks-effectively',
    question: 'How to get started with training teams to use metrics & checks effectively?',
    answer: 'Hold a workshop. Show teams how to define metrics (e.g., row count, average value) and checks (e.g., value must be in range) on a dataset. Then, demonstrate how to add a "Run checks" step to a scenario to automate data quality validation.',
  },
  {
    id: 454,
    slug: 'maintaining-a-common-kpi-dictionary-in-dataiku',
    question: 'How to get started with maintaining a common KPI dictionary in Dataiku?',
    answer: 'Use a central project\'s Wiki to create a "KPI Dictionary". For each key business metric, document its definition, the formula for calculating it, and a link to the Dataiku dataset where it can be found. This ensures everyone is using the same definitions.',
  },
  {
    id: 455,
    slug: 'automating-dashboard-refreshes-using-scenarios',
    question: 'How to get started with automating dashboard refreshes using Scenarios?',
    answer: 'Create a scenario. Add a step to "Build" all the datasets that are used by the charts on your dashboard. Then, add a "Refresh dashboard caches" step. Schedule this scenario to run regularly to keep your dashboard data up-to-date.',
  },
  {
    id: 456,
    slug: 'comparing-chart-outputs-across-dataset-versions',
    question: 'How to get started with comparing chart outputs across dataset versions?',
    answer: 'If your dataset is versioned (e.g., using snapshots or partitions), you can create two identical charts on a dashboard. Configure one to point to the old version and one to the new version. This allows for a direct visual comparison.',
  },
  {
    id: 457,
    slug: 'embedding-audit-checks-within-dashboards',
    question: 'How to get started with embedding audit checks within dashboards?',
    answer: 'Create a dataset that contains the results of your data quality checks. Then, create a chart on this dataset (e.g., a bar chart showing the number of invalid rows per column) and add it to your main project dashboard. This makes data quality visible to all users.',
  },
  {
    id: 458,
    slug: 'reviewing-filters-and-access-control-on-dashboards',
    question: 'How to get started with reviewing filters and access control on dashboards?',
    answer: 'Dashboard permissions are inherited from the project. Ensure only the correct groups have access in the project settings. For filters, teach users how to add dashboard-level filters that can be used to slice the data across multiple charts simultaneously.',
  },
  {
    id: 459,
    slug: 'exporting-dashboards-for-business-teams-regularly',
    question: 'How to get started with exporting dashboards for business teams regularly?',
    answer: 'Create a scenario that builds the dashboard data. Add a "Reporter" to the scenario. Configure the reporter to export the dashboard as a PDF or images and email it to a list of stakeholders on a recurring schedule.',
  },
  {
    id: 460,
    slug: 'linking-dashboards-to-business-context-via-text-widgets',
    question: 'How to get started with linking dashboards to business context via text widgets?',
    answer: 'On a dashboard, use "Text" tiles to add titles, section headers, and explanatory paragraphs. This is crucial for providing context. A good dashboard is not just charts; it tells a story. Use the text tiles to guide the user through that story.',
  },
  {
    id: 461,
    slug: 'defining-permission-templates-for-various-user-roles',
    question: 'How to get started with defining permission templates for various user roles?',
    answer: 'In Administration > Security, create groups that correspond to your user roles (e.g., "Data Analyst," "Data Scientist," "Business User"). For each group, define a default set of permissions (e.g., "Business User" group has read-only access to specific projects).',
  },
  {
    id: 462,
    slug: 'setting-project-level-permissions-for-data-security',
    question: 'How to get started with setting project-level permissions for data security?',
    answer: 'In your project, go to Settings > Permissions. Add the user groups that need access. Assign them the minimum required permissions. For example, a business team might only need "Reader" access, while the development team needs "Contributor" or "Administrator" access.',
  },
  {
    id: 463,
    slug: 'implementing-tag-based-governance-across-datasets',
    question: 'How to get started with implementing tag-based governance across datasets?',
    answer: 'Establish a tagging taxonomy. For example, use tags for data sensitivity (`PII`, `Confidential`), data source (`Salesforce`, `SAP`), and status (`Raw`, `Validated`). Enforce the use of these tags on all datasets. This makes your data catalog searchable and governable.',
  },
  {
    id: 464,
    slug: 'aligning-recipes-with-enterprise-data-retention-policies',
    question: 'How to get started with aligning recipes with enterprise data retention policies?',
    answer: 'If your company has a policy to delete data after a certain period, you can implement this with a Dataiku scenario. Create a scenario with a Python step that identifies old partitions in your datasets and uses the API to delete them.',
  },
  {
    id: 465,
    slug: 'logging-user-access-events-and-reviewing-anomalies',
    question: 'How to get started with logging user access events and reviewing anomalies?',
    answer: 'A Dataiku administrator can access the instance-level audit logs. These logs record all significant events, including user logins and dataset access. These logs can be exported and analyzed to look for anomalous access patterns.',
  },
  {
    id: 466,
    slug: 'educating-juniors-on-ethical-data-use-policies',
    question: 'How to get started with educating juniors on ethical data use policies?',
    answer: 'During onboarding, hold a specific session on data ethics and privacy. Review your company\'s policies. Use real-world examples to illustrate the importance of handling sensitive data responsibly. Make this a core part of their training.',
  },
  {
    id: 467,
    slug: 'documenting-third-party-plugin-usage-and-approval',
    question: 'How to get started with documenting third-party plugin usage and approval?',
    answer: 'Maintain a central registry (e.g., in a Wiki) of all approved plugins. Before a new plugin can be installed, it should go through a review process to assess its functionality, security, and maintenance status. Document the approval for each plugin in the registry.',
  },
  {
    id: 468,
    slug: 'applying-data-masking-for-sensitive-columns',
    question: 'How to get started with applying data masking for sensitive columns?',
    answer: 'In a Prepare recipe, identify columns containing sensitive data (e.g., tagged as PII). Use a "Find and Replace" processor with a regular expression to mask parts of the data (e.g., `s/....$//` to remove the last 4 characters).',
  },
  {
    id: 469,
    slug: 'setting-alerts-when-governance-rules-are-violated',
    question: 'How to get started with setting alerts when governance rules are violated?',
    answer: 'Create a scenario that runs your data quality checks. If a check fails (which represents a governance rule violation), the scenario should fail. Configure a reporter on the scenario to send an alert to the data governance team.',
  },
  {
    id: 470,
    slug: 'coordinating-with-infosec-for-data-audit-reviews',
    question: 'How to get started with coordinating with InfoSec for data audit reviews?',
    answer: 'Provide the InfoSec team with read-only access to the relevant Dataiku projects. Walk them through the lineage graphs, which demonstrate data provenance. Use the audit logs to provide evidence of who has accessed the data.',
  },
  {
    id: 471,
    slug: 'planning-migration-from-alteryx-flows-into-dataiku',
    question: 'How to get started with planning migration from Alteryx flows into Dataiku?',
    answer: 'Begin with an inventory of all Alteryx workflows. Classify them by complexity and business criticality. Create a migration plan that prioritizes the most important and feasible workflows first. This is a manual translation project, not an automatic conversion.',
  },
  {
    id: 472,
    slug: 'reverse-engineering-legacy-sql-workflows',
    question: 'How to get started with reverse engineering legacy SQL workflows?',
    answer: 'Take the SQL code from the legacy system. In Dataiku, create a SQL recipe and paste in the code. You will need to replace the hardcoded table names with references to the corresponding Dataiku input datasets. Then, you can run the recipe and validate the output.',
  },
  {
    id: 473,
    slug: 'mapping-old-etl-steps-to-visual-recipes',
    question: 'How to get started with mapping old ETL steps to visual recipes?',
    answer: 'Create a "translation matrix." List the common tools or steps from your old ETL tool in one column and their Dataiku visual recipe equivalents in the other (e.g., Alteryx "Filter" -> Dataiku Prepare "Filter" processor). This cheat sheet helps developers translate the logic.',
  },
  {
    id: 474,
    slug: 'onboarding-source-systems-like-sap-or-mdms',
    question: 'How to get started with onboarding source systems like SAP or MDMs?',
    answer: 'These systems often expose data via a SQL database layer. Work with the system administrators to get a read-only database account. Then, set up a new connection in Dataiku to this database. You can then browse and import the necessary tables as datasets.',
  },
  {
    id: 475,
    slug: 'parallelizing-migration-across-teams',
    question: 'How to get started with parallelizing migration across teams?',
    answer: 'Divide the list of workflows to be migrated into logical groups based on business domain (e.g., Finance, Marketing). Assign each group to a different development team. Ensure all teams are using the same migration standards and templates.',
  },
  {
    id: 476,
    slug: 'validating-data-consistency-across-tools',
    question: 'How to get started with validating data consistency across tools?',
    answer: 'For a period, run both the old and new pipelines in parallel. In Dataiku, import the output from the legacy tool. Use a "Stack" recipe to combine it with the output from the new Dataiku flow. Then, use a "Group" recipe to compare row counts and key metrics to ensure they match.',
  },
  {
    id: 477,
    slug: 'using-apis-to-import-export-logic-programmatically',
    question: 'How to get started with using APIs to import/export logic programmatically?',
    answer: 'The Dataiku REST API allows you to programmatically interact with projects. You can write scripts to export the definition of a flow from one project (as JSON) and import it into another. This is useful for automating the replication of project structures.',
  },
  {
    id: 478,
    slug: 'bulk-replicating-pipelines-across-projects',
    question: 'How to get started with bulk replicating pipelines across projects?',
    answer: 'Create a "template" project that contains the standard pipeline structure you want to replicate. You can then use the "Duplicate project" feature to create new copies for different teams or use cases. The new projects can then be customized as needed.',
  },
  {
    id: 479,
    slug: 'identifying-deprecated-logic-during-migration',
    question: 'How to get started with identifying deprecated logic during migration?',
    answer: 'A migration is a perfect opportunity to clean up old logic. As you review each legacy workflow, question the purpose of every step. Work with business users to confirm if a particular transformation or output is still needed. If not, do not migrate it.',
  },
  {
    id: 480,
    slug: 'educating-teams-on-post-migration-optimization',
    question: 'How to get started with educating teams on post-migration optimization?',
    answer: 'After a workflow is migrated, the job is not done. Hold a session to show the team how to use Dataiku\'s features to optimize it further, such as changing the execution engine to Spark or adding partitioning to the datasets.',
  },
  {
    id: 481,
    slug: 'defining-reusable-flow-templates',
    question: 'How to get started with defining reusable flow templates?',
    answer: 'Create a Dataiku project that serves as a template. It should contain a standard Flow Zone structure (e.g., Ingestion, Preparation, Reporting), naming conventions, and template scenarios. When a new project is needed, duplicate this template project.',
  },
  {
    id: 482,
    slug: 'building-an-internal-cookbook-of-common-recipes',
    question: 'How to get started with building an internal cookbook of common recipes?',
    answer: 'Use a central project Wiki to create a "Cookbook". Add pages for common tasks, like "How to connect to Salesforce" or "How to parse a complex date format". Include screenshots and snippets of code or recipe steps.',
  },
  {
    id: 483,
    slug: 'mentoring-juniors-through-pair-programming',
    question: 'How to get started with mentoring juniors through pair programming?',
    answer: 'Schedule regular sessions where you and a junior developer share a screen and build a Dataiku flow together. Let them "drive" (do the clicking and typing) while you guide them, explain concepts, and review the work in real-time.',
  },
  {
    id: 484,
    slug: 'holding-code-walkthrough-sessions-weekly',
    question: 'How to get started with holding code walkthrough sessions weekly?',
    answer: 'Organize a weekly meeting where one team member presents a project or a complex recipe they have been working on. This "show and tell" format is a great way for the whole team to learn from each other\'s work and share best practices.',
  },
  {
    id: 485,
    slug: 'introducing-review-before-release-checklists',
    question: 'How to get started with introducing “review before release” checklists?',
    answer: 'Create a standard checklist for what needs to be reviewed before a project is deployed. This should include items like "All recipes have descriptions," "No hardcoded variables," and "Scenario reporters are configured." This ensures quality before going to production.',
  },
  {
    id: 486,
    slug: 'promoting-experimentation-via-sandbox-flows',
    question: 'How to get started with promoting experimentation via sandbox flows?',
    answer: 'Provide every developer with their own sandbox project where they have full admin rights. Encourage them to use this space to try new features, test complex ideas, and learn without any fear of breaking production workflows.',
  },
  {
    id: 487,
    slug: 'building-a-flow-style-guide-for-all-contributors',
    question: 'How to get started with building a flow style guide for all contributors?',
    answer: 'Document a style guide in the Wiki. It should cover naming conventions, the expected structure of Flow Zones, documentation standards, and code formatting rules. This consistency is key for maintainability in a multi-user environment.',
  },
  {
    id: 488,
    slug: 'sharing-best-practices-via-newsletters-or-wikis',
    question: 'How to get started with sharing best practices via newsletters or wikis?',
    answer: 'Create a "Dataiku Center of Excellence" space (e.g., on a shared Wiki or in a monthly newsletter). Regularly publish articles on best practices, new features, and successful projects. This helps to build a strong community of practice.',
  },
  {
    id: 489,
    slug: 'encouraging-feedback-driven-tool-usage-evolution',
    question: 'How to get started with encouraging feedback-driven tool usage evolution?',
    answer: 'Hold regular feedback sessions with your Dataiku users. Ask them what is working well and what their pain points are. Use this feedback to evolve your best practices, templates, and training materials.',
  },
  {
    id: 490,
    slug: 'showcasing-case-studies-across-teams',
    question: 'How to get started with showcasing case studies across teams?',
    answer: 'When a team completes a successful project, ask them to create a short presentation or a one-page summary. Share this case study widely. This not only recognizes the team\'s work but also inspires other teams with new ideas.',
  },
  {
    id: 491,
    slug: 'creating-a-maturity-model-for-dataiku-adoption',
    question: 'How to get started with creating a maturity model for Dataiku adoption?',
    answer: 'Define several levels of maturity, from "Beginner" (basic visual flows) to "Expert" (using advanced features like APIs, plugins, and containerized execution). For each level, define the skills and practices required. This gives teams a clear roadmap for improvement.',
  },
  {
    id: 492,
    slug: 'designing-dataops-pipelines-using-dataiku',
    question: 'How to get started with designing DataOps pipelines using Dataiku?',
    answer: 'DataOps in Dataiku means applying DevOps principles to data pipelines. This involves using Git for version control, creating scenarios for automated testing and data quality checks, and using project bundles for automated deployment between environments.',
  },
  {
    id: 493,
    slug: 'defining-cross-functional-success-kpis',
    question: 'How to get started with defining cross-functional success KPIs?',
    answer: 'Work with business and IT leaders to define what success with Dataiku looks like for the organization. KPIs could include "time to deliver new analytics projects," "number of users actively using the platform," and "cost savings from decommissioned legacy systems."',
  },
  {
    id: 494,
    slug: 'measuring-time-to-delivery-on-analytics-projects',
    question: 'How to get started with measuring time-to-delivery on analytics projects?',
    answer: 'For each new project request, log the start date. When the project is delivered to production, log the end date. Tracking this "time to value" is a powerful metric to demonstrate the efficiency gains from using a platform like Dataiku.',
  },
  {
    id: 495,
    slug: 'building-ai-ml-maturity-tracking-per-department',
    question: 'How to get started with building AI/ML maturity tracking per department?',
    answer: 'Create a dashboard that tracks AI/ML adoption across the company. It could include metrics like the number of models deployed per department, the business value generated by those models, and the number of people trained in machine learning.',
  },
  {
    id: 496,
    slug: 'integrating-dataiku-into-your-wider-tech-stack',
    question: 'How to get started with integrating Dataiku into your wider tech stack?',
    answer: 'Use the Dataiku REST API. This is the key to integration. Your other systems (like schedulers, CI/CD tools, or business applications) can call the API to trigger jobs, retrieve data, or get predictions. This makes Dataiku a component in your larger architecture.',
  },
  {
    id: 497,
    slug: 'defining-a-roadmap-for-feature-upgrades',
    question: 'How to get started with defining a roadmap for feature upgrades?',
    answer: 'Regularly review the Dataiku release notes. Identify new features that would be valuable for your organization. Create a roadmap for upgrading your Dataiku instance and a communication plan for rolling out the new features to your users.',
  },
  {
    id: 498,
    slug: 'running-retrospectives-on-dataiku-projects',
    question: 'How to get started with running retrospectives on Dataiku projects?',
    answer: 'After a project is complete, hold a retrospective meeting with the project team. Use a simple framework: "What went well?", "What didn\'t go well?", "What should we do differently next time?". Document the outcomes to ensure continuous improvement.',
  },
  {
    id: 499,
    slug: 'aligning-dataiku-workflows-to-business-outcomes',
    question: 'How to get started with aligning Dataiku workflows to business outcomes?',
    answer: 'For every project, clearly define the business outcome it supports in the project Wiki. For example, "This project aims to reduce customer churn by 5%." Then, build dashboards that track the KPIs related to that outcome. This ensures your work is always tied to business value.',
  },
  {
    id: 500,
    slug: 'scaling-dataiku-practices-across-global-teams',
    question: 'How to get started with scaling Dataiku practices across global teams?',
    answer: 'Create a central "Center of Excellence" (CoE). This team is responsible for defining global best practices, providing training, managing shared resources (like plugins and connections), and facilitating knowledge sharing between the different regional teams.',
  },
];

export const getQuestionBySlug = (slug: string): Question | undefined => {
  return questions.find(q => q.slug === slug);
}

export const getQuestionById = (id: number): Question | undefined => {
  return questions.find(q => q.id === id);
}
