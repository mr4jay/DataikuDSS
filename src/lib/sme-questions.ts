
export interface Question {
  id: number;
  slug: string;
  question: string;
  answer: string;
}

export const questions: Question[] = [
  {
    id: 401,
    slug: 'defining-onboarding-workflows-for-new-dataiku-users',
    question: 'How to get started with defining onboarding workflows for new Dataiku users?',
    answer: '### 1. Introduction/Overview\nDefining a structured onboarding workflow is crucial for enabling new users to become productive in Dataiku efficiently. A good onboarding plan blends self-service learning with guided, practical exercises.\n\n### 2. Prerequisites\n- A Dataiku instance with a "Sandbox" or "Training" project space.\n- A list of core competencies you expect a new user to learn in their first month.\n\n### 3. Step-by-Step Instructions\n1.  **Create a "Welcome" Project:** Build a dedicated Dataiku project that serves as the starting point for all new users.\n2.  **Develop a Checklist in the Wiki:** In the project\\\'s Wiki, create an "Onboarding Checklist" with links to Dataiku Academy courses, key documentation, and a sequence of hands-on tasks.\n3.  **Provide Sample Datasets:** Populate the project with clean, easy-to-understand sample datasets for users to practice with.\n4.  **Assign a Mentor:** Assign an experienced team member as a mentor to guide the new user and answer questions.\n\n### 4. Resources and Tools\n- **Dataiku Academy:** The "Core Designer" learning path is essential.\n- **A templated onboarding project:** To ensure a consistent experience.\n\n### 5. Next Steps and Progression\n- Have new users present a small project they built at the end of their first month to showcase their learning.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** New users feel overwhelmed.\n- **Solution:** Break down the onboarding into a week-by-week plan to make it more manageable.',
  },
  {
    id: 402,
    slug: 'assigning-datasets-and-flows-for-hands-on-learning',
    question: 'How to get started with assigning datasets and flows for hands-on learning?',
    answer: '### 1. Introduction/Overview\nPassive learning is ineffective. New users need hands-on exercises with real data and flows. The key is to provide a safe, structured environment for this practice.\n\n### 2. Prerequisites\n- A "sandbox" project for each new user.\n- A central project with read-only "golden" datasets.\n\n### 3. Step-by-Step Instructions\n1.  **Grant Sandbox Access:** Give the new user full admin rights on their own sandbox project.\n2.  **Provide Read-Only Access to Real Data:** Give them "Reader" access to a project containing well-documented, clean production datasets.\n3.  **Create a Task List:** In their sandbox project\\\'s Wiki, create a list of tasks, such as "Build a flow to join the `customers` and `orders` datasets and calculate the total sales per customer."\n\n### 4. Resources and Tools\n- **Sandbox Projects:** To provide a safe experimentation space.\n- **Read-only Shared Projects:** To provide access to realistic data.\n\n### 5. Next Steps and Progression\n- Gradually increase the complexity of the assigned tasks as their skills grow.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The user breaks something in their sandbox.\n- **Solution:** This is a good thing! A sandbox is for learning. Help them troubleshoot the issue. It\\\'s a valuable learning experience.',
  },
  {
    id: 403,
    slug: 'introducing-standard-naming-conventions-for-datasets-and-recipes',
    question: 'How to get started with introducing standard naming conventions for datasets and recipes?',
    answer: '### 1. Introduction/Overview\nConsistent naming is a cornerstone of maintainable projects. Establishing and enforcing a standard naming convention makes flows easier to read and understand for everyone on the team.\n\n### 2. Prerequisites\n- Agreement from the team on a standard.\n\n### 3. Step-by-Step Instructions\n1.  **Define the Convention:** Hold a team meeting to agree on a simple, clear convention. A good starting point is:\n    *   **Datasets:** \`SOURCE_CONTENT_STATUS\` (e.g., \`SFDC_LEADS_CLEANED\`).\n    *   **Recipes:** \`VERB_INPUT_OUTPUT\` (e.g., \`prepare_leads_for_scoring\`).\n2.  **Document the Standard:** Create a page in your central team Wiki that clearly documents this naming convention with examples.\n3.  **Enforce via Code Reviews:** The most effective way to enforce the standard is during peer reviews of projects or pull requests. A review checklist should include "Adherence to naming conventions."\n\n### 4. Resources and Tools\n- **A central Wiki:** To document the standards.\n- **Peer Reviews:** As the enforcement mechanism.\n\n### 5. Next Steps and Progression\n- Use a linter or a custom Dataiku check to automatically flag non-compliant names.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** People forget or don\\\'t follow the standard.\n- **Solution:** Consistent enforcement during reviews is key. If a developer knows their work won\\\'t be approved without following the standard, they will quickly adopt the habit.',
  },
  {
    id: 404,
    slug: 'monitoring-new-joiners-progress-across-the-flow',
    question: 'How to get started with monitoring new joiners’ progress across the Flow?',
    answer: '### 1. Introduction/Overview\nAs an SME or manager, you need to track the progress of new team members to provide support and ensure they are learning effectively. This involves a combination of informal check-ins and reviewing their work directly in Dataiku.\n\n### 2. Prerequisites\n- A mentorship or management relationship with the new joiner.\n- Access to their sandbox or development projects.\n\n### 3. Step-by-Step Instructions\n1.  **Daily Stand-ups:** Use the daily team stand-up for a quick status update. "What did you work on yesterday? What are you working on today? Are you blocked by anything?"\n2.  **Weekly 1-on-1s:** Schedule a dedicated weekly meeting to review their work. Have them share their screen and walk you through a flow they\\\'ve built.\n3.  **Use the Project Timeline:** In their project, the **Timeline** feature gives you a quick, high-level audit trail of their recent activity (e.g., "Jane created recipe X," "Jane modified dataset Y").\n\n### 4. Resources and Tools\n- **Regularly scheduled meetings.**\n- **Dataiku\\\'s Project Timeline feature.**\n\n### 5. Next Steps and Progression\n- Set clear goals for each week of their onboarding so you have something concrete to measure their progress against.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The new joiner is stuck but is afraid to ask for help.\n- **Solution:** You must create a psychologically safe environment. In your 1-on-1s, explicitly state that it\\\'s okay to be stuck and that asking questions is a sign of a good developer.',
  },
  {
    id: 405,
    slug: 'creating-checklist-based-training-modules-for-each-feature',
    question: 'How to get started with creating checklist-based training modules for each feature?',
    answer: '### 1. Introduction/Overview\nBreaking down the vast capabilities of Dataiku into small, manageable training modules with checklists helps new users learn in a structured way and allows you to track their skill acquisition.\n\n### 2. Prerequisites\n- A deep understanding of Dataiku\\\'s features.\n- A platform for hosting the training materials (e.g., a Wiki).\n\n### 3. Step-by-Step Instructions\n1.  **Identify Core Competencies:** List the key features a new user must learn (e.g., Prepare Recipe, Join Recipe, Scenarios, Dashboards).\n2.  **Create a Module for Each Competency:** In your Wiki, create a separate page for each module.\n3.  **Develop a Checklist for Each Module:** For each module, create a checklist of specific, demonstrable skills.\n    *   **Example (Join Recipe Module):**\n        *   \\[ ] Can explain the difference between a left and inner join.\n        *   \\[ ] Can successfully perform a join on a single key.\n        *   \\[ ] Can successfully perform a join on a composite key.\n4.  **Link to Resources:** For each checklist item, provide a link to the relevant Dataiku Academy course or documentation page.\n\n### 4. Resources and Tools\n- **A Wiki or other documentation platform.**\n- **Dataiku Academy and official documentation.**\n\n### 5. Next Steps and Progression\n- Have users work through the checklists during their onboarding and demonstrate the skills to their mentor.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** Creating the modules is a lot of work.\n- **Solution:** Start small. Create modules for the 5-10 most critical features first. You can build out the rest over time. You can also assign the creation of a new module to a team member who has recently mastered that feature.',
  },
  {
    id: 406,
    slug: 'enforcing-documentation-discipline-early-in-the-project',
    question: 'How to get started with enforcing documentation discipline early in the project?',
    answer: '### 1. Introduction/Overview\nGood documentation is a habit. Enforcing this discipline from the very beginning of a new team member\\\'s journey is the best way to ensure it becomes a natural part of their workflow.\n\n### 2. Prerequisites\n- A clear team standard for what needs to be documented.\n\n### 3. Step-by-Step Instructions\n1.  **Lead by Example:** As an SME, ensure all of your own work is impeccably documented. New joiners will follow the examples they see.\n2.  **Define Clear Expectations:** The standard should be simple: **every object in the Flow must have a clear, one-sentence description.**\n3.  **Make it Part of the "Definition of Done":** A task or user story is not complete until the associated objects are documented. This should be a formal part of your process.\n4.  **Enforce During Reviews:** During peer reviews, if a recipe is missing a description, the reviewer should leave a comment asking for it to be added. The work should not be approved until it is documented.\n\n### 4. Resources and Tools\n- **The "Description" field** on all Dataiku objects.\n- **A documented team standard** in your Wiki.\n- **Peer reviews.**\n\n### 5. Next Steps and Progression\n- Showcase examples of good documentation in team meetings to highlight its value.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** "It slows down development."\n- **Solution:** Frame it as an investment. The 30 seconds it takes to write a good description will save hours of confusion for the next person who has to work on that flow. It actually speeds up the team in the long run.',
  },
  {
    id: 407,
    slug: 'building-learning-flows-that-cover-prepare-join-window-recipes',
    question: 'How to get started with building learning flows that cover Prepare, Join, Window recipes?',
    answer: '### 1. Introduction/Overview\nA simple, end-to-end learning flow is a powerful tool for teaching new users the core concepts of Dataiku in a practical way.\n\n### 2. Prerequisites\n- Two simple, related CSV files (e.g., `customers.csv`, `orders.csv`).\n- A new Dataiku project.\n\n### 3. Step-by-Step Instructions\n1.  **Ingest Data:** Upload the two CSV files to create two datasets.\n2.  **First Step (Prepare):** Create a **Prepare** recipe on the `orders` dataset. Use it to parse the date column and filter out test orders.\n3.  **Second Step (Join):** Create a **Join** recipe. Join the output of your Prepare recipe with the `customers` dataset on the `customer_id` key.\n4.  **Third Step (Window):** Create a **Window** recipe on the joined data. Use it to calculate a running total of sales for each customer. Partition by `customer_id` and order by `date`.\n5.  **Document Each Step:** Add clear descriptions to each dataset and recipe explaining its purpose.\n\n### 4. Resources and Tools\n- **Visual Recipes:** Prepare, Join, Window.\n- **Sample Data:** Simple, clean data is best for a learning exercise.\n\n### 5. Next Steps and Progression\n- Have the new user try to rebuild this entire flow themselves after you have demonstrated it.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The user gets stuck on the Window recipe configuration.\n- **Solution:** The concepts of "Partition by" and "Order by" are the most common points of confusion. Use a visual diagram on a whiteboard to explain how the window frame moves over the partitioned data.',
  },
  {
    id: 408,
    slug: 'guiding-them-through-real-data-exploration-tasks',
    question: 'How to get started with guiding them through real data exploration tasks?',
    answer: '### 1. Introduction/Overview\nData exploration is a key skill. As an SME, you can guide new users by giving them open-ended tasks that encourage them to use Dataiku\\\'s visual analysis tools to explore a dataset and find insights.\n\n### 2. Prerequisites\n- An interesting, new dataset.\n- A Dataiku project.\n\n### 3. Step-by-Step Instructions\n1.  **Provide the Data:** Upload a new dataset to a project and share it with the user.\n2.  **Ask Open-Ended Questions:** Give them a set of business questions to answer, not a set of technical instructions. For example:\n    *   "Explore this customer dataset. What can you tell me about our customer base?"\n    *   "Are there any data quality issues you can find?"\n    *   "What is the relationship between customer age and total spending?"\n3.  **Point them to the Tools:** Guide them to use the **Statistics** and **Charts** tabs on the dataset to answer these questions.\n4.  **Review their Findings:** Have them present their findings to you, explaining the charts and statistics they used.\n\n### 4. Resources and Tools\n- **Dataset Statistics Tab:** For profiling columns.\n- **Dataset Charts Tab:** For visual exploration.\n\n### 5. Next Steps and Progression\n- Encourage them to create a full **Dashboard** to summarize the key insights they found during their exploration.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The user is not sure where to start.\n- **Solution:** Give them a more specific starting point. "Start by looking at the distribution of the `country` column. What do you notice?" This can help kickstart their exploration process.',
  },
  {
    id: 409,
    slug: 'auditing-how-juniors-use-variables-metrics-and-flags',
    question: 'How to get started with auditing how juniors use variables, metrics, and flags?',
    answer: '### 1. Introduction/Overview\nAs an SME, it\\\'s important to review a junior developer\\\'s work to ensure they are using platform features correctly. Auditing their use of variables, metrics, and other settings helps catch anti-patterns early and reinforces best practices.\n\n### 2. Prerequisites\n- Access to the junior developer\\\'s projects.\n- A checklist of best practices to review against.\n\n### 3. Step-by-Step Instructions\n1.  **Review Project Variables:**\n    *   Open their project and go to **... > Variables**. Are they parameterizing their project, or are there hardcoded values in their recipes?\n2.  **Review Metrics and Checks:**\n    *   Open their key datasets and go to the **Status** tab. Have they set up any metrics or data quality checks? This is a key sign of a mature developer.\n3.  **Review Scenario Flags:**\n    *   Open their scenarios. Are they using reporters for alerting? Are they using build modes like "Forced rebuild" correctly?\n4.  **Provide Feedback:** In a 1-on-1 session, walk them through your findings and explain the "why" behind the best practices.\n\n### 4. Resources and Tools\n- **The Project Itself:** Your primary source for the audit.\n- **A Review Checklist:** To ensure you are consistent in your audits.\n\n### 5. Next Steps and Progression\n- Turn the review checklist into a self-assessment tool that developers can use on their own work before submitting it for review.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The junior developer sees the review as criticism.\n- **Solution:** Frame the session as a collaborative learning opportunity, not a test. The goal is to help them improve and build higher-quality pipelines.',
  },
  {
    id: 410,
    slug: 'encouraging-best-practices-around-branching-and-git-sync',
    question: 'How to get started with encouraging best practices around branching and Git sync?',
    answer: '### 1. Introduction/Overview\nUsing Git correctly is essential for team collaboration. As an SME, you must establish and encourage best practices for branching, committing, and syncing to prevent conflicts and maintain a clean project history.\n\n### 2. Prerequisites\n- The project is integrated with a Git repository.\n- The team is familiar with basic Git concepts.\n\n### 3. Step-by-Step Instructions\n1.  **Define a Branching Strategy:** Document a simple branching strategy. A common one is "feature branching":\n    *   \`main\` branch is for production-ready code.\n    *   All new work must be done on a new branch created from \`main\` (e.g., \`feature/add-sales-report\`).\n2.  **Enforce Pull Requests (PRs):** In your Git provider (GitHub, etc.), protect the \`main\` branch and require all changes to be merged via a pull request. Require at least one reviewer to approve the PR.\n3.  **Train on Commit Messages:** Teach the team to write clear, descriptive commit messages. A good message explains *why* a change was made.\n\n### 4. Resources and Tools\n- **Git Branch Protection Rules:** A feature in GitHub/GitLab to enforce PRs.\n- **A documented branching strategy** in your team Wiki.\n\n### 5. Next Steps and Progression\n- Integrate your Git repository with a CI/CD tool to automatically run tests on every pull request.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** Developers complain that the PR process is slow.\n- **Solution:** The team must build a culture of reviewing PRs quickly. Keep PRs small and focused on a single change to make them easier and faster to review.',
  },
  {
    id: 411,
    slug: 'defining-recipe-selection-frameworks-prepare-vs-python',
    question: 'How to get started with defining recipe selection frameworks (e.g., when to prefer “Prepare” vs “Python”)?',
    answer: '### 1. Introduction/Overview\nOne of the most common questions from new developers is "which recipe should I use?". As an SME, you should provide a simple, clear framework to guide their decision, prioritizing clarity and performance.\n\n### 2. Prerequisites\n- An understanding of the capabilities of different recipe types.\n\n### 3. Step-by-Step Instructions\n1.  **Establish the "Visual First" Principle:** The default choice should always be a visual recipe.\n2.  **Create a Decision Tree:** Document a simple decision tree in your Wiki:\n    *   **Question 1: Can this transformation be done with the visual processors in a Prepare, Join, or Group recipe?**\n        *   **Yes:** Use the visual recipe. **This is the answer 90% of the time.**\n        *   **No:** Move to Question 2.\n    *   **Question 2: Does the logic require an external library or a complex algorithm not available visually?**\n        *   **Yes:** Use a **Python Recipe**.\n        *   **No:** Re-evaluate Question 1. Is there a creative way to use a visual recipe?\n3.  **Consider Performance:** Add a note about performance: "For large datasets in a database, a visual recipe set to \\\'Run on database\\\' will almost always be faster than a Python recipe."\n\n### 4. Resources and Tools\n- **A Wiki page** with the decision tree.\n\n### 5. Next Steps and Progression\n- During reviews, if you see a Python recipe that could have been a simple visual recipe, use it as a teaching moment to reinforce the framework.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A developer always defaults to Python because it\\\'s what they know.\n- **Solution:** Explain the benefits of visual recipes in Dataiku: they are easier for others to understand (self-documenting), and they can be pushed down to more powerful engines (SQL, Spark) for better performance.',
  },
  {
    id: 412,
    slug: 'standardizing-the-transformation-rules-in-prepare-steps',
    question: 'How to get started with standardizing the transformation rules in “Prepare” steps?',
    answer: '### 1. Introduction/Overview\nMany projects require the same basic data cleaning steps. Standardizing these rules into a reusable component saves time and ensures consistency.\n\n### 2. Prerequisites\n- A set of common cleaning steps that your team performs often (e.g., trimming whitespace, converting to lowercase, handling nulls).\n\n### 3. Step-by-Step Instructions\n1.  **Create a "Standard Cleaning" Recipe:** In a shared or template project, create a Prepare recipe that contains your standard set of cleaning steps.\n2.  **Copy the Steps:** In the Prepare recipe, select all the steps in the script panel on the left and click the "Copy" button.\n3.  **Share the Logic:** This copies the JSON definition of the steps to your clipboard. You can now:\n    *   Paste this JSON into a team Wiki page called "Standard Cleaning Logic".\n    *   Email the JSON to your team.\n4.  **Reuse the Logic:** When a developer needs to apply these standard steps, they can simply copy the JSON from the Wiki and paste it into their own Prepare recipe. All the steps will be added instantly.\n\n### 4. Resources and Tools\n- **Prepare Recipe\\\'s Copy/Paste feature.**\n- **A central Wiki** to store the shared logic.\n\n### 5. Next Steps and Progression\n- For more advanced use cases, a developer can create a custom Python processor that encapsulates the logic, making it available directly in the Prepare recipe\\\'s processor list.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The shared logic doesn\\\'t apply perfectly to all datasets.\n- **Solution:** The pasted steps are just a starting point. The developer can then modify or delete individual steps to suit the specific needs of their dataset.',
  },
  {
    id: 413,
    slug: 'setting-ground-rules-for-using-stack-vs-join-recipes',
    question: 'How to get started with setting ground rules for using Stack vs Join recipes?',
    answer: '### 1. Introduction/Overview\nNew users often confuse the Stack and Join recipes. Setting clear ground rules and providing simple visual explanations is key to preventing this common mistake.\n\n### 2. Prerequisites\n- An understanding of the difference between the two recipes.\n\n### 3. Step-by-Step Instructions\n1.  **Create a "Ground Rules" Wiki Page:** In your team\\\'s Wiki, create a page for recipe guidance.\n2.  **Define the Rule Simply:** Use clear, non-technical language.\n    *   "Use **Join** when you want to add new **columns** from another dataset." (Show a picture of two tables being combined side-by-side).\n    *   "Use **Stack** when you want to add new **rows** from another dataset." (Show a picture of two tables being combined one on top of the other).\n3.  **Provide a Use Case:**\n    *   **Join Use Case:** "Joining `orders` data with `customers` data to add the customer\\\'s name to each order."\n    *   **Stack Use Case:** "Stacking `sales_2022` data with `sales_2023` data to create a single historical dataset."\n\n### 4. Resources and Tools\n- **A Wiki page** with clear definitions and visual aids.\n\n### 5. Next Steps and Progression\n- During reviews, if you see a user misusing one of these recipes, refer them back to the Wiki page.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A user tries to join two datasets with no common key.\n- **Solution:** Explain that a join requires a "lookup key" that exists in both tables. This is a fundamental concept that needs to be understood.',
  },
  {
    id: 414,
    slug: 'using-pivot-and-unpivot-recipes-for-reporting-logic',
    question: 'How to get started with using Pivot and Unpivot recipes for reporting logic?',
    answer: '### 1. Introduction/Overview\nPivoting and unpivoting are common data reshaping tasks needed for reporting. As an SME, you should guide developers on when and how to use Dataiku\\\'s dedicated visual recipes for these tasks.\n\n### 2. Prerequisites\n- A dataset that needs reshaping.\n- An understanding of "long" vs. "wide" data formats.\n\n### 3. Step-by-Step Instructions\n1.  **Explain the Use Cases:**\n    *   **Pivot:** Use the Pivot recipe to transform "long" data into "wide" data. This is common when you want to create summary tables for reports, where each category has its own column.\n    *   **Unpivot:** Use the Unpivot recipe for the reverse. This is useful when you get data from a source like Excel where it is already in a "pivoted" or "crosstab" format, and you need to normalize it into a long format for easier analysis.\n2.  **Provide a Clear Example:**\n    *   In a training project, provide a simple `sales_data_long` dataset (`Date`, `Category`, `Sales`).\n    *   Walk through using the **Pivot** recipe to turn it into `sales_data_wide` (`Date`, `Sales_Category_A`, `Sales_Category_B`).\n    *   Then, show how the **Unpivot** recipe can transform it back.\n\n### 4. Resources and Tools\n- **Pivot and Unpivot recipes.**\n- **A sample project** demonstrating the transformation.\n\n### 5. Next Steps and Progression\n- Discuss the performance implications. Pivoting can create very wide tables, so it\\\'s important to only pivot on columns with a limited number of unique values.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** The user\\\'s pivot recipe is creating hundreds of columns.\n- **Solution:** This means the column they are pivoting on has too many unique values (high cardinality). They should first use a Prepare recipe to group the values or clean them up before pivoting.',
  },
  {
    id: 415,
    slug: 'preempting-edge-cases-in-window-and-top-n-recipes',
    question: 'How to get started with preempting edge cases in Window and Top-N recipes?',
    answer: '### 1. Introduction/Overview\nThe Window and Top-N recipes are powerful, but they have settings that can lead to unexpected results if not understood. As an SME, you should proactively teach developers about these edge cases.\n\n### 2. Prerequisites\n- A deep understanding of how these recipes work.\n\n### 3. Step-by-Step Instructions\n1.  **For the Window Recipe:**\n    *   **Emphasize Ordering:** Stress that a Window function is meaningless without a defined **Order**. This is the most common mistake.\n    *   **Explain Partitioning:** Clearly explain that partitioning restarts the calculation for each group.\n    *   **Visualize the Frame:** Draw a diagram on a whiteboard showing how the "window frame" (e.g., "3 preceding rows") moves across the data.\n2.  **For the Top-N Recipe:**\n    *   **Focus on Ties:** The biggest edge case is how the recipe handles ties in the ranking column. Explain the difference between the "dense" ranking strategy and the sequential one.\n    *   **Explain "Retrieve all" vs. "Select N":** Clarify that "Retrieve all rows" can result in more than N rows if there are ties at the Nth position.\n\n### 4. Resources and Tools\n- **Whiteboards or diagramming tools** for visual explanations.\n- **Sample datasets** that specifically include ties and multiple partitions.\n\n### 5. Next Steps and Progression\n- Create a small quiz or a set of challenge questions to test the developers\\\' understanding of these edge cases.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A user says their running total from a Window recipe is wrong.\n- **Solution:** Ask them one question: "Did you set the ordering?". 99% of the time, they have forgotten to add an "Order by" clause to the recipe.',
  },
  {
    id: 416,
    slug: 'resolving-issues-when-visual-recipes-get-too-complex',
    question: 'How to get started with resolving issues when visual recipes get too complex?',
    answer: '### 1. Introduction/Overview\nWhile it\\\'s possible to create a Prepare recipe with hundreds of steps, this can become difficult to manage and debug. As an SME, you should guide developers on how to refactor complex visual logic for better maintainability.\n\n### 2. Prerequisites\n- A project with a very long and complex visual recipe.\n\n### 3. Step-by-Step Instructions\n1.  **The "Single Responsibility" Principle:** Teach the principle that each recipe should have one clear, single purpose.\n2.  **Identify Logical Blocks:** Open the long Prepare recipe. Look for logical groups of steps in the script (e.g., a set of steps for address cleaning, another set for date calculations).\n3.  **Refactor into Multiple Recipes:**\n    *   Cut the steps for the first logical block (e.g., address cleaning) from the main recipe.\n    *   Create a new, intermediate Prepare recipe that performs only these steps.\n    *   Chain the original recipe to the output of this new, smaller recipe.\n    *   Repeat this process, breaking the monolithic recipe down into a chain of smaller, single-purpose recipes. The Flow will be longer, but each step will be simpler and easier to understand.\n\n### 4. Resources and Tools\n- **The Prepare Recipe\\\'s copy/paste feature** makes it easy to move steps between recipes.\n- **Flow Zones:** Organize the new, longer chain of recipes into a Flow Zone to keep the overall Flow clean.\n\n### 5. Next Steps and Progression\n- For very complex or repetitive logic, consider creating a reusable Python function in the project library or even a custom plugin.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** "But now my Flow has so many recipes!"\n- **Solution:** Explain that this is a good thing. It makes the pipeline more modular and easier to debug. If a step fails, you know exactly which small part of the logic has the problem. Use Flow Zones to manage the visual complexity.',
  },
  {
    id: 417,
    slug: 'validating-recipe-chains-for-schema-consistency',
    question: 'How to get started with validating recipe chains for schema consistency?',
    answer: '### 1. Introduction/Overview\nIn a long chain of recipes, an upstream change can unexpectedly break a downstream recipe if it alters the schema (e.g., by changing a column name or type). As an SME, you should teach developers how to manage and validate schema consistency.\n\n### 2. Prerequisites\n- A Dataiku Flow with a multi-step recipe chain.\n\n### 3. Step-by-Step Instructions\n1.  **Proactive Check: "Propagate Schema Changes":**\n    *   When you make a change in an upstream recipe (e.g., renaming a column in a Prepare recipe), Dataiku is often aware of it.\n    *   Teach developers to look for and use the **Propagate schema changes** button, which can automatically update the configurations of downstream recipes.\n2.  **Reactive Check: The Job Log:**\n    *   If a job fails, the most common error is a schema error like "Column \\\'X\\\' not found."\n    *   Teach developers to read this error and trace it back. Open the failing recipe and look at its input dataset. Does the column exist? If not, the problem is in the upstream recipe that produced it.\n3.  **Best Practice: The "Analyze" Tool:**\n    *   Before building a new recipe on an existing dataset, encourage developers to quickly open the dataset and use the **Analyze** feature on a few key columns to check their name, type, and distribution. This can catch schema issues before a job is even run.\n\n### 4. Resources and Tools\n- **The "Propagate schema changes" feature.**\n- **The Job Log.**\n- **The Analyze feature** on datasets.\n\n### 5. Next Steps and Progression\n- Add automated schema checks to your pipeline. You can use a Python scenario step with the Dataiku API to get a dataset\\\'s schema and verify that it contains a specific set of required columns before running the main pipeline.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A downstream Join recipe is failing.\n- **Solution:** The most likely cause is that a column used in the join key was renamed or removed in an upstream Prepare recipe. The developer needs to update the Join recipe to use the new, correct key column.',
  },
  {
    id: 418,
    slug: 'enforcing-test-datasets-for-each-visual-recipe-module',
    question: 'How to get started with enforcing test datasets for each visual recipe module?',
    answer: '### 1. Introduction/Overview\nUnit testing is not just for code. You can apply the same principle to visual recipes by creating small, dedicated test datasets that contain known edge cases. This ensures your visual logic is robust.\n\n### 2. Prerequisites\n- A visual recipe that performs a critical transformation.\n\n### 3. Step-by-Step Instructions\n1.  **Create a Test Dataset:**\n    *   Manually create a small CSV file or an inline dataset that contains sample data specifically designed to test your recipe.\n    *   Include rows that represent edge cases: null values, zeros, negative numbers, strings with trailing spaces, etc.\n2.  **Create a Test Branch of the Flow:**\n    *   Duplicate your main recipe.\n    *   Change the input of this duplicated recipe to be your new test dataset. Let\\\'s call this recipe \`test_prepare_customers\`.\n3.  **Define the Expected Output:** Manually create another small dataset that represents the *correct output* you expect after the recipe has processed the test input.\n4.  **Automate the Test:**\n    *   Create a **Scenario** called "Run Unit Tests".\n    *   In the scenario, add a step to build your \`test_prepare_customers\` recipe.\n    *   Add a second step that uses a **Sync/Compare** recipe or a Python recipe to compare the actual output with the expected output. If they do not match, the step should fail, which will fail the scenario.\n\n### 4. Resources and Tools\n- **Inline Datasets:** For creating small test datasets.\n- **The Duplicate feature:** To create a test version of your recipe.\n- **Scenarios:** To automate the execution of the test.\n\n### 5. Next Steps and Progression\n- Integrate this "Run Unit Tests" scenario into your CI/CD pipeline. No code should be merged unless all the visual recipe unit tests pass.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** "This seems like a lot of setup."\n- **Solution:** It is. You don\\\'t need to do this for every single recipe. Reserve this practice for your most critical, complex, and shared visual recipes where an error would have a significant impact.',
  },
  {
    id: 419,
    slug: 'mentoring-on-when-to-add-intermediate-sync-recipes',
    question: 'How to get started with mentoring on when to add intermediate Sync recipes?',
    answer: '### 1. Introduction/Overview\nThe **Sync** recipe\\\'s main purpose is to move data from one storage location to another. As an SME, you should guide junior developers on the two primary strategic reasons to add a Sync recipe into a flow.\n\n### 2. Prerequisites\n- An understanding of Dataiku\\\'s different storage connections (e.g., filesystem, S3, Snowflake).\n\n### 3. Step-by-Step Instructions\n\n#### Use Case 1: Changing the Storage Engine\n- **When:** You have a series of recipes running on one engine (e.g., in-memory on the filesystem) but the next step needs to be on a different engine (e.g., a SQL database).\n- **Example:**\n    1. You upload a CSV file and use a Prepare recipe to clean it (this runs in-memory).\n    2. You need to join this cleaned data with a very large table in your Snowflake data warehouse.\n- **Solution:** Before the Join recipe, add a **Sync** recipe. Its input is the cleaned CSV data. For its output, change the connection to your Snowflake connection. This will create a new table in Snowflake. Now, both inputs to your Join recipe are in Snowflake, and you can run the join with the highly performant "Run on database" engine.\n\n#### Use Case 2: Creating a Major Checkpoint\n- **When:** You have a very long and complex data preparation flow. You want to "finalize" the result of this preparation into a stable, "golden" dataset before you start using it for multiple downstream purposes (like modeling and reporting).\n- **Solution:** After the last Prepare recipe in your preparation chain, add a **Sync** recipe. This creates a clean, materialized copy of your prepared data. It signals to other developers that this is a stable, trusted checkpoint in the flow.\n\n### 4. Resources and Tools\n- **The Sync Recipe.**\n- **Multiple data connections.**\n\n### 5. Next Steps and Progression\n- Explain how the Sync recipe is also used to move data between different environments (e.g., from a dev database to a prod database) during deployments.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** A user says, "Why can\\\'t I just use a Prepare recipe and change the output connection?"\n- **Solution:** Explain that other visual recipes (like Prepare) have their engine determined by their *input*. A Sync recipe is special because its primary purpose is to let you explicitly control the output connection, making it the correct tool for moving data between systems.',
  },
  {
    id: 420,
    slug: 'embedding-qa-tags-within-recipe-descriptions',
    question: 'How to get started with embedding QA tags within recipe descriptions?',
    answer: '### 1. Introduction/Overview\nIn a busy project, it can be hard to know the quality assurance (QA) status of a specific recipe. A lightweight but effective solution is to embed simple QA status tags directly in the recipe\\\'s description, making the status visible at a glance in the Flow.\n\n### 2. Prerequisites\n- A team agreement on a simple set of QA tags.\n\n### 3. Step-by-Step Instructions\n1.  **Define Your QA Tags:** Agree on a simple set of tags. For example:\n    *   `[QA: Not Started]`\n    *   `[QA: In Progress]`\n    *   `[QA: Ready for Review]`\n    *   `[QA: Approved]`\n2.  **Use in Descriptions:**\n    *   When a developer is working on a recipe, they should add the appropriate tag to the beginning of the recipe\\\'s **Description** field.\n    *   **Example Description:** `[QA: Ready for Review] This recipe joins customer and sales data.`\n3.  **Update During the Workflow:**\n    *   As the recipe moves through your QA process, the developer or reviewer updates the tag.\n    *   For example, after a peer review is complete, the reviewer changes the tag to `[QA: Approved]`.\n\n### 4. Resources and Tools\n- **The Description field** on recipes.\n\n### 5. Next Steps and Progression\n- You can use the Dataiku API to write a scenario that scans all recipes in a project and creates a report of the current QA status of each, providing a simple QA dashboard.\n\n### 6. Common Challenges and Solutions\n- **Challenge:** People forget to update the tags.\n- **Solution:** This must be part of your team\\\'s process. For example, a pull request should not be merged until the reviewer has updated the tag on the relevant recipes to `[QA: Approved]`.',
  }
];

export const getQuestionBySlug = (slug: string): Question | undefined => {
  return questions.find(q => q.slug === slug);
}

export const getQuestionById = (id: number): Question | undefined => {
  return questions.find(q => q.slug === id);
}
