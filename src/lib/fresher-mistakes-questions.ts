
export interface Question {
  id: number;
  slug: string;
  question: string;
  answer: string;
}

export const questions: Question[] = [
  {
    id: 301,
    slug: 'when-to-use-prepare-vs-python-recipe',
    question: 'How to get started with understanding when to use a Prepare recipe vs. a Python recipe?',
    answer: 'Use a Prepare recipe for common, structured data cleaning tasks like handling nulls, parsing dates, or simple formula-based transformations. Switch to a Python recipe when you need complex custom logic, integrations with external libraries (like APIs), or transformations that are too complex for visual processors.',
  },
  {
    id: 302,
    slug: 'parsing-dates-when-dataiku-misdetects-format',
    question: 'How to get started with parsing dates when Dataiku misdetects format?',
    answer: 'In a Prepare recipe, use the "Parse date" processor. If Dataiku auto-detection fails, manually specify the date format that matches your data (e.g., `yyyy-MM-dd HH:mm:ss`). You can find a list of format codes in the documentation.',
  },
  {
    id: 303,
    slug: 'handling-nulls-when-find-replace-fails',
    question: 'How to get started with handling nulls when the Find & Replace processor fails?',
    answer: 'The "Find & Replace" processor works on string values, but nulls are a special state. To handle nulls, use dedicated processors like "Impute missing values" (to fill with mean, median, etc.) or "Clear cells with invalid values" and select only "null" values.',
  },
  {
    id: 304,
    slug: 'splitting-multi-value-columns',
    question: 'How to get started with splitting multi-value columns using Split recipe?',
    answer: 'In a Prepare recipe, use the "Split column" processor. Select the column containing your multi-value string and specify the delimiter (e.g., a comma or semicolon). This will create multiple new columns, each containing one of the split values.',
  },
  {
    id: 305,
    slug: 'configuring-fuzzy-join-when-exact-match-fails',
    question: 'How to get started with configuring fuzzy join when exact match fails?',
    answer: 'Use the "Fuzzy Join" visual recipe. Select your two datasets and the columns to join on. In the settings, you can choose the similarity algorithm (e.g., Levenshtein distance) and set a threshold to control how "fuzzy" the match should be.',
  },
  {
    id: 306,
    slug: 'grouping-by-multiple-keys-correctly',
    question: 'How to get started with grouping by multiple keys correctly in Group recipe?',
    answer: 'In the "Group" recipe, simply add all the columns you want to group by in the "Group by" section. Then, define your aggregations (like sum, count, average) on the other columns. The output will be aggregated for each unique combination of the group keys.',
  },
  {
    id: 307,
    slug: 'getting-unexpected-results-in-window-recipe',
    question: 'How to get started with getting unexpected results in a Window recipe frame?',
    answer: 'The most common issue is the window frame definition. Ensure your data is correctly ordered using the "Order by" setting. Then, carefully define your window frame (e.g., "preceding" and "following" rows) to control which rows are included in the calculation for each record.',
  },
  {
    id: 308,
    slug: 'interpreting-top-n-recipe-with-duplicates',
    question: 'How to get started with interpreting Top N recipe when ranking duplicates?',
    answer: 'The "Top N" recipe has settings for handling ties. You can choose to include all tied records (which might result in more than N rows) or use a tie-breaking strategy, such as picking the first one based on another sorting column.',
  },
  {
    id: 309,
    slug: 'stacking-datasets-and-preserving-schema',
    question: 'How to get started with stacking datasets and preserving schema with Stack recipe?',
    answer: 'In the "Stack" recipe, Dataiku will try to match columns by name. If your column names are inconsistent across datasets, use the "Schema" tab in the recipe to manually map columns to ensure they are stacked correctly.',
  },
  {
    id: 310,
    slug: 'syncing-two-datasets-with-sync-recipe',
    question: 'How to get started with syncing two datasets with Sync recipe and matching granular keys?',
    answer: 'The "Sync" recipe is for synchronizing the content of one dataset to another, not for joining. It is often used to copy a dataset to a different connection. If you need to join datasets, use the "Join" recipe instead.',
  },
  {
    id: 311,
    slug: 'reordering-columns-and-maintaining-order',
    question: 'How to get started with reordering columns in Sort recipe and maintaining order?',
    answer: 'The "Sort" recipe sorts rows, not columns. To reorder columns, use a "Prepare" recipe. In the column list, you can drag and drop columns to your desired order. This order will be preserved in the output dataset.',
  },
  {
    id: 312,
    slug: 'applying-pivot-recipe-and-preventing-missing-columns',
    question: 'How to get started with applying Pivot recipe and preventing missing columns?',
    answer: 'In the "Pivot" recipe, if a category from your pivot column is not present in the data, it will not create a column for it. To ensure all expected columns are present, you may need to first join your data with a "scaffold" dataset that contains all possible categories.',
  },
  {
    id: 313,
    slug: 'filtering-vs-sampling-confusion',
    question: 'How to get started with filtering vs sampling confusion in Sample/Filter recipe?',
    answer: 'Use "Filter" to keep or remove rows based on a specific condition (e.g., `country = \'USA\'`). Use "Sample" to select a random subset of your data (e.g., 10% of all rows). Filtering is deterministic; sampling is random.',
  },
  {
    id: 314,
    slug: 'deduplicating-rows-with-distinct-recipe',
    question: 'How to get started with deduplicating rows using Distinct recipe but missing duplicates?',
    answer: 'The "Distinct" recipe removes rows that are entirely identical. If you want to deduplicate based on a specific key (e.g., customer ID), use the "Group" recipe. Group by the key and for other columns, use a "First" or "Last" aggregation to pick one record per key.',
  },
  {
    id: 315,
    slug: 'chaining-multiple-visual-recipes',
    question: 'How to get started with chaining multiple visual recipes while maintaining lineage?',
    answer: 'This is the core of Dataiku. The output dataset of one recipe becomes the input for the next. Simply select the output dataset and choose a new recipe from the right panel. Dataiku automatically connects them, building the visual lineage in your Flow.',
  },
  {
    id: 316,
    slug: 'setting-up-a-python-recipe-environment',
    question: 'How to get started with setting up a Python recipe environment (libraries, dependencies)?',
    answer: 'Go to Administration > Code Envs. Create a new Python environment. In the "Packages to install" section, add the libraries you need (e.g., `pandas`, `requests`). Then, in your Python recipe settings, select this environment.',
  },
  {
    id: 317,
    slug: 'importing-pandas-in-python-recipes',
    question: 'How to get started with importing pandas in Python recipes without runtime errors?',
    answer: 'Pandas is included by default in Dataiku\'s built-in Python environments. If you get an error, it usually means your recipe is using a code environment where pandas is not installed. Check your recipe\'s settings and the code environment\'s package list.',
  },
  {
    id: 318,
    slug: 'debugging-python-recipes',
    question: 'How to get started with debugging Python recipes when exceptions occur in execution?',
    answer: 'When a Python recipe fails, click on the job and view the "Log". It will show the full Python traceback, including the line number and error message. You can also add `print()` statements to your code to log variable values.',
  },
  {
    id: 319,
    slug: 'accessing-dataiku-datasets-via-api',
    question: 'How to get started with accessing Dataiku datasets via DSS API in code recipes?',
    answer: 'Use the `dataiku` library. To read an input dataset, use `input_dataset = dataiku.Dataset("your_input_name")`. To get it as a Pandas DataFrame, use `df = input_dataset.get_dataframe()`.',
  },
  {
    id: 320,
    slug: 'writing-sql-recipes-with-special-chars-in-names',
    question: 'How to get started with writing SQL recipes when field names contain spaces or special chars?',
    answer: 'You need to quote the column names using the specific quoting character for your database (e.g., double quotes `"` for PostgreSQL and Snowflake, or square brackets `[]` for SQL Server). For example: `SELECT "My Column" FROM ...`',
  },
  {
    id: 321,
    slug: 'handling-large-data-in-memory-errors',
    question: 'How to get started with handling large data in-memory causing out-of-memory errors?',
    answer: 'Do not use the in-memory engine (like pandas in a Python recipe) for very large datasets. Instead, use a SQL recipe or a Spark recipe to push the computation to a distributed engine. This processes the data where it lives, without pulling it into Dataiku\'s memory.',
  },
  {
    id: 322,
    slug: 'correcting-python-syntax-errors',
    question: 'How to get started with correcting indentation or syntax errors in Python code?',
    answer: 'The Python recipe editor has basic syntax highlighting. For better checks, use the "Validate" button, which will run a syntax check. For complex code, consider using the VS Code integration to edit your Python code with a full-featured IDE.',
  },
  {
    id: 323,
    slug: 'integrating-scikit-learn-pipelines-in-recipes',
    question: 'How to get started with integrating scikit‑learn pipelines in Python recipes?',
    answer: 'In your Python recipe, ensure the code environment has scikit-learn. You can then write standard scikit-learn code. Use the Dataiku API to load your data into a pandas DataFrame, apply your scikit-learn `Pipeline` to it, and write the results back to an output dataset.',
  },
  {
    id: 324,
    slug: 'connecting-to-external-apis-securely',
    question: 'How to get started with connecting to external APIs securely in code recipes?',
    answer: 'Do not hardcode API keys in your code. Store them as project variables. In your project, go to Variables > Edit. Add a new variable, mark it as a "Password" type to hide its value. Then access it in your Python recipe with `dataiku.get_custom_variables()["your_api_key_variable"]`.',
  },
  {
    id: 325,
    slug: 'parameterizing-code-recipes-with-variables',
    question: 'How to get started with parameterizing code recipes using project variables?',
    answer: 'Define variables in your project\'s "Variables" section. In your Python recipe, access them using `dataiku.get_custom_variables()`. This returns a dictionary of all variables. You can then use these variables to control your code\'s logic.',
  },
  {
    id: 326,
    slug: 'committing-code-recipes-to-git',
    question: 'How to get started with committing code recipes to Git and handling diff conflicts?',
    answer: 'If your project is connected to Git, any change to a code recipe can be committed. Go to the Git page in your project, stage the changed recipe, write a commit message, and push. If there are conflicts, Dataiku provides a visual diff tool to help you resolve them.',
  },
  {
    id: 327,
    slug: 'using-r-recipes-with-missing-packages',
    question: 'How to get started with using R recipes when R packages are not installed?',
    answer: 'Similar to Python, you need an R code environment. Go to Administration > Code Envs, create an R environment, and add the required packages (e.g., `dplyr`, `ggplot2`). Then, select this environment in your R recipe\'s settings.',
  },
  {
    id: 328,
    slug: 'breaking-long-sql-queries',
    question: 'How to get started with breaking long SQL queries by proper line continuation?',
    answer: 'SQL is flexible with whitespace. You can break your query into multiple lines for readability. It is good practice to put each clause (SELECT, FROM, WHERE, GROUP BY) on a new line. This does not affect the query\'s execution.',
  },
  {
    id: 329,
    slug: 'testing-code-logic-in-notebooks',
    question: 'How to get started with testing code logic locally using notebooks before recipe run?',
    answer: 'Notebooks are great for experimentation. Create a notebook, load a sample of your dataset, and test your Python code in the cells. Once you are confident the logic is correct, you can copy the code into a Python recipe to make it part of your automated flow.',
  },
  {
    id: 330,
    slug: 'capturing-execution-logs-from-code-recipes',
    question: 'How to get started with capturing execution logs from code recipes for troubleshooting?',
    answer: 'Use the standard Python `logging` module or simple `print()` statements in your code recipe. The output of these will appear in the job log, which is essential for debugging and understanding what your code is doing during execution.',
  },
  {
    id: 331,
    slug: 'organizing-datasets-into-flow-zones',
    question: 'How to get started with organizing datasets into Flow Zones?',
    answer: 'In your Flow, right-click on the canvas and select "Create Flow Zone". Give it a logical name (e.g., "Data Ingestion"). You can then select multiple datasets and recipes and drag them into the zone. This is key for keeping large flows organized.',
  },
  {
    id: 332,
    slug: 'renaming-datasets-without-breaking-recipes',
    question: 'How to get started with renaming datasets without breaking recipes?',
    answer: 'When you rename a dataset in the Flow, Dataiku will automatically update all the recipes that use it as an input or output. You do not need to manually change the recipe settings.',
  },
  {
    id: 333,
    slug: 'visual-clutter-in-flow',
    question: 'How to get started with visual clutter in Flow and grouping logical zones?',
    answer: 'Use Flow Zones to group related items. You can collapse a Flow Zone to hide its contents, which dramatically simplifies the visual layout of a complex project. Use clear naming conventions for your zones.',
  },
  {
    id: 334,
    slug: 'troubleshooting-broken-links-from-deleted-datasets',
    question: 'How to get started with troubleshooting broken links due to deleted datasets?',
    answer: 'If a recipe\'s input dataset is missing, it will be shown with a red outline. You need to either recreate the missing dataset or edit the recipe to connect it to a different input dataset.',
  },
  {
    id: 335,
    slug: 'viewing-dataset-lineage',
    question: 'How to get started with viewing dataset lineage and recipe dependencies clearly?',
    answer: 'The Flow itself is a lineage diagram. To see the lineage for a specific dataset, right-click on it and select "View upstream dependencies" or "View downstream dependencies". For more detail, open the dataset and go to the "Lineage" tab for a column-level view.',
  },
  {
    id: 336,
    slug: 'refreshing-schema-metadata',
    question: 'How to get started with refreshing schema metadata when source changes?',
    answer: 'If the schema of your source data changes (e.g., a new column is added), open the dataset in Dataiku. In the "Settings" tab, you can often find a "Refresh" or "Check" button that will re-read the source schema and update Dataiku\'s metadata.',
  },
  {
    id: 337,
    slug: 'tracking-downstream-effects',
    question: 'How to get started with tracking downstream effects when modifying upstream logic?',
    answer: 'Before changing an upstream recipe, use the Flow to see all of its downstream dependencies. After making the change, you will need to rebuild all the downstream datasets and models to propagate the change through your pipeline.',
  },
  {
    id: 338,
    slug: 'resolving-circular-dependencies',
    question: 'How to get started with resolving circular dependencies in Flow?',
    answer: 'Dataiku flows must be Directed Acyclic Graphs (DAGs), meaning you cannot have loops. If you create a situation where recipe A builds dataset B, and recipe C builds dataset A from dataset B, you have a circular dependency. You need to redesign your logic to break the loop.',
  },
  {
    id: 339,
    slug: 'archiving-stale-datasets-safely',
    question: 'How to get started with archiving stale datasets safely without losing lineage?',
    answer: 'Before deleting a dataset, ensure it has no downstream dependencies. If you want to archive it, you can export the project as a bundle. This saves the entire project state, including the flow and lineage, which you can restore later if needed.',
  },
  {
    id: 340,
    slug: 'multi-user-collaboration-flow-conflicts',
    question: 'How to get started with multi-user collaboration causing Flow conflicts?',
    answer: 'Use the Git integration for your project. This allows multiple users to work on different branches. When merging branches, Dataiku provides a visual diff tool to help you resolve any conflicts that arise when two users have modified the same object.',
  },
  {
    id: 341,
    slug: 'building-a-basic-scenario',
    question: 'How to get started with building a basic Scenario to schedule recipe execution?',
    answer: 'Go to the "Scenarios" page of your project. Create a new scenario. In the "Steps" tab, add a "Build / Train" step. Select the final dataset you want to build. In the "Settings" tab, add a "Time-based" trigger to schedule it.',
  },
  {
    id: 342,
    slug: 'setting-triggers-based-on-dataset-update',
    question: 'How to get started with setting triggers based on dataset update timing?',
    answer: 'In a scenario\'s "Settings" tab, you can add a "Dataset change" trigger. Select a dataset, and the scenario will automatically run whenever that dataset is updated. This is useful for event-driven pipelines.',
  },
  {
    id: 343,
    slug: 'configuring-retries-on-failure',
    question: 'How to get started with configuring retries on failure without infinite loops?',
    answer: 'This requires a Python step in your scenario. The Python script can check the outcome of a previous job step. If it failed, the script can re-run the job. Be sure to include a counter in your script to limit the number of retries and avoid an infinite loop.',
  },
  {
    id: 344,
    slug: 'handling-global-variables-in-scenarios',
    question: 'How to get started with handling global variables within Scenarios effectively?',
    answer: 'You can override project variables for a specific scenario run. In the scenario, you can define run-specific values for your variables. This is useful for running the same flow with different parameters (e.g., for different dates or regions).',
  },
  {
    id: 345,
    slug: 'setting-up-email-alerts',
    question: 'How to get started with setting up email alerts when a step fails?',
    answer: 'In your scenario, go to the "Reporters" tab. Add a new "Mail" reporter. You can configure it to send an email on scenario failure, providing the project name, scenario name, and a link to the logs.',
  },
  {
    id: 346,
    slug: 'passing-parameters-into-scenario-jobs',
    question: 'How to get started with passing parameters into Scenario jobs correctly in Python?',
    answer: 'When you trigger a scenario via the API, you can include a JSON payload with new values for your project variables. In a Python scenario step, you can also set the values of variables for subsequent steps.',
  },
  {
    id: 347,
    slug: 'organizing-scenario-steps',
    question: 'How to get started with organizing scenario steps and dependencies visually?',
    answer: 'Scenario steps run sequentially. Add steps in the order you want them to execute. You can add steps to build different datasets, run data quality checks, or execute custom Python code. Give each step a clear name to document its purpose.',
  },
  {
    id: 348,
    slug: 'executing-conditional-steps',
    question: 'How to get started with executing conditional steps depending on dataset checks?',
    answer: 'Use a Python step. The script can run a data quality check on a dataset using the API. Based on the result, the script can decide whether to run another job or to stop the scenario. `if condition: client.get_project(project_key).get_scenario(scenario_id).run()`',
  },
  {
    id: 349,
    slug: 'exporting-logs-from-scenarios',
    question: 'How to get started with exporting logs from Scenarios for debugging?',
    answer: 'From the "Last runs" tab of a scenario, click on a specific run. This will open the job page, where you can view the full log. You can download the log as a text file for offline analysis.',
  },
  {
    id: 350,
    slug: 'scheduling-incremental-runs',
    question: 'How to get started with scheduling incremental runs without redundant full refresh?',
    answer: 'Use partitioned datasets. In your scenario\'s "Build" step, instead of "Build required datasets", choose to build a specific partition (e.g., the latest available one). This tells Dataiku to only process the new data, which is much more efficient.',
  },
  {
    id: 351,
    slug: 'installing-new-plugins',
    question: 'How to get started with installing new plugins to access custom recipes?',
    answer: 'Go to Administration > Plugins. In the "Store" tab, you can browse plugins from Dataiku and the community. Find the plugin you need and click "Install". It will then be available for use in your projects.',
  },
  {
    id: 352,
    slug: 'using-a-plugin-to-wrap-code',
    question: 'How to get started with using a plugin recipe to wrap your own code visually?',
    answer: 'This requires developing a custom plugin. You write the Python backend code and a JSON configuration file that defines the UI for your recipe. This allows you to turn a complex piece of code into a simple, reusable visual component for others to use.',
  },
  {
    id: 353,
    slug: 'generating-visual-recipes-via-ai-sql-assistant',
    question: 'How to get started with generating visual recipes via AI SQL assistant?',
    answer: 'In a SQL recipe, you may see an option for an AI assistant. You can write a natural language prompt (e.g., "join this table with the customers table on the user_id field") and the AI will generate the corresponding SQL code.',
  },
  {
    id: 354,
    slug: 'enabling-natural-language-recipe-generation',
    question: 'How to get started with enabling natural language recipe generation in paid edition?',
    answer: 'This feature may depend on your Dataiku version and license. If available, it usually appears as a button or helper in recipe editors. You provide a prompt in plain English, and the AI suggests the recipe steps or code to accomplish it.',
  },
  {
    id: 355,
    slug: 'prompting-ai-to-explain-recipe-logic',
    question: 'How to get started with prompting generative AI to explain existing recipe logic?',
    answer: 'Some versions of Dataiku with AI features can explain code. In a code recipe, there might be an "Explain code" button. This sends the code to a generative AI model, which returns a natural language explanation of what the code does.',
  },
  {
    id: 356,
    slug: 'customizing-ai-prompts-to-get-accurate-sql-code',
    question: 'How to get started with customizing AI prompts to get accurate SQL code?',
    answer: 'Be specific in your prompts. Instead of "join tables", write "Perform a left join from the sales table to the customers table, using the `customer_id` column as the key". The more detail you provide, the better the generated code will be.',
  },
  {
    id: 357,
    slug: 'avoiding-over-dependence-on-ai-assistants',
    question: 'How to get started with avoiding over-dependence on AI assistants when wrong?',
    answer: 'Always treat AI-generated code as a suggestion, not a final answer. You must understand the code and verify that it is correct and does what you expect. Use it as a tool to speed up your work, not as a replacement for your own knowledge.',
  },
  {
    id: 358,
    slug: 'verifying-ai-generated-logic',
    question: 'How to get started with verifying AI-generated logic via manual review?',
    answer: 'After an AI generates code, read through it line by line. Run it on a sample of your data and check the output to ensure it is correct. Never run AI-generated code in production without thoroughly testing it first.',
  },
  {
    id: 359,
    slug: 'integrating-plugin-recipes-into-flow',
    question: 'How to get started with integrating plugin recipes into your flow for modularity?',
    answer: 'Once a plugin is installed, its recipes will appear in the "+ Recipe" menu alongside the standard visual recipes. You can use them just like any other recipe, which is great for encapsulating complex or reusable logic.',
  },
  {
    id: 360,
    slug: 'debugging-plugin-recipe-failures',
    question: 'How to get started with debugging plugin recipe failures and exceptions?',
    answer: 'Treat it like any other recipe failure. Check the job log for the error message and traceback. Since plugins are often open source, you can also look at the plugin\'s source code to understand its logic and identify the potential cause of the failure.',
  },
  {
    id: 361,
    slug: 'creating-charts-from-datasets',
    question: 'How to get started with creating charts from datasets and customizing axes?',
    answer: 'Open a dataset and go to the "Charts" tab. You can select the chart type (bar, line, scatter, etc.) and then drag and drop columns to the X and Y axes. Use the options to customize labels, colors, and other visual aspects.',
  },
  {
    id: 362,
    slug: 'building-dashboards',
    question: 'How to get started with building dashboards and arranging visuals coherently?',
    answer: 'Go to the "Dashboards" section of your project. Create a new dashboard. You can then add "Tiles", which can be charts, metrics, or dataset previews. Drag and drop the tiles to arrange them into a logical layout that tells a story.',
  },
  {
    id: 363,
    slug: 'refreshing-cached-charts',
    question: 'How to get started with refreshing cached charts when datasets update?',
    answer: 'Dashboard charts are cached for performance. If the underlying dataset is updated, you may need to manually refresh the dashboard tile or the entire dashboard to see the new data reflected in the chart.',
  },
  {
    id: 364,
    slug: 'controlling-dashboard-access-permissions',
    question: 'How to get started with controlling access permissions for dashboards?',
    answer: 'Dashboard permissions are inherited from the project settings. If a user has "Reader" access to the project, they can view the dashboards. You can also share individual dashboards with specific users or groups.',
  },
  {
    id: 365,
    slug: 'scheduling-pdf-or-excel-reports-from-dashboards',
    question: 'How to get started with scheduling PDF or Excel reports from dashboards?',
    answer: 'Create a scenario. Add a "Build" step and select your dashboard. Then, add a "Reporter" (e.g., Mail). The reporter can be configured to attach a PDF or Excel export of the dashboard to the email.',
  },
  {
    id: 366,
    slug: 'embedding-dashboards-in-external-tools',
    question: 'How to get started with embedding dashboards in external tools or portals?',
    answer: 'On a dashboard, use the "Share" feature. This can provide an HTML snippet (an `<iframe>`) that you can embed into other web pages. This requires the viewer to have access to the Dataiku instance.',
  },
  {
    id: 367,
    slug: 'adding-kpi-cards',
    question: 'How to get started with adding KPI cards using statistical summaries?',
    answer: 'First, compute your KPIs and store them in a dataset. Then, on that dataset, go to the "Status" tab > "Metrics". Compute metrics like the sum or average of your KPI column. You can then "Publish" these metrics to a dashboard as KPI cards.',
  },
  {
    id: 368,
    slug: 'resolving-missing-chart-errors',
    question: 'How to get started with resolving missing chart errors due to incompatible field types?',
    answer: 'This often happens when you try to use a string field on an axis that expects a number or a date. Go back to your dataset and use a "Prepare" recipe to convert the column to the correct data type (e.g., using "Parse date" or "Parse to integer").',
  },
  {
    id: 369,
    slug: 'exporting-visuals-to-power-bi-or-tableau',
    question: 'How to get started with exporting visuals to Power BI or Tableau properly?',
    answer: 'It is usually better to export the underlying data, not the visual. Create an "Export" recipe to write the dataset to a SQL database. Then, connect Power BI or Tableau to that database table and recreate the visual natively in your BI tool for best results.',
  },
  {
    id: 370,
    slug: 'troubleshooting-dashboard-layout-issues',
    question: 'How to get started with troubleshooting layout issues across screen sizes?',
    answer: 'Dataiku dashboards have a grid-based layout. Be mindful of how you arrange and size your tiles. Test the dashboard on different screen resolutions to see how it reflows. Use the layout options to adjust the width and height of tiles.',
  },
  {
    id: 371,
    slug: 'setting-roles-and-permissions',
    question: 'How to get started with setting roles and permissions for team collaboration?',
    answer: 'In your project, go to Settings > Permissions. Add your team members or user groups. Assign them a role like "Reader" (can only view), "Contributor" (can edit), or "Administrator" (full control). This is fundamental for secure collaboration.',
  },
  {
    id: 372,
    slug: 'applying-read-write-datasets-access',
    question: 'How to get started with applying read/write datasets access appropriately?',
    answer: 'This is managed at the project level. A user with "Reader" permissions on a project can only view datasets. A "Contributor" can create, edit, and build datasets. You cannot set permissions on individual datasets within a project.',
  },
  {
    id: 373,
    slug: 'versioning-analysis-artifacts-in-git',
    question: 'How to get started with versioning analysis artifacts in Git?',
    answer: 'Connect your project to a Git repository. This allows you to commit changes to all your project artifacts, including recipes, notebooks, and flow structure. Use a standard branching strategy (e.g., feature branches) for development.',
  },
  {
    id: 374,
    slug: 'documenting-recipes-with-in-flow-annotations',
    question: 'How to get started with documenting recipes with in-flow annotations?',
    answer: 'Use the "Description" field on every recipe to explain its purpose. This description is visible in the Flow and is essential for helping others (and your future self) understand your pipeline.',
  },
  {
    id: 375,
    slug: 'attaching-metadata-to-datasets-for-lineage',
    question: 'How to get started with attaching metadata to datasets for lineage tracking?',
    answer: 'Use tags (e.g., `source:salesforce`, `data_quality:validated`) and the "Description" field on your datasets. This metadata helps in searching for and governing your data assets. The lineage itself is tracked automatically by Dataiku.',
  },
  {
    id: 376,
    slug: 'applying-project-level-tags',
    question: 'How to get started with applying project-level tags and description consistently?',
    answer: 'On the project homepage, you can set a project description and apply tags. Establish a convention for tags (e.g., by business unit, data sensitivity) to make your projects easily discoverable and manageable.',
  },
  {
    id: 377,
    slug: 'exporting-project-documentation',
    question: 'How to get started with exporting project documentation for reviews?',
    answer: 'You can export a project as a bundle, which includes all its documentation. For sharing, you can take screenshots of the Flow, export dashboards as PDFs, and copy-paste content from the Wiki.',
  },
  {
    id: 378,
    slug: 'enforcing-governance-policies-in-dataset-access',
    question: 'How to get started with enforcing governance policies in dataset access?',
    answer: 'Use project-level permissions. If a certain group of users should not see sensitive data, make sure that data lives in a project they do not have access to. Use PII tagging to flag sensitive columns.',
  },
  {
    id: 379,
    slug: 'auditing-recipe-changes',
    question: 'How to get started with auditing recipe changes via project history logs?',
    answer: 'The project "Timeline" provides a chronological log of all changes made to the project, including who edited which recipe and when. When using Git, the commit history provides an even more detailed audit trail.',
  },
  {
    id: 380,
    slug: 'aligning-usage-with-compliance-standards',
    question: 'How to get started with aligning usage with organizational compliance standards?',
    answer: 'Work with your compliance team to understand the rules. Use Dataiku\'s governance features (access control, lineage, PII tagging, audit logs) to implement and demonstrate compliance with these standards.',
  },
  {
    id: 381,
    slug: 'profiling-long-running-recipes',
    question: 'How to get started with profiling long-running recipes for performance issues?',
    answer: 'Go to the "Jobs" menu and find a run of your main scenario. The job view will show the duration of each recipe. Identify the recipes that are taking the most time; these are your primary candidates for optimization.',
  },
  {
    id: 382,
    slug: 'switching-visual-to-code-recipes-for-efficiency',
    question: 'How to get started with switching visual recipes to code recipes for efficiency?',
    answer: 'This is not always more efficient. Visual recipes are often highly optimized to push down computation. Only switch to a code recipe if you have a complex algorithm that cannot be expressed visually or if you need to use a specific library. An inefficient Python script can be much slower than a visual recipe.',
  },
  {
    id: 383,
    slug: 'enabling-spark-mode',
    question: 'How to get started with enabling Spark mode for large dataset processing?',
    answer: 'If your Dataiku instance is connected to a Spark cluster, you can enable Spark for many recipes. In the recipe\'s settings, go to the "Advanced" tab and change the "Execution engine" to "Spark".',
  },
  {
    id: 384,
    slug: 'leveraging-push-down-sql',
    question: 'How to get started with leveraging push‑down SQL instead of in-memory compute?',
    answer: 'This is the most important performance optimization. If your data is in a database, use SQL recipes or set visual recipes to run on the database engine. This avoids pulling massive amounts of data into Dataiku and leverages the power of your database.',
  },
  {
    id: 385,
    slug: 'optimizing-join-recipes',
    question: 'How to get started with optimizing join recipes to minimize data duplication?',
    answer: 'After a join, you often have redundant columns (like the join key from both tables). Add a "Prepare" recipe immediately after the join to remove these unnecessary columns, which will reduce the size of the intermediate dataset.',
  },
  {
    id: 386,
    slug: 'caching-intermediate-datasets',
    question: 'How to get started with caching intermediate datasets to avoid recompute?',
    answer: 'Dataiku does this by default. Every dataset is a cache. To optimize it, you can change the storage format of your intermediate datasets. In a dataset\'s settings, change the format to a more performant one like "Parquet" instead of CSV.',
  },
  {
    id: 387,
    slug: 'partitioning-data-to-speed-up-group-by',
    question: 'How to get started with partitioning data to speed up group-by operations?',
    answer: 'Partition your large datasets, for example by date. When you need to run an aggregation, you can configure your recipes to only process the latest partition(s). This drastically reduces the amount of data that needs to be read and processed.',
  },
  {
    id: 388,
    slug: 'deleting-obsolete-datasets',
    question: 'How to get started with deleting obsolete datasets to free storage space?',
    answer: 'Regularly review your flows. If a dataset is no longer used as an input for any downstream recipe, model, or chart, it may be obsolete. Dataiku will warn you if you try to delete a dataset that is still in use.',
  },
  {
    id: 389,
    slug: 'comparing-runtime-metrics-across-environments',
    question: 'How to get started with comparing runtime metrics across environments?',
    answer: 'After deploying a project to a new environment (e.g., from dev to prod), run your main scenarios and check the job logs. Compare the recipe run times. A significant slowdown in production might indicate a resource allocation issue.',
  },
  {
    id: 390,
    slug: 'tuning-resource-config-of-dss-nodes',
    question: 'How to get started with tuning resource config of DSS nodes for heavy loads?',
    answer: 'This is a task for a Dataiku administrator. They can adjust the memory and CPU allocated to the Dataiku backend services. For containerized deployments, they can scale the number of nodes to handle more concurrent jobs.',
  },
  {
    id: 391,
    slug: 'using-dataiku-academy',
    question: 'How to get started with using Dataiku Academy’s Visual Recipes free videos?',
    answer: 'Go to the Dataiku Academy website. They offer free learning paths, including one for "Core Designer". These paths contain short videos and tutorials that are excellent for learning the basics of visual recipes.',
  },
  {
    id: 392,
    slug: 'browsing-dataikus-knowledge-base',
    question: 'How to get started with browsing Dataiku’s Knowledge Base for specific feature issues?',
    answer: 'The official Dataiku documentation includes a Knowledge Base with articles on common issues and how-to guides. Use the search function in the documentation to find articles related to your specific problem.',
  },
  {
    id: 393,
    slug: 'posting-questions-in-dataiku-community-forum',
    question: 'How to get started with posting questions and search in Dataiku Community forum?',
    answer: 'The Dataiku Community forum is a great resource. Before posting, search the forum to see if your question has already been answered. If not, post a new question with a clear description of your problem, what you have tried, and any error messages.',
  },
  {
    id: 394,
    slug: 'testing-sample-projects',
    question: 'How to get started with testing sample projects to understand layout and flow?',
    answer: 'Dataiku comes with several sample projects. From the homepage, create a new project from one of these samples. Explore the Flow, look at how the recipes are configured, and run the scenarios. This is a great way to learn best practices.',
  },
  {
    id: 395,
    slug: 'experimenting-in-free-edition',
    question: 'How to get started with experimenting in a trial or free edition environment?',
    answer: 'Don\'t be afraid to experiment. The free edition is a full-featured sandbox. Try importing your own data, building complex flows, and testing different recipes. You can always delete a project and start over.',
  },
  {
    id: 396,
    slug: 'following-dataiku-tips-and-tricks',
    question: 'How to get started with following top tips & tricks emails from Dataiku?',
    answer: 'Subscribe to the Dataiku newsletter or follow their blog. They often share useful tips, tricks, and tutorials that can help you learn new techniques and become more efficient.',
  },
  {
    id: 397,
    slug: 'leveraging-mentor-or-peer-reviews',
    question: 'How to get started with leveraging mentor or peer reviews on beginner projects?',
    answer: 'Ask a more experienced colleague to review your project. They can provide valuable feedback on your flow design, recipe logic, and adherence to best practices. This is one of the fastest ways to improve.',
  },
  {
    id: 398,
    slug: 'building-reusable-personal-template-flows',
    question: 'How to get started with building reusable personal template flows for practice?',
    answer: 'As you complete projects, you will develop patterns that you use frequently. You can create a project that serves as a template, containing your standard Flow Zone structure, naming conventions, and common validation recipes. You can then duplicate this project to kickstart new work.',
  },
  {
    id: 399,
    slug: 'debugging-stuck-steps-by-cleaning-recipe-caches',
    question: 'How to get started with debugging stuck steps by cleaning recipe caches?',
    answer: 'If a job seems stuck, you can try clearing the cache of the output dataset. In the dataset\'s "Advanced" settings, you can find an option to "Clear data". This will force Dataiku to re-run the recipe that produces it from scratch.',
  },
  {
    id: 400,
    slug: 'tracking-common-issues-to-build-troubleshooting-checklist',
    question: 'How to get started with tracking your common issues to build your troubleshooting checklist?',
    answer: 'Keep a personal document or a Wiki page where you note down the common problems you encounter and how you solved them. This will become your personal troubleshooting guide and will help you solve problems much faster in the future.',
  },
];

export const getQuestionBySlug = (slug: string): Question | undefined => {
  return questions.find(q => q.slug === slug);
}

export const getQuestionById = (id: number): Question | undefined => {
  return questions.find(q => q.id === id);
}
