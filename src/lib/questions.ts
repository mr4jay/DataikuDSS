
export interface Question {
  id: number;
  slug: string;
  question: string;
  answer: string;
}

export const questions: Question[] = [
  {
    id: 1,
    slug: 'building-your-first-dataiku-dss-flow',
    question: 'How to get started with building your first Dataiku DSS flow?',
    answer: 'Start by creating a new project. In your project\'s Flow, click the "+ Dataset" button to import your data. Once you have a dataset, select it and choose a visual recipe like "Prepare" from the right-hand panel to start transforming your data. Connect recipes and datasets to build a sequence of data processing steps, forming your first flow.',
  },
  {
    id: 2,
    slug: 'designing-end-to-end-etl-pipelines-in-dataiku-dss',
    question: 'How to get started with designing end-to-end ETL pipelines in Dataiku DSS?',
    answer: 'Begin by mapping out your data journey: identify your data sources, the transformations required, and the final output destination. In Dataiku, this translates to a series of recipes in a Flow. Start with ingestion recipes (e.g., from a database), apply transformation recipes (Prepare, Join, Group), and end with an export recipe to write the results to a database or file system.',
  },
  {
    id: 3,
    slug: 'ingesting-data-from-databases-into-dataiku',
    question: 'How to get started with ingesting data from databases into Dataiku?',
    answer: 'From your project Flow, click "+ Dataset" and select your database type (e.g., PostgreSQL, SQL Server). You will need to configure a connection to your database with the necessary credentials. Once connected, you can browse tables and import them as datasets into your flow.',
  },
  {
    id: 4,
    slug: 'loading-csv-excel-data-into-dataiku-dss',
    question: 'How to get started with loading CSV/Excel data into Dataiku DSS?',
    answer: 'In your project, click "+ Dataset" and choose "Upload your files". You can drag and drop your CSV or Excel files directly. Dataiku will automatically detect the format and schema. Review the preview and settings, then create the dataset to add it to your Flow.',
  },
  {
    id: 5,
    slug: 'integrating-rest-apis-as-dataiku-datasets',
    question: 'How to get started with integrating REST APIs as Dataiku datasets?',
    answer: 'Use the "API" dataset connector. In your Flow, click "+ Dataset" > "API". You will need to configure the API endpoint URL, any authentication methods (like API keys), and parameters. Dataiku can then call the API and parse the JSON response into a tabular dataset.',
  },
  {
    id: 6,
    slug: 'working-with-cloud-storage-sources-s3-gcs-azure-blob',
    question: 'How to get started with working with cloud storage sources (S3, GCS, Azure Blob)?',
    answer: 'First, set up a connection to your cloud provider in "Administration" > "Connections". Then, in your Flow, create a new dataset and select the appropriate source (e.g., "Amazon S3"). You can then browse your buckets and folders to select the files you want to use as a dataset.',
  },
  {
    id: 7,
    slug: 'combining-disparate-data-sources-into-unified-datasets',
    question: 'How to get started with combining disparate data sources into unified datasets?',
    answer: 'Use the "Join" or "Stack" visual recipes. Import your different datasets into the Flow. Select one dataset, then choose the "Join" recipe to combine it with another based on a common key. Use the "Stack" recipe to append datasets with the same schema.',
  },
  {
    id: 8,
    slug: 'creating-multi-step-recipe-chains',
    question: 'How to get started with creating multi-step recipe chains?',
    answer: 'A multi-step recipe chain is simply a sequence in your Flow. Start with a dataset, apply a recipe (e.g., Prepare). The output of this recipe is a new dataset. You can then select this output dataset and apply another recipe (e.g., Join). This creates a chain of dependencies that defines your data pipeline.',
  },
  {
    id: 9,
    slug: 'structuring-reusable-flow-zones',
    question: 'How to get started with structuring reusable Flow Zones?',
    answer: 'Flow Zones help organize large projects. In your Flow, right-click on the canvas and select "Create Flow Zone". Give it a name that represents a logical part of your project (e.g., "Data Ingestion", "Feature Engineering"). You can then drag and drop related datasets and recipes into this zone to keep your Flow clean and modular.',
  },
  {
    id: 10,
    slug: 'implementing-branching-and-looping-in-dataiku-flows',
    question: 'How to get started with implementing branching and looping in Dataiku flows?',
    answer: 'While Dataiku flows are primarily directed acyclic graphs, you can implement conditional logic (branching) using Scenarios with Python code to check a condition and run different jobs. Looping can be achieved by creating a scenario that calls itself or processes data iteratively, for example by using variables to process data for different dates or categories.',
  },
  {
    id: 11,
    slug: 'using-prepare-recipes-for-data-cleaning',
    question: 'How to get started with using Prepare recipes for data cleaning?',
    answer: 'The Prepare recipe is the workhorse of data cleaning. Select a dataset and click "Prepare". In the recipe, you can add steps from the processor library. Common cleaning steps include handling missing values (e.g., "Clear cells with invalid values"), parsing dates, splitting text, and using formulas for custom transformations.',
  },
  {
    id: 12,
    slug: 'writing-python-recipes-in-dataiku-dss',
    question: 'How to get started with writing Python recipes in Dataiku DSS?',
    answer: 'Select a dataset in your Flow, then from the right panel choose "+ Recipe" > "Python". This opens a code editor. Use the Dataiku API to read your input dataset as a Pandas DataFrame (`dataiku.Dataset("input_dataset_name").get_dataframe()`), perform your transformations, and write the result to an output dataset.',
  },
  {
    id: 13,
    slug: 'writing-sql-recipes-inside-dataiku',
    question: 'How to get started with writing SQL recipes inside Dataiku?',
    answer: 'If your datasets are on a SQL database, you can use a SQL recipe. Select your input dataset, then choose "+ Recipe" > "SQL". Write your SQL query in the editor. You can reference input datasets directly by their name in the `FROM` clause. The result of your query will be the output dataset.',
  },
  {
    id: 14,
    slug: 'adding-custom-python-code-into-workflows',
    question: 'How to get started with adding custom Python code into workflows?',
    answer: 'Custom Python code can be added via Python recipes, notebooks, or by creating custom plugins. For in-flow logic, a Python recipe is ideal. For exploratory analysis, use a Python notebook. For reusable components, consider developing a custom Python-based plugin.',
  },
  {
    id: 15,
    slug: 'combining-r-code-and-dataiku-recipes',
    question: 'How to get started with combining R code and Dataiku recipes?',
    answer: 'Similar to Python, you can use an R recipe. Select an input dataset, click "+ Recipe" > "R". Use the Dataiku R API to read data, apply your R code, and write back the output. This allows you to integrate R-based transformations seamlessly into your visual recipe flows.',
  },
  {
    id: 16,
    slug: 'using-macros-and-global-variables-in-recipes',
    question: 'How to get started with using macros and global variables in recipes?',
    answer: 'Define variables in your project\'s "Variables" section. You can then reference these variables in your recipes (e.g., in formulas or filters) using the `${variable_name}` syntax. This is useful for parameterizing your flows, for example, by using a variable for a file path or a date filter.',
  },
  {
    id: 17,
    slug: 'parsing-json-and-nested-data-in-dataiku',
    question: 'How to get started with parsing JSON and nested data in Dataiku?',
    answer: 'In a Prepare recipe, if you have a column with JSON data, use the "Unnest object" or "Flatten object" processor. This will extract the keys from the JSON object into separate columns, making the nested data accessible for analysis.',
  },
  {
    id: 18,
    slug: 'feature-engineering-using-formula-steps',
    question: 'How to get started with feature engineering using formula steps?',
    answer: 'In a Prepare recipe, the "Formula" processor is very powerful for creating new features. You can write expressions using a syntax similar to spreadsheet formulas to perform mathematical operations, manipulate strings, use conditional logic (if/then/else), and create new columns based on existing ones.',
  },
  {
    id: 19,
    slug: 'implementing-fuzzy-joins-in-dataiku',
    question: 'How to get started with implementing fuzzy joins in Dataiku?',
    answer: 'Dataiku offers a "Fuzzy Join" visual recipe. This is useful when you need to join datasets on keys that are similar but not identical (e.g., "Dataiku Inc." and "Dataiku"). Select your two datasets and the columns to join on, then configure the similarity metric and threshold.',
  },
  {
    id: 20,
    slug: 'handling-missing-values-via-recipes',
    question: 'How to get started with handling missing values via recipes?',
    answer: 'In a Prepare recipe, select a column with missing values. From the processor library, you have several options: "Remove rows" to delete them, "Impute" to fill with a mean, median, or constant value, or "Create indicator" to make a new column that flags where values were missing.',
  },
  {
    id: 21,
    slug: 'using-automl-in-dataiku-dss',
    question: 'How to get started with using AutoML in Dataiku DSS?',
    answer: 'From your Flow, select a clean, prepared dataset. In the right panel, click "Lab", then "New Analysis". Choose "Prediction" or "Clustering", and select your target variable. Dataiku\'s AutoML will automatically handle feature engineering, train several models, and present a leaderboard with performance metrics.',
  },
  {
    id: 22,
    slug: 'building-a-random-forest-classifier-visually',
    question: 'How to get started with building a random forest classifier visually?',
    answer: 'In the Visual Analysis Lab for a prediction task, AutoML will often train a Random Forest model. You can also explicitly select it from the list of algorithms. You can then tune its hyperparameters, like the number of trees and max depth, directly in the UI.',
  },
  {
    id: 23,
    slug: 'building-regression-models-in-dataiku-dss',
    question: 'How to get started with building regression models in Dataiku DSS?',
    answer: 'The process is similar to classification. In the Visual Analysis Lab, create a new analysis. If your target variable is numerical, Dataiku will automatically set the task as regression. It will then train and evaluate suitable regression models like Linear Regression, Ridge, or Gradient Boosted Trees.',
  },
  {
    id: 24,
    slug: 'evaluating-models-roc-f1-precision-recall',
    question: 'How to get started with evaluating models (ROC, F1, precision/recall)?',
    answer: 'After a model is trained in the Visual Analysis Lab, the results page provides extensive evaluation metrics. For classifiers, you will find the ROC curve, confusion matrix, precision, recall, and F1-score. You can analyze these to understand your model\'s performance and choose the best one for your use case.',
  },
  {
    id: 25,
    slug: 'extracting-feature-importance-from-model-runs',
    question: 'How to get started with extracting feature importance from model runs?',
    answer: 'On the model results page, there is a "Feature Importance" section. This shows which input variables had the most influence on the model\'s predictions. For tree-based models, this is often based on Gini impurity or information gain.',
  },
  {
    id: 26,
    slug: 'implementing-cross-validation-and-a-b-tests',
    question: 'How to get started with implementing cross-validation and A/B tests?',
    answer: 'Cross-validation is a standard part of the model training process in Dataiku and is configured in the "Design" tab of the analysis. For A/B testing deployed models, you can create two versions of your model (or two different models) in the Flow and use a "Split" recipe to route traffic to both, then compare their performance on live data.',
  },
  {
    id: 27,
    slug: 'training-deep-learning-models-using-notebooks',
    question: 'How to get started with training deep learning models using notebooks?',
    answer: 'Dataiku supports deep learning frameworks like TensorFlow and Keras. Create a Python notebook and set up a code environment with the necessary libraries. You can then write your deep learning code as you normally would, using the Dataiku API to load data and save your trained model back to the Flow.',
  },
  {
    id: 28,
    slug: 'integrating-scikit-learn-and-tensorflow-in-dataiku',
    question: 'How to get started with integrating scikit-learn and TensorFlow in Dataiku?',
    answer: 'These libraries are the backbone of Dataiku\'s visual machine learning. You can also use them directly in Python recipes or notebooks. Create a code environment, add `scikit-learn` or `tensorflow` as a dependency, and then you are free to use their full functionality within your Dataiku project.',
  },
  {
    id: 29,
    slug: 'deploying-ml-models-inside-dataiku',
    question: 'How to get started with deploying ML models inside Dataiku?',
    answer: 'From a trained model in the Visual Analysis Lab, click the "Deploy" button. This creates a "Saved Model" object in your Flow. You can then use this deployed model with a "Score" recipe to make predictions on new data batches.',
  },
  {
    id: 30,
    slug: 'monitoring-model-performance-over-time',
    question: 'How to get started with monitoring model performance over time?',
    answer: 'Once a model is deployed, you can create a "Model Evaluation" recipe to track its performance. This recipe compares the model\'s predictions against new ground truth data. You can set up scenarios to run this evaluation regularly and alert you if the model\'s performance degrades (model drift).',
  },
  {
    id: 31,
    slug: 'building-scenarios-in-dataiku-dss',
    question: 'How to get started with building Scenarios in Dataiku DSS?',
    answer: 'Navigate to the "Scenarios" section of your project. Create a new scenario and give it a name. In the "Steps" tab, add steps like "Build / Train" to rebuild a dataset or retrain a model. You can also add custom Python code steps for more complex logic.',
  },
  {
    id: 32,
    slug: 'scheduling-pipelines-via-triggers-or-cron',
    question: 'How to get started with scheduling pipelines via triggers or CRON?',
    answer: 'In a Scenario, go to the "Settings" tab. You can add "Triggers" to define when the scenario should run. A "Time-based" trigger lets you set a schedule (e.g., daily, hourly), similar to CRON jobs. You can also trigger scenarios based on changes in data or via the API.',
  },
  {
    id: 33,
    slug: 'automating-model-retraining-workflows',
    question: 'How to get started with automating model retraining workflows?',
    answer: 'Create a scenario that includes a step to "Train" your saved model. This step will rebuild the model using the latest data from its input dataset. Schedule this scenario to run periodically (e.g., weekly) to keep your model fresh.',
  },
  {
    id: 34,
    slug: 'configuring-success-failure-email-alerts',
    question: 'How to get started with configuring success/failure email alerts?',
    answer: 'In a Scenario, go to the "Reporters" tab. Add a new reporter and select "Mail". You can configure it to send an email on success, failure, or completion of the scenario run. This is essential for monitoring your automated jobs.',
  },
  {
    id: 35,
    slug: 'building-data-quality-validation-steps-in-scenarios',
    question: 'How to get started with building data quality validation steps in Scenarios?',
    answer: 'First, define data quality rules on your dataset in the "Status" tab. Then, in a scenario, add a step to "Run checks". You can configure the scenario to fail if the data quality checks do not pass, preventing bad data from moving through your pipeline.',
  },
  {
    id: 36,
    slug: 'using-scenarios-to-automate-data-ingestion-jobs',
    question: 'How to get started with using scenarios to automate data ingestion jobs?',
    answer: 'Create a scenario whose main step is to build the datasets that represent your ingested data. For example, a step to "Build" your S3 dataset. Schedule this scenario to run regularly to fetch the latest data from your sources.',
  },
  {
    id: 37,
    slug: 'managing-flow-dependencies-programmatically',
    question: 'How to get started with managing flow dependencies programmatically?',
    answer: 'In a scenario, you can use a Python step to interact with the Dataiku API. You can get handles to datasets or recipes, check their status, and decide which parts of the flow to build. This gives you fine-grained control over the execution of your pipeline.',
  },
  {
    id: 38,
    slug: 'integrating-dataiku-jobs-into-ci-cd-pipelines',
    question: 'How to get started with integrating Dataiku jobs into CI/CD pipelines?',
    answer: 'Your CI/CD tool (like Jenkins or Azure DevOps) can use the Dataiku REST API to trigger scenario runs. You can have your CI/CD pipeline first deploy the project code to Dataiku and then trigger a scenario to run tests or build production datasets.',
  },
  {
    id: 39,
    slug: 'linking-dataiku-with-jenkins-or-azure-devops',
    question: 'How to get started with linking Dataiku with Jenkins or Azure DevOps?',
    answer: 'The primary method is via the Dataiku REST API. Create an API key in Dataiku with the necessary permissions. In your Jenkins or Azure DevOps pipeline script, make an HTTP request to the Dataiku API endpoint for running a scenario.',
  },
  {
    id: 40,
    slug: 'using-rest-apis-to-trigger-dataiku-scenarios',
    question: 'How to get started with using REST APIs to trigger Dataiku scenarios?',
    answer: 'Generate an API key in Dataiku. Find the endpoint for running a scenario in the API documentation. It will look something like `POST /projects/{projectKey}/scenarios/{scenarioId}/run`. Use a tool like `curl` or any programming language to make a POST request to this endpoint to trigger the scenario.',
  },
  {
    id: 41,
    slug: 'connecting-dataiku-to-aws-redshift',
    question: 'How to get started with connecting Dataiku to AWS Redshift?',
    answer: 'In Dataiku, go to "Administration" > "Connections". Create a new connection of type "Redshift". You will need to provide the host, database name, user, and password for your Redshift cluster. Once saved, you can use this connection to create new datasets from Redshift tables.',
  },
  {
    id: 42,
    slug: 'connecting-dataiku-to-snowflake-data-warehouse',
    question: 'How to get started with connecting Dataiku to Snowflake data warehouse?',
    answer: 'Go to "Administration" > "Connections" and create a "Snowflake" connection. Fill in your Snowflake account URL, user credentials, and default warehouse/database. Dataiku can then leverage Snowflake for both data storage and computation (push-down).',
  },
  {
    id: 43,
    slug: 'using-dataiku-with-gcp-big-query',
    question: 'How to get started with using Dataiku with GCP big-query?',
    answer: 'Set up a Google Cloud Platform connection in "Administration" > "Connections", authenticating with a service account. Then, you can create datasets from BigQuery tables. Dataiku will push down computation to BigQuery whenever possible for optimal performance.',
  },
  {
    id: 44,
    slug: 'integrating-hadoop-or-spark-with-dataiku',
    question: 'How to get started with integrating Hadoop or Spark with Dataiku?',
    answer: 'Dataiku needs to be installed on an edge node of your Hadoop/Spark cluster. The installation process will guide you in configuring Dataiku to connect to HDFS and use YARN for submitting Spark jobs. This allows recipes to run on Spark.',
  },
  {
    id: 45,
    slug: 'running-spark-based-recipes-in-dss',
    question: 'How to get started with running Spark-based recipes in DSS?',
    answer: 'Once Dataiku is connected to a Spark cluster, you can change the execution engine for many visual recipes (like Prepare, Join) from the default "In-Memory" to "Spark". You can also write PySpark code directly in Python recipes.',
  },
  {
    id: 46,
    slug: 'leveraging-cloud-compute-for-large-scale-pipelines',
    question: 'How to get started with leveraging cloud compute for large-scale pipelines?',
    answer: 'When using data warehouses like Snowflake or BigQuery, Dataiku automatically pushes down computation. For other cases, you can configure Dataiku to spin up temporary cloud compute clusters (e.g., on Kubernetes or through services like Databricks) to run specific, resource-intensive recipes.',
  },
  {
    id: 47,
    slug: 'deploying-dataiku-in-cloud-environments',
    question: 'How to get started with deploying Dataiku in cloud environments?',
    answer: 'Dataiku can be deployed on a cloud virtual machine (like an EC2 instance on AWS). For more scalable deployments, consider using the official Dataiku images for Docker or Kubernetes, which allows for containerized and managed deployments.',
  },
  {
    id: 48,
    slug: 'using-dataiku-on-kubernetes-docker',
    question: 'How to get started with using Dataiku on Kubernetes/Docker?',
    answer: 'Dataiku provides official Docker images. You can use these images to run Dataiku as a container. For production, it is recommended to use Kubernetes to manage the Dataiku containers, handle scaling, and ensure high availability.',
  },
  {
    id: 49,
    slug: 'optimizing-performance-on-big-data-jobs',
    question: 'How to get started with optimizing performance on big data jobs?',
    answer: 'The key is to push computation to where the data lives. Use SQL-based recipes for data in databases. Use Spark for data in HDFS or cloud storage. In the "Prepare" recipe, check the "Execution Engine" and ensure it is set to Spark or your database for large datasets, not "In-Memory".',
  },
  {
    id: 50,
    slug: 'migrating-alteryx-workflows-into-dataiku',
    question: 'How to get started with migrating Alteryx workflows into Dataiku?',
    answer: 'There is no direct automatic conversion. The process involves manually recreating the Alteryx workflow logic in Dataiku. Map Alteryx tools to Dataiku visual recipes (e.g., Alteryx "Join" tool becomes a Dataiku "Join" recipe). This is a good opportunity to refactor and optimize the pipeline.',
  },
  {
    id: 51,
    slug: 'creating-dashboards-inside-dataiku',
    question: 'How to get started with creating dashboards inside Dataiku?',
    answer: 'In your project, go to the "Dashboards" section. Create a new dashboard. Then, click "Add Tile" to add content. You can add charts, metrics, dataset previews, and text. Charts must first be created in the "Charts" tab of a dataset.',
  },
  {
    id: 52,
    slug: 'exporting-datasets-to-tableau-power-bi',
    question: 'How to get started with exporting datasets to Tableau/Power BI?',
    answer: 'You can create an "Export" recipe that writes a dataset to a format and location that your BI tool can read (e.g., a SQL database table). Some BI tools also have connectors that can read directly from Dataiku-managed datasets.',
  },
  {
    id: 53,
    slug: 'embedding-dataiku-insights-into-bi-tools',
    question: 'How to get started with embedding Dataiku insights into BI tools?',
    answer: 'You can export datasets from Dataiku to be consumed by BI tools. Additionally, you can embed individual Dataiku charts or dashboards into other web applications (including some BI tools) using their "Share" feature, which provides an HTML embed snippet.',
  },
  {
    id: 54,
    slug: 'designing-kpi-dashboards-within-dss',
    question: 'How to get started with designing KPI dashboards within DSS?',
    answer: 'First, use recipes to compute your KPIs and store them in a dataset. Then, in the "Metrics" tab of that dataset, you can define and compute metrics. These metrics can then be added as tiles to a Dataiku dashboard to create a focused KPI monitoring view.',
  },
  {
    id: 55,
    slug: 'scheduling-excel-report-generation-in-dataiku',
    question: 'How to get started with scheduling Excel report generation in Dataiku?',
    answer: 'Create an "Export" recipe that outputs your dataset in Excel format. Then, create a scenario that builds this export recipe. Schedule this scenario to run as needed. The output Excel file can be stored on the Dataiku server or sent to a cloud storage location.',
  },
  {
    id: 56,
    slug: 'automating-slack-or-email-report-distribution',
    question: 'How to get started with automating slack or email report distribution?',
    answer: 'In a scenario, under the "Reporters" tab, you can add Slack or Mail reporters. You can configure them to send a message upon scenario completion. To attach a report, you can include a link to the exported file or dashboard.',
  },
  {
    id: 57,
    slug: 'exporting-model-predictions-for-stakeholder-review',
    question: 'How to get started with exporting model predictions for stakeholder review?',
    answer: 'After using a "Score" recipe to generate predictions on new data, the output dataset contains the predictions. You can then use an "Export" recipe to save this dataset as a CSV or Excel file, which can be easily shared with stakeholders.',
  },
  {
    id: 58,
    slug: 'building-stakeholder-ready-reports-in-dataiku',
    question: 'How to get started with building stakeholder-ready reports in Dataiku?',
    answer: 'Use Dataiku Dashboards. They are designed for this purpose. Combine charts, key metrics, and text tiles with explanations to create a narrative. You can then share a link to the dashboard with stakeholders.',
  },
  {
    id: 59,
    slug: 'documenting-pipelines-and-outputs-effectively',
    question: 'How to get started with documenting pipelines and outputs effectively?',
    answer: 'Use the built-in documentation features. Give every object (dataset, recipe) a clear name and description. Use the project "Wiki" for high-level documentation. In the Flow, use comments and Flow Zones to explain different parts of your pipeline.',
  },
  {
    id: 60,
    slug: 'training-end-users-on-self-service-analytics',
    question: 'How to get started with training end users on self-service analytics?',
    answer: 'Create well-documented, "golden" datasets for them to use. Build example dashboards and Dataiku applications that they can use and learn from. The Dataiku Academy also provides excellent resources for training new users.',
  },
  {
    id: 61,
    slug: 'documenting-dataiku-flows-and-steps',
    question: 'How to get started with documenting Dataiku flows and steps?',
    answer: 'Every object in the Flow has a "Summary" tab where you can add a description. For recipes, you can add comments to individual steps. Use the project Wiki for overall documentation and a "TODO" list.',
  },
  {
    id: 62,
    slug: 'annotating-datasets-and-recipes-for-governance',
    question: 'How to get started with annotating datasets and recipes for governance?',
    answer: 'Use tags to categorize your datasets and recipes (e.g., "PII", "Finance"). You can also add custom metadata in the "Summary" tab of each object. This information is searchable and helps in governing your projects.',
  },
  {
    id: 63,
    slug: 'applying-data-quality-checks-automatically',
    question: 'How to get started with applying data quality checks automatically?',
    answer: 'Define data quality rules in the "Status" tab of a dataset. Then, create a scenario with a step to "Run checks" on that dataset. Schedule this scenario to run after your data ingestion to automatically validate new data.',
  },
  {
    id: 64,
    slug: 'creating-reusable-metadata-and-standards',
    question: 'How to get started with creating reusable metadata and standards?',
    answer: 'Establish a tagging convention for your projects. Create project templates that include standard Flow Zones and naming conventions. For code, create a library of reusable functions in the project\'s library folder.',
  },
  {
    id: 65,
    slug: 'implementing-lineage-and-impact-tracing',
    question: 'How to get started with implementing lineage and impact tracing?',
    answer: 'Dataiku automatically tracks lineage. In the Flow, you can see the upstream and downstream dependencies of any object. In the "Lineage" tab of a dataset, you can see a detailed, column-level graph of how each column was created.',
  },
  {
    id: 66,
    slug: 'applying-access-controls-and-permissions',
    question: 'How to get started with applying access controls and permissions?',
    answer: 'Access control is managed through user groups. In "Administration" > "Security", you can create groups and assign permissions to them (e.g., read, write, administer) on a per-project basis.',
  },
  {
    id: 67,
    slug: 'version-controlling-dataiku-projects-using-git',
    question: 'How to get started with version controlling Dataiku projects using Git?',
    answer: 'In the project settings, you can link your Dataiku project to a remote Git repository (like on GitHub or GitLab). This allows you to commit changes, create branches, and pull updates, integrating your Dataiku development into a standard Git workflow.',
  },
  {
    id: 68,
    slug: 'maintaining-audit-trails-within-dss',
    question: 'How to get started with maintaining audit trails within DSS?',
    answer: 'Dataiku automatically logs all changes made to a project in the "Timeline" view. For more detailed auditing, especially for regulatory compliance, you can look at the backend logs of the Dataiku instance.',
  },
  {
    id: 69,
    slug: 'aligning-pipelines-with-compliance-policies',
    question: 'How to get started with aligning pipelines with compliance policies?',
    answer: 'Use features like tagging for PII data, access controls to restrict who can see sensitive data, and thorough documentation. The column-level lineage is also crucial for demonstrating to auditors how data is being used.',
  },
  {
    id: 70,
    slug: 'establishing-coding-best-practices-in-team',
    question: 'How to get started with establishing coding best practices in team?',
    answer: 'Create a set of guidelines for your team. This can include naming conventions, a standard structure for Python code in recipes, and a requirement for comments. Use the project Wiki to document these best practices. Code reviews can be done through the Git integration.',
  },
  {
    id: 71,
    slug: 'embedding-generative-ai-models-in-pipelines',
    question: 'How to get started with embedding Generative AI models in pipelines?',
    answer: 'Use a Python recipe to call a generative AI model\'s API (e.g., from OpenAI or Hugging Face). You can pass data from a Dataiku dataset as prompts to the model and write the generated text back to an output dataset.',
  },
  {
    id: 72,
    slug: 'building-nlp-text-analytics-flows',
    question: 'How to get started with building NLP/text analytics flows?',
    answer: 'In a Prepare recipe, use the "Text processing" processors. These can perform tasks like tokenization, stop word removal, and sentiment analysis. For more advanced NLP, use a Python recipe with libraries like NLTK or spaCy.',
  },
  {
    id: 73,
    slug: 'integrating-labeling-management-workflows',
    question: 'How to get started with integrating labeling management workflows?',
    answer: 'Dataiku has a "Labeling" plugin that allows you to create tasks for manually labeling data, for example, for training a text classification or computer vision model. This is integrated directly into the Flow.',
  },
  {
    id: 74,
    slug: 'using-time-series-modeling-in-dataiku',
    question: 'How to get started with using time-series modeling in Dataiku?',
    answer: 'Dataiku provides a "Time Series Forecasting" visual analysis lab. You will need a dataset with a date column and a numerical series to forecast. The lab helps you with resampling, feature extraction from the date, and training forecasting models.',
  },
  {
    id: 75,
    slug: 'implementing-computer-vision-pipelines',
    question: 'How to get started with implementing computer vision pipelines?',
    answer: 'You will typically use a Python recipe or notebook with libraries like OpenCV or Pillow to process images. For modeling, you can use deep learning frameworks like Keras/TensorFlow to train models on your image data, which can be stored as folders of files managed by Dataiku.',
  },
  {
    id: 76,
    slug: 'connecting-dataiku-to-kubernetes-clusters',
    question: 'How to get started with connecting Dataiku to Kubernetes clusters?',
    answer: 'In "Administration" > "Containerized Execution", you can configure a connection to a Kubernetes cluster. This allows Dataiku to spin up ephemeral pods to run specific recipes or notebooks, which is great for managing computational resources.',
  },
  {
    id: 77,
    slug: 'using-dataiku-apis-for-advanced-control',
    question: 'How to get started with using Dataiku APIs for advanced control?',
    answer: 'There are two main APIs: the Python API for use within Dataiku (in recipes and notebooks) and the REST API for external control. Start by exploring the Python API to programmatically manipulate objects in your flow. The REST API is for integration with other systems.',
  },
  {
    id: 78,
    slug: 'adding-custom-plugins-to-dss',
    question: 'How to get started with adding custom plugins to DSS?',
    answer: 'Plugins are developed locally in a special developer mode of Dataiku. You can create your own visual recipes, dataset connectors, or processors using a mix of Python and JSON configuration files. Once developed, the plugin can be packaged and installed on other Dataiku instances.',
  },
  {
    id: 79,
    slug: 'deploying-dataiku-rest-endpoints',
    question: 'How to get started with deploying Dataiku REST endpoints?',
    answer: 'You can deploy a trained model as a real-time API endpoint. This is done through the "API Deployer" service. You create an API service, add your trained model as an endpoint, and Dataiku will manage the serving infrastructure.',
  },
  {
    id: 80,
    slug: 'building-interactive-dataiku-apps',
    question: 'How to get started with building interactive Dataiku apps?',
    answer: 'Dataiku Apps are web applications that can be built within a project. You can create a "Web App" from the project view. Options include a standard HTML/CSS/JS app, or apps built with Python frameworks like Dash or Streamlit, which let you create interactive UIs backed by your data and models.',
  },
  {
    id: 81,
    slug: 'collaborating-with-business-analysts-and-data-scientists',
    question: 'How to get started with collaborating with business analysts and data scientists?',
    answer: 'Dataiku is designed for this collaboration. Use the Flow as a common language. BAs can use visual recipes, while DSs can add code recipes. Use the "Discussions" feature on every object to ask questions and share findings. The Wiki is great for shared documentation.',
  },
  {
    id: 82,
    slug: 'translating-business-needs-into-dss-pipelines',
    question: 'How to get started with translating business needs into DSS pipelines?',
    answer: 'Start by breaking down the business problem into a series of logical steps: What data is needed? How does it need to be cleaned and transformed? What is the final output (a dataset, a model, a chart)? Each of these steps can then be implemented as a recipe or object in a Dataiku Flow.',
  },
  {
    id: 83,
    slug: 'reviewing-code-and-mentoring-junior-workers',
    question: 'How to get started with reviewing code and mentoring junior workers?',
    answer: 'If using the Git integration, you can use standard pull request workflows for code review. For visual recipes, sit with the junior developer and review the steps in their Prepare recipe, explaining the logic and suggesting improvements. The visual nature of the Flow makes it easy to see what they are doing.',
  },
  {
    id: 84,
    slug: 'gathering-requirements-for-dataiku-projects',
    question: 'How to get started with gathering requirements for Dataiku projects?',
    answer: 'Work with stakeholders to define the project goals, the key questions to be answered, the available data sources, and the desired outcomes. Create a project brief in the Wiki to document these requirements before you start building.',
  },
  {
    id: 85,
    slug: 'writing-technical-specs-and-process-docs',
    question: 'How to get started with writing technical specs and process docs?',
    answer: 'The project Wiki is the perfect place for this. Create pages for technical specifications, data dictionary, and process documentation. The combination of the visual Flow and the Wiki provides a comprehensive documentation package.',
  },
  {
    id: 86,
    slug: 'participating-in-agile-scrum-development-flows',
    question: 'How to get started with participating in Agile/Scrum development flows?',
    answer: 'Dataiku projects fit well into an Agile framework. Each user story or task can correspond to building a part of the Dataiku Flow. Use the project "TODO" list as a simple backlog. The Git integration allows for sprint-based development with branching and merging.',
  },
  {
    id: 87,
    slug: 'communicating-progress-to-non-technical-stakeholders',
    question: 'How to get started with communicating progress to non-technical stakeholders?',
    answer: 'Use Dataiku Dashboards to share key results and visualizations. The Flow itself, when well-organized with Zones, can also be a useful tool to show the overall process in a simplified way. Schedule regular demos to walk them through the progress.',
  },
  {
    id: 88,
    slug: 'troubleshooting-live-dataiku-jobs',
    question: 'How to get started with troubleshooting live Dataiku jobs?',
    answer: 'When a scenario run fails, go to the "Last runs" tab of the scenario. Click on the failed run to see the logs. The logs will show which step failed and provide a detailed error message. This is the starting point for any troubleshooting.',
  },
  {
    id: 89,
    slug: 'resolving-pipeline-failures-and-bottlenecks',
    question: 'How to get started with resolving pipeline failures and bottlenecks?',
    answer: 'For failures, check the job logs to identify the root cause (e.g., bad data, code error, connection issue). For bottlenecks, use the job timings to see which recipes are taking the longest to run. Then you can focus on optimizing that specific part of the flow, for example by changing the execution engine.',
  },
  {
    id: 90,
    slug: 'staying-updated-with-the-latest-dss-features',
    question: 'How to get started with staying updated with the latest DSS features?',
    answer: 'Read the release notes for each new version of Dataiku. Follow the Dataiku blog and community forums. The Dataiku Academy also regularly updates its content to cover new features.',
  },
  {
    id: 91,
    slug: 'navigating-the-dataiku-academy-learning-paths',
    question: 'How to get started with navigating the Dataiku Academy learning paths?',
    answer: 'The Dataiku Academy offers structured learning paths for different roles (e.g., "Core Designer", "Advanced Designer", "Developer"). Start with the "Core Designer" path to get a solid foundation in the key concepts of Dataiku. The paths consist of videos, tutorials, and quizzes.',
  },
  {
    id: 92,
    slug: 'using-the-dataiku-developer-guide-and-api-docs',
    question: 'How to get started with using the Dataiku Developer guide and API docs?',
    answer: 'The official documentation is your best friend. The developer guide provides information on topics like creating plugins. The API docs (both for Python and REST) provide detailed references for all available functions and endpoints, with examples.',
  },
  {
    id: 93,
    slug: 'joining-the-dataiku-community-forums',
    question: 'How to get started with joining the Dataiku Community forums?',
    answer: 'The Dataiku Community is a great place to ask questions, share your work, and learn from other users. You can browse existing topics or post a new question if you are stuck. It is a very active and helpful community.',
  },
  {
    id: 94,
    slug: 'earning-dataiku-core-designer-certification',
    question: 'How to get started with earning Dataiku Core Designer certification?',
    answer: 'Follow the "Core Designer" learning path on the Dataiku Academy. Once you have completed the modules and feel confident with the material, you can take the certification exam. The certification demonstrates your proficiency in the fundamental concepts of Dataiku.',
  },
  {
    id: 95,
    slug: 'experimenting-in-a-sandbox-dss-instance',
    question: 'How to get started with experimenting in a sandbox DSS instance?',
    answer: 'Having a sandbox environment is crucial for learning. Don\'t be afraid to try new things. Create new projects, import different kinds of data, and experiment with all the different visual recipes. The worst that can happen is you delete the project and start over.',
  },
  {
    id: 96,
    slug: 'running-example-projects-tutorial-flows',
    question: 'How to get started with running example projects/tutorial flows?',
    answer: 'Dataiku comes with several sample projects that cover common use cases like customer churn prediction. Create a new project from one of these samples and explore the Flow. See how the datasets and recipes are connected. Re-run the scenarios to see how it all works.',
  },
  {
    id: 97,
    slug: 'benchmarking-performance-on-sandbox-datasets',
    question: 'How to get started with benchmarking performance on sandbox datasets?',
    answer: 'Try building the same data transformation pipeline in different ways. For example, using only visual recipes vs. a Python recipe. Or, for a large dataset, compare the performance of the "In-Memory" engine vs. the "Spark" engine. This will help you understand the performance implications of your design choices.',
  },
  {
    id: 98,
    slug: 'running-pocs-combining-dataiku-and-ml-frameworks',
    question: 'How to get started with running POCs combining Dataiku and ML frameworks?',
    answer: 'Define a small, focused proof-of-concept (POC). For example, "Can we use a pre-trained model from Hugging Face to classify customer feedback?". Then, in a Dataiku project, use a Python recipe to implement this POC, demonstrating how the external ML framework can be integrated into a Dataiku pipeline.',
  },
  {
    id: 99,
    slug: 'evaluating-dataiku-for-specific-business-cases',
    question: 'How to get started with evaluating Dataiku for specific business cases?',
    answer: 'Take a real but small-scale business problem and try to solve it end-to-end in Dataiku. This is the best way to see how Dataiku would work for your specific needs. Document the process, the challenges, and the results to build a business case.',
  },
  {
    id: 100,
    slug: 'building-your-personal-dataiku-learning-portfolio',
    question: 'How to get started with building your personal Dataiku learning portfolio?',
    answer: 'As you learn, create projects that showcase your skills. You could take a public dataset (e.g., from Kaggle) and build a complete project around it in Dataiku, including data preparation, modeling, and a dashboard. Document your work in the project Wiki. This portfolio can be very valuable for job applications.',
  },
  {
    id: 101,
    slug: 'reviewing-your-daily-dataiku-project-dashboard',
    question: 'How to get started with reviewing your daily Dataiku project dashboard?',
    answer: 'Begin your day by opening your primary project and navigating to the "Dashboards" section. Check key metric tiles for any significant changes, review the latest charts for unexpected trends, and look at the project\'s recent activity feed to understand what has been updated.',
  },
  {
    id: 102,
    slug: 'checking-scenario-logs-for-overnight-failures',
    question: 'How to get started with checking scenario logs for overnight failures?',
    answer: 'Go to the "Scenarios" page in your project. Sort the scenarios by "Last run" and look for any with a "Failed" status. Click on a failed run to view its log, which will detail the exact step that failed and provide an error message to begin your troubleshooting.',
  },
  {
    id: 103,
    slug: 'prioritizing-high-severity-pipeline-alerts',
    question: 'How to get started with prioritizing high-severity pipeline alerts?',
    answer: 'Check your email or Slack for any automated failure alerts from your scenarios. The alert message should specify the project and scenario name. Prioritize alerts related to production pipelines or those that block downstream dependencies for other teams.',
  },
  {
    id: 104,
    slug: 'examining-data-freshness-and-quality-metrics',
    question: 'How to get started with examining data freshness and quality metrics?',
    answer: 'On your key datasets, navigate to the "Status" tab. Here, you can see when the data was last built (freshness). Then, go to "Metrics" to review the results of your data quality rules. Look for any new invalid, empty, or outlier values that need investigation.',
  },
  {
    id: 105,
    slug: 'syncing-with-data-engineering-team-on-data-availability',
    question: 'How to get started with syncing with data engineering team on data availability?',
    answer: 'Establish a communication channel (like a daily stand-up or a dedicated Slack channel). Confirm the status of upstream data sources that your Dataiku projects depend on. Ask if there were any delays or schema changes in the source systems that might impact your pipelines.',
  },
  {
    id: 106,
    slug: 'scheduling-the-days-workflow-runs',
    question: 'How to get started with scheduling the day\'s workflow runs?',
    answer: 'While most critical workflows should be automated with triggers, some ad-hoc runs may be necessary. Go to the "Scenarios" page, select the scenario you need to run, and click the "Run" button. This is useful for kicking off development or testing jobs.',
  },
  {
    id: 107,
    slug: 'defining-rois-and-kpis-to-track-in-your-project',
    question: 'How to get started with defining ROIs and KPIs to track in your project?',
    answer: 'Collaborate with business stakeholders to understand what defines success. In the project Wiki, document the Key Performance Indicators (KPIs) and the expected Return on Investment (ROI). Then, create recipes to calculate these KPIs and display them on a project dashboard.',
  },
  {
    id: 108,
    slug: 'reviewing-pull-requests-in-shared-dataiku-libraries',
    question: 'How to get started with reviewing pull requests in shared Dataiku libraries?',
    answer: 'If your project is connected to Git, navigate to your Git provider (e.g., GitHub). Review any open pull requests that modify the shared Python libraries in your project. Check the code for correctness, style, and potential impact on existing flows before approving.',
  },
  {
    id: 109,
    slug: 'discussing-flow-enhancements-with-stakeholders',
    question: 'How to get started with discussing flow enhancements with stakeholders?',
    answer: 'Use a Dataiku dashboard or the Flow itself as a visual aid. Schedule a meeting to walk stakeholders through the current process and the proposed changes. Use the "Discussions" feature on datasets or recipes to capture feedback directly within Dataiku.',
  },
  {
    id: 110,
    slug: 'planning-feature-engineering-tasks-for-new-releases',
    question: 'How to get started with planning feature engineering tasks for new releases?',
    answer: 'Create a new Flow Zone in your project for the upcoming release. Use the project\'s "TODO" list or a Wiki page to list the new features to be created. This involves identifying which new columns need to be created in your Prepare recipes to improve model performance.',
  },
  {
    id: 111,
    slug: 'monitoring-scheduled-dataiku-scenarios',
    question: 'How to get started with monitoring scheduled Dataiku scenarios?',
    answer: 'Navigate to the "Scenarios" section. The main view provides a summary of all scenarios, their last run status (success, failed), and the next scheduled run time. This gives you a high-level overview of the health of your automated pipelines.',
  },
  {
    id: 112,
    slug: 'investigating-recipe-failure-and-error-logs',
    question: 'How to get started with investigating recipe failure and error logs?',
    answer: 'When a job fails, open the job\'s log. The log is structured by recipe steps. Find the step that failed. The error message will often tell you exactly what went wrong, such as a data type mismatch in a Join recipe or a syntax error in a Python recipe.',
  },
  {
    id: 113,
    slug: 'implementing-retry-logic-in-scenarios',
    question: 'How to get started with implementing retry logic in scenarios?',
    answer: 'While Dataiku doesn\'t have a built-in "retry" button for steps, you can implement it using a Python step in your scenario. The Python code can check the outcome of a previous step and, if it failed, can trigger the job again up to a certain number of retries.',
  },
  {
    id: 114,
    slug: 'optimizing-recipe-runtime-on-large-datasets',
    question: 'How to get started with optimizing recipe runtime on large datasets?',
    answer: 'Identify slow recipes by checking job run times. The most common optimization is to change the execution engine from "In-Memory" to a distributed engine like Spark or to push down the computation to a SQL database. This is done in the "Advanced" tab of the recipe settings.',
  },
  {
    id: 115,
    slug: 'balancing-compute-vs-in-memory-recipe-execution',
    question: 'How to get started with balancing compute vs in-memory recipe execution?',
    answer: 'Use the in-memory engine for smaller datasets or recipes that are not supported by distributed engines. For large datasets and compatible recipes (like Prepare, Join, Group), always prefer pushing down to a database or Spark to avoid pulling all data into Dataiku\'s memory.',
  },
  {
    id: 116,
    slug: 'enforcing-error-handling-branches-in-flows',
    question: 'How to get started with enforcing error-handling branches in flows?',
    answer: 'Use a scenario with Python steps for conditional logic. After a main job runs, a Python step can check its outcome. If it failed, the script can trigger a different branch of the flow, for example, to send a notification or to run a cleanup job.',
  },
  {
    id: 117,
    slug: 'tuning-global-variables-for-better-pipeline-reuse',
    question: 'How to get started with tuning global variables for better pipeline reuse?',
    answer: 'In Project Settings > Variables, define variables for things that change between environments or runs, like file paths, date ranges, or model versions. Use the `${variable_name}` syntax in your recipes. This allows you to change the behavior of the entire flow without editing individual recipes.',
  },
  {
    id: 118,
    slug: 'refactoring-flow-zones-for-modularity',
    question: 'How to get started with refactoring Flow Zones for modularity?',
    answer: 'Review your Flow. If it\'s becoming too complex, identify logical sections (e.g., data ingestion, feature engineering, modeling, reporting). Create a Flow Zone for each section and move the relevant items into it. This makes the Flow much easier to read and maintain.',
  },
  {
    id: 119,
    slug: 'cleaning-up-obsolete-datasets-to-reduce-clutter',
    question: 'How to get started with cleaning up obsolete datasets to reduce clutter?',
    answer: 'Regularly review your Flow for datasets that are no longer used as inputs to any other recipe or model. These can often be deleted. Be careful to check dependencies first; Dataiku will warn you if you try to delete something that is still in use.',
  },
  {
    id: 120,
    slug: 'version-controlling-flows-using-git-integration',
    question: 'How to get started with version controlling flows using Git integration?',
    answer: 'In the project settings, connect your project to a remote Git repository. After making changes to your flow, go to the Git page in the project, stage your changes, write a descriptive commit message, and push them to the remote repository. This creates a history of your project.',
  },
  {
    id: 121,
    slug: 'building-and-iterating-on-prepare-recipes',
    question: 'How to get started with building and iterating on Prepare recipes?',
    answer: 'Select a dataset and choose the "Prepare" recipe. Add processors from the library on the left to clean, transform, and create new columns. The recipe is interactive; you see the results of each step immediately. Continuously add and refine steps until your data is in the desired shape.',
  },
  {
    id: 122,
    slug: 'using-python-recipes-for-complex-transforms',
    question: 'How to get started with using Python recipes for complex transforms?',
    answer: 'When a visual recipe isn\'t enough, use a Python recipe. It gives you the full power of Python and libraries like Pandas. Read your input dataset into a DataFrame, perform your custom transformations in code, and write the resulting DataFrame to the output dataset.',
  },
  {
    id: 123,
    slug: 'writing-sql-recipes-for-relational-joins',
    question: 'How to get started with writing SQL recipes for relational joins?',
    answer: 'If your data is in a SQL database, a SQL recipe is highly efficient. Select your input datasets and choose the "SQL" recipe. Write a standard SQL query, referencing the input tables by their names. This pushes the entire computation down to the database.',
  },
  {
    id: 124,
    slug: 'creating-reusable-macros-for-repetitive-logic',
    question: 'How to get started with creating reusable macros for repetitive logic?',
    answer: 'In Project Settings > Macros, you can define project-level macros. These can be written in Python and can perform actions like running a series of jobs or creating datasets. They are useful for encapsulating logic that you need to reuse across multiple scenarios.',
  },
  {
    id: 125,
    slug: 'implementing-data-quality-rules-in-flows',
    question: 'How to get started with implementing data quality rules in flows?',
    answer: 'On any dataset, go to the "Status" tab and then "Metrics". Here you can define checks, such as "column must be unique" or "values must be within a range". You can then create a scenario step to "Run checks" and fail the job if the data quality is not met.',
  },
  {
    id: 126,
    slug: 'imputing-missing-values-across-multiple-datasets',
    question: 'How to get started with imputing missing values across multiple datasets?',
    answer: 'In a Prepare recipe, select a column with missing data. The processor library has several options for imputation: filling with the mean, median, mode, a constant value, or using a more advanced algorithm. Apply this step to all relevant columns.',
  },
  {
    id: 127,
    slug: 'engineering-timestamp-and-lag-features-for-time-series',
    question: 'How to get started with engineering timestamp and lag features for time series?',
    answer: 'In a Prepare recipe, use the "Parse Date" processor on your timestamp column. Then, you can use the "Extract date components" processor to get the year, month, day of week, etc. For lag features, you would typically use a Window recipe to get the value from a previous row.',
  },
  {
    id: 128,
    slug: 'converting-json-and-nested-attributes-into-flat-fields',
    question: 'How to get started with converting JSON and nested attributes into flat fields?',
    answer: 'If you have a column containing JSON strings, use the "Unnest object" processor in a Prepare recipe. Dataiku will parse the JSON and create a new column for each top-level key in the object, effectively flattening the structure.',
  },
  {
    id: 129,
    slug: 'norming-categorical-variables-for-modeling',
    question: 'How to get started with norming categorical variables for modeling?',
    answer: 'The most common method is one-hot encoding. In a Prepare recipe, select a categorical column and use the "Dummy-encode" processor. This will create a new binary (0/1) column for each unique value in the original category, making it suitable for most ML models.',
  },
  {
    id: 130,
    slug: 'caching-intermediate-data-for-reuse-in-downstream-steps',
    question: 'How to get started with caching intermediate data for reuse in downstream steps?',
    answer: 'Dataiku does this automatically. Every output dataset of a recipe is a form of cache. When you run a downstream recipe, Dataiku will not re-run the upstream recipes if their inputs haven\'t changed. For performance, you can set the storage format of these intermediate datasets (e.g., to Parquet).',
  },
  {
    id: 131,
    slug: 'triggering-automl-experiments-for-classification-or-regression',
    question: 'How to get started with triggering AutoML experiments for classification or regression?',
    answer: 'Select your prepared dataset, click "Lab" in the right panel, and "New Analysis". Choose your target variable. Dataiku will automatically detect if it\'s a classification or regression task. In the "Design" tab, click "Train" to start the AutoML process.',
  },
  {
    id: 132,
    slug: 'comparing-model-runs-using-evaluation-dashboards',
    question: 'How to get started with comparing model runs using evaluation dashboards?',
    answer: 'After an AutoML experiment finishes, the "Results" tab shows a leaderboard of all the models that were trained. You can sort them by metrics like AUC, F1-score, or R-squared. Click on multiple models and "Compare" to see a side-by-side view of their performance.',
  },
  {
    id: 133,
    slug: 'extracting-feature-importance-and-shap-insights',
    question: 'How to get started with extracting feature importance and SHAP insights?',
    answer: 'In the results page for a single trained model, there are tabs for "Feature Importance" (which gives a global overview) and "Individual Explanations" (which uses methods like SHAP values to explain predictions for single data points).',
  },
  {
    id: 134,
    slug: 'retraining-models-on-fresh-datasets-using-dataiku-scenarios',
    question: 'How to get started with retraining models on fresh datasets using Dataiku scenarios?',
    answer: 'Once you have deployed a model, create a scenario. Add a step to "Build" the input dataset for the model, followed by a step to "Train" the saved model. Schedule this scenario to run periodically to keep your model updated with the latest data.',
  },
  {
    id: 135,
    slug: 'integrating-scikit-learn-pipelines-in-notebooks',
    question: 'How to get started with integrating scikit-learn pipelines in notebooks?',
    answer: 'Create a Python notebook in Dataiku. Ensure your code environment has scikit-learn installed. You can then write standard scikit-learn code, including creating `Pipeline` objects. Use the `dataiku` library to load your data into a Pandas DataFrame to feed into your scikit-learn pipeline.',
  },
  {
    id: 136,
    slug: 'using-tensorflow-or-pytorch-through-code-recipes',
    question: 'How to get started with using TensorFlow or PyTorch through code recipes?',
    answer: 'Create a Python recipe and make sure you select a code environment that has TensorFlow or PyTorch installed. You can then write your deep learning code, load data using the Dataiku API, train your model, and save the model file to a managed folder.',
  },
  {
    id: 137,
    slug: 'performing-cross-validation-manually-in-python-code',
    question: 'How to get started with performing cross-validation manually in Python code?',
    answer: 'While Dataiku\'s AutoML does this automatically, you can do it yourself in a Python recipe or notebook. Use scikit-learn\'s `KFold` or `StratifiedKFold` to create the data splits. Then, loop through the splits, training your model on the training fold and evaluating on the test fold.',
  },
  {
    id: 138,
    slug: 'selecting-best-model-and-packaging-as-a-saved-model',
    question: 'How to get started with selecting best model and packaging as a Saved Model?',
    answer: 'From the model results leaderboard in the Visual Analysis Lab, select the model that performs best on your chosen metric. Click the "Deploy" button. This will create a "Saved Model" object in your Flow, which is the official, versioned package of your trained model.',
  },
  {
    id: 139,
    slug: 'defining-custom-saved-models-via-pythonmodel-module',
    question: 'How to get started with defining custom saved models via PythonModel module?',
    answer: 'For models trained in Python code, you can create a custom model class that inherits from `dataiku.custom_model.CustomModel`. You need to implement methods for training and prediction. This allows you to integrate a completely custom model into the Dataiku framework so it can be deployed and monitored like a visual model.',
  },
  {
    id: 140,
    slug: 'visualizing-confusion-matrices-and-roc-curves',
    question: 'How to get started with visualizing confusion matrices and ROC curves?',
    answer: 'On the results page of a trained classification model, these are available by default. Go to the "Performance" section. You will find interactive plots for the Confusion Matrix, ROC AUC curve, lift charts, and more.',
  },
  {
    id: 141,
    slug: 'deploying-models-with-dataiku-api-node',
    question: 'How to get started with deploying models with Dataiku API node?',
    answer: 'First, deploy your trained model from the lab to the Flow. Then, you need an "API Deployer" instance. From there, you can create a new API service and add your saved model as an endpoint. This makes the model available for real-time predictions via a REST API.',
  },
  {
    id: 142,
    slug: 'creating-real-time-scoring-endpoints-for-apps',
    question: 'How to get started with creating real-time scoring endpoints for apps?',
    answer: 'Once your model is deployed as an endpoint on the API node, Dataiku provides the URL and code snippets (e.g., in Python or curl) to call it. Your external application can then make an HTTP request with the feature data to this endpoint and will receive the prediction in real-time.',
  },
  {
    id: 143,
    slug: 'building-dashboards-to-monitor-model-drift',
    question: 'How to get started with building dashboards to monitor model drift?',
    answer: 'On your saved model in the Flow, create a "Model Evaluation" recipe. This compares predictions on new data to the ground truth. The results of this recipe (like changes in accuracy, AUC, or data distributions) can then be published to a dashboard to track performance over time.',
  },
  {
    id: 144,
    slug: 'setting-up-drift-alerts-for-key-features',
    question: 'How to get started with setting up drift alerts for key features?',
    answer: 'In the "Model Views" of a saved model, you can enable drift analysis. This tracks the statistical distribution of your input features. You can then create a scenario step that checks for drift and, if it exceeds a threshold, runs a reporter to send an email or Slack alert.',
  },
  {
    id: 145,
    slug: 'automating-output-delivery-via-email-or-slack',
    question: 'How to get started with automating output delivery via email or Slack?',
    answer: 'Create a scenario that builds your desired output (like a dataset or dashboard). In the "Reporters" tab of the scenario, add a new reporter for "Mail" or "Slack". Configure it to run on scenario completion and customize the message, which can include links to the output.',
  },
  {
    id: 146,
    slug: 'scheduling-periodic-model-performance-audits',
    question: 'How to get started with scheduling periodic model performance audits?',
    answer: 'Create a scenario that runs a "Model Evaluation" recipe on your deployed model using the latest data. Schedule this scenario to run on a regular basis (e.g., weekly or monthly). The output will be a historical record of your model\'s performance.',
  },
  {
    id: 147,
    slug: 'pushing-model-outputs-into-bi-tools-like-power-bi',
    question: 'How to get started with pushing model outputs into BI tools like Power BI?',
    answer: 'After running a "Score" recipe, your model\'s predictions are in a dataset. Create an "Export" recipe to write this dataset to a location your BI tool can access, like a SQL database table. Then, configure your Power BI report to read from that table.',
  },
  {
    id: 148,
    slug: 'configuring-promotion-from-dev-to-prod-environments',
    question: 'How to get started with configuring promotion from dev to prod environments?',
    answer: 'The standard way is using project bundles. From your development project, create a bundle. Then, on your production Dataiku instance, import this bundle. The import wizard helps you re-map connections and other settings to match the production environment.',
  },
  {
    id: 149,
    slug: 'documenting-deployment-configurations-and-environments',
    question: 'How to get started with documenting deployment configurations and environments?',
    answer: 'Use the project Wiki to create a page dedicated to deployment. Document the connections, variables, and any manual steps required for promoting the project. This is crucial for ensuring smooth and repeatable deployments.',
  },
  {
    id: 150,
    slug: 'troubleshooting-scoring-api-latency-or-failure',
    question: 'How to get started with troubleshooting scoring API latency or failure?',
    answer: 'In the API Deployer, look at the logs for your API endpoint. It will show any errors that occurred during prediction. For latency issues, check the complexity of your model and the resources allocated to the API node. You may need to scale up the node or simplify your model.',
  },
  {
    id: 151,
    slug: 'connecting-to-cloud-systems-aws-redshift-snowflake-bigquery',
    question: 'How to get started with connecting to cloud systems (AWS Redshift, Snowflake, BigQuery)?',
    answer: 'Go to Administration > Connections. Select your cloud data warehouse type. You will need to provide the server address, credentials, and other connection details. Once configured, this connection can be used across all your Dataiku projects.',
  },
  {
    id: 152,
    slug: 'connecting-to-rest-or-soap-apis-as-data-sources',
    question: 'How to get started with connecting to REST or SOAP APIs as data sources?',
    answer: 'From the Flow, create a new dataset using the "API" plugin. Configure the endpoint URL, authentication method (e.g., API key in header), and any parameters. Dataiku can then query this API and parse the JSON/XML response into a dataset.',
  },
  {
    id: 153,
    slug: 'integrating-with-enterprise-flat-file-data-feeds-csv-sftp',
    question: 'How to get started with integrating with enterprise flat-file data feeds (CSV, SFTP)?',
    answer: 'Create a new dataset and choose "Files from SFTP". You will need to configure a connection to the SFTP server. Then you can browse for the file and import it. Dataiku can automatically detect CSV formatting and schema.',
  },
  {
    id: 154,
    slug: 'managing-credentials-and-secrets-securely-in-dataiku',
    question: 'How to get started with managing credentials and secrets securely in Dataiku?',
    answer: 'Avoid hardcoding credentials. Use the built-in secrets management. In connection settings, you can define credentials that are stored securely. For more advanced use, integrate Dataiku with an external secrets vault like HashiCorp Vault or AWS Secrets Manager.',
  },
  {
    id: 155,
    slug: 'implementing-data-load-jobs-from-nosql-sources',
    question: 'How to get started with implementing data load jobs from NoSQL sources?',
    answer: 'Dataiku has connectors for several NoSQL databases like MongoDB and Elasticsearch. Go to Administration > Connections to set up the connection. Then, in a project, you can create a dataset that reads from a collection or index in your NoSQL database.',
  },
  {
    id: 156,
    slug: 'triggering-dataiku-jobs-from-external-schedulers-e-g-airflow',
    question: 'How to get started with triggering Dataiku jobs from external schedulers (e.g. Airflow)?',
    answer: 'Use the Dataiku REST API. Generate an API key in Dataiku. In your Airflow DAG, use the `HttpOperator` to make a POST request to the Dataiku API endpoint for running a scenario. This allows you to integrate Dataiku into a larger data orchestration ecosystem.',
  },
  {
    id: 157,
    slug: 'exporting-processed-data-back-to-cloud-buckets',
    question: 'How to get started with exporting processed data back to cloud buckets?',
    answer: 'Select your final, processed dataset. Choose the "Export" recipe. For the storage location, select your cloud connection (e.g., Amazon S3) and specify the bucket and path where you want to save the output file (e.g., as a CSV or Parquet file).',
  },
  {
    id: 158,
    slug: 'configuring-dataiku-connections-for-live-streaming-sources',
    question: 'How to get started with configuring Dataiku connections for live streaming sources?',
    answer: 'Dataiku has connectors for streaming services like Kafka. In Administration > Connections, set up a connection to your Kafka cluster. You can then create a streaming endpoint in a project that subscribes to a Kafka topic and can apply real-time transformations.',
  },
  {
    id: 159,
    slug: 'integrating-dataiku-with-bi-dashboards-tableau-power-bi',
    question: 'How to get started with integrating Dataiku with BI dashboards (Tableau, Power BI)?',
    answer: 'The primary method is to export a dataset from Dataiku to a SQL database that your BI tool is connected to. You can automate this export with a scenario. For some BI tools, you can also embed individual charts from Dataiku directly into the BI dashboard.',
  },
  {
    id: 160,
    slug: 'migrating-legacy-alteryx-workflows-into-dataiku-flows',
    question: 'How to get started with migrating legacy Alteryx workflows into Dataiku flows?',
    answer: 'This is a manual process of translation. Go through your Alteryx workflow tool by tool. For each tool, find the equivalent visual recipe in Dataiku (e.g., Alteryx "Select" is similar to processors in a Dataiku "Prepare" recipe). Rebuild the logic step-by-step in a Dataiku Flow.',
  },
  {
    id: 161,
    slug: 'writing-reusable-python-utility-libraries-in-dataiku',
    question: 'How to get started with writing reusable Python utility libraries in Dataiku?',
    answer: 'In your project, go to the "Libraries" section. Here you can create Python files (`.py`). Define your reusable functions in these files. From any Python recipe or notebook in the project, you can then import these functions and use them.',
  },
  {
    id: 162,
    slug: 'unit-testing-python-recipes-with-builtin-test-frameworks',
    question: 'How to get started with unit testing Python recipes with builtin test frameworks?',
    answer: 'In a Python recipe, you can create a test case. Dataiku provides a framework where you can define test inputs (as small, sample DataFrames) and assert the expected output after your recipe\'s code has run. This helps ensure your code is correct.',
  },
  {
    id: 163,
    slug: 'using-dataikus-vs-code-or-pycharm-integrations',
    question: 'How to get started with using Dataiku’s VS Code or PyCharm integrations?',
    answer: 'Dataiku allows you to link your project to a local copy of the code. This lets you edit Python recipes and libraries in your preferred IDE (like VS Code). The changes you save locally are synced back to the Dataiku project, combining the power of a real IDE with the Dataiku platform.',
  },
  {
    id: 164,
    slug: 'participating-in-peer-code-reviews-of-flow-libraries',
    question: 'How to get started with participating in peer code reviews of flow libraries?',
    answer: 'When using the Git integration for your project, follow a standard pull request workflow. When a team member makes changes to a Python library, they create a pull request. You can then review the code changes in your Git provider (e.g., GitHub), add comments, and approve it.',
  },
  {
    id: 165,
    slug: 'collaborating-via-project-level-discussions-and-comments',
    question: 'How to get started with collaborating via project-level discussions and comments?',
    answer: 'Every object in Dataiku (dataset, recipe, dashboard) has a "Discussions" panel. Use this to ask questions, make suggestions, or document decisions. You can @-mention colleagues to notify them. This keeps the conversation tied to the specific context.',
  },
  {
    id: 166,
    slug: 'training-junior-team-members-on-dss-methods',
    question: 'How to get started with training junior team members on DSS methods?',
    answer: 'Pair programming is very effective. Sit with them and build a flow together. Start with visual recipes and explain the logic. Point them to the Dataiku Academy for structured learning paths. Review their work and provide constructive feedback.',
  },
  {
    id: 167,
    slug: 'maintaining-knowledge-artifacts-in-project-wikis',
    question: 'How to get started with maintaining knowledge artifacts in project Wikis?',
    answer: 'The project Wiki is a powerful tool for documentation. Create pages for the project overview, data dictionary, technical specifications, and meeting notes. A well-maintained Wiki is essential for project continuity and onboarding new team members.',
  },
  {
    id: 168,
    slug: 'merging-and-resolving-conflicts-in-git-based-projects',
    question: 'How to get started with merging and resolving conflicts in Git-based projects?',
    answer: 'When you pull changes from the remote Git repository, Dataiku will show you if there are any conflicts (e.g., if you and another developer edited the same recipe). Dataiku provides a visual diff tool to help you see the conflicting changes and choose which version to keep.',
  },
  {
    id: 169,
    slug: 'documenting-code-use-cases-in-project-readme-files',
    question: 'How to get started with documenting code use-cases in project ReadMe files?',
    answer: 'While the Wiki is for detailed documentation, the project homepage can serve as a "ReadMe". Use a text tile on the project homepage to provide a high-level summary of the project, its purpose, and links to the key dashboards or outputs.',
  },
  {
    id: 170,
    slug: 'capturing-business-logic-as-spec-documents',
    question: 'How to get started with capturing business logic as spec documents?',
    answer: 'Before building, create a specification document in the project Wiki. Translate the business requirements into a technical plan. For example, "To calculate customer churn, we will define an active customer as one who has made a purchase in the last 6 months." This links the code back to the business logic.',
  },
  {
    id: 171,
    slug: 'setting-project-access-control-and-permissions',
    question: 'How to get started with setting project access control and permissions?',
    answer: 'In your project, go to Settings > Permissions. Here you can add users or groups and assign them a role (e.g., Reader, Contributor, Administrator). This allows you to control who can see or edit your project, which is crucial for governance.',
  },
  {
    id: 172,
    slug: 'tracking-flow-and-dataset-lineage-in-dss',
    question: 'How to get started with tracking flow and dataset lineage in DSS?',
    answer: 'Lineage is an automatic feature. From any dataset in the Flow, you can right-click and see its upstream and downstream dependencies. For more detail, open the dataset and go to the "Lineage" tab for a column-level graph showing exactly how each field was created.',
  },
  {
    id: 173,
    slug: 'applying-data-governance-templates-and-standards',
    question: 'How to get started with applying data governance templates and standards?',
    answer: 'Establish a set of standards for your organization, such as a mandatory tagging policy for datasets containing PII. You can create a project template that includes standard governance checks and naming conventions to enforce consistency across projects.',
  },
  {
    id: 174,
    slug: 'embedding-data-privacy-logic-in-code-recipes',
    question: 'How to get started with embedding data privacy logic in code recipes?',
    answer: 'In a Python recipe, you can write code to anonymize or mask sensitive data. For example, you could hash email addresses or remove personal identifiers from a dataset before it is used for modeling. Use project variables to control which columns are considered sensitive.',
  },
  {
    id: 175,
    slug: 'enabling-approvals-for-model-deployments-via-governance-flows',
    question: 'How to get started with enabling approvals for model deployments via governance flows?',
    answer: 'Dataiku provides features for a governed model lifecycle. You can require a "sign-off" from specific users before a model can be deployed to production. This creates an audit trail and ensures that models are reviewed before they are put into use.',
  },
  {
    id: 176,
    slug: 'auditing-user-actions-and-modifications-in-dss',
    question: 'How to get started with auditing user actions and modifications in DSS?',
    answer: 'The project "Timeline" shows a log of all changes made to the project, who made them, and when. For a more global view, a Dataiku administrator can access the instance-level audit logs which provide a comprehensive record of all user activities.',
  },
  {
    id: 177,
    slug: 'versioning-datasets-and-models-for-reproducibility',
    question: 'How to get started with versioning datasets and models for reproducibility?',
    answer: 'When you deploy a model from the lab, it becomes a versioned "Saved Model" in the Flow. For datasets, if you need to preserve a specific version, you can use a Python recipe to copy the dataset to a new, version-named dataset (e.g., `customers_2023_01_15`).',
  },
  {
    id: 178,
    slug: 'reviewing-compliance-criteria-for-pipelines',
    question: 'How to get started with reviewing compliance criteria for pipelines?',
    answer: 'Work with your compliance or legal team to understand the requirements (e.g., GDPR, CCPA). Document these in the project Wiki. Use Dataiku features like PII tagging, access controls, and lineage graphs to demonstrate that your pipeline meets these criteria.',
  },
  {
    id: 179,
    slug: 'archiving-stale-flows-while-preserving-lineage',
    question: 'How to get started with archiving stale flows while preserving lineage?',
    answer: 'Instead of deleting an old project, you can export it as a project bundle and store it elsewhere. This serves as an archive. The exported bundle contains all the project metadata, including the flow and lineage information.',
  },
  {
    id: 180,
    slug: 'capturing-metadata-in-preparation-and-modeling-steps',
    question: 'How to get started with capturing metadata in preparation and modeling steps?',
    answer: 'Use the "Description" field on every dataset and recipe to explain its purpose. Use tags to categorize objects (e.g., `data_source:sap`, `status:production`). For columns, you can add descriptions in the "Settings" tab of a dataset. This metadata is searchable and essential for governance.',
  },
  {
    id: 181,
    slug: 'profiling-heavy-recipes-for-execution-bottlenecks',
    question: 'How to get started with profiling heavy recipes for execution bottlenecks?',
    answer: 'When a job runs, Dataiku records the time taken for each recipe. Go to the "Jobs" menu and find your job. The timings will show you which recipes are the slowest. This is the first step in identifying where to focus your optimization efforts.',
  },
  {
    id: 182,
    slug: 'switching-memory-vs-push-down-mode-per-recipe',
    question: 'How to get started with switching memory vs push-down mode per recipe?',
    answer: 'In the settings of a visual recipe (like Prepare or Join), go to the "Advanced" tab. You will find a dropdown for the "Execution Engine". For large datasets stored in a database or Hadoop, change this from the default "In-Memory" to your database or Spark engine.',
  },
  {
    id: 183,
    slug: 'distributing-compute-via-spark-in-large-scale-jobs',
    question: 'How to get started with distributing compute via Spark in large-scale jobs?',
    answer: 'If Dataiku is connected to a Spark cluster, you can write PySpark code in a Python recipe. Set the recipe to run on Spark. This allows you to leverage the full power of distributed computing for massive data transformations.',
  },
  {
    id: 184,
    slug: 'optimizing-join-strategies-on-key-datasets',
    question: 'How to get started with optimizing join strategies on key datasets?',
    answer: 'In a "Join" recipe, go to the "Advanced" tab. You can influence the join strategy. For example, in a Spark engine, you can provide hints for a "broadcast" join if one of your datasets is small, which can significantly improve performance.',
  },
  {
    id: 185,
    slug: 'partitioning-datasets-to-improve-performance',
    question: 'How to get started with partitioning datasets to improve performance?',
    answer: 'When your data has a clear dimension like time or country, you can partition it. In the "Settings" of a dataset, enable partitioning and choose the column to partition on (e.g., a date column). When you build the dataset, Dataiku will create a separate file for each partition. This allows recipes to only process the partitions they need, which is much faster.',
  },
  {
    id: 186,
    slug: 'minimizing-data-duplication-in-project-flows',
    question: 'How to get started with minimizing data duplication in project flows?',
    answer: 'Create centralized, "golden" datasets in one project or Flow Zone. Have other projects or flows read from these datasets instead of creating their own copies. This reduces storage costs and ensures everyone is working from the same source of truth.',
  },
  {
    id: 187,
    slug: 'leveraging-caching-and-reusable-datasets',
    question: 'How to get started with leveraging caching and reusable datasets?',
    answer: 'This is a core concept in Dataiku. The output of every recipe is cached. If you have a complex data preparation flow that is used by multiple downstream models, build it once. The downstream recipes will reuse the cached output, saving computation time.',
  },
  {
    id: 188,
    slug: 'trimming-intermediate-dataset-dimensions-to-reduce-size',
    question: 'How to get started with trimming intermediate dataset dimensions to reduce size?',
    answer: 'After a join or aggregation, you often have columns that are no longer needed. Add a "Prepare" recipe immediately after and use the "Remove" processor to delete these unnecessary columns. This makes the intermediate dataset smaller and faster to process downstream.',
  },
  {
    id: 189,
    slug: 'tuning-resource-allocation-for-api-nodes',
    question: 'How to get started with tuning resource allocation for API nodes?',
    answer: 'In the API Deployer, you can configure the infrastructure for your API services. This includes setting the number of replicas (for high availability) and the memory and CPU allocated to each replica. You may need to increase these resources for complex models or high-traffic endpoints.',
  },
  {
    id: 190,
    slug: 'benchmarking-run-times-between-dev-and-prod-environments',
    question: 'How to get started with benchmarking run-times between dev and prod environments?',
    answer: 'After deploying a project to production, run the main scenarios and record the job timings. Compare these to the timings from your development environment. Differences can highlight configuration issues or resource constraints in the production environment that need to be addressed.',
  },
  {
    id: 191,
    slug: 'reviewing-release-notes-for-latest-dataiku-dss-versions',
    question: 'How to get started with reviewing release notes for latest Dataiku DSS versions?',
    answer: 'Bookmark the official Dataiku documentation page for release notes. With each new major and minor release, read through the notes to understand the new features, bug fixes, and any potential breaking changes. This helps you plan for upgrades and adopt new functionality.',
  },
  {
    id: 192,
    slug: 'trying-new-plugins-and-features-from-dataiku-community',
    question: 'How to get started with trying new plugins and features from Dataiku community?',
    answer: 'In Dataiku, go to Administration > Plugins > Store. Here you can browse a catalog of plugins built by Dataiku and the community. You can install them directly from this interface to add new functionality to your instance, like new dataset connectors or visual recipes.',
  },
  {
    id: 193,
    slug: 'exploring-tutorials-from-dataiku-developer-guide',
    question: 'How to get started with exploring tutorials from Dataiku Developer Guide?',
    answer: 'The official Dataiku documentation contains a Developer guide with tutorials on advanced topics like creating custom plugins, using the APIs, and advanced deployment scenarios. Set aside time to work through these tutorials in a sandbox environment.',
  },
  {
    id: 194,
    slug: 'documenting-lessons-learned-in-project-retrospectives',
    question: 'How to get started with documenting lessons learned in project retrospectives?',
    answer: 'After a project milestone, hold a retrospective with the team. Discuss what went well and what could be improved. Document the key takeaways in the project Wiki. This creates a knowledge base that helps the team avoid repeating mistakes in future projects.',
  },
  {
    id: 195,
    slug: 'participating-in-team-brown-bag-sessions-or-friday-university',
    question: 'How to get started with participating in team brown-bag sessions or Friday University?',
    answer: 'Organize informal learning sessions where team members can share what they are working on or a new technique they have learned in Dataiku. This is a great way to spread knowledge across the team. You could present a cool project you built or a new feature you discovered.',
  },
  {
    id: 196,
    slug: 'applying-best-practices-from-git-based-workflows',
    question: 'How to get started with applying best practices from Git-based workflows?',
    answer: 'Even though much of Dataiku is visual, apply software engineering best practices. Use feature branches in Git for new development, write clear commit messages, use pull requests for code review, and tag releases. This brings discipline and reproducibility to your data projects.',
  },
  {
    id: 197,
    slug: 'evaluating-metrics-drift-to-guide-model-retraining',
    question: 'How to get started with evaluating metrics drift to guide model retraining?',
    answer: 'Regularly run a "Model Evaluation" recipe on your deployed model. If you see a significant drop in a key metric like AUC or accuracy, it is a strong signal that the model needs to be retrained on more recent data to adapt to changing patterns.',
  },
  {
    id: 198,
    slug: 'experimenting-with-new-ml-approaches-in-sandbox-flows',
    question: 'How to get started with experimenting with new ML approaches in sandbox flows?',
    answer: 'Don\'t be afraid to try new things. In a safe sandbox project, create a branch of your main flow. In this branch, try a different modeling approach, a new feature engineering technique, or a different algorithm. This is how you innovate and find better solutions.',
  },
  {
    id: 199,
    slug: 'benchmarking-dss-pipelines-versus-airflow-alteryx-alternatives',
    question: 'How to get started with benchmarking DSS pipelines versus Airflow/Alteryx alternatives?',
    answer: 'Take a representative data pipeline and build it in both Dataiku and another tool like Alteryx. Then, measure the performance of both, not just in terms of run time, but also development time, maintainability, and governance capabilities. This can help justify the choice of platform.',
  },
  {
    id: 200,
    slug: 'creating-your-personal-portfolio-of-dataiku-flows-and-dashboards',
    question: 'How to get started with creating your personal portfolio of Dataiku flows and dashboards?',
    answer: 'In a personal sandbox instance or project, work on projects that interest you using public datasets. Build end-to-end flows, create insightful dashboards, and document your work well. You can then take screenshots or create a presentation of this portfolio to showcase your skills to potential employers.',
  },
];

export const getQuestionBySlug = (slug: string): Question | undefined => {
  return questions.find(q => q.slug === slug);
}

export const getQuestionById = (id: number): Question | undefined => {
  return questions.find(q => q.id === id);
}
