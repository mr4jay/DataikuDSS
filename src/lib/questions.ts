export interface Question {
  id: number;
  slug: string;
  question: string;
  answer: string;
}

export const questions: Question[] = [
  {
    id: 1,
    slug: 'building-your-first-dataiku-dss-flow',
    question: 'How to get started with building your first Dataiku DSS flow?',
    answer: 'Start by creating a new project. In your project\'s Flow, click the "+ Dataset" button to import your data. Once you have a dataset, select it and choose a visual recipe like "Prepare" from the right-hand panel to start transforming your data. Connect recipes and datasets to build a sequence of data processing steps, forming your first flow.',
  },
  {
    id: 2,
    slug: 'designing-end-to-end-etl-pipelines-in-dataiku-dss',
    question: 'How to get started with designing end-to-end ETL pipelines in Dataiku DSS?',
    answer: 'Begin by mapping out your data journey: identify your data sources, the transformations required, and the final output destination. In Dataiku, this translates to a series of recipes in a Flow. Start with ingestion recipes (e.g., from a database), apply transformation recipes (Prepare, Join, Group), and end with an export recipe to write the results to a database or file system.',
  },
  {
    id: 3,
    slug: 'ingesting-data-from-databases-into-dataiku',
    question: 'How to get started with ingesting data from databases into Dataiku?',
    answer: 'From your project Flow, click "+ Dataset" and select your database type (e.g., PostgreSQL, SQL Server). You will need to configure a connection to your database with the necessary credentials. Once connected, you can browse tables and import them as datasets into your flow.',
  },
  {
    id: 4,
    slug: 'loading-csv-excel-data-into-dataiku-dss',
    question: 'How to get started with loading CSV/Excel data into Dataiku DSS?',
    answer: 'In your project, click "+ Dataset" and choose "Upload your files". You can drag and drop your CSV or Excel files directly. Dataiku will automatically detect the format and schema. Review the preview and settings, then create the dataset to add it to your Flow.',
  },
  {
    id: 5,
    slug: 'integrating-rest-apis-as-dataiku-datasets',
    question: 'How to get started with integrating REST APIs as Dataiku datasets?',
    answer: 'Use the "API" dataset connector. In your Flow, click "+ Dataset" > "API". You will need to configure the API endpoint URL, any authentication methods (like API keys), and parameters. Dataiku can then call the API and parse the JSON response into a tabular dataset.',
  },
  {
    id: 6,
    slug: 'working-with-cloud-storage-sources-s3-gcs-azure-blob',
    question: 'How to get started with working with cloud storage sources (S3, GCS, Azure Blob)?',
    answer: 'First, set up a connection to your cloud provider in "Administration" > "Connections". Then, in your Flow, create a new dataset and select the appropriate source (e.g., "Amazon S3"). You can then browse your buckets and folders to select the files you want to use as a dataset.',
  },
  {
    id: 7,
    slug: 'combining-disparate-data-sources-into-unified-datasets',
    question: 'How to get started with combining disparate data sources into unified datasets?',
    answer: 'Use the "Join" or "Stack" visual recipes. Import your different datasets into the Flow. Select one dataset, then choose the "Join" recipe to combine it with another based on a common key. Use the "Stack" recipe to append datasets with the same schema.',
  },
  {
    id: 8,
    slug: 'creating-multi-step-recipe-chains',
    question: 'How to get started with creating multi-step recipe chains?',
    answer: 'A multi-step recipe chain is simply a sequence in your Flow. Start with a dataset, apply a recipe (e.g., Prepare). The output of this recipe is a new dataset. You can then select this output dataset and apply another recipe (e.g., Join). This creates a chain of dependencies that defines your data pipeline.',
  },
  {
    id: 9,
    slug: 'structuring-reusable-flow-zones',
    question: 'How to get started with structuring reusable Flow Zones?',
    answer: 'Flow Zones help organize large projects. In your Flow, right-click on the canvas and select "Create Flow Zone". Give it a name that represents a logical part of your project (e.g., "Data Ingestion", "Feature Engineering"). You can then drag and drop related datasets and recipes into this zone to keep your Flow clean and modular.',
  },
  {
    id: 10,
    slug: 'implementing-branching-and-looping-in-dataiku-flows',
    question: 'How to get started with implementing branching and looping in Dataiku flows?',
    answer: 'While Dataiku flows are primarily directed acyclic graphs, you can implement conditional logic (branching) using Scenarios with Python code to check a condition and run different jobs. Looping can be achieved by creating a scenario that calls itself or processes data iteratively, for example by using variables to process data for different dates or categories.',
  },
  {
    id: 11,
    slug: 'using-prepare-recipes-for-data-cleaning',
    question: 'How to get started with using Prepare recipes for data cleaning?',
    answer: 'The Prepare recipe is the workhorse of data cleaning. Select a dataset and click "Prepare". In the recipe, you can add steps from the processor library. Common cleaning steps include handling missing values (e.g., "Clear cells with invalid values"), parsing dates, splitting text, and using formulas for custom transformations.',
  },
  {
    id: 12,
    slug: 'writing-python-recipes-in-dataiku-dss',
    question: 'How to get started with writing Python recipes in Dataiku DSS?',
    answer: 'Select a dataset in your Flow, then from the right panel choose "+ Recipe" > "Python". This opens a code editor. Use the Dataiku API to read your input dataset as a Pandas DataFrame (`dataiku.Dataset("input_dataset_name").get_dataframe()`), perform your transformations, and write the result to an output dataset.',
  },
  {
    id: 13,
    slug: 'writing-sql-recipes-inside-dataiku',
    question: 'How to get started with writing SQL recipes inside Dataiku?',
    answer: 'If your datasets are on a SQL database, you can use a SQL recipe. Select your input dataset, then choose "+ Recipe" > "SQL". Write your SQL query in the editor. You can reference input datasets directly by their name in the `FROM` clause. The result of your query will be the output dataset.',
  },
  {
    id: 14,
    slug: 'adding-custom-python-code-into-workflows',
    question: 'How to get started with adding custom Python code into workflows?',
    answer: 'Custom Python code can be added via Python recipes, notebooks, or by creating custom plugins. For in-flow logic, a Python recipe is ideal. For exploratory analysis, use a Python notebook. For reusable components, consider developing a custom Python-based plugin.',
  },
  {
    id: 15,
    slug: 'combining-r-code-and-dataiku-recipes',
    question: 'How to get started with combining R code and Dataiku recipes?',
    answer: 'Similar to Python, you can use an R recipe. Select an input dataset, click "+ Recipe" > "R". Use the Dataiku R API to read data, apply your R code, and write back the output. This allows you to integrate R-based transformations seamlessly into your visual recipe flows.',
  },
  {
    id: 16,
    slug: 'using-macros-and-global-variables-in-recipes',
    question: 'How to get started with using macros and global variables in recipes?',
    answer: 'Define variables in your project\'s "Variables" section. You can then reference these variables in your recipes (e.g., in formulas or filters) using the `${variable_name}` syntax. This is useful for parameterizing your flows, for example, by using a variable for a file path or a date filter.',
  },
  {
    id: 17,
    slug: 'parsing-json-and-nested-data-in-dataiku',
    question: 'How to get started with parsing JSON and nested data in Dataiku?',
    answer: 'In a Prepare recipe, if you have a column with JSON data, use the "Unnest object" or "Flatten object" processor. This will extract the keys from the JSON object into separate columns, making the nested data accessible for analysis.',
  },
  {
    id: 18,
    slug: 'feature-engineering-using-formula-steps',
    question: 'How to get started with feature engineering using formula steps?',
    answer: 'In a Prepare recipe, the "Formula" processor is very powerful for creating new features. You can write expressions using a syntax similar to spreadsheet formulas to perform mathematical operations, manipulate strings, use conditional logic (if/then/else), and create new columns based on existing ones.',
  },
  {
    id: 19,
    slug: 'implementing-fuzzy-joins-in-dataiku',
    question: 'How to get started with implementing fuzzy joins in Dataiku?',
    answer: 'Dataiku offers a "Fuzzy Join" visual recipe. This is useful when you need to join datasets on keys that are similar but not identical (e.g., "Dataiku Inc." and "Dataiku"). Select your two datasets and the columns to join on, then configure the similarity metric and threshold.',
  },
  {
    id: 20,
    slug: 'handling-missing-values-via-recipes',
    question: 'How to get started with handling missing values via recipes?',
    answer: 'In a Prepare recipe, select a column with missing values. From the processor library, you have several options: "Remove rows" to delete them, "Impute" to fill with a mean, median, or constant value, or "Create indicator" to make a new column that flags where values were missing.',
  },
  {
    id: 21,
    slug: 'using-automl-in-dataiku-dss',
    question: 'How to get started with using AutoML in Dataiku DSS?',
    answer: 'From your Flow, select a clean, prepared dataset. In the right panel, click "Lab", then "New Analysis". Choose "Prediction" or "Clustering", and select your target variable. Dataiku\'s AutoML will automatically handle feature engineering, train several models, and present a leaderboard with performance metrics.',
  },
  {
    id: 22,
    slug: 'building-a-random-forest-classifier-visually',
    question: 'How to get started with building a random forest classifier visually?',
    answer: 'In the Visual Analysis Lab for a prediction task, AutoML will often train a Random Forest model. You can also explicitly select it from the list of algorithms. You can then tune its hyperparameters, like the number of trees and max depth, directly in the UI.',
  },
  {
    id: 23,
    slug: 'building-regression-models-in-dataiku-dss',
    question: 'How to get started with building regression models in Dataiku DSS?',
    answer: 'The process is similar to classification. In the Visual Analysis Lab, create a new analysis. If your target variable is numerical, Dataiku will automatically set the task as regression. It will then train and evaluate suitable regression models like Linear Regression, Ridge, or Gradient Boosted Trees.',
  },
  {
    id: 24,
    slug: 'evaluating-models-roc-f1-precision-recall',
    question: 'How to get started with evaluating models (ROC, F1, precision/recall)?',
    answer: 'After a model is trained in the Visual Analysis Lab, the results page provides extensive evaluation metrics. For classifiers, you will find the ROC curve, confusion matrix, precision, recall, and F1-score. You can analyze these to understand your model\'s performance and choose the best one for your use case.',
  },
  {
    id: 25,
    slug: 'extracting-feature-importance-from-model-runs',
    question: 'How to get started with extracting feature importance from model runs?',
    answer: 'On the model results page, there is a "Feature Importance" section. This shows which input variables had the most influence on the model\'s predictions. For tree-based models, this is often based on Gini impurity or information gain.',
  },
  {
    id: 26,
    slug: 'implementing-cross-validation-and-a-b-tests',
    question: 'How to get started with implementing cross-validation and A/B tests?',
    answer: 'Cross-validation is a standard part of the model training process in Dataiku and is configured in the "Design" tab of the analysis. For A/B testing deployed models, you can create two versions of your model (or two different models) in the Flow and use a "Split" recipe to route traffic to both, then compare their performance on live data.',
  },
  {
    id: 27,
    slug: 'training-deep-learning-models-using-notebooks',
    question: 'How to get started with training deep learning models using notebooks?',
    answer: 'Dataiku supports deep learning frameworks like TensorFlow and Keras. Create a Python notebook and set up a code environment with the necessary libraries. You can then write your deep learning code as you normally would, using the Dataiku API to load data and save your trained model back to the Flow.',
  },
  {
    id: 28,
    slug: 'integrating-scikit-learn-and-tensorflow-in-dataiku',
    question: 'How to get started with integrating scikit-learn and TensorFlow in Dataiku?',
    answer: 'These libraries are the backbone of Dataiku\'s visual machine learning. You can also use them directly in Python recipes or notebooks. Create a code environment, add `scikit-learn` or `tensorflow` as a dependency, and then you are free to use their full functionality within your Dataiku project.',
  },
  {
    id: 29,
    slug: 'deploying-ml-models-inside-dataiku',
    question: 'How to get started with deploying ML models inside Dataiku?',
    answer: 'From a trained model in the Visual Analysis Lab, click the "Deploy" button. This creates a "Saved Model" object in your Flow. You can then use this deployed model with a "Score" recipe to make predictions on new data batches.',
  },
  {
    id: 30,
    slug: 'monitoring-model-performance-over-time',
    question: 'How to get started with monitoring model performance over time?',
    answer: 'Once a model is deployed, you can create a "Model Evaluation" recipe to track its performance. This recipe compares the model\'s predictions against new ground truth data. You can set up scenarios to run this evaluation regularly and alert you if the model\'s performance degrades (model drift).',
  },
  {
    id: 31,
    slug: 'building-scenarios-in-dataiku-dss',
    question: 'How to get started with building Scenarios in Dataiku DSS?',
    answer: 'Navigate to the "Scenarios" section of your project. Create a new scenario and give it a name. In the "Steps" tab, add steps like "Build / Train" to rebuild a dataset or retrain a model. You can also add custom Python code steps for more complex logic.',
  },
  {
    id: 32,
    slug: 'scheduling-pipelines-via-triggers-or-cron',
    question: 'How to get started with scheduling pipelines via triggers or CRON?',
    answer: 'In a Scenario, go to the "Settings" tab. You can add "Triggers" to define when the scenario should run. A "Time-based" trigger lets you set a schedule (e.g., daily, hourly), similar to CRON jobs. You can also trigger scenarios based on changes in data or via the API.',
  },
  {
    id: 33,
    slug: 'automating-model-retraining-workflows',
    question: 'How to get started with automating model retraining workflows?',
    answer: 'Create a scenario that includes a step to "Train" your saved model. This step will rebuild the model using the latest data from its input dataset. Schedule this scenario to run periodically (e.g., weekly) to keep your model fresh.',
  },
  {
    id: 34,
    slug: 'configuring-success-failure-email-alerts',
    question: 'How to get started with configuring success/failure email alerts?',
    answer: 'In a Scenario, go to the "Reporters" tab. Add a new reporter and select "Mail". You can configure it to send an email on success, failure, or completion of the scenario run. This is essential for monitoring your automated jobs.',
  },
  {
    id: 35,
    slug: 'building-data-quality-validation-steps-in-scenarios',
    question: 'How to get started with building data quality validation steps in Scenarios?',
    answer: 'First, define data quality rules on your dataset in the "Status" tab. Then, in a scenario, add a step to "Run checks". You can configure the scenario to fail if the data quality checks do not pass, preventing bad data from moving through your pipeline.',
  },
  {
    id: 36,
    slug: 'using-scenarios-to-automate-data-ingestion-jobs',
    question: 'How to get started with using scenarios to automate data ingestion jobs?',
    answer: 'Create a scenario whose main step is to build the datasets that represent your ingested data. For example, a step to "Build" your S3 dataset. Schedule this scenario to run regularly to fetch the latest data from your sources.',
  },
  {
    id: 37,
    slug: 'managing-flow-dependencies-programmatically',
    question: 'How to get started with managing flow dependencies programmatically?',
    answer: 'In a scenario, you can use a Python step to interact with the Dataiku API. You can get handles to datasets or recipes, check their status, and decide which parts of the flow to build. This gives you fine-grained control over the execution of your pipeline.',
  },
  {
    id: 38,
    slug: 'integrating-dataiku-jobs-into-ci-cd-pipelines',
    question: 'How to get started with integrating Dataiku jobs into CI/CD pipelines?',
    answer: 'Your CI/CD tool (like Jenkins or Azure DevOps) can use the Dataiku REST API to trigger scenario runs. You can have your CI/CD pipeline first deploy the project code to Dataiku and then trigger a scenario to run tests or build production datasets.',
  },
  {
    id: 39,
    slug: 'linking-dataiku-with-jenkins-or-azure-devops',
    question: 'How to get started with linking Dataiku with Jenkins or Azure DevOps?',
    answer: 'The primary method is via the Dataiku REST API. Create an API key in Dataiku with the necessary permissions. In your Jenkins or Azure DevOps pipeline script, make an HTTP request to the Dataiku API endpoint for running a scenario.',
  },
  {
    id: 40,
    slug: 'using-rest-apis-to-trigger-dataiku-scenarios',
    question: 'How to get started with using REST APIs to trigger Dataiku scenarios?',
    answer: 'Generate an API key in Dataiku. Find the endpoint for running a scenario in the API documentation. It will look something like `POST /projects/{projectKey}/scenarios/{scenarioId}/run`. Use a tool like `curl` or any programming language to make a POST request to this endpoint to trigger the scenario.',
  },
  {
    id: 41,
    slug: 'connecting-dataiku-to-aws-redshift',
    question: 'How to get started with connecting Dataiku to AWS Redshift?',
    answer: 'In Dataiku, go to "Administration" > "Connections". Create a new connection of type "Redshift". You will need to provide the host, database name, user, and password for your Redshift cluster. Once saved, you can use this connection to create new datasets from Redshift tables.',
  },
  {
    id: 42,
    slug: 'connecting-dataiku-to-snowflake-data-warehouse',
    question: 'How to get started with connecting Dataiku to Snowflake data warehouse?',
    answer: 'Go to "Administration" > "Connections" and create a "Snowflake" connection. Fill in your Snowflake account URL, user credentials, and default warehouse/database. Dataiku can then leverage Snowflake for both data storage and computation (push-down).',
  },
  {
    id: 43,
    slug: 'using-dataiku-with-gcp-big-query',
    question: 'How to get started with using Dataiku with GCP big-query?',
    answer: 'Set up a Google Cloud Platform connection in "Administration" > "Connections", authenticating with a service account. Then, you can create datasets from BigQuery tables. Dataiku will push down computation to BigQuery whenever possible for optimal performance.',
  },
  {
    id: 44,
    slug: 'integrating-hadoop-or-spark-with-dataiku',
    question: 'How to get started with integrating Hadoop or Spark with Dataiku?',
    answer: 'Dataiku needs to be installed on an edge node of your Hadoop/Spark cluster. The installation process will guide you in configuring Dataiku to connect to HDFS and use YARN for submitting Spark jobs. This allows recipes to run on Spark.',
  },
  {
    id: 45,
    slug: 'running-spark-based-recipes-in-dss',
    question: 'How to get started with running Spark-based recipes in DSS?',
    answer: 'Once Dataiku is connected to a Spark cluster, you can change the execution engine for many visual recipes (like Prepare, Join) from the default "In-Memory" to "Spark". You can also write PySpark code directly in Python recipes.',
  },
  {
    id: 46,
    slug: 'leveraging-cloud-compute-for-large-scale-pipelines',
    question: 'How to get started with leveraging cloud compute for large-scale pipelines?',
    answer: 'When using data warehouses like Snowflake or BigQuery, Dataiku automatically pushes down computation. For other cases, you can configure Dataiku to spin up temporary cloud compute clusters (e.g., on Kubernetes or through services like Databricks) to run specific, resource-intensive recipes.',
  },
  {
    id: 47,
    slug: 'deploying-dataiku-in-cloud-environments',
    question: 'How to get started with deploying Dataiku in cloud environments?',
    answer: 'Dataiku can be deployed on a cloud virtual machine (like an EC2 instance on AWS). For more scalable deployments, consider using the official Dataiku images for Docker or Kubernetes, which allows for containerized and managed deployments.',
  },
  {
    id: 48,
    slug: 'using-dataiku-on-kubernetes-docker',
    question: 'How to get started with using Dataiku on Kubernetes/Docker?',
    answer: 'Dataiku provides official Docker images. You can use these images to run Dataiku as a container. For production, it is recommended to use Kubernetes to manage the Dataiku containers, handle scaling, and ensure high availability.',
  },
  {
    id: 49,
    slug: 'optimizing-performance-on-big-data-jobs',
    question: 'How to get started with optimizing performance on big data jobs?',
    answer: 'The key is to push computation to where the data lives. Use SQL-based recipes for data in databases. Use Spark for data in HDFS or cloud storage. In the "Prepare" recipe, check the "Execution Engine" and ensure it is set to Spark or your database for large datasets, not "In-Memory".',
  },
  {
    id: 50,
    slug: 'migrating-alteryx-workflows-into-dataiku',
    question: 'How to get started with migrating Alteryx workflows into Dataiku?',
    answer: 'There is no direct automatic conversion. The process involves manually recreating the Alteryx workflow logic in Dataiku. Map Alteryx tools to Dataiku visual recipes (e.g., Alteryx "Join" tool becomes a Dataiku "Join" recipe). This is a good opportunity to refactor and optimize the pipeline.',
  },
  {
    id: 51,
    slug: 'creating-dashboards-inside-dataiku',
    question: 'How to get started with creating dashboards inside Dataiku?',
    answer: 'In your project, go to the "Dashboards" section. Create a new dashboard. Then, click "Add Tile" to add content. You can add charts, metrics, dataset previews, and text. Charts must first be created in the "Charts" tab of a dataset.',
  },
  {
    id: 52,
    slug: 'exporting-datasets-to-tableau-power-bi',
    question: 'How to get started with exporting datasets to Tableau/Power BI?',
    answer: 'You can create an "Export" recipe that writes a dataset to a format and location that your BI tool can read (e.g., a SQL database table). Some BI tools also have connectors that can read directly from Dataiku-managed datasets.',
  },
  {
    id: 53,
    slug: 'embedding-dataiku-insights-into-bi-tools',
    question: 'How to get started with embedding Dataiku insights into BI tools?',
    answer: 'You can export datasets from Dataiku to be consumed by BI tools. Additionally, you can embed individual Dataiku charts or dashboards into other web applications (including some BI tools) using their "Share" feature, which provides an HTML embed snippet.',
  },
  {
    id: 54,
    slug: 'designing-kpi-dashboards-within-dss',
    question: 'How to get started with designing KPI dashboards within DSS?',
    answer: 'First, use recipes to compute your KPIs and store them in a dataset. Then, in the "Metrics" tab of that dataset, you can define and compute metrics. These metrics can then be added as tiles to a Dataiku dashboard to create a focused KPI monitoring view.',
  },
  {
    id: 55,
    slug: 'scheduling-excel-report-generation-in-dataiku',
    question: 'How to get started with scheduling Excel report generation in Dataiku?',
    answer: 'Create an "Export" recipe that outputs your dataset in Excel format. Then, create a scenario that builds this export recipe. Schedule this scenario to run as needed. The output Excel file can be stored on the Dataiku server or sent to a cloud storage location.',
  },
  {
    id: 56,
    slug: 'automating-slack-or-email-report-distribution',
    question: 'How to get started with automating slack or email report distribution?',
    answer: 'In a scenario, under the "Reporters" tab, you can add Slack or Mail reporters. You can configure them to send a message upon scenario completion. To attach a report, you can include a link to the exported file or dashboard.',
  },
  {
    id: 57,
    slug: 'exporting-model-predictions-for-stakeholder-review',
    question: 'How to get started with exporting model predictions for stakeholder review?',
    answer: 'After using a "Score" recipe to generate predictions on new data, the output dataset contains the predictions. You can then use an "Export" recipe to save this dataset as a CSV or Excel file, which can be easily shared with stakeholders.',
  },
  {
    id: 58,
    slug: 'building-stakeholder-ready-reports-in-dataiku',
    question: 'How to get started with building stakeholder-ready reports in Dataiku?',
    answer: 'Use Dataiku Dashboards. They are designed for this purpose. Combine charts, key metrics, and text tiles with explanations to create a narrative. You can then share a link to the dashboard with stakeholders.',
  },
  {
    id: 59,
    slug: 'documenting-pipelines-and-outputs-effectively',
    question: 'How to get started with documenting pipelines and outputs effectively?',
    answer: 'Use the built-in documentation features. Give every object (dataset, recipe) a clear name and description. Use the project "Wiki" for high-level documentation. In the Flow, use comments and Flow Zones to explain different parts of your pipeline.',
  },
  {
    id: 60,
    slug: 'training-end-users-on-self-service-analytics',
    question: 'How to get started with training end users on self-service analytics?',
    answer: 'Create well-documented, "golden" datasets for them to use. Build example dashboards and Dataiku applications that they can use and learn from. The Dataiku Academy also provides excellent resources for training new users.',
  },
  {
    id: 61,
    slug: 'documenting-dataiku-flows-and-steps',
    question: 'How to get started with documenting Dataiku flows and steps?',
    answer: 'Every object in the Flow has a "Summary" tab where you can add a description. For recipes, you can add comments to individual steps. Use the project Wiki for overall documentation and a "TODO" list.',
  },
  {
    id: 62,
    slug: 'annotating-datasets-and-recipes-for-governance',
    question: 'How to get started with annotating datasets and recipes for governance?',
    answer: 'Use tags to categorize your datasets and recipes (e.g., "PII", "Finance"). You can also add custom metadata in the "Summary" tab of each object. This information is searchable and helps in governing your projects.',
  },
  {
    id: 63,
    slug: 'applying-data-quality-checks-automatically',
    question: 'How to get started with applying data quality checks automatically?',
    answer: 'Define data quality rules in the "Status" tab of a dataset. Then, create a scenario with a step to "Run checks" on that dataset. Schedule this scenario to run after your data ingestion to automatically validate new data.',
  },
  {
    id: 64,
    slug: 'creating-reusable-metadata-and-standards',
    question: 'How to get started with creating reusable metadata and standards?',
    answer: 'Establish a tagging convention for your projects. Create project templates that include standard Flow Zones and naming conventions. For code, create a library of reusable functions in the project\'s library folder.',
  },
  {
    id: 65,
    slug: 'implementing-lineage-and-impact-tracing',
    question: 'How to get started with implementing lineage and impact tracing?',
    answer: 'Dataiku automatically tracks lineage. In the Flow, you can see the upstream and downstream dependencies of any object. In the "Lineage" tab of a dataset, you can see a detailed, column-level graph of how each column was created.',
  },
  {
    id: 66,
    slug: 'applying-access-controls-and-permissions',
    question: 'How to get started with applying access controls and permissions?',
    answer: 'Access control is managed through user groups. In "Administration" > "Security", you can create groups and assign permissions to them (e.g., read, write, administer) on a per-project basis.',
  },
  {
    id: 67,
    slug: 'version-controlling-dataiku-projects-using-git',
    question: 'How to get started with version controlling Dataiku projects using Git?',
    answer: 'In the project settings, you can link your Dataiku project to a remote Git repository (like on GitHub or GitLab). This allows you to commit changes, create branches, and pull updates, integrating your Dataiku development into a standard Git workflow.',
  },
  {
    id: 68,
    slug: 'maintaining-audit-trails-within-dss',
    question: 'How to get started with maintaining audit trails within DSS?',
    answer: 'Dataiku automatically logs all changes made to a project in the "Timeline" view. For more detailed auditing, especially for regulatory compliance, you can look at the backend logs of the Dataiku instance.',
  },
  {
    id: 69,
    slug: 'aligning-pipelines-with-compliance-policies',
    question: 'How to get started with aligning pipelines with compliance policies?',
    answer: 'Use features like tagging for PII data, access controls to restrict who can see sensitive data, and thorough documentation. The column-level lineage is also crucial for demonstrating to auditors how data is being used.',
  },
  {
    id: 70,
    slug: 'establishing-coding-best-practices-in-team',
    question: 'How to get started with establishing coding best practices in team?',
    answer: 'Create a set of guidelines for your team. This can include naming conventions, a standard structure for Python code in recipes, and a requirement for comments. Use the project Wiki to document these best practices. Code reviews can be done through the Git integration.',
  },
  {
    id: 71,
    slug: 'embedding-generative-ai-models-in-pipelines',
    question: 'How to get started with embedding Generative AI models in pipelines?',
    answer: 'Use a Python recipe to call a generative AI model\'s API (e.g., from OpenAI or Hugging Face). You can pass data from a Dataiku dataset as prompts to the model and write the generated text back to an output dataset.',
  },
  {
    id: 72,
    slug: 'building-nlp-text-analytics-flows',
    question: 'How to get started with building NLP/text analytics flows?',
    answer: 'In a Prepare recipe, use the "Text processing" processors. These can perform tasks like tokenization, stop word removal, and sentiment analysis. For more advanced NLP, use a Python recipe with libraries like NLTK or spaCy.',
  },
  {
    id: 73,
    slug: 'integrating-labeling-management-workflows',
    question: 'How to get started with integrating labeling management workflows?',
    answer: 'Dataiku has a "Labeling" plugin that allows you to create tasks for manually labeling data, for example, for training a text classification or computer vision model. This is integrated directly into the Flow.',
  },
  {
    id: 74,
    slug: 'using-time-series-modeling-in-dataiku',
    question: 'How to get started with using time-series modeling in Dataiku?',
    answer: 'Dataiku provides a "Time Series Forecasting" visual analysis lab. You will need a dataset with a date column and a numerical series to forecast. The lab helps you with resampling, feature extraction from the date, and training forecasting models.',
  },
  {
    id: 75,
    slug: 'implementing-computer-vision-pipelines',
    question: 'How to get started with implementing computer vision pipelines?',
    answer: 'You will typically use a Python recipe or notebook with libraries like OpenCV or Pillow to process images. For modeling, you can use deep learning frameworks like Keras/TensorFlow to train models on your image data, which can be stored as folders of files managed by Dataiku.',
  },
  {
    id: 76,
    slug: 'connecting-dataiku-to-kubernetes-clusters',
    question: 'How to get started with connecting Dataiku to Kubernetes clusters?',
    answer: 'In "Administration" > "Containerized Execution", you can configure a connection to a Kubernetes cluster. This allows Dataiku to spin up ephemeral pods to run specific recipes or notebooks, which is great for managing computational resources.',
  },
  {
    id: 77,
    slug: 'using-dataiku-apis-for-advanced-control',
    question: 'How to get started with using Dataiku APIs for advanced control?',
    answer: 'There are two main APIs: the Python API for use within Dataiku (in recipes and notebooks) and the REST API for external control. Start by exploring the Python API to programmatically manipulate objects in your flow. The REST API is for integration with other systems.',
  },
  {
    id: 78,
    slug: 'adding-custom-plugins-to-dss',
    question: 'How to get started with adding custom plugins to DSS?',
    answer: 'Plugins are developed locally in a special developer mode of Dataiku. You can create your own visual recipes, dataset connectors, or processors using a mix of Python and JSON configuration files. Once developed, the plugin can be packaged and installed on other Dataiku instances.',
  },
  {
    id: 79,
    slug: 'deploying-dataiku-rest-endpoints',
    question: 'How to get started with deploying Dataiku REST endpoints?',
    answer: 'You can deploy a trained model as a real-time API endpoint. This is done through the "API Deployer" service. You create an API service, add your trained model as an endpoint, and Dataiku will manage the serving infrastructure.',
  },
  {
    id: 80,
    slug: 'building-interactive-dataiku-apps',
    question: 'How to get started with building interactive Dataiku apps?',
    answer: 'Dataiku Apps are web applications that can be built within a project. You can create a "Web App" from the project view. Options include a standard HTML/CSS/JS app, or apps built with Python frameworks like Dash or Streamlit, which let you create interactive UIs backed by your data and models.',
  },
  {
    id: 81,
    slug: 'collaborating-with-business-analysts-and-data-scientists',
    question: 'How to get started with collaborating with business analysts and data scientists?',
    answer: 'Dataiku is designed for this collaboration. Use the Flow as a common language. BAs can use visual recipes, while DSs can add code recipes. Use the "Discussions" feature on every object to ask questions and share findings. The Wiki is great for shared documentation.',
  },
  {
    id: 82,
    slug: 'translating-business-needs-into-dss-pipelines',
    question: 'How to get started with translating business needs into DSS pipelines?',
    answer: 'Start by breaking down the business problem into a series of logical steps: What data is needed? How does it need to be cleaned and transformed? What is the final output (a dataset, a model, a chart)? Each of these steps can then be implemented as a recipe or object in a Dataiku Flow.',
  },
  {
    id: 83,
    slug: 'reviewing-code-and-mentoring-junior-workers',
    question: 'How to get started with reviewing code and mentoring junior workers?',
    answer: 'If using the Git integration, you can use standard pull request workflows for code review. For visual recipes, sit with the junior developer and review the steps in their Prepare recipe, explaining the logic and suggesting improvements. The visual nature of the Flow makes it easy to see what they are doing.',
  },
  {
    id: 84,
    slug: 'gathering-requirements-for-dataiku-projects',
    question: 'How to get started with gathering requirements for Dataiku projects?',
    answer: 'Work with stakeholders to define the project goals, the key questions to be answered, the available data sources, and the desired outcomes. Create a project brief in the Wiki to document these requirements before you start building.',
  },
  {
    id: 85,
    slug: 'writing-technical-specs-and-process-docs',
    question: 'How to get started with writing technical specs and process docs?',
    answer: 'The project Wiki is the perfect place for this. Create pages for technical specifications, data dictionary, and process documentation. The combination of the visual Flow and the Wiki provides a comprehensive documentation package.',
  },
  {
    id: 86,
    slug: 'participating-in-agile-scrum-development-flows',
    question: 'How to get started with participating in Agile/Scrum development flows?',
    answer: 'Dataiku projects fit well into an Agile framework. Each user story or task can correspond to building a part of the Dataiku Flow. Use the project "TODO" list as a simple backlog. The Git integration allows for sprint-based development with branching and merging.',
  },
  {
    id: 87,
    slug: 'communicating-progress-to-non-technical-stakeholders',
    question: 'How to get started with communicating progress to non-technical stakeholders?',
    answer: 'Use Dataiku Dashboards to share key results and visualizations. The Flow itself, when well-organized with Zones, can also be a useful tool to show the overall process in a simplified way. Schedule regular demos to walk them through the progress.',
  },
  {
    id: 88,
    slug: 'troubleshooting-live-dataiku-jobs',
    question: 'How to get started with troubleshooting live Dataiku jobs?',
    answer: 'When a scenario run fails, go to the "Last runs" tab of the scenario. Click on the failed run to see the logs. The logs will show which step failed and provide a detailed error message. This is the starting point for any troubleshooting.',
  },
  {
    id: 89,
    slug: 'resolving-pipeline-failures-and-bottlenecks',
    question: 'How to get started with resolving pipeline failures and bottlenecks?',
    answer: 'For failures, check the job logs to identify the root cause (e.g., bad data, code error, connection issue). For bottlenecks, use the job timings to see which recipes are taking the longest to run. Then you can focus on optimizing that specific part of the flow, for example by changing the execution engine.',
  },
  {
    id: 90,
    slug: 'staying-updated-with-the-latest-dss-features',
    question: 'How to get started with staying updated with the latest DSS features?',
    answer: 'Read the release notes for each new version of Dataiku. Follow the Dataiku blog and community forums. The Dataiku Academy also regularly updates its content to cover new features.',
  },
  {
    id: 91,
    slug: 'navigating-the-dataiku-academy-learning-paths',
    question: 'How to get started with navigating the Dataiku Academy learning paths?',
    answer: 'The Dataiku Academy offers structured learning paths for different roles (e.g., "Core Designer", "Advanced Designer", "Developer"). Start with the "Core Designer" path to get a solid foundation in the key concepts of Dataiku. The paths consist of videos, tutorials, and quizzes.',
  },
  {
    id: 92,
    slug: 'using-the-dataiku-developer-guide-and-api-docs',
    question: 'How to get started with using the Dataiku Developer guide and API docs?',
    answer: 'The official documentation is your best friend. The developer guide provides information on topics like creating plugins. The API docs (both for Python and REST) provide detailed references for all available functions and endpoints, with examples.',
  },
  {
    id: 93,
    slug: 'joining-the-dataiku-community-forums',
    question: 'How to get started with joining the Dataiku Community forums?',
    answer: 'The Dataiku Community is a great place to ask questions, share your work, and learn from other users. You can browse existing topics or post a new question if you are stuck. It is a very active and helpful community.',
  },
  {
    id: 94,
    slug: 'earning-dataiku-core-designer-certification',
    question: 'How to get started with earning Dataiku Core Designer certification?',
    answer: 'Follow the "Core Designer" learning path on the Dataiku Academy. Once you have completed the modules and feel confident with the material, you can take the certification exam. The certification demonstrates your proficiency in the fundamental concepts of Dataiku.',
  },
  {
    id: 95,
    slug: 'experimenting-in-a-sandbox-dss-instance',
    question: 'How to get started with experimenting in a sandbox DSS instance?',
    answer: 'Having a sandbox environment is crucial for learning. Don\'t be afraid to try new things. Create new projects, import different kinds of data, and experiment with all the different visual recipes. The worst that can happen is you delete the project and start over.',
  },
  {
    id: 96,
    slug: 'running-example-projects-tutorial-flows',
    question: 'How to get started with running example projects/tutorial flows?',
    answer: 'Dataiku comes with several sample projects that cover common use cases like customer churn prediction. Create a new project from one of these samples and explore the Flow. See how the datasets and recipes are connected. Re-run the scenarios to see how it all works.',
  },
  {
    id: 97,
    slug: 'benchmarking-performance-on-sandbox-datasets',
    question: 'How to get started with benchmarking performance on sandbox datasets?',
    answer: 'Try building the same data transformation pipeline in different ways. For example, using only visual recipes vs. a Python recipe. Or, for a large dataset, compare the performance of the "In-Memory" engine vs. the "Spark" engine. This will help you understand the performance implications of your design choices.',
  },
  {
    id: 98,
    slug: 'running-pocs-combining-dataiku-and-ml-frameworks',
    question: 'How to get started with running POCs combining Dataiku and ML frameworks?',
    answer: 'Define a small, focused proof-of-concept (POC). For example, "Can we use a pre-trained model from Hugging Face to classify customer feedback?". Then, in a Dataiku project, use a Python recipe to implement this POC, demonstrating how the external ML framework can be integrated into a Dataiku pipeline.',
  },
  {
    id: 99,
    slug: 'evaluating-dataiku-for-specific-business-cases',
    question: 'How to get started with evaluating Dataiku for specific business cases?',
    answer: 'Take a real but small-scale business problem and try to solve it end-to-end in Dataiku. This is the best way to see how Dataiku would work for your specific needs. Document the process, the challenges, and the results to build a business case.',
  },
  {
    id: 100,
    slug: 'building-your-personal-dataiku-learning-portfolio',
    question: 'How to get started with building your personal Dataiku learning portfolio?',
    answer: 'As you learn, create projects that showcase your skills. You could take a public dataset (e.g., from Kaggle) and build a complete project around it in Dataiku, including data preparation, modeling, and a dashboard. Document your work in the project Wiki. This portfolio can be very valuable for job applications.',
  },
];

export const getQuestionBySlug = (slug: string): Question | undefined => {
  return questions.find(q => q.slug === slug);
}

export const getQuestionById = (id: number): Question | undefined => {
  return questions.find(q => q.id === id);
}
